{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "RUN_DIR = \"/home/sequenzia/dev/rl-project\"\n",
    "ZOO_DIR = \"/home/sequenzia/dev/repos/rl-baselines3-zoo/rl_zoo3\"\n",
    "LOG_DIR = \"/home/sequenzia/dev/rl-project/trained-agents\"\n",
    "CONFIG_DIR = \"/home/sequenzia/dev/rl-project/configs\"\n",
    "TENSORBOARD_DIR = \"/home/sequenzia/dev/rl-project/logs/tensorboard\"\n",
    "\n",
    "SEED = 43\n",
    "\n",
    "SAVE_FREQ = 10000\n",
    "EVAL_FREQ = 10000\n",
    "EVAL_EPISODES = 5\n",
    "VERBOSE = 1\n",
    "\n",
    "PROJECT_NAME = \"Solen-RL-Project\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakout\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROM = \"Breakout\"\n",
    "\n",
    "ENV_ID = f\"{ROM}NoFrameskip-v4\"\n",
    "\n",
    "CMD = f\"cd {RUN_DIR} && python {ZOO_DIR}/train.py\"\n",
    "CMD += f\" --env {ENV_ID}\"\n",
    "CMD += f\" --log-folder {LOG_DIR}\"\n",
    "CMD += f\" --tensorboard-log {TENSORBOARD_DIR}\"\n",
    "CMD += f\" --wandb-project-name {PROJECT_NAME}\"\n",
    "CMD += f\" --seed {SEED}\"\n",
    "CMD += f\" --save-freq {SAVE_FREQ}\"\n",
    "CMD += f\" --eval-freq {EVAL_FREQ}\"\n",
    "CMD += f\" --eval-episodes {EVAL_EPISODES}\"\n",
    "CMD += f\" --verbose {VERBOSE}\"\n",
    "CMD += f\" --device cuda\"\n",
    "CMD += f\" --track\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== BreakoutNoFrameskip-v4 ==========\n",
      "Seed: 43\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mappliedtheta\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sequenzia/dev/rl-project/logs/wandb/run-20231218_004833-oraji4ep\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mBreakoutNoFrameskip-v4__a2c__1702878511\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/appliedtheta/Solen-RL-Project\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/appliedtheta/Solen-RL-Project/runs/oraji4ep\u001b[0m\n",
      "Loading hyperparameters from: /home/sequenzia/dev/rl-project/configs/a2c.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('ent_coef', 0.01),\n",
      "             ('env_wrapper',\n",
      "              ['stable_baselines3.common.atari_wrappers.AtariWrapper']),\n",
      "             ('frame_stack', 4),\n",
      "             ('n_envs', 16),\n",
      "             ('n_timesteps', 20000000.0),\n",
      "             ('policy', 'CnnPolicy'),\n",
      "             ('policy_kwargs',\n",
      "              'dict(optimizer_class=RMSpropTFLike, '\n",
      "              'optimizer_kwargs=dict(eps=1e-5))'),\n",
      "             ('vf_coef', 0.25)])\n",
      "Using 16 environments\n",
      "Creating test environment\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "Stacking 4 frames\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Stacking 4 frames\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Using cuda device\n",
      "Log path: /home/sequenzia/dev/rl-project/trained-agents/a2c/BreakoutNoFrameskip-v4_1\n",
      "Logging to runs/BreakoutNoFrameskip-v4__a2c__1702878511/BreakoutNoFrameskip-v4/A2C_1\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 687      |\n",
      "|    ep_rew_mean        | 1.08     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | -0.00827 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.094    |\n",
      "|    value_loss         | 0.073    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=1.20 +/- 1.17\n",
      "Episode length: 714.60 +/- 207.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 715      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | -0.0431  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124      |\n",
      "|    policy_loss        | 0.0718   |\n",
      "|    value_loss         | 0.061    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 737      |\n",
      "|    ep_rew_mean        | 1.36     |\n",
      "| time/                 |          |\n",
      "|    fps                | 760      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | -0.0212  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.0312   |\n",
      "|    value_loss         | 0.0265   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=1.80 +/- 1.60\n",
      "Episode length: 785.20 +/- 253.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 785      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.0496   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 249      |\n",
      "|    policy_loss        | -0.0127  |\n",
      "|    value_loss         | 0.0149   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 767       |\n",
      "|    ep_rew_mean        | 1.57      |\n",
      "| time/                 |           |\n",
      "|    fps                | 767       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -0.000141 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 0.0632    |\n",
      "|    value_loss         | 0.0465    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=1.20 +/- 0.98\n",
      "Episode length: 723.00 +/- 168.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 723      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | -0.00285 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 374      |\n",
      "|    policy_loss        | 0.0713   |\n",
      "|    value_loss         | 0.0779   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 759      |\n",
      "|    ep_rew_mean        | 1.53     |\n",
      "| time/                 |          |\n",
      "|    fps                | 784      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 40       |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | -0.0215  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.00996  |\n",
      "|    value_loss         | 0.0285   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=1.00 +/- 1.55\n",
      "Episode length: 670.40 +/- 243.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 670      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | -0.0293  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.0412  |\n",
      "|    value_loss         | 0.0197   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 746      |\n",
      "|    ep_rew_mean     | 1.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 785      |\n",
      "|    iterations      | 500      |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 730      |\n",
      "|    ep_rew_mean        | 1.27     |\n",
      "| time/                 |          |\n",
      "|    fps                | 804      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 59       |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -0.212   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.00916 |\n",
      "|    value_loss         | 0.000538 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=1.00 +/- 0.89\n",
      "Episode length: 709.60 +/- 167.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 710      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 50000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -0.0214  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 624      |\n",
      "|    policy_loss        | -0.0245  |\n",
      "|    value_loss         | 0.00151  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 706      |\n",
      "|    ep_rew_mean        | 1.16     |\n",
      "| time/                 |          |\n",
      "|    fps                | 807      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 69       |\n",
      "|    total_timesteps    | 56000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.0286   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.0104   |\n",
      "|    value_loss         | 0.0251   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=1.40 +/- 1.85\n",
      "Episode length: 743.00 +/- 325.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 743      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 60000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -0.103   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 749      |\n",
      "|    policy_loss        | -0.0723  |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 712      |\n",
      "|    ep_rew_mean        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    fps                | 804      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 79       |\n",
      "|    total_timesteps    | 64000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -4.25    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00931 |\n",
      "|    value_loss         | 0.000402 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=2.00 +/- 2.10\n",
      "Episode length: 881.00 +/- 383.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 881      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 70000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | -0.0269  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 874      |\n",
      "|    policy_loss        | -0.0629  |\n",
      "|    value_loss         | 0.0118   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 751      |\n",
      "|    ep_rew_mean        | 1.43     |\n",
      "| time/                 |          |\n",
      "|    fps                | 803      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 89       |\n",
      "|    total_timesteps    | 72000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | -0.544   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.0263  |\n",
      "|    value_loss         | 0.00141  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=1.20 +/- 1.47\n",
      "Episode length: 708.20 +/- 257.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 708      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -0.00503 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.082   |\n",
      "|    value_loss         | 0.0236   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 752      |\n",
      "|    ep_rew_mean     | 1.48     |\n",
      "| time/              |          |\n",
      "|    fps             | 803      |\n",
      "|    iterations      | 1000     |\n",
      "|    time_elapsed    | 99       |\n",
      "|    total_timesteps | 80000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 713      |\n",
      "|    ep_rew_mean        | 1.21     |\n",
      "| time/                 |          |\n",
      "|    fps                | 814      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 108      |\n",
      "|    total_timesteps    | 88000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.276    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.0317  |\n",
      "|    value_loss         | 0.00155  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=1.80 +/- 1.47\n",
      "Episode length: 820.80 +/- 261.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 821      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 90000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.0563   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1124     |\n",
      "|    policy_loss        | -0.0335  |\n",
      "|    value_loss         | 0.00601  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 719      |\n",
      "|    ep_rew_mean        | 1.22     |\n",
      "| time/                 |          |\n",
      "|    fps                | 815      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 117      |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.0151   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.0158   |\n",
      "|    value_loss         | 0.0392   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=0.20 +/- 0.40\n",
      "Episode length: 560.40 +/- 79.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 560      |\n",
      "|    mean_reward        | 0.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 100000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -0.11    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1249     |\n",
      "|    policy_loss        | -0.0963  |\n",
      "|    value_loss         | 0.0282   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 748      |\n",
      "|    ep_rew_mean        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    fps                | 819      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 126      |\n",
      "|    total_timesteps    | 104000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.00118  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.0891   |\n",
      "|    value_loss         | 0.0722   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=1.40 +/- 0.49\n",
      "Episode length: 747.00 +/- 69.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 747      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 110000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.354    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1374     |\n",
      "|    policy_loss        | -0.0153  |\n",
      "|    value_loss         | 0.000629 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 734      |\n",
      "|    ep_rew_mean        | 1.36     |\n",
      "| time/                 |          |\n",
      "|    fps                | 818      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 136      |\n",
      "|    total_timesteps    | 112000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.0205   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.0212   |\n",
      "|    value_loss         | 0.039    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=1.40 +/- 0.80\n",
      "Episode length: 705.80 +/- 111.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 706      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 120000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.398    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.0144  |\n",
      "|    value_loss         | 0.000265 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 732      |\n",
      "|    ep_rew_mean     | 1.35     |\n",
      "| time/              |          |\n",
      "|    fps             | 819      |\n",
      "|    iterations      | 1500     |\n",
      "|    time_elapsed    | 146      |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 750      |\n",
      "|    ep_rew_mean        | 1.37     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 154      |\n",
      "|    total_timesteps    | 128000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.0183   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.00943  |\n",
      "|    value_loss         | 0.0394   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=1.20 +/- 0.75\n",
      "Episode length: 712.80 +/- 136.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 713      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 130000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.164    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1624     |\n",
      "|    policy_loss        | -0.0174  |\n",
      "|    value_loss         | 0.00181  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 708      |\n",
      "|    ep_rew_mean        | 1.1      |\n",
      "| time/                 |          |\n",
      "|    fps                | 825      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 164      |\n",
      "|    total_timesteps    | 136000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.0436   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.0703   |\n",
      "|    value_loss         | 0.0748   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=1.60 +/- 1.62\n",
      "Episode length: 819.00 +/- 314.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 819      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 140000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.0489   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1749     |\n",
      "|    policy_loss        | -0.0725  |\n",
      "|    value_loss         | 0.0345   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 710      |\n",
      "|    ep_rew_mean        | 1.14     |\n",
      "| time/                 |          |\n",
      "|    fps                | 822      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 175      |\n",
      "|    total_timesteps    | 144000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.158    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.0238  |\n",
      "|    value_loss         | 0.00216  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=1.60 +/- 1.50\n",
      "Episode length: 775.20 +/- 223.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 775      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 150000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.0314   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1874     |\n",
      "|    policy_loss        | 0.0301   |\n",
      "|    value_loss         | 0.0518   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 740      |\n",
      "|    ep_rew_mean        | 1.35     |\n",
      "| time/                 |          |\n",
      "|    fps                | 820      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 185      |\n",
      "|    total_timesteps    | 152000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.159    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -0.0323  |\n",
      "|    value_loss         | 0.00211  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=1.80 +/- 1.60\n",
      "Episode length: 824.40 +/- 305.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 824      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 160000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.0777   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.0536   |\n",
      "|    value_loss         | 0.0471   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 713      |\n",
      "|    ep_rew_mean     | 1.2      |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 2000     |\n",
      "|    time_elapsed    | 195      |\n",
      "|    total_timesteps | 160000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 710      |\n",
      "|    ep_rew_mean        | 1.17     |\n",
      "| time/                 |          |\n",
      "|    fps                | 824      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 203      |\n",
      "|    total_timesteps    | 168000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.388    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | -0.0168  |\n",
      "|    value_loss         | 0.000944 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=0.80 +/- 0.40\n",
      "Episode length: 666.00 +/- 72.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 666      |\n",
      "|    mean_reward        | 0.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 170000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.0717   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2124     |\n",
      "|    policy_loss        | 0.0262   |\n",
      "|    value_loss         | 0.0382   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 710      |\n",
      "|    ep_rew_mean        | 1.16     |\n",
      "| time/                 |          |\n",
      "|    fps                | 823      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 213      |\n",
      "|    total_timesteps    | 176000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.496    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | -0.00347 |\n",
      "|    value_loss         | 0.000155 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=0.80 +/- 0.98\n",
      "Episode length: 655.60 +/- 170.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 656      |\n",
      "|    mean_reward        | 0.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 180000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.0653   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2249     |\n",
      "|    policy_loss        | -0.046   |\n",
      "|    value_loss         | 0.0052   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 712      |\n",
      "|    ep_rew_mean        | 1.18     |\n",
      "| time/                 |          |\n",
      "|    fps                | 822      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 223      |\n",
      "|    total_timesteps    | 184000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.0362   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 0.0951   |\n",
      "|    value_loss         | 0.0857   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=1.40 +/- 1.74\n",
      "Episode length: 728.80 +/- 271.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 729      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 190000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.026    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2374     |\n",
      "|    policy_loss        | 0.106    |\n",
      "|    value_loss         | 0.0814   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 729      |\n",
      "|    ep_rew_mean        | 1.33     |\n",
      "| time/                 |          |\n",
      "|    fps                | 820      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 233      |\n",
      "|    total_timesteps    | 192000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.496    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | -0.0148  |\n",
      "|    value_loss         | 0.00105  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 610.60 +/- 114.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 611      |\n",
      "|    mean_reward        | 0.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 200000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.0578   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | 0.023    |\n",
      "|    value_loss         | 0.0366   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 745      |\n",
      "|    ep_rew_mean     | 1.4      |\n",
      "| time/              |          |\n",
      "|    fps             | 821      |\n",
      "|    iterations      | 2500     |\n",
      "|    time_elapsed    | 243      |\n",
      "|    total_timesteps | 200000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 740      |\n",
      "|    ep_rew_mean        | 1.43     |\n",
      "| time/                 |          |\n",
      "|    fps                | 825      |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 251      |\n",
      "|    total_timesteps    | 208000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.0827   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | -0.0466  |\n",
      "|    value_loss         | 0.00645  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=1.40 +/- 0.80\n",
      "Episode length: 739.40 +/- 174.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 739      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 210000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | -0.00339 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2624     |\n",
      "|    policy_loss        | -0.0399  |\n",
      "|    value_loss         | 0.00579  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 712      |\n",
      "|    ep_rew_mean        | 1.24     |\n",
      "| time/                 |          |\n",
      "|    fps                | 825      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 261      |\n",
      "|    total_timesteps    | 216000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.132    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | 0.151    |\n",
      "|    value_loss         | 0.106    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=1.60 +/- 1.36\n",
      "Episode length: 780.60 +/- 223.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 781      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 220000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.16     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2749     |\n",
      "|    policy_loss        | -0.0363  |\n",
      "|    value_loss         | 0.00416  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 732      |\n",
      "|    ep_rew_mean        | 1.32     |\n",
      "| time/                 |          |\n",
      "|    fps                | 824      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 271      |\n",
      "|    total_timesteps    | 224000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.202    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | -0.0695  |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=1.80 +/- 1.60\n",
      "Episode length: 836.60 +/- 282.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 837      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 230000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.198    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2874     |\n",
      "|    policy_loss        | 0.0382   |\n",
      "|    value_loss         | 0.0307   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 750      |\n",
      "|    ep_rew_mean        | 1.41     |\n",
      "| time/                 |          |\n",
      "|    fps                | 823      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 281      |\n",
      "|    total_timesteps    | 232000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.305    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | -0.0214  |\n",
      "|    value_loss         | 0.00165  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=1.40 +/- 1.02\n",
      "Episode length: 717.80 +/- 134.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 718      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 240000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.157    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 0.0781   |\n",
      "|    value_loss         | 0.0651   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 716      |\n",
      "|    ep_rew_mean     | 1.21     |\n",
      "| time/              |          |\n",
      "|    fps             | 822      |\n",
      "|    iterations      | 3000     |\n",
      "|    time_elapsed    | 291      |\n",
      "|    total_timesteps | 240000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 735      |\n",
      "|    ep_rew_mean        | 1.31     |\n",
      "| time/                 |          |\n",
      "|    fps                | 825      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 300      |\n",
      "|    total_timesteps    | 248000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.308    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | -0.0248  |\n",
      "|    value_loss         | 0.00162  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=2.00 +/- 1.90\n",
      "Episode length: 838.80 +/- 312.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 839      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 250000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.308    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3124     |\n",
      "|    policy_loss        | -0.0306  |\n",
      "|    value_loss         | 0.00443  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 747      |\n",
      "|    ep_rew_mean        | 1.38     |\n",
      "| time/                 |          |\n",
      "|    fps                | 822      |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 311      |\n",
      "|    total_timesteps    | 256000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.0609   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | -0.0278  |\n",
      "|    value_loss         | 0.00453  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=1.20 +/- 1.47\n",
      "Episode length: 728.00 +/- 262.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 728      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 260000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.272    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3249     |\n",
      "|    policy_loss        | -0.0197  |\n",
      "|    value_loss         | 0.00106  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 721      |\n",
      "|    ep_rew_mean        | 1.24     |\n",
      "| time/                 |          |\n",
      "|    fps                | 820      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 321      |\n",
      "|    total_timesteps    | 264000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.435    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | -0.0226  |\n",
      "|    value_loss         | 0.0051   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=1.80 +/- 1.60\n",
      "Episode length: 822.20 +/- 291.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 822      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 270000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | -0.148   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3374     |\n",
      "|    policy_loss        | -0.0583  |\n",
      "|    value_loss         | 0.00789  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 729      |\n",
      "|    ep_rew_mean        | 1.31     |\n",
      "| time/                 |          |\n",
      "|    fps                | 818      |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 332      |\n",
      "|    total_timesteps    | 272000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.779    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | -0.022   |\n",
      "|    value_loss         | 0.00165  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=0.20 +/- 0.40\n",
      "Episode length: 567.20 +/- 73.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 567      |\n",
      "|    mean_reward        | 0.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 280000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.319    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | 0.0434   |\n",
      "|    value_loss         | 0.0498   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 738      |\n",
      "|    ep_rew_mean     | 1.42     |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 3500     |\n",
      "|    time_elapsed    | 342      |\n",
      "|    total_timesteps | 280000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 726      |\n",
      "|    ep_rew_mean        | 1.33     |\n",
      "| time/                 |          |\n",
      "|    fps                | 820      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 350      |\n",
      "|    total_timesteps    | 288000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.669    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | 0.00804  |\n",
      "|    value_loss         | 0.00177  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=2.00 +/- 1.79\n",
      "Episode length: 850.40 +/- 300.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 850      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 290000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.244    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3624     |\n",
      "|    policy_loss        | 0.0681   |\n",
      "|    value_loss         | 0.0669   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 705      |\n",
      "|    ep_rew_mean        | 1.17     |\n",
      "| time/                 |          |\n",
      "|    fps                | 819      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 361      |\n",
      "|    total_timesteps    | 296000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.716    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | -0.0194  |\n",
      "|    value_loss         | 0.00377  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=1.40 +/- 1.20\n",
      "Episode length: 749.40 +/- 205.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 749      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 300000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.0788   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3749     |\n",
      "|    policy_loss        | -0.03    |\n",
      "|    value_loss         | 0.00343  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 709      |\n",
      "|    ep_rew_mean        | 1.16     |\n",
      "| time/                 |          |\n",
      "|    fps                | 819      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 371      |\n",
      "|    total_timesteps    | 304000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.742    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | -0.0268  |\n",
      "|    value_loss         | 0.00139  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=1.20 +/- 1.47\n",
      "Episode length: 709.00 +/- 229.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 709      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 310000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.521    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3874     |\n",
      "|    policy_loss        | 0.0434   |\n",
      "|    value_loss         | 0.0339   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 726      |\n",
      "|    ep_rew_mean        | 1.26     |\n",
      "| time/                 |          |\n",
      "|    fps                | 819      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 380      |\n",
      "|    total_timesteps    | 312000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | -0.658   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | -0.0184  |\n",
      "|    value_loss         | 0.00424  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=1.00 +/- 0.89\n",
      "Episode length: 695.00 +/- 158.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 695      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 320000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.226    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 0.0779   |\n",
      "|    value_loss         | 0.0615   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 719      |\n",
      "|    ep_rew_mean     | 1.29     |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 4000     |\n",
      "|    time_elapsed    | 390      |\n",
      "|    total_timesteps | 320000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 743      |\n",
      "|    ep_rew_mean        | 1.43     |\n",
      "| time/                 |          |\n",
      "|    fps                | 821      |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 399      |\n",
      "|    total_timesteps    | 328000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.66     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | -0.011   |\n",
      "|    value_loss         | 0.00101  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=2.80 +/- 2.04\n",
      "Episode length: 977.80 +/- 335.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 978      |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 330000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.401    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4124     |\n",
      "|    policy_loss        | -0.00776 |\n",
      "|    value_loss         | 0.0227   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 732      |\n",
      "|    ep_rew_mean        | 1.32     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 410      |\n",
      "|    total_timesteps    | 336000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.504    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | 0.113    |\n",
      "|    value_loss         | 0.0661   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 611.00 +/- 97.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 611      |\n",
      "|    mean_reward        | 0.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 340000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.64     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4249     |\n",
      "|    policy_loss        | -0.0381  |\n",
      "|    value_loss         | 0.00543  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 722      |\n",
      "|    ep_rew_mean        | 1.27     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 420      |\n",
      "|    total_timesteps    | 344000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | -0.0996  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | -0.0652  |\n",
      "|    value_loss         | 0.0178   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=0.80 +/- 0.75\n",
      "Episode length: 653.80 +/- 115.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 654      |\n",
      "|    mean_reward        | 0.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 350000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.441    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4374     |\n",
      "|    policy_loss        | -0.0175  |\n",
      "|    value_loss         | 0.0307   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 732      |\n",
      "|    ep_rew_mean        | 1.32     |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 430      |\n",
      "|    total_timesteps    | 352000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.64     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | -0.00471 |\n",
      "|    value_loss         | 0.0033   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=1.20 +/- 0.75\n",
      "Episode length: 716.00 +/- 137.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 716      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 360000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.278    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | 0.0503   |\n",
      "|    value_loss         | 0.0317   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 730      |\n",
      "|    ep_rew_mean     | 1.31     |\n",
      "| time/              |          |\n",
      "|    fps             | 816      |\n",
      "|    iterations      | 4500     |\n",
      "|    time_elapsed    | 440      |\n",
      "|    total_timesteps | 360000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 739      |\n",
      "|    ep_rew_mean        | 1.41     |\n",
      "| time/                 |          |\n",
      "|    fps                | 819      |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 449      |\n",
      "|    total_timesteps    | 368000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.711    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | 0.032    |\n",
      "|    value_loss         | 0.00928  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=0.80 +/- 1.17\n",
      "Episode length: 659.40 +/- 193.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 659      |\n",
      "|    mean_reward        | 0.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 370000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.467    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4624     |\n",
      "|    policy_loss        | 0.0431   |\n",
      "|    value_loss         | 0.00848  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 702      |\n",
      "|    ep_rew_mean        | 1.11     |\n",
      "| time/                 |          |\n",
      "|    fps                | 818      |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 459      |\n",
      "|    total_timesteps    | 376000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.639    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | -0.0282  |\n",
      "|    value_loss         | 0.0133   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 605.00 +/- 95.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 605      |\n",
      "|    mean_reward        | 0.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 380000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.473    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4749     |\n",
      "|    policy_loss        | 0.000122 |\n",
      "|    value_loss         | 0.00585  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 689      |\n",
      "|    ep_rew_mean        | 0.98     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 469      |\n",
      "|    total_timesteps    | 384000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.576    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | -0.0616  |\n",
      "|    value_loss         | 0.0282   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=1.00 +/- 1.10\n",
      "Episode length: 670.40 +/- 177.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 670      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 390000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.388    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4874     |\n",
      "|    policy_loss        | -0.0149  |\n",
      "|    value_loss         | 0.00275  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 706      |\n",
      "|    ep_rew_mean        | 1.13     |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 480      |\n",
      "|    total_timesteps    | 392000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.811    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | 0.0661   |\n",
      "|    value_loss         | 0.0189   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=1.40 +/- 0.80\n",
      "Episode length: 740.60 +/- 125.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 741      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 400000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.818    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | 0.02     |\n",
      "|    value_loss         | 0.0229   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 721      |\n",
      "|    ep_rew_mean     | 1.23     |\n",
      "| time/              |          |\n",
      "|    fps             | 815      |\n",
      "|    iterations      | 5000     |\n",
      "|    time_elapsed    | 490      |\n",
      "|    total_timesteps | 400000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 728      |\n",
      "|    ep_rew_mean        | 1.28     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 498      |\n",
      "|    total_timesteps    | 408000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | 0.00821  |\n",
      "|    value_loss         | 0.00442  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=1.80 +/- 1.17\n",
      "Episode length: 846.40 +/- 206.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 846      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 410000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.717    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5124     |\n",
      "|    policy_loss        | 0.0375   |\n",
      "|    value_loss         | 0.0119   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 768      |\n",
      "|    ep_rew_mean        | 1.49     |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 509      |\n",
      "|    total_timesteps    | 416000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.698    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | -0.136   |\n",
      "|    value_loss         | 0.0307   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=0.80 +/- 0.75\n",
      "Episode length: 636.20 +/- 109.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 636      |\n",
      "|    mean_reward        | 0.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 420000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.766    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5249     |\n",
      "|    policy_loss        | 0.0112   |\n",
      "|    value_loss         | 0.0184   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 793      |\n",
      "|    ep_rew_mean        | 1.69     |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 519      |\n",
      "|    total_timesteps    | 424000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.686    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | -0.00669 |\n",
      "|    value_loss         | 0.00796  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=1.00 +/- 0.89\n",
      "Episode length: 666.60 +/- 109.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 667      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 430000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -0.175   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5374     |\n",
      "|    policy_loss        | -0.0142  |\n",
      "|    value_loss         | 0.00437  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 744      |\n",
      "|    ep_rew_mean        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 528      |\n",
      "|    total_timesteps    | 432000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.491    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | -0.0309  |\n",
      "|    value_loss         | 0.0682   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=1.60 +/- 1.62\n",
      "Episode length: 795.00 +/- 274.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 795      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 440000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.47     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | -0.0335  |\n",
      "|    value_loss         | 0.0237   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 759      |\n",
      "|    ep_rew_mean     | 1.51     |\n",
      "| time/              |          |\n",
      "|    fps             | 815      |\n",
      "|    iterations      | 5500     |\n",
      "|    time_elapsed    | 539      |\n",
      "|    total_timesteps | 440000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 744      |\n",
      "|    ep_rew_mean        | 1.45     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 5600     |\n",
      "|    time_elapsed       | 548      |\n",
      "|    total_timesteps    | 448000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.469    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | 0.00342  |\n",
      "|    value_loss         | 0.0214   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=1.20 +/- 1.17\n",
      "Episode length: 686.60 +/- 167.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 687      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 450000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.831    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5624     |\n",
      "|    policy_loss        | 0.0263   |\n",
      "|    value_loss         | 0.0202   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 714      |\n",
      "|    ep_rew_mean        | 1.22     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 557      |\n",
      "|    total_timesteps    | 456000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -2.41    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | -0.0707  |\n",
      "|    value_loss         | 0.0126   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=1.40 +/- 1.02\n",
      "Episode length: 759.80 +/- 178.17\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 760       |\n",
      "|    mean_reward        | 1.4       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 460000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | 0.765     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5749      |\n",
      "|    policy_loss        | -0.000724 |\n",
      "|    value_loss         | 0.00371   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 738      |\n",
      "|    ep_rew_mean        | 1.34     |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 567      |\n",
      "|    total_timesteps    | 464000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.834    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | -0.0103  |\n",
      "|    value_loss         | 0.0309   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=3.00 +/- 2.45\n",
      "Episode length: 1008.40 +/- 392.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+03 |\n",
      "|    mean_reward        | 3        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 470000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.723    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5874     |\n",
      "|    policy_loss        | 0.0463   |\n",
      "|    value_loss         | 0.00792  |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 785      |\n",
      "|    ep_rew_mean        | 1.58     |\n",
      "| time/                 |          |\n",
      "|    fps                | 815      |\n",
      "|    iterations         | 5900     |\n",
      "|    time_elapsed       | 578      |\n",
      "|    total_timesteps    | 472000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.739    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | 0.000708 |\n",
      "|    value_loss         | 0.00295  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=3.60 +/- 3.07\n",
      "Episode length: 1034.40 +/- 395.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+03 |\n",
      "|    mean_reward        | 3.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 480000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.696    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | -0.0805  |\n",
      "|    value_loss         | 0.0155   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 1.48     |\n",
      "| time/              |          |\n",
      "|    fps             | 814      |\n",
      "|    iterations      | 6000     |\n",
      "|    time_elapsed    | 589      |\n",
      "|    total_timesteps | 480000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 737      |\n",
      "|    ep_rew_mean        | 1.44     |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 6100     |\n",
      "|    time_elapsed       | 597      |\n",
      "|    total_timesteps    | 488000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.655    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6099     |\n",
      "|    policy_loss        | 0.0533   |\n",
      "|    value_loss         | 0.0362   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=3.40 +/- 2.42\n",
      "Episode length: 1018.80 +/- 248.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+03 |\n",
      "|    mean_reward        | 3.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 490000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.632    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6124     |\n",
      "|    policy_loss        | -0.0319  |\n",
      "|    value_loss         | 0.0155   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 750      |\n",
      "|    ep_rew_mean        | 1.46     |\n",
      "| time/                 |          |\n",
      "|    fps                | 815      |\n",
      "|    iterations         | 6200     |\n",
      "|    time_elapsed       | 608      |\n",
      "|    total_timesteps    | 496000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.713    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | 0.0885   |\n",
      "|    value_loss         | 0.0707   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=1.60 +/- 1.36\n",
      "Episode length: 786.40 +/- 215.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 786      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 500000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.829    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6249     |\n",
      "|    policy_loss        | 0.0638   |\n",
      "|    value_loss         | 0.0286   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 759      |\n",
      "|    ep_rew_mean        | 1.49     |\n",
      "| time/                 |          |\n",
      "|    fps                | 814      |\n",
      "|    iterations         | 6300     |\n",
      "|    time_elapsed       | 618      |\n",
      "|    total_timesteps    | 504000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.525    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | 0.102    |\n",
      "|    value_loss         | 0.0436   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=2.20 +/- 1.94\n",
      "Episode length: 926.60 +/- 354.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 927      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 510000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.672    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6374     |\n",
      "|    policy_loss        | -0.182   |\n",
      "|    value_loss         | 0.0958   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 778      |\n",
      "|    ep_rew_mean        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    fps                | 814      |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 628      |\n",
      "|    total_timesteps    | 512000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | -0.0552  |\n",
      "|    value_loss         | 0.0153   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 579.40 +/- 108.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 579      |\n",
      "|    mean_reward        | 0.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 520000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.831    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6499     |\n",
      "|    policy_loss        | -0.0122  |\n",
      "|    value_loss         | 0.00223  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 731      |\n",
      "|    ep_rew_mean     | 1.31     |\n",
      "| time/              |          |\n",
      "|    fps             | 814      |\n",
      "|    iterations      | 6500     |\n",
      "|    time_elapsed    | 638      |\n",
      "|    total_timesteps | 520000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 699      |\n",
      "|    ep_rew_mean        | 1.07     |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 6600     |\n",
      "|    time_elapsed       | 647      |\n",
      "|    total_timesteps    | 528000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.658    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | -0.0126  |\n",
      "|    value_loss         | 0.00707  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=2.80 +/- 3.31\n",
      "Episode length: 867.40 +/- 393.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 867      |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 530000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.895    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6624     |\n",
      "|    policy_loss        | 0.0286   |\n",
      "|    value_loss         | 0.0185   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 722      |\n",
      "|    ep_rew_mean        | 1.21     |\n",
      "| time/                 |          |\n",
      "|    fps                | 815      |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 657      |\n",
      "|    total_timesteps    | 536000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.898    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | 0.0137   |\n",
      "|    value_loss         | 0.0111   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=0.40 +/- 0.49\n",
      "Episode length: 599.00 +/- 97.04\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 599       |\n",
      "|    mean_reward        | 0.4       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 540000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | 0.868     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6749      |\n",
      "|    policy_loss        | -0.000693 |\n",
      "|    value_loss         | 0.0136    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 734      |\n",
      "|    ep_rew_mean        | 1.3      |\n",
      "| time/                 |          |\n",
      "|    fps                | 815      |\n",
      "|    iterations         | 6800     |\n",
      "|    time_elapsed       | 666      |\n",
      "|    total_timesteps    | 544000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6799     |\n",
      "|    policy_loss        | -0.0416  |\n",
      "|    value_loss         | 0.0155   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=3.00 +/- 0.89\n",
      "Episode length: 1027.80 +/- 201.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+03 |\n",
      "|    mean_reward        | 3        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 550000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.716    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6874     |\n",
      "|    policy_loss        | -0.157   |\n",
      "|    value_loss         | 0.12     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 732      |\n",
      "|    ep_rew_mean        | 1.32     |\n",
      "| time/                 |          |\n",
      "|    fps                | 814      |\n",
      "|    iterations         | 6900     |\n",
      "|    time_elapsed       | 677      |\n",
      "|    total_timesteps    | 552000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | -0.0424  |\n",
      "|    value_loss         | 0.0115   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=0.80 +/- 0.98\n",
      "Episode length: 661.60 +/- 165.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 662      |\n",
      "|    mean_reward        | 0.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 560000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.586    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | 0.117    |\n",
      "|    value_loss         | 0.0261   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 779      |\n",
      "|    ep_rew_mean     | 1.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 814      |\n",
      "|    iterations      | 7000     |\n",
      "|    time_elapsed    | 687      |\n",
      "|    total_timesteps | 560000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 755      |\n",
      "|    ep_rew_mean        | 1.43     |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 7100     |\n",
      "|    time_elapsed       | 695      |\n",
      "|    total_timesteps    | 568000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.821    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7099     |\n",
      "|    policy_loss        | 0.00709  |\n",
      "|    value_loss         | 0.0361   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=2.60 +/- 3.26\n",
      "Episode length: 957.00 +/- 558.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 957      |\n",
      "|    mean_reward        | 2.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 570000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.889    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7124     |\n",
      "|    policy_loss        | 0.0632   |\n",
      "|    value_loss         | 0.02     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 756      |\n",
      "|    ep_rew_mean        | 1.5      |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 7200     |\n",
      "|    time_elapsed       | 705      |\n",
      "|    total_timesteps    | 576000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.697    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7199     |\n",
      "|    policy_loss        | -0.0123  |\n",
      "|    value_loss         | 0.00662  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=1.00 +/- 1.55\n",
      "Episode length: 704.80 +/- 298.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 705      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 580000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7249     |\n",
      "|    policy_loss        | -0.00414 |\n",
      "|    value_loss         | 0.0264   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 777      |\n",
      "|    ep_rew_mean        | 1.7      |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 7300     |\n",
      "|    time_elapsed       | 715      |\n",
      "|    total_timesteps    | 584000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.861    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7299     |\n",
      "|    policy_loss        | 0.0285   |\n",
      "|    value_loss         | 0.0264   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=2.00 +/- 1.67\n",
      "Episode length: 863.00 +/- 298.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 863      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 590000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.786    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7374     |\n",
      "|    policy_loss        | 0.0154   |\n",
      "|    value_loss         | 0.0854   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 770      |\n",
      "|    ep_rew_mean        | 1.7      |\n",
      "| time/                 |          |\n",
      "|    fps                | 815      |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 725      |\n",
      "|    total_timesteps    | 592000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | -0.00153 |\n",
      "|    value_loss         | 0.0123   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=2.20 +/- 3.92\n",
      "Episode length: 780.20 +/- 470.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 780      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 600000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7499     |\n",
      "|    policy_loss        | 0.0284   |\n",
      "|    value_loss         | 0.0283   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 773      |\n",
      "|    ep_rew_mean     | 1.65     |\n",
      "| time/              |          |\n",
      "|    fps             | 815      |\n",
      "|    iterations      | 7500     |\n",
      "|    time_elapsed    | 735      |\n",
      "|    total_timesteps | 600000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 825      |\n",
      "|    ep_rew_mean        | 1.94     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 744      |\n",
      "|    total_timesteps    | 608000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.758    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | 0.0368   |\n",
      "|    value_loss         | 0.0131   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=1.00 +/- 1.26\n",
      "Episode length: 692.40 +/- 212.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 692      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 610000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7624     |\n",
      "|    policy_loss        | -0.0218  |\n",
      "|    value_loss         | 0.0317   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 811      |\n",
      "|    ep_rew_mean        | 1.84     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 753      |\n",
      "|    total_timesteps    | 616000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | 0.036    |\n",
      "|    value_loss         | 0.02     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=1.80 +/- 2.23\n",
      "Episode length: 839.20 +/- 388.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 839      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 620000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7749     |\n",
      "|    policy_loss        | -0.00408 |\n",
      "|    value_loss         | 0.0195   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 773      |\n",
      "|    ep_rew_mean        | 1.66     |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 7800     |\n",
      "|    time_elapsed       | 764      |\n",
      "|    total_timesteps    | 624000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.818    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7799     |\n",
      "|    policy_loss        | -0.0363  |\n",
      "|    value_loss         | 0.0235   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 522.00 +/- 7.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 522      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 630000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.581    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7874     |\n",
      "|    policy_loss        | -0.0328  |\n",
      "|    value_loss         | 0.0718   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 797      |\n",
      "|    ep_rew_mean        | 1.79     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 7900     |\n",
      "|    time_elapsed       | 773      |\n",
      "|    total_timesteps    | 632000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.784    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7899     |\n",
      "|    policy_loss        | 0.0676   |\n",
      "|    value_loss         | 0.0539   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=2.00 +/- 1.79\n",
      "Episode length: 875.40 +/- 333.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 875      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 640000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | 0.0127   |\n",
      "|    value_loss         | 0.00368  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 804      |\n",
      "|    ep_rew_mean     | 1.83     |\n",
      "| time/              |          |\n",
      "|    fps             | 816      |\n",
      "|    iterations      | 8000     |\n",
      "|    time_elapsed    | 783      |\n",
      "|    total_timesteps | 640000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 771      |\n",
      "|    ep_rew_mean        | 1.66     |\n",
      "| time/                 |          |\n",
      "|    fps                | 818      |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 791      |\n",
      "|    total_timesteps    | 648000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | -0.0388  |\n",
      "|    value_loss         | 0.0111   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=5.80 +/- 5.04\n",
      "Episode length: 1240.00 +/- 496.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.24e+03 |\n",
      "|    mean_reward        | 5.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 650000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8124     |\n",
      "|    policy_loss        | -0.0115  |\n",
      "|    value_loss         | 0.0205   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 760      |\n",
      "|    ep_rew_mean        | 1.52     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 8200     |\n",
      "|    time_elapsed       | 802      |\n",
      "|    total_timesteps    | 656000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8199     |\n",
      "|    policy_loss        | -0.0116  |\n",
      "|    value_loss         | 0.00412  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=3.20 +/- 5.91\n",
      "Episode length: 853.00 +/- 592.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 853      |\n",
      "|    mean_reward        | 3.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 660000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8249     |\n",
      "|    policy_loss        | 0.0707   |\n",
      "|    value_loss         | 0.0357   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 778      |\n",
      "|    ep_rew_mean        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 8300     |\n",
      "|    time_elapsed       | 812      |\n",
      "|    total_timesteps    | 664000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.651    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8299     |\n",
      "|    policy_loss        | 0.102    |\n",
      "|    value_loss         | 0.0625   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=3.00 +/- 2.00\n",
      "Episode length: 1017.00 +/- 336.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+03 |\n",
      "|    mean_reward        | 3        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 670000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.683    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8374     |\n",
      "|    policy_loss        | 0.0139   |\n",
      "|    value_loss         | 0.0207   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 832      |\n",
      "|    ep_rew_mean        | 2.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 8400     |\n",
      "|    time_elapsed       | 822      |\n",
      "|    total_timesteps    | 672000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.898    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8399     |\n",
      "|    policy_loss        | 0.0395   |\n",
      "|    value_loss         | 0.017    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=2.20 +/- 0.98\n",
      "Episode length: 878.40 +/- 166.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 878      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 680000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8499     |\n",
      "|    policy_loss        | -0.0359  |\n",
      "|    value_loss         | 0.0103   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 2.22     |\n",
      "| time/              |          |\n",
      "|    fps             | 816      |\n",
      "|    iterations      | 8500     |\n",
      "|    time_elapsed    | 833      |\n",
      "|    total_timesteps | 680000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 775      |\n",
      "|    ep_rew_mean        | 1.61     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 8600     |\n",
      "|    time_elapsed       | 841      |\n",
      "|    total_timesteps    | 688000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.831    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8599     |\n",
      "|    policy_loss        | 0.00093  |\n",
      "|    value_loss         | 0.027    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=3.20 +/- 3.87\n",
      "Episode length: 985.60 +/- 503.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 986      |\n",
      "|    mean_reward        | 3.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 690000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8624     |\n",
      "|    policy_loss        | 0.0247   |\n",
      "|    value_loss         | 0.0156   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 805      |\n",
      "|    ep_rew_mean        | 1.81     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 8700     |\n",
      "|    time_elapsed       | 851      |\n",
      "|    total_timesteps    | 696000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | 0.026    |\n",
      "|    value_loss         | 0.015    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=1.80 +/- 2.23\n",
      "Episode length: 833.80 +/- 393.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 834      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 700000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8749     |\n",
      "|    policy_loss        | -0.0394  |\n",
      "|    value_loss         | 0.0114   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 804      |\n",
      "|    ep_rew_mean        | 1.81     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 8800     |\n",
      "|    time_elapsed       | 861      |\n",
      "|    total_timesteps    | 704000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8799     |\n",
      "|    policy_loss        | 0.0544   |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=1.60 +/- 1.50\n",
      "Episode length: 766.40 +/- 247.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 766      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 710000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.898    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8874     |\n",
      "|    policy_loss        | -0.103   |\n",
      "|    value_loss         | 0.0236   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 841      |\n",
      "|    ep_rew_mean        | 2.02     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 8900     |\n",
      "|    time_elapsed       | 871      |\n",
      "|    total_timesteps    | 712000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8899     |\n",
      "|    policy_loss        | -0.0425  |\n",
      "|    value_loss         | 0.015    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=3.80 +/- 3.25\n",
      "Episode length: 1109.80 +/- 483.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.11e+03 |\n",
      "|    mean_reward        | 3.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 720000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | -0.0635  |\n",
      "|    value_loss         | 0.0334   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 820      |\n",
      "|    ep_rew_mean     | 1.86     |\n",
      "| time/              |          |\n",
      "|    fps             | 816      |\n",
      "|    iterations      | 9000     |\n",
      "|    time_elapsed    | 882      |\n",
      "|    total_timesteps | 720000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 855      |\n",
      "|    ep_rew_mean        | 2.11     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 890      |\n",
      "|    total_timesteps    | 728000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | 0.0273   |\n",
      "|    value_loss         | 0.00499  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=4.20 +/- 3.54\n",
      "Episode length: 1145.80 +/- 450.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.15e+03 |\n",
      "|    mean_reward        | 4.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 730000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9124     |\n",
      "|    policy_loss        | 0.00878  |\n",
      "|    value_loss         | 0.0049   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 828      |\n",
      "|    ep_rew_mean        | 1.99     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 9200     |\n",
      "|    time_elapsed       | 900      |\n",
      "|    total_timesteps    | 736000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9199     |\n",
      "|    policy_loss        | 0.0114   |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=1.20 +/- 1.60\n",
      "Episode length: 691.60 +/- 237.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 692      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 740000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9249     |\n",
      "|    policy_loss        | -0.0961  |\n",
      "|    value_loss         | 0.0278   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 819      |\n",
      "|    ep_rew_mean        | 1.99     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 9300     |\n",
      "|    time_elapsed       | 910      |\n",
      "|    total_timesteps    | 744000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9299     |\n",
      "|    policy_loss        | -0.0308  |\n",
      "|    value_loss         | 0.00904  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=1.80 +/- 1.83\n",
      "Episode length: 830.60 +/- 328.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 831      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 750000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.86     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9374     |\n",
      "|    policy_loss        | 0.039    |\n",
      "|    value_loss         | 0.0113   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 826      |\n",
      "|    ep_rew_mean        | 2.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 9400     |\n",
      "|    time_elapsed       | 920      |\n",
      "|    total_timesteps    | 752000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9399     |\n",
      "|    policy_loss        | -0.0223  |\n",
      "|    value_loss         | 0.00242  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=1.20 +/- 1.60\n",
      "Episode length: 715.40 +/- 266.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 715      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 760000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | -0.0188  |\n",
      "|    value_loss         | 0.0111   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 783      |\n",
      "|    ep_rew_mean     | 1.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 817      |\n",
      "|    iterations      | 9500     |\n",
      "|    time_elapsed    | 929      |\n",
      "|    total_timesteps | 760000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 747      |\n",
      "|    ep_rew_mean        | 1.45     |\n",
      "| time/                 |          |\n",
      "|    fps                | 818      |\n",
      "|    iterations         | 9600     |\n",
      "|    time_elapsed       | 937      |\n",
      "|    total_timesteps    | 768000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9599     |\n",
      "|    policy_loss        | -0.0506  |\n",
      "|    value_loss         | 0.0077   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=1.00 +/- 1.10\n",
      "Episode length: 681.60 +/- 172.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 682      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 770000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9624     |\n",
      "|    policy_loss        | 0.0307   |\n",
      "|    value_loss         | 0.0153   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 814      |\n",
      "|    ep_rew_mean        | 2.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 819      |\n",
      "|    iterations         | 9700     |\n",
      "|    time_elapsed       | 947      |\n",
      "|    total_timesteps    | 776000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9699     |\n",
      "|    policy_loss        | -0.0781  |\n",
      "|    value_loss         | 0.0294   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=0.80 +/- 1.60\n",
      "Episode length: 658.40 +/- 271.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 658      |\n",
      "|    mean_reward        | 0.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 780000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9749     |\n",
      "|    policy_loss        | -0.00414 |\n",
      "|    value_loss         | 0.00686  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 883      |\n",
      "|    ep_rew_mean        | 2.49     |\n",
      "| time/                 |          |\n",
      "|    fps                | 819      |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 956      |\n",
      "|    total_timesteps    | 784000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | -0.00749 |\n",
      "|    value_loss         | 0.00629  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=2.80 +/- 2.32\n",
      "Episode length: 975.40 +/- 381.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 975      |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 790000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9874     |\n",
      "|    policy_loss        | 0.0436   |\n",
      "|    value_loss         | 0.0172   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 888      |\n",
      "|    ep_rew_mean        | 2.44     |\n",
      "| time/                 |          |\n",
      "|    fps                | 819      |\n",
      "|    iterations         | 9900     |\n",
      "|    time_elapsed       | 966      |\n",
      "|    total_timesteps    | 792000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9899     |\n",
      "|    policy_loss        | -0.0277  |\n",
      "|    value_loss         | 0.00727  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=4.80 +/- 4.35\n",
      "Episode length: 1138.60 +/- 542.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+03 |\n",
      "|    mean_reward        | 4.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 800000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9999     |\n",
      "|    policy_loss        | -0.00865 |\n",
      "|    value_loss         | 0.00482  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 913      |\n",
      "|    ep_rew_mean     | 2.52     |\n",
      "| time/              |          |\n",
      "|    fps             | 819      |\n",
      "|    iterations      | 10000    |\n",
      "|    time_elapsed    | 976      |\n",
      "|    total_timesteps | 800000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 861      |\n",
      "|    ep_rew_mean        | 2.25     |\n",
      "| time/                 |          |\n",
      "|    fps                | 820      |\n",
      "|    iterations         | 10100    |\n",
      "|    time_elapsed       | 984      |\n",
      "|    total_timesteps    | 808000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10099    |\n",
      "|    policy_loss        | -0.0278  |\n",
      "|    value_loss         | 0.00644  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=2.80 +/- 3.66\n",
      "Episode length: 888.60 +/- 469.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 889      |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 810000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10124    |\n",
      "|    policy_loss        | 0.0307   |\n",
      "|    value_loss         | 0.00469  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 764      |\n",
      "|    ep_rew_mean        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    fps                | 820      |\n",
      "|    iterations         | 10200    |\n",
      "|    time_elapsed       | 994      |\n",
      "|    total_timesteps    | 816000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10199    |\n",
      "|    policy_loss        | -0.0224  |\n",
      "|    value_loss         | 0.00255  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=1.00 +/- 0.89\n",
      "Episode length: 683.20 +/- 131.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 683      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 820000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10249    |\n",
      "|    policy_loss        | -0.0215  |\n",
      "|    value_loss         | 0.00657  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 815      |\n",
      "|    ep_rew_mean        | 1.95     |\n",
      "| time/                 |          |\n",
      "|    fps                | 821      |\n",
      "|    iterations         | 10300    |\n",
      "|    time_elapsed       | 1003     |\n",
      "|    total_timesteps    | 824000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10299    |\n",
      "|    policy_loss        | -0.0442  |\n",
      "|    value_loss         | 0.00814  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=4.40 +/- 3.44\n",
      "Episode length: 1186.20 +/- 508.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.19e+03 |\n",
      "|    mean_reward        | 4.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 830000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10374    |\n",
      "|    policy_loss        | 0.0392   |\n",
      "|    value_loss         | 0.00781  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 864      |\n",
      "|    ep_rew_mean        | 2.26     |\n",
      "| time/                 |          |\n",
      "|    fps                | 820      |\n",
      "|    iterations         | 10400    |\n",
      "|    time_elapsed       | 1014     |\n",
      "|    total_timesteps    | 832000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10399    |\n",
      "|    policy_loss        | 0.0322   |\n",
      "|    value_loss         | 0.00609  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=1.20 +/- 1.47\n",
      "Episode length: 700.00 +/- 223.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 700      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 840000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10499    |\n",
      "|    policy_loss        | 0.0343   |\n",
      "|    value_loss         | 0.00703  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 2.26     |\n",
      "| time/              |          |\n",
      "|    fps             | 820      |\n",
      "|    iterations      | 10500    |\n",
      "|    time_elapsed    | 1024     |\n",
      "|    total_timesteps | 840000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 833      |\n",
      "|    ep_rew_mean        | 1.98     |\n",
      "| time/                 |          |\n",
      "|    fps                | 821      |\n",
      "|    iterations         | 10600    |\n",
      "|    time_elapsed       | 1032     |\n",
      "|    total_timesteps    | 848000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10599    |\n",
      "|    policy_loss        | -0.052   |\n",
      "|    value_loss         | 0.0116   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=2.80 +/- 3.66\n",
      "Episode length: 926.60 +/- 500.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 927      |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 850000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10624    |\n",
      "|    policy_loss        | -0.0356  |\n",
      "|    value_loss         | 0.0103   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 817      |\n",
      "|    ep_rew_mean        | 1.94     |\n",
      "| time/                 |          |\n",
      "|    fps                | 821      |\n",
      "|    iterations         | 10700    |\n",
      "|    time_elapsed       | 1042     |\n",
      "|    total_timesteps    | 856000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10699    |\n",
      "|    policy_loss        | -0.114   |\n",
      "|    value_loss         | 0.0193   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=2.80 +/- 3.31\n",
      "Episode length: 880.80 +/- 404.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 881      |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 860000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10749    |\n",
      "|    policy_loss        | 0.00897  |\n",
      "|    value_loss         | 0.00887  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 900      |\n",
      "|    ep_rew_mean        | 2.53     |\n",
      "| time/                 |          |\n",
      "|    fps                | 820      |\n",
      "|    iterations         | 10800    |\n",
      "|    time_elapsed       | 1052     |\n",
      "|    total_timesteps    | 864000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10799    |\n",
      "|    policy_loss        | 0.00779  |\n",
      "|    value_loss         | 0.0035   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=1.60 +/- 1.85\n",
      "Episode length: 803.60 +/- 326.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 804      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 870000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10874    |\n",
      "|    policy_loss        | 0.0135   |\n",
      "|    value_loss         | 0.00214  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 837      |\n",
      "|    ep_rew_mean        | 2.1      |\n",
      "| time/                 |          |\n",
      "|    fps                | 821      |\n",
      "|    iterations         | 10900    |\n",
      "|    time_elapsed       | 1062     |\n",
      "|    total_timesteps    | 872000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10899    |\n",
      "|    policy_loss        | 0.0568   |\n",
      "|    value_loss         | 0.016    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=3.60 +/- 2.42\n",
      "Episode length: 1137.20 +/- 420.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+03 |\n",
      "|    mean_reward        | 3.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 880000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10999    |\n",
      "|    policy_loss        | 0.0303   |\n",
      "|    value_loss         | 0.00632  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 2.3      |\n",
      "| time/              |          |\n",
      "|    fps             | 820      |\n",
      "|    iterations      | 11000    |\n",
      "|    time_elapsed    | 1072     |\n",
      "|    total_timesteps | 880000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 839      |\n",
      "|    ep_rew_mean        | 2.04     |\n",
      "| time/                 |          |\n",
      "|    fps                | 821      |\n",
      "|    iterations         | 11100    |\n",
      "|    time_elapsed       | 1080     |\n",
      "|    total_timesteps    | 888000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11099    |\n",
      "|    policy_loss        | 0.0225   |\n",
      "|    value_loss         | 0.00651  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=1.80 +/- 3.60\n",
      "Episode length: 743.80 +/- 440.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 744      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 890000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11124    |\n",
      "|    policy_loss        | 0.00659  |\n",
      "|    value_loss         | 0.00124  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 844      |\n",
      "|    ep_rew_mean        | 2.21     |\n",
      "| time/                 |          |\n",
      "|    fps                | 821      |\n",
      "|    iterations         | 11200    |\n",
      "|    time_elapsed       | 1090     |\n",
      "|    total_timesteps    | 896000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11199    |\n",
      "|    policy_loss        | 0.0155   |\n",
      "|    value_loss         | 0.00145  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=0.80 +/- 0.98\n",
      "Episode length: 642.40 +/- 154.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 642      |\n",
      "|    mean_reward        | 0.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 900000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11249    |\n",
      "|    policy_loss        | -0.0102  |\n",
      "|    value_loss         | 0.00554  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 857      |\n",
      "|    ep_rew_mean        | 2.32     |\n",
      "| time/                 |          |\n",
      "|    fps                | 821      |\n",
      "|    iterations         | 11300    |\n",
      "|    time_elapsed       | 1100     |\n",
      "|    total_timesteps    | 904000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11299    |\n",
      "|    policy_loss        | 0.0601   |\n",
      "|    value_loss         | 0.0177   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=2.40 +/- 3.50\n",
      "Episode length: 842.80 +/- 432.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 843      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 910000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11374    |\n",
      "|    policy_loss        | -0.00404 |\n",
      "|    value_loss         | 0.00266  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 808      |\n",
      "|    ep_rew_mean        | 1.92     |\n",
      "| time/                 |          |\n",
      "|    fps                | 821      |\n",
      "|    iterations         | 11400    |\n",
      "|    time_elapsed       | 1110     |\n",
      "|    total_timesteps    | 912000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11399    |\n",
      "|    policy_loss        | -0.00756 |\n",
      "|    value_loss         | 0.0134   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=2.00 +/- 2.45\n",
      "Episode length: 879.20 +/- 430.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 879      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 920000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11499    |\n",
      "|    policy_loss        | 0.00349  |\n",
      "|    value_loss         | 0.00141  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 2.25     |\n",
      "| time/              |          |\n",
      "|    fps             | 821      |\n",
      "|    iterations      | 11500    |\n",
      "|    time_elapsed    | 1120     |\n",
      "|    total_timesteps | 920000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 926      |\n",
      "|    ep_rew_mean        | 2.67     |\n",
      "| time/                 |          |\n",
      "|    fps                | 822      |\n",
      "|    iterations         | 11600    |\n",
      "|    time_elapsed       | 1128     |\n",
      "|    total_timesteps    | 928000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11599    |\n",
      "|    policy_loss        | -0.00897 |\n",
      "|    value_loss         | 0.00602  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=4.40 +/- 4.32\n",
      "Episode length: 1095.20 +/- 515.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.1e+03  |\n",
      "|    mean_reward        | 4.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 930000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11624    |\n",
      "|    policy_loss        | 0.014    |\n",
      "|    value_loss         | 0.00375  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 911      |\n",
      "|    ep_rew_mean        | 2.59     |\n",
      "| time/                 |          |\n",
      "|    fps                | 822      |\n",
      "|    iterations         | 11700    |\n",
      "|    time_elapsed       | 1138     |\n",
      "|    total_timesteps    | 936000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11699    |\n",
      "|    policy_loss        | 0.00408  |\n",
      "|    value_loss         | 0.00106  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=1.80 +/- 1.72\n",
      "Episode length: 814.20 +/- 316.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 814      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 940000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11749    |\n",
      "|    policy_loss        | 0.00288  |\n",
      "|    value_loss         | 0.00608  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 809      |\n",
      "|    ep_rew_mean        | 1.88     |\n",
      "| time/                 |          |\n",
      "|    fps                | 821      |\n",
      "|    iterations         | 11800    |\n",
      "|    time_elapsed       | 1148     |\n",
      "|    total_timesteps    | 944000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11799    |\n",
      "|    policy_loss        | -0.0183  |\n",
      "|    value_loss         | 0.0024   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=3.00 +/- 5.06\n",
      "Episode length: 874.00 +/- 517.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 874      |\n",
      "|    mean_reward        | 3        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 950000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11874    |\n",
      "|    policy_loss        | -0.0167  |\n",
      "|    value_loss         | 0.00264  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 749      |\n",
      "|    ep_rew_mean        | 1.53     |\n",
      "| time/                 |          |\n",
      "|    fps                | 821      |\n",
      "|    iterations         | 11900    |\n",
      "|    time_elapsed       | 1159     |\n",
      "|    total_timesteps    | 952000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11899    |\n",
      "|    policy_loss        | 0.0609   |\n",
      "|    value_loss         | 0.0158   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=0.60 +/- 1.20\n",
      "Episode length: 627.20 +/- 184.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 627      |\n",
      "|    mean_reward        | 0.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 960000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11999    |\n",
      "|    policy_loss        | 0.00078  |\n",
      "|    value_loss         | 0.00134  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 1.6      |\n",
      "| time/              |          |\n",
      "|    fps             | 821      |\n",
      "|    iterations      | 12000    |\n",
      "|    time_elapsed    | 1168     |\n",
      "|    total_timesteps | 960000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 801      |\n",
      "|    ep_rew_mean        | 1.86     |\n",
      "| time/                 |          |\n",
      "|    fps                | 822      |\n",
      "|    iterations         | 12100    |\n",
      "|    time_elapsed       | 1176     |\n",
      "|    total_timesteps    | 968000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12099    |\n",
      "|    policy_loss        | 0.0118   |\n",
      "|    value_loss         | 0.00835  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=2.00 +/- 3.10\n",
      "Episode length: 772.00 +/- 382.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 772      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 970000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12124    |\n",
      "|    policy_loss        | 0.0222   |\n",
      "|    value_loss         | 0.00162  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 820      |\n",
      "|    ep_rew_mean        | 2.02     |\n",
      "| time/                 |          |\n",
      "|    fps                | 822      |\n",
      "|    iterations         | 12200    |\n",
      "|    time_elapsed       | 1186     |\n",
      "|    total_timesteps    | 976000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12199    |\n",
      "|    policy_loss        | -0.0318  |\n",
      "|    value_loss         | 0.00488  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=3.20 +/- 4.96\n",
      "Episode length: 887.00 +/- 521.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 887      |\n",
      "|    mean_reward        | 3.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 980000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12249    |\n",
      "|    policy_loss        | 0.0457   |\n",
      "|    value_loss         | 0.0123   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 859      |\n",
      "|    ep_rew_mean        | 2.3      |\n",
      "| time/                 |          |\n",
      "|    fps                | 822      |\n",
      "|    iterations         | 12300    |\n",
      "|    time_elapsed       | 1196     |\n",
      "|    total_timesteps    | 984000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12299    |\n",
      "|    policy_loss        | 0.0246   |\n",
      "|    value_loss         | 0.00947  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 579.80 +/- 118.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 580      |\n",
      "|    mean_reward        | 0.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 990000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12374    |\n",
      "|    policy_loss        | -0.00296 |\n",
      "|    value_loss         | 0.00972  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 822      |\n",
      "|    ep_rew_mean        | 2.04     |\n",
      "| time/                 |          |\n",
      "|    fps                | 822      |\n",
      "|    iterations         | 12400    |\n",
      "|    time_elapsed       | 1206     |\n",
      "|    total_timesteps    | 992000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.842    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12399    |\n",
      "|    policy_loss        | 0.0562   |\n",
      "|    value_loss         | 0.0258   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=1.40 +/- 1.96\n",
      "Episode length: 753.20 +/- 351.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 753      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | -0.00323 |\n",
      "|    value_loss         | 0.00249  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 2.11     |\n",
      "| time/              |          |\n",
      "|    fps             | 822      |\n",
      "|    iterations      | 12500    |\n",
      "|    time_elapsed    | 1215     |\n",
      "|    total_timesteps | 1000000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 789      |\n",
      "|    ep_rew_mean        | 1.73     |\n",
      "| time/                 |          |\n",
      "|    fps                | 823      |\n",
      "|    iterations         | 12600    |\n",
      "|    time_elapsed       | 1223     |\n",
      "|    total_timesteps    | 1008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12599    |\n",
      "|    policy_loss        | 0.0301   |\n",
      "|    value_loss         | 0.0116   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1010000, episode_reward=2.00 +/- 2.28\n",
      "Episode length: 859.40 +/- 373.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 859      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1010000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12624    |\n",
      "|    policy_loss        | -0.0161  |\n",
      "|    value_loss         | 0.00567  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 811      |\n",
      "|    ep_rew_mean        | 1.81     |\n",
      "| time/                 |          |\n",
      "|    fps                | 823      |\n",
      "|    iterations         | 12700    |\n",
      "|    time_elapsed       | 1233     |\n",
      "|    total_timesteps    | 1016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12699    |\n",
      "|    policy_loss        | -0.0114  |\n",
      "|    value_loss         | 0.00262  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=4.20 +/- 2.93\n",
      "Episode length: 1128.20 +/- 380.67\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 1.13e+03  |\n",
      "|    mean_reward        | 4.2       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1020000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.36     |\n",
      "|    explained_variance | 0.995     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12749     |\n",
      "|    policy_loss        | -0.000224 |\n",
      "|    value_loss         | 0.00123   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 824      |\n",
      "|    ep_rew_mean        | 1.9      |\n",
      "| time/                 |          |\n",
      "|    fps                | 823      |\n",
      "|    iterations         | 12800    |\n",
      "|    time_elapsed       | 1243     |\n",
      "|    total_timesteps    | 1024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12799    |\n",
      "|    policy_loss        | 0.0628   |\n",
      "|    value_loss         | 0.00973  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1030000, episode_reward=3.60 +/- 3.38\n",
      "Episode length: 1063.80 +/- 462.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+03 |\n",
      "|    mean_reward        | 3.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1030000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12874    |\n",
      "|    policy_loss        | 0.0297   |\n",
      "|    value_loss         | 0.00682  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 846      |\n",
      "|    ep_rew_mean        | 2.07     |\n",
      "| time/                 |          |\n",
      "|    fps                | 823      |\n",
      "|    iterations         | 12900    |\n",
      "|    time_elapsed       | 1253     |\n",
      "|    total_timesteps    | 1032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12899    |\n",
      "|    policy_loss        | -0.0211  |\n",
      "|    value_loss         | 0.00118  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=1.40 +/- 1.96\n",
      "Episode length: 761.00 +/- 350.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 761      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12999    |\n",
      "|    policy_loss        | -0.0141  |\n",
      "|    value_loss         | 0.00702  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 2.1      |\n",
      "| time/              |          |\n",
      "|    fps             | 823      |\n",
      "|    iterations      | 13000    |\n",
      "|    time_elapsed    | 1263     |\n",
      "|    total_timesteps | 1040000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 870      |\n",
      "|    ep_rew_mean        | 2.27     |\n",
      "| time/                 |          |\n",
      "|    fps                | 824      |\n",
      "|    iterations         | 13100    |\n",
      "|    time_elapsed       | 1271     |\n",
      "|    total_timesteps    | 1048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13099    |\n",
      "|    policy_loss        | 0.00735  |\n",
      "|    value_loss         | 0.00474  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1050000, episode_reward=2.00 +/- 1.41\n",
      "Episode length: 850.80 +/- 234.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 851      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13124    |\n",
      "|    policy_loss        | -0.0134  |\n",
      "|    value_loss         | 0.00135  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 895      |\n",
      "|    ep_rew_mean        | 2.37     |\n",
      "| time/                 |          |\n",
      "|    fps                | 824      |\n",
      "|    iterations         | 13200    |\n",
      "|    time_elapsed       | 1280     |\n",
      "|    total_timesteps    | 1056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13199    |\n",
      "|    policy_loss        | -0.00503 |\n",
      "|    value_loss         | 0.00234  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=2.60 +/- 2.06\n",
      "Episode length: 952.40 +/- 381.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 952      |\n",
      "|    mean_reward        | 2.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13249    |\n",
      "|    policy_loss        | 0.0554   |\n",
      "|    value_loss         | 0.045    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 920      |\n",
      "|    ep_rew_mean        | 2.53     |\n",
      "| time/                 |          |\n",
      "|    fps                | 824      |\n",
      "|    iterations         | 13300    |\n",
      "|    time_elapsed       | 1290     |\n",
      "|    total_timesteps    | 1064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13299    |\n",
      "|    policy_loss        | 0.027    |\n",
      "|    value_loss         | 0.00769  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1070000, episode_reward=1.00 +/- 0.89\n",
      "Episode length: 668.80 +/- 140.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 669      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1070000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13374    |\n",
      "|    policy_loss        | 0.00492  |\n",
      "|    value_loss         | 0.0019   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 854      |\n",
      "|    ep_rew_mean        | 2.17     |\n",
      "| time/                 |          |\n",
      "|    fps                | 824      |\n",
      "|    iterations         | 13400    |\n",
      "|    time_elapsed       | 1299     |\n",
      "|    total_timesteps    | 1072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13399    |\n",
      "|    policy_loss        | -0.0242  |\n",
      "|    value_loss         | 0.0197   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=1.40 +/- 1.96\n",
      "Episode length: 758.20 +/- 341.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 758      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13499    |\n",
      "|    policy_loss        | 0.00192  |\n",
      "|    value_loss         | 0.000575 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 891      |\n",
      "|    ep_rew_mean     | 2.47     |\n",
      "| time/              |          |\n",
      "|    fps             | 824      |\n",
      "|    iterations      | 13500    |\n",
      "|    time_elapsed    | 1309     |\n",
      "|    total_timesteps | 1080000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 830      |\n",
      "|    ep_rew_mean        | 2.04     |\n",
      "| time/                 |          |\n",
      "|    fps                | 825      |\n",
      "|    iterations         | 13600    |\n",
      "|    time_elapsed       | 1317     |\n",
      "|    total_timesteps    | 1088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13599    |\n",
      "|    policy_loss        | -0.0259  |\n",
      "|    value_loss         | 0.00957  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1090000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 569.80 +/- 113.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 570      |\n",
      "|    mean_reward        | 0.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1090000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13624    |\n",
      "|    policy_loss        | 0.0295   |\n",
      "|    value_loss         | 0.00432  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 833      |\n",
      "|    ep_rew_mean        | 2.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 826      |\n",
      "|    iterations         | 13700    |\n",
      "|    time_elapsed       | 1326     |\n",
      "|    total_timesteps    | 1096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13699    |\n",
      "|    policy_loss        | 0.0257   |\n",
      "|    value_loss         | 0.0149   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=1.00 +/- 1.26\n",
      "Episode length: 672.00 +/- 200.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 672      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13749    |\n",
      "|    policy_loss        | 0.00895  |\n",
      "|    value_loss         | 0.00816  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 922      |\n",
      "|    ep_rew_mean        | 2.68     |\n",
      "| time/                 |          |\n",
      "|    fps                | 826      |\n",
      "|    iterations         | 13800    |\n",
      "|    time_elapsed       | 1335     |\n",
      "|    total_timesteps    | 1104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13799    |\n",
      "|    policy_loss        | -0.019   |\n",
      "|    value_loss         | 0.00396  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1110000, episode_reward=4.00 +/- 3.16\n",
      "Episode length: 1106.60 +/- 422.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.11e+03 |\n",
      "|    mean_reward        | 4        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1110000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.779    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13874    |\n",
      "|    policy_loss        | -0.0819  |\n",
      "|    value_loss         | 0.0326   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 868      |\n",
      "|    ep_rew_mean        | 2.32     |\n",
      "| time/                 |          |\n",
      "|    fps                | 826      |\n",
      "|    iterations         | 13900    |\n",
      "|    time_elapsed       | 1345     |\n",
      "|    total_timesteps    | 1112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13899    |\n",
      "|    policy_loss        | 0.0219   |\n",
      "|    value_loss         | 0.00153  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=1.00 +/- 1.26\n",
      "Episode length: 687.60 +/- 211.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 688      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.703    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13999    |\n",
      "|    policy_loss        | 0.0408   |\n",
      "|    value_loss         | 0.00915  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 2.25     |\n",
      "| time/              |          |\n",
      "|    fps             | 826      |\n",
      "|    iterations      | 14000    |\n",
      "|    time_elapsed    | 1355     |\n",
      "|    total_timesteps | 1120000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 783      |\n",
      "|    ep_rew_mean        | 1.82     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 14100    |\n",
      "|    time_elapsed       | 1363     |\n",
      "|    total_timesteps    | 1128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14099    |\n",
      "|    policy_loss        | 0.0207   |\n",
      "|    value_loss         | 0.0099   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1130000, episode_reward=3.20 +/- 2.32\n",
      "Episode length: 1054.60 +/- 410.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+03 |\n",
      "|    mean_reward        | 3.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1130000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14124    |\n",
      "|    policy_loss        | -0.0187  |\n",
      "|    value_loss         | 0.00387  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 874      |\n",
      "|    ep_rew_mean        | 2.41     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 14200    |\n",
      "|    time_elapsed       | 1373     |\n",
      "|    total_timesteps    | 1136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14199    |\n",
      "|    policy_loss        | -0.0256  |\n",
      "|    value_loss         | 0.00562  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=1.20 +/- 1.47\n",
      "Episode length: 735.20 +/- 256.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 735      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14249    |\n",
      "|    policy_loss        | 0.0146   |\n",
      "|    value_loss         | 0.00119  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 855      |\n",
      "|    ep_rew_mean        | 2.15     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 14300    |\n",
      "|    time_elapsed       | 1382     |\n",
      "|    total_timesteps    | 1144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14299    |\n",
      "|    policy_loss        | -0.0168  |\n",
      "|    value_loss         | 0.00328  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1150000, episode_reward=3.80 +/- 3.43\n",
      "Episode length: 1080.00 +/- 469.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+03 |\n",
      "|    mean_reward        | 3.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14374    |\n",
      "|    policy_loss        | -0.0482  |\n",
      "|    value_loss         | 0.00528  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 817      |\n",
      "|    ep_rew_mean        | 1.93     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 14400    |\n",
      "|    time_elapsed       | 1392     |\n",
      "|    total_timesteps    | 1152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14399    |\n",
      "|    policy_loss        | 0.0376   |\n",
      "|    value_loss         | 0.00779  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 524.00 +/- 10.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 524      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14499    |\n",
      "|    policy_loss        | 0.00771  |\n",
      "|    value_loss         | 0.00252  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 2.33     |\n",
      "| time/              |          |\n",
      "|    fps             | 827      |\n",
      "|    iterations      | 14500    |\n",
      "|    time_elapsed    | 1401     |\n",
      "|    total_timesteps | 1160000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 818      |\n",
      "|    ep_rew_mean        | 1.96     |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 14600    |\n",
      "|    time_elapsed       | 1409     |\n",
      "|    total_timesteps    | 1168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14599    |\n",
      "|    policy_loss        | -0.0122  |\n",
      "|    value_loss         | 0.00167  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1170000, episode_reward=2.80 +/- 2.32\n",
      "Episode length: 1001.60 +/- 394.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1e+03    |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1170000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14624    |\n",
      "|    policy_loss        | 0.00345  |\n",
      "|    value_loss         | 0.00114  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 803      |\n",
      "|    ep_rew_mean        | 1.81     |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 14700    |\n",
      "|    time_elapsed       | 1420     |\n",
      "|    total_timesteps    | 1176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14699    |\n",
      "|    policy_loss        | 0.0393   |\n",
      "|    value_loss         | 0.0113   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1180000, episode_reward=1.00 +/- 0.89\n",
      "Episode length: 665.20 +/- 124.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 665      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14749    |\n",
      "|    policy_loss        | 0.0167   |\n",
      "|    value_loss         | 0.00217  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 770      |\n",
      "|    ep_rew_mean        | 1.58     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 14800    |\n",
      "|    time_elapsed       | 1430     |\n",
      "|    total_timesteps    | 1184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14799    |\n",
      "|    policy_loss        | 0.00265  |\n",
      "|    value_loss         | 0.00107  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1190000, episode_reward=4.80 +/- 2.93\n",
      "Episode length: 1259.00 +/- 389.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.26e+03 |\n",
      "|    mean_reward        | 4.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1190000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14874    |\n",
      "|    policy_loss        | 0.0073   |\n",
      "|    value_loss         | 0.001    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 771      |\n",
      "|    ep_rew_mean        | 1.5      |\n",
      "| time/                 |          |\n",
      "|    fps                | 826      |\n",
      "|    iterations         | 14900    |\n",
      "|    time_elapsed       | 1441     |\n",
      "|    total_timesteps    | 1192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14899    |\n",
      "|    policy_loss        | -0.00542 |\n",
      "|    value_loss         | 0.00395  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=2.20 +/- 1.83\n",
      "Episode length: 893.20 +/- 325.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 893      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14999    |\n",
      "|    policy_loss        | 0.00564  |\n",
      "|    value_loss         | 0.00383  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 814      |\n",
      "|    ep_rew_mean     | 1.79     |\n",
      "| time/              |          |\n",
      "|    fps             | 826      |\n",
      "|    iterations      | 15000    |\n",
      "|    time_elapsed    | 1452     |\n",
      "|    total_timesteps | 1200000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 815      |\n",
      "|    ep_rew_mean        | 1.9      |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 15100    |\n",
      "|    time_elapsed       | 1460     |\n",
      "|    total_timesteps    | 1208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15099    |\n",
      "|    policy_loss        | 0.0174   |\n",
      "|    value_loss         | 0.00927  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1210000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 519.00 +/- 8.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 519      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1210000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15124    |\n",
      "|    policy_loss        | 0.012    |\n",
      "|    value_loss         | 0.00303  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 820      |\n",
      "|    ep_rew_mean        | 2.02     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 15200    |\n",
      "|    time_elapsed       | 1469     |\n",
      "|    total_timesteps    | 1216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15199    |\n",
      "|    policy_loss        | -0.00787 |\n",
      "|    value_loss         | 0.00565  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 585.80 +/- 118.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 586      |\n",
      "|    mean_reward        | 0.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15249    |\n",
      "|    policy_loss        | -0.0203  |\n",
      "|    value_loss         | 0.00261  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 820      |\n",
      "|    ep_rew_mean        | 1.96     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 15300    |\n",
      "|    time_elapsed       | 1479     |\n",
      "|    total_timesteps    | 1224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15299    |\n",
      "|    policy_loss        | -0.0145  |\n",
      "|    value_loss         | 0.00181  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1230000, episode_reward=2.00 +/- 2.19\n",
      "Episode length: 862.40 +/- 400.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 862      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1230000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15374    |\n",
      "|    policy_loss        | -0.041   |\n",
      "|    value_loss         | 0.0145   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 758      |\n",
      "|    ep_rew_mean        | 1.48     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 15400    |\n",
      "|    time_elapsed       | 1489     |\n",
      "|    total_timesteps    | 1232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15399    |\n",
      "|    policy_loss        | 0.00405  |\n",
      "|    value_loss         | 0.000665 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=7.00 +/- 3.03\n",
      "Episode length: 1454.80 +/- 346.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.45e+03 |\n",
      "|    mean_reward        | 7        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15499    |\n",
      "|    policy_loss        | -0.0481  |\n",
      "|    value_loss         | 0.0208   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 1.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 826      |\n",
      "|    iterations      | 15500    |\n",
      "|    time_elapsed    | 1500     |\n",
      "|    total_timesteps | 1240000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 776      |\n",
      "|    ep_rew_mean        | 1.65     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 15600    |\n",
      "|    time_elapsed       | 1508     |\n",
      "|    total_timesteps    | 1248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15599    |\n",
      "|    policy_loss        | 0.0258   |\n",
      "|    value_loss         | 0.00299  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1250000, episode_reward=0.80 +/- 1.60\n",
      "Episode length: 644.00 +/- 249.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 644      |\n",
      "|    mean_reward        | 0.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15624    |\n",
      "|    policy_loss        | -0.00491 |\n",
      "|    value_loss         | 0.00404  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 854      |\n",
      "|    ep_rew_mean        | 2.26     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 15700    |\n",
      "|    time_elapsed       | 1517     |\n",
      "|    total_timesteps    | 1256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15699    |\n",
      "|    policy_loss        | 0.00822  |\n",
      "|    value_loss         | 0.00343  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 521.60 +/- 11.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 522      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15749    |\n",
      "|    policy_loss        | -0.00177 |\n",
      "|    value_loss         | 0.0196   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 877      |\n",
      "|    ep_rew_mean        | 2.42     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 15800    |\n",
      "|    time_elapsed       | 1527     |\n",
      "|    total_timesteps    | 1264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15799    |\n",
      "|    policy_loss        | -0.00683 |\n",
      "|    value_loss         | 0.00298  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1270000, episode_reward=4.20 +/- 3.06\n",
      "Episode length: 1157.40 +/- 408.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+03 |\n",
      "|    mean_reward        | 4.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1270000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15874    |\n",
      "|    policy_loss        | -0.0529  |\n",
      "|    value_loss         | 0.0163   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 806      |\n",
      "|    ep_rew_mean        | 1.9      |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 15900    |\n",
      "|    time_elapsed       | 1537     |\n",
      "|    total_timesteps    | 1272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15899    |\n",
      "|    policy_loss        | -0.0143  |\n",
      "|    value_loss         | 0.00418  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=1.40 +/- 1.96\n",
      "Episode length: 750.40 +/- 344.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 750      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15999    |\n",
      "|    policy_loss        | -0.00636 |\n",
      "|    value_loss         | 0.00765  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 2.15     |\n",
      "| time/              |          |\n",
      "|    fps             | 827      |\n",
      "|    iterations      | 16000    |\n",
      "|    time_elapsed    | 1547     |\n",
      "|    total_timesteps | 1280000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 857      |\n",
      "|    ep_rew_mean        | 2.11     |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 16100    |\n",
      "|    time_elapsed       | 1554     |\n",
      "|    total_timesteps    | 1288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16099    |\n",
      "|    policy_loss        | -0.00481 |\n",
      "|    value_loss         | 0.00239  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1290000, episode_reward=1.00 +/- 2.00\n",
      "Episode length: 691.00 +/- 353.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 691      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1290000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16124    |\n",
      "|    policy_loss        | 0.0384   |\n",
      "|    value_loss         | 0.00483  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 863      |\n",
      "|    ep_rew_mean        | 2.17     |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 16200    |\n",
      "|    time_elapsed       | 1564     |\n",
      "|    total_timesteps    | 1296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16199    |\n",
      "|    policy_loss        | -0.011   |\n",
      "|    value_loss         | 0.00442  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=1.20 +/- 1.60\n",
      "Episode length: 757.80 +/- 299.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 758      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16249    |\n",
      "|    policy_loss        | -0.0166  |\n",
      "|    value_loss         | 0.0104   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 789      |\n",
      "|    ep_rew_mean        | 1.71     |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 16300    |\n",
      "|    time_elapsed       | 1573     |\n",
      "|    total_timesteps    | 1304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16299    |\n",
      "|    policy_loss        | -0.039   |\n",
      "|    value_loss         | 0.00938  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1310000, episode_reward=2.80 +/- 2.48\n",
      "Episode length: 914.60 +/- 290.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 915      |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1310000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16374    |\n",
      "|    policy_loss        | 0.000997 |\n",
      "|    value_loss         | 0.000558 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 789      |\n",
      "|    ep_rew_mean        | 1.62     |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 16400    |\n",
      "|    time_elapsed       | 1584     |\n",
      "|    total_timesteps    | 1312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16399    |\n",
      "|    policy_loss        | 0.00238  |\n",
      "|    value_loss         | 0.00118  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=1.80 +/- 2.23\n",
      "Episode length: 835.40 +/- 388.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 835      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16499    |\n",
      "|    policy_loss        | -0.0501  |\n",
      "|    value_loss         | 0.017    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 825      |\n",
      "|    ep_rew_mean     | 1.95     |\n",
      "| time/              |          |\n",
      "|    fps             | 828      |\n",
      "|    iterations      | 16500    |\n",
      "|    time_elapsed    | 1593     |\n",
      "|    total_timesteps | 1320000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 931      |\n",
      "|    ep_rew_mean        | 2.72     |\n",
      "| time/                 |          |\n",
      "|    fps                | 829      |\n",
      "|    iterations         | 16600    |\n",
      "|    time_elapsed       | 1601     |\n",
      "|    total_timesteps    | 1328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16599    |\n",
      "|    policy_loss        | 0.00133  |\n",
      "|    value_loss         | 0.000745 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1330000, episode_reward=2.60 +/- 2.15\n",
      "Episode length: 973.40 +/- 380.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 973      |\n",
      "|    mean_reward        | 2.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1330000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16624    |\n",
      "|    policy_loss        | 0.00318  |\n",
      "|    value_loss         | 0.00124  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 895      |\n",
      "|    ep_rew_mean        | 2.53     |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 16700    |\n",
      "|    time_elapsed       | 1611     |\n",
      "|    total_timesteps    | 1336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16699    |\n",
      "|    policy_loss        | 0.00816  |\n",
      "|    value_loss         | 0.00234  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=2.00 +/- 1.10\n",
      "Episode length: 844.40 +/- 179.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 844      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16749    |\n",
      "|    policy_loss        | 0.0551   |\n",
      "|    value_loss         | 0.0214   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 811      |\n",
      "|    ep_rew_mean        | 1.92     |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 16800    |\n",
      "|    time_elapsed       | 1621     |\n",
      "|    total_timesteps    | 1344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16799    |\n",
      "|    policy_loss        | 0.0612   |\n",
      "|    value_loss         | 0.0119   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1350000, episode_reward=0.20 +/- 0.40\n",
      "Episode length: 543.00 +/- 38.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 543      |\n",
      "|    mean_reward        | 0.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16874    |\n",
      "|    policy_loss        | -0.0061  |\n",
      "|    value_loss         | 0.00474  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 826      |\n",
      "|    ep_rew_mean        | 2.02     |\n",
      "| time/                 |          |\n",
      "|    fps                | 829      |\n",
      "|    iterations         | 16900    |\n",
      "|    time_elapsed       | 1630     |\n",
      "|    total_timesteps    | 1352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16899    |\n",
      "|    policy_loss        | -0.007   |\n",
      "|    value_loss         | 0.00185  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=1.00 +/- 1.26\n",
      "Episode length: 678.40 +/- 197.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 678      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16999    |\n",
      "|    policy_loss        | -0.00971 |\n",
      "|    value_loss         | 0.00318  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 2.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 829      |\n",
      "|    iterations      | 17000    |\n",
      "|    time_elapsed    | 1639     |\n",
      "|    total_timesteps | 1360000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 877      |\n",
      "|    ep_rew_mean        | 2.25     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 17100    |\n",
      "|    time_elapsed       | 1647     |\n",
      "|    total_timesteps    | 1368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17099    |\n",
      "|    policy_loss        | 0.000622 |\n",
      "|    value_loss         | 0.00266  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1370000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 588.20 +/- 120.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 588      |\n",
      "|    mean_reward        | 0.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1370000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17124    |\n",
      "|    policy_loss        | -0.0269  |\n",
      "|    value_loss         | 0.00542  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 857      |\n",
      "|    ep_rew_mean        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 17200    |\n",
      "|    time_elapsed       | 1656     |\n",
      "|    total_timesteps    | 1376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17199    |\n",
      "|    policy_loss        | -0.0159  |\n",
      "|    value_loss         | 0.00147  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=2.40 +/- 1.96\n",
      "Episode length: 920.20 +/- 360.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 920      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17249    |\n",
      "|    policy_loss        | 0.0153   |\n",
      "|    value_loss         | 0.00106  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 881      |\n",
      "|    ep_rew_mean        | 2.37     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 17300    |\n",
      "|    time_elapsed       | 1666     |\n",
      "|    total_timesteps    | 1384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17299    |\n",
      "|    policy_loss        | -0.00998 |\n",
      "|    value_loss         | 0.00135  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1390000, episode_reward=2.60 +/- 2.33\n",
      "Episode length: 970.40 +/- 412.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 970      |\n",
      "|    mean_reward        | 2.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1390000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17374    |\n",
      "|    policy_loss        | 0.0165   |\n",
      "|    value_loss         | 0.00825  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 871      |\n",
      "|    ep_rew_mean        | 2.28     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 17400    |\n",
      "|    time_elapsed       | 1676     |\n",
      "|    total_timesteps    | 1392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17399    |\n",
      "|    policy_loss        | 0.0216   |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1400000, episode_reward=2.00 +/- 1.10\n",
      "Episode length: 835.00 +/- 205.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 835      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17499    |\n",
      "|    policy_loss        | 0.0019   |\n",
      "|    value_loss         | 0.00137  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 837      |\n",
      "|    ep_rew_mean     | 2.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 830      |\n",
      "|    iterations      | 17500    |\n",
      "|    time_elapsed    | 1686     |\n",
      "|    total_timesteps | 1400000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 825      |\n",
      "|    ep_rew_mean        | 1.94     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 17600    |\n",
      "|    time_elapsed       | 1694     |\n",
      "|    total_timesteps    | 1408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17599    |\n",
      "|    policy_loss        | -0.015   |\n",
      "|    value_loss         | 0.00207  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1410000, episode_reward=2.20 +/- 2.04\n",
      "Episode length: 896.20 +/- 363.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 896      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1410000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17624    |\n",
      "|    policy_loss        | -0.0307  |\n",
      "|    value_loss         | 0.00792  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 845      |\n",
      "|    ep_rew_mean        | 2.11     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 17700    |\n",
      "|    time_elapsed       | 1704     |\n",
      "|    total_timesteps    | 1416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17699    |\n",
      "|    policy_loss        | -0.031   |\n",
      "|    value_loss         | 0.00408  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1420000, episode_reward=1.00 +/- 1.26\n",
      "Episode length: 671.80 +/- 196.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 672      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17749    |\n",
      "|    policy_loss        | 0.0274   |\n",
      "|    value_loss         | 0.0122   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 839      |\n",
      "|    ep_rew_mean        | 2.09     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 17800    |\n",
      "|    time_elapsed       | 1714     |\n",
      "|    total_timesteps    | 1424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17799    |\n",
      "|    policy_loss        | 0.0205   |\n",
      "|    value_loss         | 0.0199   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1430000, episode_reward=4.60 +/- 4.80\n",
      "Episode length: 1129.60 +/- 491.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+03 |\n",
      "|    mean_reward        | 4.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1430000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17874    |\n",
      "|    policy_loss        | 0.00549  |\n",
      "|    value_loss         | 0.00183  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 820      |\n",
      "|    ep_rew_mean        | 1.99     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 17900    |\n",
      "|    time_elapsed       | 1724     |\n",
      "|    total_timesteps    | 1432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17899    |\n",
      "|    policy_loss        | 0.018    |\n",
      "|    value_loss         | 0.00451  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=2.80 +/- 2.48\n",
      "Episode length: 1013.00 +/- 447.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+03 |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17999    |\n",
      "|    policy_loss        | 0.00851  |\n",
      "|    value_loss         | 0.00264  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 796      |\n",
      "|    ep_rew_mean     | 1.75     |\n",
      "| time/              |          |\n",
      "|    fps             | 830      |\n",
      "|    iterations      | 18000    |\n",
      "|    time_elapsed    | 1734     |\n",
      "|    total_timesteps | 1440000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 797      |\n",
      "|    ep_rew_mean        | 1.79     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 18100    |\n",
      "|    time_elapsed       | 1742     |\n",
      "|    total_timesteps    | 1448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.763    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18099    |\n",
      "|    policy_loss        | 0.0118   |\n",
      "|    value_loss         | 0.0534   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1450000, episode_reward=1.80 +/- 1.33\n",
      "Episode length: 820.60 +/- 224.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 821      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18124    |\n",
      "|    policy_loss        | 0.00315  |\n",
      "|    value_loss         | 0.0126   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 799      |\n",
      "|    ep_rew_mean        | 1.86     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 18200    |\n",
      "|    time_elapsed       | 1752     |\n",
      "|    total_timesteps    | 1456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18199    |\n",
      "|    policy_loss        | 0.0139   |\n",
      "|    value_loss         | 0.000724 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1460000, episode_reward=1.40 +/- 1.20\n",
      "Episode length: 737.60 +/- 187.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 738      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18249    |\n",
      "|    policy_loss        | 0.00572  |\n",
      "|    value_loss         | 0.00128  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 803      |\n",
      "|    ep_rew_mean        | 1.85     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 18300    |\n",
      "|    time_elapsed       | 1761     |\n",
      "|    total_timesteps    | 1464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18299    |\n",
      "|    policy_loss        | -0.0313  |\n",
      "|    value_loss         | 0.00276  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1470000, episode_reward=2.00 +/- 1.26\n",
      "Episode length: 838.20 +/- 218.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 838      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1470000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18374    |\n",
      "|    policy_loss        | 0.00918  |\n",
      "|    value_loss         | 0.00508  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 834      |\n",
      "|    ep_rew_mean        | 1.98     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 18400    |\n",
      "|    time_elapsed       | 1771     |\n",
      "|    total_timesteps    | 1472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18399    |\n",
      "|    policy_loss        | -0.00207 |\n",
      "|    value_loss         | 0.0017   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1480000, episode_reward=2.40 +/- 3.38\n",
      "Episode length: 828.00 +/- 416.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 828      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18499    |\n",
      "|    policy_loss        | -0.00743 |\n",
      "|    value_loss         | 0.00341  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 1.97     |\n",
      "| time/              |          |\n",
      "|    fps             | 831      |\n",
      "|    iterations      | 18500    |\n",
      "|    time_elapsed    | 1780     |\n",
      "|    total_timesteps | 1480000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 845      |\n",
      "|    ep_rew_mean        | 2.07     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 18600    |\n",
      "|    time_elapsed       | 1788     |\n",
      "|    total_timesteps    | 1488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18599    |\n",
      "|    policy_loss        | -0.0108  |\n",
      "|    value_loss         | 0.00245  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1490000, episode_reward=3.00 +/- 2.28\n",
      "Episode length: 1033.80 +/- 398.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+03 |\n",
      "|    mean_reward        | 3        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1490000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18624    |\n",
      "|    policy_loss        | 0.00152  |\n",
      "|    value_loss         | 0.00189  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 821      |\n",
      "|    ep_rew_mean        | 1.89     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 18700    |\n",
      "|    time_elapsed       | 1799     |\n",
      "|    total_timesteps    | 1496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18699    |\n",
      "|    policy_loss        | 0.0496   |\n",
      "|    value_loss         | 0.0226   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=3.60 +/- 3.14\n",
      "Episode length: 1046.00 +/- 416.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+03 |\n",
      "|    mean_reward        | 3.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18749    |\n",
      "|    policy_loss        | 0.0116   |\n",
      "|    value_loss         | 0.021    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 817      |\n",
      "|    ep_rew_mean        | 1.92     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 18800    |\n",
      "|    time_elapsed       | 1808     |\n",
      "|    total_timesteps    | 1504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18799    |\n",
      "|    policy_loss        | -0.0022  |\n",
      "|    value_loss         | 0.00163  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1510000, episode_reward=1.40 +/- 1.50\n",
      "Episode length: 782.40 +/- 260.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 782      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1510000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.823    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18874    |\n",
      "|    policy_loss        | 0.0738   |\n",
      "|    value_loss         | 0.0326   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 831      |\n",
      "|    ep_rew_mean        | 2.05     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 18900    |\n",
      "|    time_elapsed       | 1818     |\n",
      "|    total_timesteps    | 1512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18899    |\n",
      "|    policy_loss        | -0.00423 |\n",
      "|    value_loss         | 0.0107   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1520000, episode_reward=1.80 +/- 3.60\n",
      "Episode length: 741.40 +/- 439.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 741      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.774    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18999    |\n",
      "|    policy_loss        | 0.0984   |\n",
      "|    value_loss         | 0.0422   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 822      |\n",
      "|    ep_rew_mean     | 1.98     |\n",
      "| time/              |          |\n",
      "|    fps             | 831      |\n",
      "|    iterations      | 19000    |\n",
      "|    time_elapsed    | 1828     |\n",
      "|    total_timesteps | 1520000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 829      |\n",
      "|    ep_rew_mean        | 1.98     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 19100    |\n",
      "|    time_elapsed       | 1836     |\n",
      "|    total_timesteps    | 1528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19099    |\n",
      "|    policy_loss        | 0.0331   |\n",
      "|    value_loss         | 0.00421  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1530000, episode_reward=2.00 +/- 3.52\n",
      "Episode length: 775.20 +/- 432.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 775      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1530000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19124    |\n",
      "|    policy_loss        | -0.0329  |\n",
      "|    value_loss         | 0.0115   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 833      |\n",
      "|    ep_rew_mean        | 2.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 19200    |\n",
      "|    time_elapsed       | 1846     |\n",
      "|    total_timesteps    | 1536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19199    |\n",
      "|    policy_loss        | -0.0194  |\n",
      "|    value_loss         | 0.0017   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1540000, episode_reward=4.80 +/- 2.48\n",
      "Episode length: 1254.20 +/- 293.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.25e+03 |\n",
      "|    mean_reward        | 4.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19249    |\n",
      "|    policy_loss        | -0.0455  |\n",
      "|    value_loss         | 0.0104   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 851      |\n",
      "|    ep_rew_mean        | 2.21     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 19300    |\n",
      "|    time_elapsed       | 1856     |\n",
      "|    total_timesteps    | 1544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19299    |\n",
      "|    policy_loss        | 0.0115   |\n",
      "|    value_loss         | 0.00454  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1550000, episode_reward=3.60 +/- 3.14\n",
      "Episode length: 1031.00 +/- 417.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+03 |\n",
      "|    mean_reward        | 3.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19374    |\n",
      "|    policy_loss        | -0.00291 |\n",
      "|    value_loss         | 0.00445  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 882      |\n",
      "|    ep_rew_mean        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 19400    |\n",
      "|    time_elapsed       | 1867     |\n",
      "|    total_timesteps    | 1552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19399    |\n",
      "|    policy_loss        | 0.0654   |\n",
      "|    value_loss         | 0.0207   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1560000, episode_reward=2.60 +/- 3.38\n",
      "Episode length: 896.40 +/- 423.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 896      |\n",
      "|    mean_reward        | 2.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19499    |\n",
      "|    policy_loss        | -0.028   |\n",
      "|    value_loss         | 0.00318  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 2.34     |\n",
      "| time/              |          |\n",
      "|    fps             | 830      |\n",
      "|    iterations      | 19500    |\n",
      "|    time_elapsed    | 1878     |\n",
      "|    total_timesteps | 1560000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 908      |\n",
      "|    ep_rew_mean        | 2.52     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 19600    |\n",
      "|    time_elapsed       | 1886     |\n",
      "|    total_timesteps    | 1568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19599    |\n",
      "|    policy_loss        | 0.00798  |\n",
      "|    value_loss         | 0.0148   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1570000, episode_reward=2.80 +/- 1.72\n",
      "Episode length: 979.00 +/- 287.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 979      |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1570000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19624    |\n",
      "|    policy_loss        | 0.0301   |\n",
      "|    value_loss         | 0.00486  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 891      |\n",
      "|    ep_rew_mean        | 2.33     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 19700    |\n",
      "|    time_elapsed       | 1896     |\n",
      "|    total_timesteps    | 1576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19699    |\n",
      "|    policy_loss        | -0.0143  |\n",
      "|    value_loss         | 0.00647  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1580000, episode_reward=5.00 +/- 2.83\n",
      "Episode length: 1243.80 +/- 375.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.24e+03 |\n",
      "|    mean_reward        | 5        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19749    |\n",
      "|    policy_loss        | -0.01    |\n",
      "|    value_loss         | 0.0028   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 835      |\n",
      "|    ep_rew_mean        | 1.98     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 19800    |\n",
      "|    time_elapsed       | 1906     |\n",
      "|    total_timesteps    | 1584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19799    |\n",
      "|    policy_loss        | -0.00202 |\n",
      "|    value_loss         | 0.00523  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1590000, episode_reward=2.20 +/- 2.04\n",
      "Episode length: 885.40 +/- 354.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 885      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1590000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19874    |\n",
      "|    policy_loss        | 0.0129   |\n",
      "|    value_loss         | 0.00127  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 799      |\n",
      "|    ep_rew_mean        | 1.87     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 19900    |\n",
      "|    time_elapsed       | 1916     |\n",
      "|    total_timesteps    | 1592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19899    |\n",
      "|    policy_loss        | 0.0146   |\n",
      "|    value_loss         | 0.00117  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=1.20 +/- 1.17\n",
      "Episode length: 701.20 +/- 187.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 701      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19999    |\n",
      "|    policy_loss        | 0.0572   |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 2.2      |\n",
      "| time/              |          |\n",
      "|    fps             | 830      |\n",
      "|    iterations      | 20000    |\n",
      "|    time_elapsed    | 1925     |\n",
      "|    total_timesteps | 1600000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 902      |\n",
      "|    ep_rew_mean        | 2.58     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 20100    |\n",
      "|    time_elapsed       | 1933     |\n",
      "|    total_timesteps    | 1608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20099    |\n",
      "|    policy_loss        | 0.022    |\n",
      "|    value_loss         | 0.0318   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1610000, episode_reward=5.40 +/- 2.87\n",
      "Episode length: 1270.80 +/- 310.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.27e+03 |\n",
      "|    mean_reward        | 5.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1610000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20124    |\n",
      "|    policy_loss        | -0.0545  |\n",
      "|    value_loss         | 0.0116   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 928      |\n",
      "|    ep_rew_mean        | 2.65     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 20200    |\n",
      "|    time_elapsed       | 1943     |\n",
      "|    total_timesteps    | 1616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20199    |\n",
      "|    policy_loss        | -0.0369  |\n",
      "|    value_loss         | 0.00722  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1620000, episode_reward=1.00 +/- 0.89\n",
      "Episode length: 674.60 +/- 122.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 675      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1620000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20249    |\n",
      "|    policy_loss        | -0.0147  |\n",
      "|    value_loss         | 0.00165  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 882      |\n",
      "|    ep_rew_mean        | 2.28     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 20300    |\n",
      "|    time_elapsed       | 1953     |\n",
      "|    total_timesteps    | 1624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20299    |\n",
      "|    policy_loss        | -0.0356  |\n",
      "|    value_loss         | 0.0162   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1630000, episode_reward=0.60 +/- 1.20\n",
      "Episode length: 625.40 +/- 185.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 625      |\n",
      "|    mean_reward        | 0.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1630000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.835    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20374    |\n",
      "|    policy_loss        | -0.0969  |\n",
      "|    value_loss         | 0.0435   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 857      |\n",
      "|    ep_rew_mean        | 2.21     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 20400    |\n",
      "|    time_elapsed       | 1962     |\n",
      "|    total_timesteps    | 1632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20399    |\n",
      "|    policy_loss        | 0.0272   |\n",
      "|    value_loss         | 0.011    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1640000, episode_reward=1.40 +/- 1.85\n",
      "Episode length: 750.40 +/- 341.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 750      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20499    |\n",
      "|    policy_loss        | -0.0115  |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 2.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 831      |\n",
      "|    iterations      | 20500    |\n",
      "|    time_elapsed    | 1972     |\n",
      "|    total_timesteps | 1640000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 855      |\n",
      "|    ep_rew_mean        | 2.25     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 20600    |\n",
      "|    time_elapsed       | 1980     |\n",
      "|    total_timesteps    | 1648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20599    |\n",
      "|    policy_loss        | -0.0773  |\n",
      "|    value_loss         | 0.0106   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1650000, episode_reward=1.80 +/- 1.47\n",
      "Episode length: 848.80 +/- 254.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 849      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20624    |\n",
      "|    policy_loss        | -0.00123 |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 807      |\n",
      "|    ep_rew_mean        | 1.83     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 20700    |\n",
      "|    time_elapsed       | 1989     |\n",
      "|    total_timesteps    | 1656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20699    |\n",
      "|    policy_loss        | -0.042   |\n",
      "|    value_loss         | 0.0143   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1660000, episode_reward=3.20 +/- 3.06\n",
      "Episode length: 977.00 +/- 357.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 977      |\n",
      "|    mean_reward        | 3.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20749    |\n",
      "|    policy_loss        | 0.0486   |\n",
      "|    value_loss         | 0.0142   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 824      |\n",
      "|    ep_rew_mean        | 1.97     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 20800    |\n",
      "|    time_elapsed       | 1999     |\n",
      "|    total_timesteps    | 1664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20799    |\n",
      "|    policy_loss        | -0.0493  |\n",
      "|    value_loss         | 0.0205   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1670000, episode_reward=2.40 +/- 1.50\n",
      "Episode length: 885.80 +/- 246.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 886      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1670000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20874    |\n",
      "|    policy_loss        | 0.0339   |\n",
      "|    value_loss         | 0.0132   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 845      |\n",
      "|    ep_rew_mean        | 2.07     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 20900    |\n",
      "|    time_elapsed       | 2009     |\n",
      "|    total_timesteps    | 1672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20899    |\n",
      "|    policy_loss        | 0.00709  |\n",
      "|    value_loss         | 0.00617  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1680000, episode_reward=2.00 +/- 1.67\n",
      "Episode length: 855.40 +/- 295.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 855      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20999    |\n",
      "|    policy_loss        | 0.0102   |\n",
      "|    value_loss         | 0.0149   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | 2.25     |\n",
      "| time/              |          |\n",
      "|    fps             | 831      |\n",
      "|    iterations      | 21000    |\n",
      "|    time_elapsed    | 2019     |\n",
      "|    total_timesteps | 1680000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 885      |\n",
      "|    ep_rew_mean        | 2.29     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 21100    |\n",
      "|    time_elapsed       | 2027     |\n",
      "|    total_timesteps    | 1688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21099    |\n",
      "|    policy_loss        | -0.0115  |\n",
      "|    value_loss         | 0.00124  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1690000, episode_reward=3.40 +/- 3.50\n",
      "Episode length: 974.40 +/- 470.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 974      |\n",
      "|    mean_reward        | 3.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1690000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.844    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21124    |\n",
      "|    policy_loss        | 0.024    |\n",
      "|    value_loss         | 0.0285   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 899      |\n",
      "|    ep_rew_mean        | 2.42     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 21200    |\n",
      "|    time_elapsed       | 2036     |\n",
      "|    total_timesteps    | 1696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21199    |\n",
      "|    policy_loss        | -0.0139  |\n",
      "|    value_loss         | 0.00533  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1700000, episode_reward=2.80 +/- 4.26\n",
      "Episode length: 915.60 +/- 578.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 916      |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21249    |\n",
      "|    policy_loss        | 0.059    |\n",
      "|    value_loss         | 0.0145   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 930      |\n",
      "|    ep_rew_mean        | 2.61     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 21300    |\n",
      "|    time_elapsed       | 2046     |\n",
      "|    total_timesteps    | 1704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21299    |\n",
      "|    policy_loss        | -0.0293  |\n",
      "|    value_loss         | 0.00767  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1710000, episode_reward=3.20 +/- 1.17\n",
      "Episode length: 1054.40 +/- 249.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+03 |\n",
      "|    mean_reward        | 3.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1710000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21374    |\n",
      "|    policy_loss        | -0.0158  |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 910      |\n",
      "|    ep_rew_mean        | 2.39     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 21400    |\n",
      "|    time_elapsed       | 2056     |\n",
      "|    total_timesteps    | 1712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21399    |\n",
      "|    policy_loss        | 0.00208  |\n",
      "|    value_loss         | 0.00402  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1720000, episode_reward=4.80 +/- 3.06\n",
      "Episode length: 1232.80 +/- 398.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.23e+03 |\n",
      "|    mean_reward        | 4.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.905    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21499    |\n",
      "|    policy_loss        | 0.0929   |\n",
      "|    value_loss         | 0.0295   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 952      |\n",
      "|    ep_rew_mean     | 2.73     |\n",
      "| time/              |          |\n",
      "|    fps             | 832      |\n",
      "|    iterations      | 21500    |\n",
      "|    time_elapsed    | 2066     |\n",
      "|    total_timesteps | 1720000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 891      |\n",
      "|    ep_rew_mean        | 2.38     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 21600    |\n",
      "|    time_elapsed       | 2074     |\n",
      "|    total_timesteps    | 1728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21599    |\n",
      "|    policy_loss        | 0.000332 |\n",
      "|    value_loss         | 0.00178  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1730000, episode_reward=2.40 +/- 2.42\n",
      "Episode length: 904.20 +/- 421.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 904      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1730000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21624    |\n",
      "|    policy_loss        | -0.0312  |\n",
      "|    value_loss         | 0.0176   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 873      |\n",
      "|    ep_rew_mean        | 2.28     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 21700    |\n",
      "|    time_elapsed       | 2084     |\n",
      "|    total_timesteps    | 1736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21699    |\n",
      "|    policy_loss        | 0.0398   |\n",
      "|    value_loss         | 0.00622  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1740000, episode_reward=4.20 +/- 4.58\n",
      "Episode length: 1040.20 +/- 451.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+03 |\n",
      "|    mean_reward        | 4.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21749    |\n",
      "|    policy_loss        | -0.0125  |\n",
      "|    value_loss         | 0.0131   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 859      |\n",
      "|    ep_rew_mean        | 2.19     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 21800    |\n",
      "|    time_elapsed       | 2094     |\n",
      "|    total_timesteps    | 1744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21799    |\n",
      "|    policy_loss        | -0.0428  |\n",
      "|    value_loss         | 0.0151   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1750000, episode_reward=2.80 +/- 3.19\n",
      "Episode length: 895.20 +/- 389.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 895      |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21874    |\n",
      "|    policy_loss        | -0.00862 |\n",
      "|    value_loss         | 0.00636  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 817      |\n",
      "|    ep_rew_mean        | 1.91     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 21900    |\n",
      "|    time_elapsed       | 2104     |\n",
      "|    total_timesteps    | 1752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21899    |\n",
      "|    policy_loss        | -0.00798 |\n",
      "|    value_loss         | 0.0106   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1760000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 624.20 +/- 123.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 624      |\n",
      "|    mean_reward        | 0.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21999    |\n",
      "|    policy_loss        | -0.0056  |\n",
      "|    value_loss         | 0.00498  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 832      |\n",
      "|    ep_rew_mean     | 2.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 832      |\n",
      "|    iterations      | 22000    |\n",
      "|    time_elapsed    | 2114     |\n",
      "|    total_timesteps | 1760000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 877      |\n",
      "|    ep_rew_mean        | 2.33     |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 22100    |\n",
      "|    time_elapsed       | 2122     |\n",
      "|    total_timesteps    | 1768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.758    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22099    |\n",
      "|    policy_loss        | 0.0863   |\n",
      "|    value_loss         | 0.0544   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1770000, episode_reward=1.60 +/- 2.24\n",
      "Episode length: 785.20 +/- 413.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 785      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1770000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22124    |\n",
      "|    policy_loss        | -0.0291  |\n",
      "|    value_loss         | 0.00751  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 894      |\n",
      "|    ep_rew_mean        | 2.6      |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 22200    |\n",
      "|    time_elapsed       | 2131     |\n",
      "|    total_timesteps    | 1776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22199    |\n",
      "|    policy_loss        | 0.0715   |\n",
      "|    value_loss         | 0.0222   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1780000, episode_reward=3.60 +/- 3.07\n",
      "Episode length: 1031.40 +/- 407.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+03 |\n",
      "|    mean_reward        | 3.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22249    |\n",
      "|    policy_loss        | -0.0298  |\n",
      "|    value_loss         | 0.00562  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 897      |\n",
      "|    ep_rew_mean        | 2.52     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 22300    |\n",
      "|    time_elapsed       | 2141     |\n",
      "|    total_timesteps    | 1784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22299    |\n",
      "|    policy_loss        | 0.0225   |\n",
      "|    value_loss         | 0.00798  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1790000, episode_reward=0.80 +/- 0.75\n",
      "Episode length: 644.20 +/- 115.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 644      |\n",
      "|    mean_reward        | 0.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1790000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.853    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22374    |\n",
      "|    policy_loss        | 0.0268   |\n",
      "|    value_loss         | 0.0195   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 886      |\n",
      "|    ep_rew_mean        | 2.42     |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 22400    |\n",
      "|    time_elapsed       | 2151     |\n",
      "|    total_timesteps    | 1792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22399    |\n",
      "|    policy_loss        | 0.0257   |\n",
      "|    value_loss         | 0.0063   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1800000, episode_reward=2.60 +/- 2.06\n",
      "Episode length: 971.20 +/- 372.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 971      |\n",
      "|    mean_reward        | 2.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22499    |\n",
      "|    policy_loss        | -0.011   |\n",
      "|    value_loss         | 0.00785  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 895      |\n",
      "|    ep_rew_mean     | 2.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 832      |\n",
      "|    iterations      | 22500    |\n",
      "|    time_elapsed    | 2161     |\n",
      "|    total_timesteps | 1800000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 898      |\n",
      "|    ep_rew_mean        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 22600    |\n",
      "|    time_elapsed       | 2169     |\n",
      "|    total_timesteps    | 1808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.747    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22599    |\n",
      "|    policy_loss        | -0.0763  |\n",
      "|    value_loss         | 0.0273   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1810000, episode_reward=2.40 +/- 1.62\n",
      "Episode length: 917.60 +/- 285.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 918      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1810000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22624    |\n",
      "|    policy_loss        | 0.00849  |\n",
      "|    value_loss         | 0.00391  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 909      |\n",
      "|    ep_rew_mean        | 2.48     |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 22700    |\n",
      "|    time_elapsed       | 2178     |\n",
      "|    total_timesteps    | 1816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22699    |\n",
      "|    policy_loss        | -0.0188  |\n",
      "|    value_loss         | 0.00796  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1820000, episode_reward=1.60 +/- 1.36\n",
      "Episode length: 778.20 +/- 237.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 778      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22749    |\n",
      "|    policy_loss        | -0.105   |\n",
      "|    value_loss         | 0.024    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 903      |\n",
      "|    ep_rew_mean        | 2.49     |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 22800    |\n",
      "|    time_elapsed       | 2188     |\n",
      "|    total_timesteps    | 1824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22799    |\n",
      "|    policy_loss        | 0.0067   |\n",
      "|    value_loss         | 0.013    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1830000, episode_reward=1.00 +/- 2.00\n",
      "Episode length: 680.00 +/- 320.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 680      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1830000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22874    |\n",
      "|    policy_loss        | 0.0379   |\n",
      "|    value_loss         | 0.0117   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 894      |\n",
      "|    ep_rew_mean        | 2.43     |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 22900    |\n",
      "|    time_elapsed       | 2198     |\n",
      "|    total_timesteps    | 1832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22899    |\n",
      "|    policy_loss        | -0.0408  |\n",
      "|    value_loss         | 0.00834  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1840000, episode_reward=2.00 +/- 1.79\n",
      "Episode length: 865.80 +/- 319.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 866      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22999    |\n",
      "|    policy_loss        | 0.0149   |\n",
      "|    value_loss         | 0.0169   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 2.11     |\n",
      "| time/              |          |\n",
      "|    fps             | 833      |\n",
      "|    iterations      | 23000    |\n",
      "|    time_elapsed    | 2208     |\n",
      "|    total_timesteps | 1840000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 871      |\n",
      "|    ep_rew_mean        | 2.36     |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 23100    |\n",
      "|    time_elapsed       | 2216     |\n",
      "|    total_timesteps    | 1848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23099    |\n",
      "|    policy_loss        | 0.00666  |\n",
      "|    value_loss         | 0.0015   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1850000, episode_reward=2.80 +/- 1.72\n",
      "Episode length: 988.00 +/- 315.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 988      |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1850000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23124    |\n",
      "|    policy_loss        | 0.063    |\n",
      "|    value_loss         | 0.0122   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 892      |\n",
      "|    ep_rew_mean        | 2.59     |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 23200    |\n",
      "|    time_elapsed       | 2227     |\n",
      "|    total_timesteps    | 1856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23199    |\n",
      "|    policy_loss        | -0.044   |\n",
      "|    value_loss         | 0.0133   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1860000, episode_reward=2.40 +/- 1.85\n",
      "Episode length: 934.40 +/- 295.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 934      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1860000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23249    |\n",
      "|    policy_loss        | 0.00699  |\n",
      "|    value_loss         | 0.00883  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 866      |\n",
      "|    ep_rew_mean        | 2.39     |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 23300    |\n",
      "|    time_elapsed       | 2237     |\n",
      "|    total_timesteps    | 1864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.839    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23299    |\n",
      "|    policy_loss        | -0.0813  |\n",
      "|    value_loss         | 0.0355   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1870000, episode_reward=4.60 +/- 3.38\n",
      "Episode length: 1204.80 +/- 448.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.2e+03  |\n",
      "|    mean_reward        | 4.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1870000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23374    |\n",
      "|    policy_loss        | 0.0371   |\n",
      "|    value_loss         | 0.0138   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 811      |\n",
      "|    ep_rew_mean        | 2.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 23400    |\n",
      "|    time_elapsed       | 2248     |\n",
      "|    total_timesteps    | 1872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23399    |\n",
      "|    policy_loss        | 0.0035   |\n",
      "|    value_loss         | 0.00597  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1880000, episode_reward=4.00 +/- 3.63\n",
      "Episode length: 1093.80 +/- 512.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.09e+03 |\n",
      "|    mean_reward        | 4        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.836    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23499    |\n",
      "|    policy_loss        | 0.0423   |\n",
      "|    value_loss         | 0.0325   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 2.39     |\n",
      "| time/              |          |\n",
      "|    fps             | 832      |\n",
      "|    iterations      | 23500    |\n",
      "|    time_elapsed    | 2258     |\n",
      "|    total_timesteps | 1880000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 938      |\n",
      "|    ep_rew_mean        | 2.92     |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 23600    |\n",
      "|    time_elapsed       | 2266     |\n",
      "|    total_timesteps    | 1888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23599    |\n",
      "|    policy_loss        | -0.0311  |\n",
      "|    value_loss         | 0.00353  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1890000, episode_reward=2.20 +/- 2.04\n",
      "Episode length: 893.00 +/- 360.29\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 893       |\n",
      "|    mean_reward        | 2.2       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1890000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.35     |\n",
      "|    explained_variance | 0.966     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23624     |\n",
      "|    policy_loss        | -0.000857 |\n",
      "|    value_loss         | 0.00959   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 950      |\n",
      "|    ep_rew_mean        | 2.92     |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 23700    |\n",
      "|    time_elapsed       | 2276     |\n",
      "|    total_timesteps    | 1896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23699    |\n",
      "|    policy_loss        | -0.00163 |\n",
      "|    value_loss         | 0.00733  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1900000, episode_reward=3.20 +/- 1.94\n",
      "Episode length: 1023.00 +/- 341.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+03 |\n",
      "|    mean_reward        | 3.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1900000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23749    |\n",
      "|    policy_loss        | -0.0505  |\n",
      "|    value_loss         | 0.0143   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 964      |\n",
      "|    ep_rew_mean        | 2.87     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 23800    |\n",
      "|    time_elapsed       | 2286     |\n",
      "|    total_timesteps    | 1904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23799    |\n",
      "|    policy_loss        | -0.0413  |\n",
      "|    value_loss         | 0.00953  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1910000, episode_reward=3.00 +/- 2.10\n",
      "Episode length: 1026.80 +/- 425.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+03 |\n",
      "|    mean_reward        | 3        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1910000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23874    |\n",
      "|    policy_loss        | -0.0345  |\n",
      "|    value_loss         | 0.0192   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 943      |\n",
      "|    ep_rew_mean        | 2.73     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 23900    |\n",
      "|    time_elapsed       | 2296     |\n",
      "|    total_timesteps    | 1912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.896    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23899    |\n",
      "|    policy_loss        | -0.00602 |\n",
      "|    value_loss         | 0.0176   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1920000, episode_reward=1.60 +/- 0.80\n",
      "Episode length: 746.60 +/- 160.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 747      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23999    |\n",
      "|    policy_loss        | 0.0016   |\n",
      "|    value_loss         | 0.00902  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 921      |\n",
      "|    ep_rew_mean     | 2.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 832      |\n",
      "|    iterations      | 24000    |\n",
      "|    time_elapsed    | 2305     |\n",
      "|    total_timesteps | 1920000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 921      |\n",
      "|    ep_rew_mean        | 2.66     |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 24100    |\n",
      "|    time_elapsed       | 2314     |\n",
      "|    total_timesteps    | 1928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.773    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24099    |\n",
      "|    policy_loss        | -0.00871 |\n",
      "|    value_loss         | 0.0385   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1930000, episode_reward=2.80 +/- 2.14\n",
      "Episode length: 1040.00 +/- 411.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+03 |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1930000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.639    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24124    |\n",
      "|    policy_loss        | 0.0405   |\n",
      "|    value_loss         | 0.0641   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 924      |\n",
      "|    ep_rew_mean        | 2.72     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 24200    |\n",
      "|    time_elapsed       | 2324     |\n",
      "|    total_timesteps    | 1936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24199    |\n",
      "|    policy_loss        | 0.00862  |\n",
      "|    value_loss         | 0.00623  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1940000, episode_reward=1.80 +/- 1.94\n",
      "Episode length: 784.40 +/- 303.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 784      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24249    |\n",
      "|    policy_loss        | -0.0187  |\n",
      "|    value_loss         | 0.0136   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 952      |\n",
      "|    ep_rew_mean        | 2.9      |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 24300    |\n",
      "|    time_elapsed       | 2333     |\n",
      "|    total_timesteps    | 1944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24299    |\n",
      "|    policy_loss        | -0.0359  |\n",
      "|    value_loss         | 0.00771  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1950000, episode_reward=5.00 +/- 3.85\n",
      "Episode length: 1195.00 +/- 486.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.2e+03  |\n",
      "|    mean_reward        | 5        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1950000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.814    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24374    |\n",
      "|    policy_loss        | 0.0464   |\n",
      "|    value_loss         | 0.0238   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 988      |\n",
      "|    ep_rew_mean        | 3.15     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 24400    |\n",
      "|    time_elapsed       | 2344     |\n",
      "|    total_timesteps    | 1952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.869    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24399    |\n",
      "|    policy_loss        | 0.0381   |\n",
      "|    value_loss         | 0.0179   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1960000, episode_reward=5.20 +/- 3.19\n",
      "Episode length: 1317.20 +/- 383.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.32e+03 |\n",
      "|    mean_reward        | 5.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24499    |\n",
      "|    policy_loss        | -0.0353  |\n",
      "|    value_loss         | 0.0146   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 929      |\n",
      "|    ep_rew_mean     | 2.69     |\n",
      "| time/              |          |\n",
      "|    fps             | 832      |\n",
      "|    iterations      | 24500    |\n",
      "|    time_elapsed    | 2355     |\n",
      "|    total_timesteps | 1960000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 964      |\n",
      "|    ep_rew_mean        | 2.98     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 24600    |\n",
      "|    time_elapsed       | 2363     |\n",
      "|    total_timesteps    | 1968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24599    |\n",
      "|    policy_loss        | -0.0213  |\n",
      "|    value_loss         | 0.0204   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1970000, episode_reward=1.40 +/- 1.02\n",
      "Episode length: 735.60 +/- 169.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 736      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1970000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24624    |\n",
      "|    policy_loss        | -0.0768  |\n",
      "|    value_loss         | 0.018    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 932      |\n",
      "|    ep_rew_mean        | 2.72     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 24700    |\n",
      "|    time_elapsed       | 2372     |\n",
      "|    total_timesteps    | 1976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24699    |\n",
      "|    policy_loss        | -0.0335  |\n",
      "|    value_loss         | 0.00622  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1980000, episode_reward=3.00 +/- 2.28\n",
      "Episode length: 998.80 +/- 409.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 999      |\n",
      "|    mean_reward        | 3        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24749    |\n",
      "|    policy_loss        | 0.0289   |\n",
      "|    value_loss         | 0.0119   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 903      |\n",
      "|    ep_rew_mean        | 2.56     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 24800    |\n",
      "|    time_elapsed       | 2383     |\n",
      "|    total_timesteps    | 1984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24799    |\n",
      "|    policy_loss        | -0.0248  |\n",
      "|    value_loss         | 0.00344  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1990000, episode_reward=0.60 +/- 0.49\n",
      "Episode length: 587.00 +/- 62.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 587      |\n",
      "|    mean_reward        | 0.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1990000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24874    |\n",
      "|    policy_loss        | 0.0228   |\n",
      "|    value_loss         | 0.023    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 937      |\n",
      "|    ep_rew_mean        | 2.74     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 24900    |\n",
      "|    time_elapsed       | 2392     |\n",
      "|    total_timesteps    | 1992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24899    |\n",
      "|    policy_loss        | 0.0192   |\n",
      "|    value_loss         | 0.00426  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2000000, episode_reward=6.40 +/- 4.63\n",
      "Episode length: 1356.60 +/- 575.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.36e+03 |\n",
      "|    mean_reward        | 6.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24999    |\n",
      "|    policy_loss        | 6.54e-05 |\n",
      "|    value_loss         | 0.00209  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 979      |\n",
      "|    ep_rew_mean     | 3.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 832      |\n",
      "|    iterations      | 25000    |\n",
      "|    time_elapsed    | 2403     |\n",
      "|    total_timesteps | 2000000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 980      |\n",
      "|    ep_rew_mean        | 3.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 25100    |\n",
      "|    time_elapsed       | 2411     |\n",
      "|    total_timesteps    | 2008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25099    |\n",
      "|    policy_loss        | 0.00279  |\n",
      "|    value_loss         | 0.013    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2010000, episode_reward=4.40 +/- 3.61\n",
      "Episode length: 1193.20 +/- 511.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.19e+03 |\n",
      "|    mean_reward        | 4.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2010000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25124    |\n",
      "|    policy_loss        | -0.0192  |\n",
      "|    value_loss         | 0.013    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 965      |\n",
      "|    ep_rew_mean        | 2.91     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 25200    |\n",
      "|    time_elapsed       | 2421     |\n",
      "|    total_timesteps    | 2016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.896    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25199    |\n",
      "|    policy_loss        | 0.000127 |\n",
      "|    value_loss         | 0.0192   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2020000, episode_reward=1.60 +/- 1.36\n",
      "Episode length: 783.40 +/- 192.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 783      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25249    |\n",
      "|    policy_loss        | -0.00604 |\n",
      "|    value_loss         | 0.0056   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 932      |\n",
      "|    ep_rew_mean        | 2.68     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 25300    |\n",
      "|    time_elapsed       | 2430     |\n",
      "|    total_timesteps    | 2024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25299    |\n",
      "|    policy_loss        | -0.00614 |\n",
      "|    value_loss         | 0.00358  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2030000, episode_reward=2.20 +/- 3.43\n",
      "Episode length: 822.20 +/- 416.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 822      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2030000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25374    |\n",
      "|    policy_loss        | 0.0164   |\n",
      "|    value_loss         | 0.00554  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 965      |\n",
      "|    ep_rew_mean        | 2.94     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 25400    |\n",
      "|    time_elapsed       | 2440     |\n",
      "|    total_timesteps    | 2032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25399    |\n",
      "|    policy_loss        | 0.0242   |\n",
      "|    value_loss         | 0.0208   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2040000, episode_reward=3.80 +/- 3.25\n",
      "Episode length: 1063.60 +/- 434.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+03 |\n",
      "|    mean_reward        | 3.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25499    |\n",
      "|    policy_loss        | 0.0056   |\n",
      "|    value_loss         | 0.00888  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 973      |\n",
      "|    ep_rew_mean     | 3.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 832      |\n",
      "|    iterations      | 25500    |\n",
      "|    time_elapsed    | 2450     |\n",
      "|    total_timesteps | 2040000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 952      |\n",
      "|    ep_rew_mean        | 2.93     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 25600    |\n",
      "|    time_elapsed       | 2458     |\n",
      "|    total_timesteps    | 2048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25599    |\n",
      "|    policy_loss        | -0.0117  |\n",
      "|    value_loss         | 0.0154   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2050000, episode_reward=1.80 +/- 1.47\n",
      "Episode length: 765.20 +/- 226.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 765      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.76     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25624    |\n",
      "|    policy_loss        | 0.00168  |\n",
      "|    value_loss         | 0.0182   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 903      |\n",
      "|    ep_rew_mean        | 2.57     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 25700    |\n",
      "|    time_elapsed       | 2468     |\n",
      "|    total_timesteps    | 2056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25699    |\n",
      "|    policy_loss        | -0.0242  |\n",
      "|    value_loss         | 0.00958  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2060000, episode_reward=2.20 +/- 1.72\n",
      "Episode length: 886.60 +/- 303.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 887      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25749    |\n",
      "|    policy_loss        | -0.061   |\n",
      "|    value_loss         | 0.0083   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 930      |\n",
      "|    ep_rew_mean        | 2.7      |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 25800    |\n",
      "|    time_elapsed       | 2478     |\n",
      "|    total_timesteps    | 2064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25799    |\n",
      "|    policy_loss        | -0.0641  |\n",
      "|    value_loss         | 0.0228   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2070000, episode_reward=5.00 +/- 3.35\n",
      "Episode length: 1176.80 +/- 391.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.18e+03 |\n",
      "|    mean_reward        | 5        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2070000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25874    |\n",
      "|    policy_loss        | 0.00155  |\n",
      "|    value_loss         | 0.0127   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 974      |\n",
      "|    ep_rew_mean        | 2.97     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 25900    |\n",
      "|    time_elapsed       | 2488     |\n",
      "|    total_timesteps    | 2072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.903    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25899    |\n",
      "|    policy_loss        | 0.0581   |\n",
      "|    value_loss         | 0.0284   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2080000, episode_reward=1.20 +/- 1.17\n",
      "Episode length: 661.40 +/- 130.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 661      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.808    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25999    |\n",
      "|    policy_loss        | 0.0153   |\n",
      "|    value_loss         | 0.0205   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.03e+03 |\n",
      "|    ep_rew_mean     | 3.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 832      |\n",
      "|    iterations      | 26000    |\n",
      "|    time_elapsed    | 2497     |\n",
      "|    total_timesteps | 2080000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | 3.29     |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 26100    |\n",
      "|    time_elapsed       | 2505     |\n",
      "|    total_timesteps    | 2088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.789    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26099    |\n",
      "|    policy_loss        | 0.11     |\n",
      "|    value_loss         | 0.0784   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2090000, episode_reward=3.60 +/- 3.61\n",
      "Episode length: 1055.80 +/- 447.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+03 |\n",
      "|    mean_reward        | 3.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2090000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26124    |\n",
      "|    policy_loss        | 0.0924   |\n",
      "|    value_loss         | 0.0373   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 920      |\n",
      "|    ep_rew_mean        | 2.77     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 26200    |\n",
      "|    time_elapsed       | 2516     |\n",
      "|    total_timesteps    | 2096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26199    |\n",
      "|    policy_loss        | 0.0094   |\n",
      "|    value_loss         | 0.0156   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2100000, episode_reward=3.00 +/- 3.29\n",
      "Episode length: 936.40 +/- 422.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 936      |\n",
      "|    mean_reward        | 3        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26249    |\n",
      "|    policy_loss        | 0.0281   |\n",
      "|    value_loss         | 0.0245   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 886      |\n",
      "|    ep_rew_mean        | 2.49     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 26300    |\n",
      "|    time_elapsed       | 2526     |\n",
      "|    total_timesteps    | 2104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26299    |\n",
      "|    policy_loss        | -0.0335  |\n",
      "|    value_loss         | 0.00397  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2110000, episode_reward=2.00 +/- 0.89\n",
      "Episode length: 770.00 +/- 129.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 770      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2110000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26374    |\n",
      "|    policy_loss        | -0.0247  |\n",
      "|    value_loss         | 0.0203   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 977      |\n",
      "|    ep_rew_mean        | 3.15     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 26400    |\n",
      "|    time_elapsed       | 2535     |\n",
      "|    total_timesteps    | 2112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.891    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26399    |\n",
      "|    policy_loss        | 0.0453   |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2120000, episode_reward=2.80 +/- 3.66\n",
      "Episode length: 902.80 +/- 476.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 903      |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26499    |\n",
      "|    policy_loss        | 0.0069   |\n",
      "|    value_loss         | 0.00718  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.03e+03 |\n",
      "|    ep_rew_mean     | 3.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 832      |\n",
      "|    iterations      | 26500    |\n",
      "|    time_elapsed    | 2545     |\n",
      "|    total_timesteps | 2120000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 998      |\n",
      "|    ep_rew_mean        | 3.31     |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 26600    |\n",
      "|    time_elapsed       | 2553     |\n",
      "|    total_timesteps    | 2128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26599    |\n",
      "|    policy_loss        | -0.0155  |\n",
      "|    value_loss         | 0.0147   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2130000, episode_reward=1.20 +/- 1.47\n",
      "Episode length: 693.80 +/- 236.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 694      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2130000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.886    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26624    |\n",
      "|    policy_loss        | 0.0216   |\n",
      "|    value_loss         | 0.0336   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 926      |\n",
      "|    ep_rew_mean        | 2.85     |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 26700    |\n",
      "|    time_elapsed       | 2563     |\n",
      "|    total_timesteps    | 2136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26699    |\n",
      "|    policy_loss        | 0.0158   |\n",
      "|    value_loss         | 0.0201   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2140000, episode_reward=2.80 +/- 3.66\n",
      "Episode length: 931.00 +/- 493.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 931      |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26749    |\n",
      "|    policy_loss        | -0.00913 |\n",
      "|    value_loss         | 0.0191   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 912      |\n",
      "|    ep_rew_mean        | 2.68     |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 26800    |\n",
      "|    time_elapsed       | 2573     |\n",
      "|    total_timesteps    | 2144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26799    |\n",
      "|    policy_loss        | 0.00233  |\n",
      "|    value_loss         | 0.0065   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2150000, episode_reward=1.40 +/- 1.85\n",
      "Episode length: 770.00 +/- 364.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 770      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.671    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26874    |\n",
      "|    policy_loss        | -0.0157  |\n",
      "|    value_loss         | 0.0178   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 933      |\n",
      "|    ep_rew_mean        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 26900    |\n",
      "|    time_elapsed       | 2582     |\n",
      "|    total_timesteps    | 2152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26899    |\n",
      "|    policy_loss        | -0.0287  |\n",
      "|    value_loss         | 0.00255  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2160000, episode_reward=5.00 +/- 3.29\n",
      "Episode length: 1171.40 +/- 385.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.17e+03 |\n",
      "|    mean_reward        | 5        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.84     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26999    |\n",
      "|    policy_loss        | 0.118    |\n",
      "|    value_loss         | 0.0517   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 956      |\n",
      "|    ep_rew_mean     | 2.96     |\n",
      "| time/              |          |\n",
      "|    fps             | 832      |\n",
      "|    iterations      | 27000    |\n",
      "|    time_elapsed    | 2593     |\n",
      "|    total_timesteps | 2160000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 981      |\n",
      "|    ep_rew_mean        | 3.1      |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 27100    |\n",
      "|    time_elapsed       | 2601     |\n",
      "|    total_timesteps    | 2168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.905    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27099    |\n",
      "|    policy_loss        | -0.0212  |\n",
      "|    value_loss         | 0.00909  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2170000, episode_reward=2.20 +/- 2.40\n",
      "Episode length: 906.00 +/- 436.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 906      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2170000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27124    |\n",
      "|    policy_loss        | -0.0242  |\n",
      "|    value_loss         | 0.0132   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 968      |\n",
      "|    ep_rew_mean        | 2.99     |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 27200    |\n",
      "|    time_elapsed       | 2611     |\n",
      "|    total_timesteps    | 2176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27199    |\n",
      "|    policy_loss        | -0.0265  |\n",
      "|    value_loss         | 0.0183   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2180000, episode_reward=2.40 +/- 1.74\n",
      "Episode length: 918.00 +/- 304.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 918      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27249    |\n",
      "|    policy_loss        | -0.00125 |\n",
      "|    value_loss         | 0.00418  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 976      |\n",
      "|    ep_rew_mean        | 3.08     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 27300    |\n",
      "|    time_elapsed       | 2621     |\n",
      "|    total_timesteps    | 2184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.444    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27299    |\n",
      "|    policy_loss        | -0.0151  |\n",
      "|    value_loss         | 0.0226   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2190000, episode_reward=2.40 +/- 2.24\n",
      "Episode length: 916.00 +/- 384.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 916      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2190000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27374    |\n",
      "|    policy_loss        | -0.0182  |\n",
      "|    value_loss         | 0.00594  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 943      |\n",
      "|    ep_rew_mean        | 2.94     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 27400    |\n",
      "|    time_elapsed       | 2632     |\n",
      "|    total_timesteps    | 2192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27399    |\n",
      "|    policy_loss        | 0.0115   |\n",
      "|    value_loss         | 0.0016   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2200000, episode_reward=2.40 +/- 2.24\n",
      "Episode length: 921.20 +/- 397.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 921      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.83     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27499    |\n",
      "|    policy_loss        | 0.0207   |\n",
      "|    value_loss         | 0.0589   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 968      |\n",
      "|    ep_rew_mean     | 3.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 832      |\n",
      "|    iterations      | 27500    |\n",
      "|    time_elapsed    | 2642     |\n",
      "|    total_timesteps | 2200000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 998      |\n",
      "|    ep_rew_mean        | 3.23     |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 27600    |\n",
      "|    time_elapsed       | 2650     |\n",
      "|    total_timesteps    | 2208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27599    |\n",
      "|    policy_loss        | -0.0481  |\n",
      "|    value_loss         | 0.0114   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2210000, episode_reward=2.40 +/- 2.06\n",
      "Episode length: 915.60 +/- 408.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 916      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2210000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.776    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27624    |\n",
      "|    policy_loss        | 0.0565   |\n",
      "|    value_loss         | 0.0561   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06e+03 |\n",
      "|    ep_rew_mean        | 3.75     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 27700    |\n",
      "|    time_elapsed       | 2660     |\n",
      "|    total_timesteps    | 2216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27699    |\n",
      "|    policy_loss        | -0.0344  |\n",
      "|    value_loss         | 0.00649  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2220000, episode_reward=5.00 +/- 3.85\n",
      "Episode length: 1296.40 +/- 547.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.3e+03  |\n",
      "|    mean_reward        | 5        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27749    |\n",
      "|    policy_loss        | -0.046   |\n",
      "|    value_loss         | 0.00563  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06e+03 |\n",
      "|    ep_rew_mean        | 3.7      |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 27800    |\n",
      "|    time_elapsed       | 2671     |\n",
      "|    total_timesteps    | 2224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27799    |\n",
      "|    policy_loss        | 0.0215   |\n",
      "|    value_loss         | 0.00746  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2230000, episode_reward=0.40 +/- 0.49\n",
      "Episode length: 557.80 +/- 60.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 558      |\n",
      "|    mean_reward        | 0.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2230000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27874    |\n",
      "|    policy_loss        | 0.00659  |\n",
      "|    value_loss         | 0.00911  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 935      |\n",
      "|    ep_rew_mean        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 27900    |\n",
      "|    time_elapsed       | 2681     |\n",
      "|    total_timesteps    | 2232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27899    |\n",
      "|    policy_loss        | 0.0369   |\n",
      "|    value_loss         | 0.00953  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2240000, episode_reward=3.20 +/- 1.94\n",
      "Episode length: 1046.60 +/- 309.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+03 |\n",
      "|    mean_reward        | 3.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27999    |\n",
      "|    policy_loss        | -0.019   |\n",
      "|    value_loss         | 0.0204   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 962      |\n",
      "|    ep_rew_mean     | 3.08     |\n",
      "| time/              |          |\n",
      "|    fps             | 832      |\n",
      "|    iterations      | 28000    |\n",
      "|    time_elapsed    | 2691     |\n",
      "|    total_timesteps | 2240000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 952      |\n",
      "|    ep_rew_mean        | 3.05     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 28100    |\n",
      "|    time_elapsed       | 2699     |\n",
      "|    total_timesteps    | 2248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28099    |\n",
      "|    policy_loss        | 0.0563   |\n",
      "|    value_loss         | 0.0263   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2250000, episode_reward=4.20 +/- 4.49\n",
      "Episode length: 1024.60 +/- 461.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+03 |\n",
      "|    mean_reward        | 4.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28124    |\n",
      "|    policy_loss        | -0.0967  |\n",
      "|    value_loss         | 0.0204   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02e+03 |\n",
      "|    ep_rew_mean        | 3.44     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 28200    |\n",
      "|    time_elapsed       | 2709     |\n",
      "|    total_timesteps    | 2256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28199    |\n",
      "|    policy_loss        | 0.0108   |\n",
      "|    value_loss         | 0.00686  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2260000, episode_reward=4.20 +/- 3.54\n",
      "Episode length: 1183.00 +/- 591.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.18e+03 |\n",
      "|    mean_reward        | 4.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28249    |\n",
      "|    policy_loss        | -0.0412  |\n",
      "|    value_loss         | 0.0132   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 953      |\n",
      "|    ep_rew_mean        | 2.95     |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 28300    |\n",
      "|    time_elapsed       | 2720     |\n",
      "|    total_timesteps    | 2264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28299    |\n",
      "|    policy_loss        | 0.015    |\n",
      "|    value_loss         | 0.00536  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2270000, episode_reward=4.00 +/- 1.55\n",
      "Episode length: 1203.60 +/- 307.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.2e+03  |\n",
      "|    mean_reward        | 4        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2270000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28374    |\n",
      "|    policy_loss        | -0.0553  |\n",
      "|    value_loss         | 0.0112   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 988      |\n",
      "|    ep_rew_mean        | 3.11     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 28400    |\n",
      "|    time_elapsed       | 2730     |\n",
      "|    total_timesteps    | 2272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28399    |\n",
      "|    policy_loss        | 0.0258   |\n",
      "|    value_loss         | 0.0138   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2280000, episode_reward=5.00 +/- 2.97\n",
      "Episode length: 1222.40 +/- 385.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.22e+03 |\n",
      "|    mean_reward        | 5        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28499    |\n",
      "|    policy_loss        | -0.0727  |\n",
      "|    value_loss         | 0.013    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.01e+03 |\n",
      "|    ep_rew_mean     | 3.21     |\n",
      "| time/              |          |\n",
      "|    fps             | 831      |\n",
      "|    iterations      | 28500    |\n",
      "|    time_elapsed    | 2741     |\n",
      "|    total_timesteps | 2280000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+03 |\n",
      "|    ep_rew_mean        | 3.9      |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 28600    |\n",
      "|    time_elapsed       | 2749     |\n",
      "|    total_timesteps    | 2288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.885    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28599    |\n",
      "|    policy_loss        | 0.0226   |\n",
      "|    value_loss         | 0.0248   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2290000, episode_reward=3.80 +/- 1.94\n",
      "Episode length: 1143.60 +/- 409.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+03 |\n",
      "|    mean_reward        | 3.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2290000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.787    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28624    |\n",
      "|    policy_loss        | -0.0521  |\n",
      "|    value_loss         | 0.0302   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+03  |\n",
      "|    ep_rew_mean        | 4        |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 28700    |\n",
      "|    time_elapsed       | 2760     |\n",
      "|    total_timesteps    | 2296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28699    |\n",
      "|    policy_loss        | 0.0154   |\n",
      "|    value_loss         | 0.0178   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2300000, episode_reward=1.80 +/- 1.33\n",
      "Episode length: 784.80 +/- 202.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 785      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28749    |\n",
      "|    policy_loss        | 0.0448   |\n",
      "|    value_loss         | 0.0167   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+03 |\n",
      "|    ep_rew_mean        | 3.94     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 28800    |\n",
      "|    time_elapsed       | 2770     |\n",
      "|    total_timesteps    | 2304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.843    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28799    |\n",
      "|    policy_loss        | -0.0416  |\n",
      "|    value_loss         | 0.0174   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2310000, episode_reward=4.80 +/- 2.48\n",
      "Episode length: 1320.60 +/- 423.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.32e+03 |\n",
      "|    mean_reward        | 4.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2310000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.84     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28874    |\n",
      "|    policy_loss        | -0.0346  |\n",
      "|    value_loss         | 0.0174   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 984      |\n",
      "|    ep_rew_mean        | 3.18     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 28900    |\n",
      "|    time_elapsed       | 2781     |\n",
      "|    total_timesteps    | 2312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28899    |\n",
      "|    policy_loss        | 0.0143   |\n",
      "|    value_loss         | 0.00728  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2320000, episode_reward=3.60 +/- 3.38\n",
      "Episode length: 993.60 +/- 394.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 994      |\n",
      "|    mean_reward        | 3.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28999    |\n",
      "|    policy_loss        | 0.0441   |\n",
      "|    value_loss         | 0.0199   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 925      |\n",
      "|    ep_rew_mean     | 2.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 831      |\n",
      "|    iterations      | 29000    |\n",
      "|    time_elapsed    | 2791     |\n",
      "|    total_timesteps | 2320000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 919      |\n",
      "|    ep_rew_mean        | 2.58     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 29100    |\n",
      "|    time_elapsed       | 2799     |\n",
      "|    total_timesteps    | 2328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29099    |\n",
      "|    policy_loss        | -0.0471  |\n",
      "|    value_loss         | 0.00927  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2330000, episode_reward=1.40 +/- 1.02\n",
      "Episode length: 700.60 +/- 139.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 701      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2330000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29124    |\n",
      "|    policy_loss        | -0.0193  |\n",
      "|    value_loss         | 0.0118   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 910      |\n",
      "|    ep_rew_mean        | 2.56     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 29200    |\n",
      "|    time_elapsed       | 2809     |\n",
      "|    total_timesteps    | 2336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29199    |\n",
      "|    policy_loss        | 0.0263   |\n",
      "|    value_loss         | 0.0188   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2340000, episode_reward=3.40 +/- 3.93\n",
      "Episode length: 993.20 +/- 535.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 993      |\n",
      "|    mean_reward        | 3.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.835    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29249    |\n",
      "|    policy_loss        | 0.00918  |\n",
      "|    value_loss         | 0.0185   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 888      |\n",
      "|    ep_rew_mean        | 2.52     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 29300    |\n",
      "|    time_elapsed       | 2819     |\n",
      "|    total_timesteps    | 2344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29299    |\n",
      "|    policy_loss        | -0.0131  |\n",
      "|    value_loss         | 0.00846  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2350000, episode_reward=4.40 +/- 2.80\n",
      "Episode length: 1181.40 +/- 379.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.18e+03 |\n",
      "|    mean_reward        | 4.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29374    |\n",
      "|    policy_loss        | 0.0384   |\n",
      "|    value_loss         | 0.017    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 937      |\n",
      "|    ep_rew_mean        | 2.84     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 29400    |\n",
      "|    time_elapsed       | 2830     |\n",
      "|    total_timesteps    | 2352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.815    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29399    |\n",
      "|    policy_loss        | 0.0772   |\n",
      "|    value_loss         | 0.0297   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2360000, episode_reward=4.20 +/- 2.48\n",
      "Episode length: 1063.20 +/- 304.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+03 |\n",
      "|    mean_reward        | 4.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29499    |\n",
      "|    policy_loss        | 0.07     |\n",
      "|    value_loss         | 0.0118   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 981      |\n",
      "|    ep_rew_mean     | 3.09     |\n",
      "| time/              |          |\n",
      "|    fps             | 830      |\n",
      "|    iterations      | 29500    |\n",
      "|    time_elapsed    | 2841     |\n",
      "|    total_timesteps | 2360000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+03 |\n",
      "|    ep_rew_mean        | 3.45     |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 29600    |\n",
      "|    time_elapsed       | 2849     |\n",
      "|    total_timesteps    | 2368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29599    |\n",
      "|    policy_loss        | -0.0818  |\n",
      "|    value_loss         | 0.0149   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2370000, episode_reward=4.40 +/- 3.72\n",
      "Episode length: 1202.20 +/- 493.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.2e+03  |\n",
      "|    mean_reward        | 4.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2370000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.891    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29624    |\n",
      "|    policy_loss        | 0.0482   |\n",
      "|    value_loss         | 0.0306   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02e+03 |\n",
      "|    ep_rew_mean        | 3.4      |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 29700    |\n",
      "|    time_elapsed       | 2860     |\n",
      "|    total_timesteps    | 2376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29699    |\n",
      "|    policy_loss        | 0.0135   |\n",
      "|    value_loss         | 0.0141   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2380000, episode_reward=2.80 +/- 1.47\n",
      "Episode length: 989.20 +/- 260.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 989      |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29749    |\n",
      "|    policy_loss        | 0.00464  |\n",
      "|    value_loss         | 0.00442  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+03 |\n",
      "|    ep_rew_mean        | 3.54     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 29800    |\n",
      "|    time_elapsed       | 2870     |\n",
      "|    total_timesteps    | 2384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29799    |\n",
      "|    policy_loss        | 0.0358   |\n",
      "|    value_loss         | 0.0189   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2390000, episode_reward=3.40 +/- 2.87\n",
      "Episode length: 1088.80 +/- 482.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.09e+03 |\n",
      "|    mean_reward        | 3.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2390000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.84     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29874    |\n",
      "|    policy_loss        | 0.0448   |\n",
      "|    value_loss         | 0.0274   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+03 |\n",
      "|    ep_rew_mean        | 3.42     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 29900    |\n",
      "|    time_elapsed       | 2880     |\n",
      "|    total_timesteps    | 2392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29899    |\n",
      "|    policy_loss        | -0.0183  |\n",
      "|    value_loss         | 0.0105   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2400000, episode_reward=3.00 +/- 2.68\n",
      "Episode length: 1006.80 +/- 505.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+03 |\n",
      "|    mean_reward        | 3        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29999    |\n",
      "|    policy_loss        | 0.00604  |\n",
      "|    value_loss         | 0.00949  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 993      |\n",
      "|    ep_rew_mean     | 3.21     |\n",
      "| time/              |          |\n",
      "|    fps             | 830      |\n",
      "|    iterations      | 30000    |\n",
      "|    time_elapsed    | 2890     |\n",
      "|    total_timesteps | 2400000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 958      |\n",
      "|    ep_rew_mean        | 2.95     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 30100    |\n",
      "|    time_elapsed       | 2898     |\n",
      "|    total_timesteps    | 2408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30099    |\n",
      "|    policy_loss        | -0.0532  |\n",
      "|    value_loss         | 0.0109   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2410000, episode_reward=1.00 +/- 1.10\n",
      "Episode length: 661.80 +/- 177.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 662      |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2410000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30124    |\n",
      "|    policy_loss        | -0.0682  |\n",
      "|    value_loss         | 0.0273   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 956      |\n",
      "|    ep_rew_mean        | 2.91     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 30200    |\n",
      "|    time_elapsed       | 2907     |\n",
      "|    total_timesteps    | 2416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.87     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30199    |\n",
      "|    policy_loss        | 0.0537   |\n",
      "|    value_loss         | 0.029    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2420000, episode_reward=4.60 +/- 3.32\n",
      "Episode length: 1159.60 +/- 421.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+03 |\n",
      "|    mean_reward        | 4.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30249    |\n",
      "|    policy_loss        | -0.0394  |\n",
      "|    value_loss         | 0.00758  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 965      |\n",
      "|    ep_rew_mean        | 2.95     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 30300    |\n",
      "|    time_elapsed       | 2918     |\n",
      "|    total_timesteps    | 2424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.877    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30299    |\n",
      "|    policy_loss        | 0.0448   |\n",
      "|    value_loss         | 0.0332   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2430000, episode_reward=4.00 +/- 2.68\n",
      "Episode length: 1132.80 +/- 379.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+03 |\n",
      "|    mean_reward        | 4        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2430000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30374    |\n",
      "|    policy_loss        | -0.0522  |\n",
      "|    value_loss         | 0.00956  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | 3.25     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 30400    |\n",
      "|    time_elapsed       | 2928     |\n",
      "|    total_timesteps    | 2432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30399    |\n",
      "|    policy_loss        | -0.037   |\n",
      "|    value_loss         | 0.0167   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2440000, episode_reward=4.20 +/- 3.76\n",
      "Episode length: 1117.40 +/- 475.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+03 |\n",
      "|    mean_reward        | 4.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30499    |\n",
      "|    policy_loss        | -0.0229  |\n",
      "|    value_loss         | 0.00742  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.02e+03 |\n",
      "|    ep_rew_mean     | 3.24     |\n",
      "| time/              |          |\n",
      "|    fps             | 830      |\n",
      "|    iterations      | 30500    |\n",
      "|    time_elapsed    | 2938     |\n",
      "|    total_timesteps | 2440000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+03 |\n",
      "|    ep_rew_mean        | 3.4      |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 30600    |\n",
      "|    time_elapsed       | 2946     |\n",
      "|    total_timesteps    | 2448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30599    |\n",
      "|    policy_loss        | -0.00955 |\n",
      "|    value_loss         | 0.00642  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2450000, episode_reward=4.00 +/- 2.83\n",
      "Episode length: 1091.20 +/- 372.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.09e+03 |\n",
      "|    mean_reward        | 4        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.819    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30624    |\n",
      "|    policy_loss        | 0.0133   |\n",
      "|    value_loss         | 0.0362   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+03 |\n",
      "|    ep_rew_mean        | 3.47     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 30700    |\n",
      "|    time_elapsed       | 2956     |\n",
      "|    total_timesteps    | 2456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30699    |\n",
      "|    policy_loss        | 0.0333   |\n",
      "|    value_loss         | 0.0114   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2460000, episode_reward=4.00 +/- 1.67\n",
      "Episode length: 1238.60 +/- 358.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.24e+03 |\n",
      "|    mean_reward        | 4        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.822    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30749    |\n",
      "|    policy_loss        | 0.0684   |\n",
      "|    value_loss         | 0.0637   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+03 |\n",
      "|    ep_rew_mean        | 3.45     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 30800    |\n",
      "|    time_elapsed       | 2967     |\n",
      "|    total_timesteps    | 2464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30799    |\n",
      "|    policy_loss        | -0.0804  |\n",
      "|    value_loss         | 0.0157   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2470000, episode_reward=5.00 +/- 3.22\n",
      "Episode length: 1241.60 +/- 432.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.24e+03 |\n",
      "|    mean_reward        | 5        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2470000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30874    |\n",
      "|    policy_loss        | -0.0178  |\n",
      "|    value_loss         | 0.0101   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+03 |\n",
      "|    ep_rew_mean        | 3.47     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 30900    |\n",
      "|    time_elapsed       | 2978     |\n",
      "|    total_timesteps    | 2472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30899    |\n",
      "|    policy_loss        | -0.00498 |\n",
      "|    value_loss         | 0.0153   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2480000, episode_reward=4.60 +/- 1.02\n",
      "Episode length: 1303.00 +/- 224.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.3e+03  |\n",
      "|    mean_reward        | 4.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30999    |\n",
      "|    policy_loss        | 0.00323  |\n",
      "|    value_loss         | 0.0508   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.01e+03 |\n",
      "|    ep_rew_mean     | 3.14     |\n",
      "| time/              |          |\n",
      "|    fps             | 829      |\n",
      "|    iterations      | 31000    |\n",
      "|    time_elapsed    | 2989     |\n",
      "|    total_timesteps | 2480000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+03 |\n",
      "|    ep_rew_mean        | 3.4      |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 31100    |\n",
      "|    time_elapsed       | 2997     |\n",
      "|    total_timesteps    | 2488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31099    |\n",
      "|    policy_loss        | -0.0514  |\n",
      "|    value_loss         | 0.0107   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2490000, episode_reward=2.20 +/- 2.04\n",
      "Episode length: 914.80 +/- 389.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 915      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2490000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.889    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31124    |\n",
      "|    policy_loss        | -0.0744  |\n",
      "|    value_loss         | 0.0179   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06e+03 |\n",
      "|    ep_rew_mean        | 3.54     |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 31200    |\n",
      "|    time_elapsed       | 3006     |\n",
      "|    total_timesteps    | 2496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.844    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31199    |\n",
      "|    policy_loss        | 0.0849   |\n",
      "|    value_loss         | 0.0284   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2500000, episode_reward=4.40 +/- 3.26\n",
      "Episode length: 1159.00 +/- 445.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+03 |\n",
      "|    mean_reward        | 4.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31249    |\n",
      "|    policy_loss        | -0.0392  |\n",
      "|    value_loss         | 0.02     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+03 |\n",
      "|    ep_rew_mean        | 3.75     |\n",
      "| time/                 |          |\n",
      "|    fps                | 829      |\n",
      "|    iterations         | 31300    |\n",
      "|    time_elapsed       | 3017     |\n",
      "|    total_timesteps    | 2504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.893    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31299    |\n",
      "|    policy_loss        | 0.00927  |\n",
      "|    value_loss         | 0.0241   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2510000, episode_reward=5.40 +/- 2.73\n",
      "Episode length: 1334.20 +/- 347.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.33e+03 |\n",
      "|    mean_reward        | 5.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2510000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31374    |\n",
      "|    policy_loss        | 0.0473   |\n",
      "|    value_loss         | 0.0155   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+03  |\n",
      "|    ep_rew_mean        | 3.93     |\n",
      "| time/                 |          |\n",
      "|    fps                | 829      |\n",
      "|    iterations         | 31400    |\n",
      "|    time_elapsed       | 3028     |\n",
      "|    total_timesteps    | 2512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31399    |\n",
      "|    policy_loss        | -0.0486  |\n",
      "|    value_loss         | 0.0109   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=3.00 +/- 1.55\n",
      "Episode length: 1004.80 +/- 306.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1e+03    |\n",
      "|    mean_reward        | 3        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31499    |\n",
      "|    policy_loss        | 0.005    |\n",
      "|    value_loss         | 0.0163   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.09e+03 |\n",
      "|    ep_rew_mean     | 3.9      |\n",
      "| time/              |          |\n",
      "|    fps             | 829      |\n",
      "|    iterations      | 31500    |\n",
      "|    time_elapsed    | 3038     |\n",
      "|    total_timesteps | 2520000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+03  |\n",
      "|    ep_rew_mean        | 3.92     |\n",
      "| time/                 |          |\n",
      "|    fps                | 829      |\n",
      "|    iterations         | 31600    |\n",
      "|    time_elapsed       | 3045     |\n",
      "|    total_timesteps    | 2528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31599    |\n",
      "|    policy_loss        | 0.0197   |\n",
      "|    value_loss         | 0.0205   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2530000, episode_reward=2.60 +/- 0.80\n",
      "Episode length: 973.80 +/- 156.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 974      |\n",
      "|    mean_reward        | 2.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2530000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31624    |\n",
      "|    policy_loss        | 0.026    |\n",
      "|    value_loss         | 0.0117   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.13e+03 |\n",
      "|    ep_rew_mean        | 4.13     |\n",
      "| time/                 |          |\n",
      "|    fps                | 829      |\n",
      "|    iterations         | 31700    |\n",
      "|    time_elapsed       | 3055     |\n",
      "|    total_timesteps    | 2536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31699    |\n",
      "|    policy_loss        | 0.0265   |\n",
      "|    value_loss         | 0.0184   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2540000, episode_reward=3.20 +/- 2.04\n",
      "Episode length: 1017.60 +/- 323.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+03 |\n",
      "|    mean_reward        | 3.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.802    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31749    |\n",
      "|    policy_loss        | -0.14    |\n",
      "|    value_loss         | 0.0555   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.13e+03 |\n",
      "|    ep_rew_mean        | 4.22     |\n",
      "| time/                 |          |\n",
      "|    fps                | 829      |\n",
      "|    iterations         | 31800    |\n",
      "|    time_elapsed       | 3066     |\n",
      "|    total_timesteps    | 2544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31799    |\n",
      "|    policy_loss        | -0.0675  |\n",
      "|    value_loss         | 0.0179   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2550000, episode_reward=7.00 +/- 2.97\n",
      "Episode length: 1465.80 +/- 361.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.47e+03 |\n",
      "|    mean_reward        | 7        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31874    |\n",
      "|    policy_loss        | -0.0321  |\n",
      "|    value_loss         | 0.0087   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.19e+03 |\n",
      "|    ep_rew_mean        | 4.61     |\n",
      "| time/                 |          |\n",
      "|    fps                | 829      |\n",
      "|    iterations         | 31900    |\n",
      "|    time_elapsed       | 3077     |\n",
      "|    total_timesteps    | 2552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31899    |\n",
      "|    policy_loss        | -0.0537  |\n",
      "|    value_loss         | 0.0102   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2560000, episode_reward=6.00 +/- 4.60\n",
      "Episode length: 1337.80 +/- 485.31\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 1.34e+03  |\n",
      "|    mean_reward        | 6         |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 2560000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.23     |\n",
      "|    explained_variance | 0.941     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 31999     |\n",
      "|    policy_loss        | -0.000672 |\n",
      "|    value_loss         | 0.0102    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.16e+03 |\n",
      "|    ep_rew_mean     | 4.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 828      |\n",
      "|    iterations      | 32000    |\n",
      "|    time_elapsed    | 3088     |\n",
      "|    total_timesteps | 2560000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14e+03 |\n",
      "|    ep_rew_mean        | 4.24     |\n",
      "| time/                 |          |\n",
      "|    fps                | 829      |\n",
      "|    iterations         | 32100    |\n",
      "|    time_elapsed       | 3096     |\n",
      "|    total_timesteps    | 2568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.896    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32099    |\n",
      "|    policy_loss        | -0.0463  |\n",
      "|    value_loss         | 0.0138   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2570000, episode_reward=4.00 +/- 1.55\n",
      "Episode length: 1132.60 +/- 348.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+03 |\n",
      "|    mean_reward        | 4        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2570000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.903    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32124    |\n",
      "|    policy_loss        | 0.00199  |\n",
      "|    value_loss         | 0.0167   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14e+03 |\n",
      "|    ep_rew_mean        | 4.16     |\n",
      "| time/                 |          |\n",
      "|    fps                | 829      |\n",
      "|    iterations         | 32200    |\n",
      "|    time_elapsed       | 3106     |\n",
      "|    total_timesteps    | 2576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.817    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32199    |\n",
      "|    policy_loss        | 0.00397  |\n",
      "|    value_loss         | 0.0425   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2580000, episode_reward=7.20 +/- 3.82\n",
      "Episode length: 1543.40 +/- 492.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.54e+03 |\n",
      "|    mean_reward        | 7.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.82     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32249    |\n",
      "|    policy_loss        | -0.0237  |\n",
      "|    value_loss         | 0.0279   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.16e+03 |\n",
      "|    ep_rew_mean        | 4.31     |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 32300    |\n",
      "|    time_elapsed       | 3117     |\n",
      "|    total_timesteps    | 2584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32299    |\n",
      "|    policy_loss        | -0.0242  |\n",
      "|    value_loss         | 0.0196   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2590000, episode_reward=6.40 +/- 5.82\n",
      "Episode length: 1307.20 +/- 618.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.31e+03 |\n",
      "|    mean_reward        | 6.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2590000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32374    |\n",
      "|    policy_loss        | 0.0581   |\n",
      "|    value_loss         | 0.0254   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.16e+03 |\n",
      "|    ep_rew_mean        | 4.31     |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 32400    |\n",
      "|    time_elapsed       | 3127     |\n",
      "|    total_timesteps    | 2592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.885    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32399    |\n",
      "|    policy_loss        | 0.0679   |\n",
      "|    value_loss         | 0.0301   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2600000, episode_reward=5.40 +/- 3.01\n",
      "Episode length: 1236.60 +/- 300.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.24e+03 |\n",
      "|    mean_reward        | 5.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.847    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32499    |\n",
      "|    policy_loss        | -0.0428  |\n",
      "|    value_loss         | 0.0219   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.14e+03 |\n",
      "|    ep_rew_mean     | 4.14     |\n",
      "| time/              |          |\n",
      "|    fps             | 828      |\n",
      "|    iterations      | 32500    |\n",
      "|    time_elapsed    | 3138     |\n",
      "|    total_timesteps | 2600000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15e+03 |\n",
      "|    ep_rew_mean        | 4.14     |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 32600    |\n",
      "|    time_elapsed       | 3146     |\n",
      "|    total_timesteps    | 2608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32599    |\n",
      "|    policy_loss        | 0.014    |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2610000, episode_reward=3.20 +/- 2.99\n",
      "Episode length: 1038.20 +/- 447.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+03 |\n",
      "|    mean_reward        | 3.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2610000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.841    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32624    |\n",
      "|    policy_loss        | -0.015   |\n",
      "|    value_loss         | 0.0239   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.2e+03  |\n",
      "|    ep_rew_mean        | 4.45     |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 32700    |\n",
      "|    time_elapsed       | 3156     |\n",
      "|    total_timesteps    | 2616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32699    |\n",
      "|    policy_loss        | 0.0164   |\n",
      "|    value_loss         | 0.0136   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2620000, episode_reward=2.60 +/- 1.50\n",
      "Episode length: 925.40 +/- 264.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 925      |\n",
      "|    mean_reward        | 2.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2620000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.648    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32749    |\n",
      "|    policy_loss        | 0.0143   |\n",
      "|    value_loss         | 0.035    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+03 |\n",
      "|    ep_rew_mean        | 4.73     |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 32800    |\n",
      "|    time_elapsed       | 3165     |\n",
      "|    total_timesteps    | 2624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32799    |\n",
      "|    policy_loss        | 0.0278   |\n",
      "|    value_loss         | 0.0388   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2630000, episode_reward=3.20 +/- 2.48\n",
      "Episode length: 1046.60 +/- 406.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+03 |\n",
      "|    mean_reward        | 3.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2630000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.709    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32874    |\n",
      "|    policy_loss        | 0.0783   |\n",
      "|    value_loss         | 0.0694   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+03 |\n",
      "|    ep_rew_mean        | 4.95     |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 32900    |\n",
      "|    time_elapsed       | 3176     |\n",
      "|    total_timesteps    | 2632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.81     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32899    |\n",
      "|    policy_loss        | 0.0712   |\n",
      "|    value_loss         | 0.0425   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2640000, episode_reward=2.80 +/- 1.17\n",
      "Episode length: 955.20 +/- 210.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 955      |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32999    |\n",
      "|    policy_loss        | -0.032   |\n",
      "|    value_loss         | 0.0173   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 5.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 828      |\n",
      "|    iterations      | 33000    |\n",
      "|    time_elapsed    | 3186     |\n",
      "|    total_timesteps | 2640000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+03 |\n",
      "|    ep_rew_mean        | 4.94     |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 33100    |\n",
      "|    time_elapsed       | 3195     |\n",
      "|    total_timesteps    | 2648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.855    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33099    |\n",
      "|    policy_loss        | 0.0652   |\n",
      "|    value_loss         | 0.0461   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2650000, episode_reward=3.40 +/- 1.02\n",
      "Episode length: 982.40 +/- 228.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 982      |\n",
      "|    mean_reward        | 3.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.842    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33124    |\n",
      "|    policy_loss        | 0.16     |\n",
      "|    value_loss         | 0.0428   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+03 |\n",
      "|    ep_rew_mean        | 5.05     |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 33200    |\n",
      "|    time_elapsed       | 3205     |\n",
      "|    total_timesteps    | 2656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33199    |\n",
      "|    policy_loss        | 0.0256   |\n",
      "|    value_loss         | 0.0337   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2660000, episode_reward=6.00 +/- 4.00\n",
      "Episode length: 1334.20 +/- 428.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.33e+03 |\n",
      "|    mean_reward        | 6        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33249    |\n",
      "|    policy_loss        | -0.0845  |\n",
      "|    value_loss         | 0.0385   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+03 |\n",
      "|    ep_rew_mean        | 4.91     |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 33300    |\n",
      "|    time_elapsed       | 3216     |\n",
      "|    total_timesteps    | 2664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33299    |\n",
      "|    policy_loss        | 0.0522   |\n",
      "|    value_loss         | 0.0271   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2670000, episode_reward=5.60 +/- 2.58\n",
      "Episode length: 1401.80 +/- 373.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.4e+03  |\n",
      "|    mean_reward        | 5.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2670000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.796    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33374    |\n",
      "|    policy_loss        | 0.0642   |\n",
      "|    value_loss         | 0.0437   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+03 |\n",
      "|    ep_rew_mean        | 4.99     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 33400    |\n",
      "|    time_elapsed       | 3227     |\n",
      "|    total_timesteps    | 2672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33399    |\n",
      "|    policy_loss        | -0.0889  |\n",
      "|    value_loss         | 0.0172   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2680000, episode_reward=5.00 +/- 4.65\n",
      "Episode length: 1294.40 +/- 664.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.29e+03 |\n",
      "|    mean_reward        | 5        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.787    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33499    |\n",
      "|    policy_loss        | 0.04     |\n",
      "|    value_loss         | 0.0515   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.19e+03 |\n",
      "|    ep_rew_mean     | 4.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 827      |\n",
      "|    iterations      | 33500    |\n",
      "|    time_elapsed    | 3238     |\n",
      "|    total_timesteps | 2680000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+03 |\n",
      "|    ep_rew_mean        | 5.25     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 33600    |\n",
      "|    time_elapsed       | 3246     |\n",
      "|    total_timesteps    | 2688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33599    |\n",
      "|    policy_loss        | -0.0159  |\n",
      "|    value_loss         | 0.0109   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2690000, episode_reward=4.20 +/- 3.76\n",
      "Episode length: 1081.40 +/- 503.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+03 |\n",
      "|    mean_reward        | 4.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2690000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.787    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33624    |\n",
      "|    policy_loss        | 0.00883  |\n",
      "|    value_loss         | 0.044    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+03 |\n",
      "|    ep_rew_mean        | 4.96     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 33700    |\n",
      "|    time_elapsed       | 3257     |\n",
      "|    total_timesteps    | 2696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.807    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33699    |\n",
      "|    policy_loss        | -0.0206  |\n",
      "|    value_loss         | 0.0528   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2700000, episode_reward=4.40 +/- 3.01\n",
      "Episode length: 1112.40 +/- 404.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.11e+03 |\n",
      "|    mean_reward        | 4.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.721    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33749    |\n",
      "|    policy_loss        | -0.0758  |\n",
      "|    value_loss         | 0.0658   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+03 |\n",
      "|    ep_rew_mean        | 4.91     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 33800    |\n",
      "|    time_elapsed       | 3267     |\n",
      "|    total_timesteps    | 2704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33799    |\n",
      "|    policy_loss        | 0.0473   |\n",
      "|    value_loss         | 0.0343   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2710000, episode_reward=4.60 +/- 1.02\n",
      "Episode length: 1303.20 +/- 222.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.3e+03  |\n",
      "|    mean_reward        | 4.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2710000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.871    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33874    |\n",
      "|    policy_loss        | 0.0318   |\n",
      "|    value_loss         | 0.0208   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.29e+03 |\n",
      "|    ep_rew_mean        | 5.33     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 33900    |\n",
      "|    time_elapsed       | 3278     |\n",
      "|    total_timesteps    | 2712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33899    |\n",
      "|    policy_loss        | -0.0306  |\n",
      "|    value_loss         | 0.00934  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2720000, episode_reward=5.20 +/- 4.96\n",
      "Episode length: 1176.40 +/- 544.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.18e+03 |\n",
      "|    mean_reward        | 5.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.829    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33999    |\n",
      "|    policy_loss        | 0.0602   |\n",
      "|    value_loss         | 0.0324   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 5.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 827      |\n",
      "|    iterations      | 34000    |\n",
      "|    time_elapsed    | 3288     |\n",
      "|    total_timesteps | 2720000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.28e+03 |\n",
      "|    ep_rew_mean        | 5.28     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 34100    |\n",
      "|    time_elapsed       | 3295     |\n",
      "|    total_timesteps    | 2728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34099    |\n",
      "|    policy_loss        | -0.0231  |\n",
      "|    value_loss         | 0.0284   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2730000, episode_reward=1.40 +/- 1.02\n",
      "Episode length: 766.40 +/- 201.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 766      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2730000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34124    |\n",
      "|    policy_loss        | -0.0496  |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+03 |\n",
      "|    ep_rew_mean        | 4.83     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 34200    |\n",
      "|    time_elapsed       | 3305     |\n",
      "|    total_timesteps    | 2736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.87     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34199    |\n",
      "|    policy_loss        | 0.0664   |\n",
      "|    value_loss         | 0.0372   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2740000, episode_reward=6.60 +/- 3.38\n",
      "Episode length: 1413.20 +/- 487.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.41e+03 |\n",
      "|    mean_reward        | 6.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.628    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34249    |\n",
      "|    policy_loss        | -0.0262  |\n",
      "|    value_loss         | 0.0624   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.21e+03 |\n",
      "|    ep_rew_mean        | 4.8      |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 34300    |\n",
      "|    time_elapsed       | 3315     |\n",
      "|    total_timesteps    | 2744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34299    |\n",
      "|    policy_loss        | -0.00826 |\n",
      "|    value_loss         | 0.0281   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2750000, episode_reward=3.80 +/- 3.71\n",
      "Episode length: 986.40 +/- 477.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 986      |\n",
      "|    mean_reward        | 3.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34374    |\n",
      "|    policy_loss        | -0.025   |\n",
      "|    value_loss         | 0.0138   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+03 |\n",
      "|    ep_rew_mean        | 5.22     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 34400    |\n",
      "|    time_elapsed       | 3325     |\n",
      "|    total_timesteps    | 2752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34399    |\n",
      "|    policy_loss        | -0.0294  |\n",
      "|    value_loss         | 0.00923  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2760000, episode_reward=4.60 +/- 1.50\n",
      "Episode length: 1242.20 +/- 230.23\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 1.24e+03  |\n",
      "|    mean_reward        | 4.6       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 2760000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.24     |\n",
      "|    explained_variance | 0.877     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 34499     |\n",
      "|    policy_loss        | -0.000751 |\n",
      "|    value_loss         | 0.0293    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.24e+03 |\n",
      "|    ep_rew_mean     | 5.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 827      |\n",
      "|    iterations      | 34500    |\n",
      "|    time_elapsed    | 3335     |\n",
      "|    total_timesteps | 2760000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.28e+03 |\n",
      "|    ep_rew_mean        | 5.39     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 34600    |\n",
      "|    time_elapsed       | 3343     |\n",
      "|    total_timesteps    | 2768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.275    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34599    |\n",
      "|    policy_loss        | 0.0318   |\n",
      "|    value_loss         | 0.0676   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2770000, episode_reward=5.00 +/- 2.19\n",
      "Episode length: 1338.80 +/- 436.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.34e+03 |\n",
      "|    mean_reward        | 5        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2770000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34624    |\n",
      "|    policy_loss        | -0.0315  |\n",
      "|    value_loss         | 0.0263   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.37e+03 |\n",
      "|    ep_rew_mean        | 5.92     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 34700    |\n",
      "|    time_elapsed       | 3354     |\n",
      "|    total_timesteps    | 2776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34699    |\n",
      "|    policy_loss        | 0.0301   |\n",
      "|    value_loss         | 0.0275   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2780000, episode_reward=8.40 +/- 4.03\n",
      "Episode length: 1707.00 +/- 522.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.71e+03 |\n",
      "|    mean_reward        | 8.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34749    |\n",
      "|    policy_loss        | 0.0101   |\n",
      "|    value_loss         | 0.00793  |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.34e+03 |\n",
      "|    ep_rew_mean        | 5.93     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 34800    |\n",
      "|    time_elapsed       | 3365     |\n",
      "|    total_timesteps    | 2784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34799    |\n",
      "|    policy_loss        | -0.0166  |\n",
      "|    value_loss         | 0.0108   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2790000, episode_reward=8.80 +/- 4.58\n",
      "Episode length: 1761.60 +/- 573.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.76e+03 |\n",
      "|    mean_reward        | 8.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2790000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.895    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34874    |\n",
      "|    policy_loss        | 0.0551   |\n",
      "|    value_loss         | 0.0285   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.39e+03 |\n",
      "|    ep_rew_mean        | 6.27     |\n",
      "| time/                 |          |\n",
      "|    fps                | 826      |\n",
      "|    iterations         | 34900    |\n",
      "|    time_elapsed       | 3377     |\n",
      "|    total_timesteps    | 2792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.825    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34899    |\n",
      "|    policy_loss        | 0.0878   |\n",
      "|    value_loss         | 0.0371   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2800000, episode_reward=5.20 +/- 2.93\n",
      "Episode length: 1268.40 +/- 408.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.27e+03 |\n",
      "|    mean_reward        | 5.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34999    |\n",
      "|    policy_loss        | -0.0398  |\n",
      "|    value_loss         | 0.0224   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 6.42     |\n",
      "| time/              |          |\n",
      "|    fps             | 826      |\n",
      "|    iterations      | 35000    |\n",
      "|    time_elapsed    | 3387     |\n",
      "|    total_timesteps | 2800000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.36e+03 |\n",
      "|    ep_rew_mean        | 6.27     |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 35100    |\n",
      "|    time_elapsed       | 3395     |\n",
      "|    total_timesteps    | 2808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35099    |\n",
      "|    policy_loss        | -0.0429  |\n",
      "|    value_loss         | 0.0199   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2810000, episode_reward=6.00 +/- 1.26\n",
      "Episode length: 1467.60 +/- 244.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.47e+03 |\n",
      "|    mean_reward        | 6        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2810000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35124    |\n",
      "|    policy_loss        | -0.00102 |\n",
      "|    value_loss         | 0.0135   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.37e+03 |\n",
      "|    ep_rew_mean        | 6.09     |\n",
      "| time/                 |          |\n",
      "|    fps                | 826      |\n",
      "|    iterations         | 35200    |\n",
      "|    time_elapsed       | 3405     |\n",
      "|    total_timesteps    | 2816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35199    |\n",
      "|    policy_loss        | -0.0164  |\n",
      "|    value_loss         | 0.0148   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2820000, episode_reward=3.80 +/- 0.98\n",
      "Episode length: 1089.80 +/- 192.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.09e+03 |\n",
      "|    mean_reward        | 3.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35249    |\n",
      "|    policy_loss        | -0.0673  |\n",
      "|    value_loss         | 0.0117   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.39e+03 |\n",
      "|    ep_rew_mean        | 6.28     |\n",
      "| time/                 |          |\n",
      "|    fps                | 826      |\n",
      "|    iterations         | 35300    |\n",
      "|    time_elapsed       | 3415     |\n",
      "|    total_timesteps    | 2824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.806    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35299    |\n",
      "|    policy_loss        | -0.0299  |\n",
      "|    value_loss         | 0.0317   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2830000, episode_reward=6.80 +/- 2.64\n",
      "Episode length: 1445.60 +/- 276.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.45e+03 |\n",
      "|    mean_reward        | 6.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2830000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.906    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35374    |\n",
      "|    policy_loss        | 0.0594   |\n",
      "|    value_loss         | 0.02     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.37e+03 |\n",
      "|    ep_rew_mean        | 6.29     |\n",
      "| time/                 |          |\n",
      "|    fps                | 826      |\n",
      "|    iterations         | 35400    |\n",
      "|    time_elapsed       | 3426     |\n",
      "|    total_timesteps    | 2832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.868    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35399    |\n",
      "|    policy_loss        | 0.0163   |\n",
      "|    value_loss         | 0.0296   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2840000, episode_reward=6.20 +/- 2.86\n",
      "Episode length: 1306.80 +/- 424.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.31e+03 |\n",
      "|    mean_reward        | 6.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35499    |\n",
      "|    policy_loss        | 0.0323   |\n",
      "|    value_loss         | 0.0144   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.39e+03 |\n",
      "|    ep_rew_mean     | 6.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 826      |\n",
      "|    iterations      | 35500    |\n",
      "|    time_elapsed    | 3437     |\n",
      "|    total_timesteps | 2840000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.39e+03 |\n",
      "|    ep_rew_mean        | 6.79     |\n",
      "| time/                 |          |\n",
      "|    fps                | 826      |\n",
      "|    iterations         | 35600    |\n",
      "|    time_elapsed       | 3445     |\n",
      "|    total_timesteps    | 2848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.19    |\n",
      "|    explained_variance | 0.571    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35599    |\n",
      "|    policy_loss        | 0.0445   |\n",
      "|    value_loss         | 0.0551   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2850000, episode_reward=8.80 +/- 3.37\n",
      "Episode length: 1675.40 +/- 421.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.68e+03 |\n",
      "|    mean_reward        | 8.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2850000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.839    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35624    |\n",
      "|    policy_loss        | -0.0786  |\n",
      "|    value_loss         | 0.0414   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.41e+03 |\n",
      "|    ep_rew_mean        | 7.07     |\n",
      "| time/                 |          |\n",
      "|    fps                | 826      |\n",
      "|    iterations         | 35700    |\n",
      "|    time_elapsed       | 3456     |\n",
      "|    total_timesteps    | 2856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.896    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35699    |\n",
      "|    policy_loss        | 0.0488   |\n",
      "|    value_loss         | 0.0225   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2860000, episode_reward=11.20 +/- 5.19\n",
      "Episode length: 1647.40 +/- 577.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.65e+03 |\n",
      "|    mean_reward        | 11.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2860000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.19    |\n",
      "|    explained_variance | 0.41     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35749    |\n",
      "|    policy_loss        | 0.101    |\n",
      "|    value_loss         | 0.107    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.38e+03 |\n",
      "|    ep_rew_mean        | 6.93     |\n",
      "| time/                 |          |\n",
      "|    fps                | 825      |\n",
      "|    iterations         | 35800    |\n",
      "|    time_elapsed       | 3468     |\n",
      "|    total_timesteps    | 2864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35799    |\n",
      "|    policy_loss        | -0.0262  |\n",
      "|    value_loss         | 0.0217   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2870000, episode_reward=9.60 +/- 3.01\n",
      "Episode length: 1715.00 +/- 374.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.72e+03 |\n",
      "|    mean_reward        | 9.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2870000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | 0.897    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35874    |\n",
      "|    policy_loss        | -0.0379  |\n",
      "|    value_loss         | 0.026    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.38e+03 |\n",
      "|    ep_rew_mean        | 6.91     |\n",
      "| time/                 |          |\n",
      "|    fps                | 824      |\n",
      "|    iterations         | 35900    |\n",
      "|    time_elapsed       | 3481     |\n",
      "|    total_timesteps    | 2872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.897    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35899    |\n",
      "|    policy_loss        | -0.0251  |\n",
      "|    value_loss         | 0.0259   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2880000, episode_reward=8.20 +/- 4.26\n",
      "Episode length: 1447.00 +/- 498.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.45e+03 |\n",
      "|    mean_reward        | 8.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35999    |\n",
      "|    policy_loss        | 0.00463  |\n",
      "|    value_loss         | 0.0232   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.43e+03 |\n",
      "|    ep_rew_mean     | 7.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 824      |\n",
      "|    iterations      | 36000    |\n",
      "|    time_elapsed    | 3492     |\n",
      "|    total_timesteps | 2880000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.46e+03 |\n",
      "|    ep_rew_mean        | 6.97     |\n",
      "| time/                 |          |\n",
      "|    fps                | 825      |\n",
      "|    iterations         | 36100    |\n",
      "|    time_elapsed       | 3500     |\n",
      "|    total_timesteps    | 2888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36099    |\n",
      "|    policy_loss        | -0.00396 |\n",
      "|    value_loss         | 0.0223   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2890000, episode_reward=5.00 +/- 1.67\n",
      "Episode length: 1253.80 +/- 312.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.25e+03 |\n",
      "|    mean_reward        | 5        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2890000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.67     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36124    |\n",
      "|    policy_loss        | -0.0418  |\n",
      "|    value_loss         | 0.0459   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.47e+03 |\n",
      "|    ep_rew_mean        | 7.11     |\n",
      "| time/                 |          |\n",
      "|    fps                | 824      |\n",
      "|    iterations         | 36200    |\n",
      "|    time_elapsed       | 3510     |\n",
      "|    total_timesteps    | 2896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36199    |\n",
      "|    policy_loss        | -0.0919  |\n",
      "|    value_loss         | 0.0416   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2900000, episode_reward=6.80 +/- 2.48\n",
      "Episode length: 1453.40 +/- 306.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.45e+03 |\n",
      "|    mean_reward        | 6.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2900000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36249    |\n",
      "|    policy_loss        | 0.0394   |\n",
      "|    value_loss         | 0.0279   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.44e+03 |\n",
      "|    ep_rew_mean        | 6.82     |\n",
      "| time/                 |          |\n",
      "|    fps                | 824      |\n",
      "|    iterations         | 36300    |\n",
      "|    time_elapsed       | 3521     |\n",
      "|    total_timesteps    | 2904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36299    |\n",
      "|    policy_loss        | 0.0515   |\n",
      "|    value_loss         | 0.0141   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2910000, episode_reward=7.80 +/- 2.56\n",
      "Episode length: 1678.60 +/- 314.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.68e+03 |\n",
      "|    mean_reward        | 7.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2910000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36374    |\n",
      "|    policy_loss        | -0.00462 |\n",
      "|    value_loss         | 0.0251   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.4e+03  |\n",
      "|    ep_rew_mean        | 6.66     |\n",
      "| time/                 |          |\n",
      "|    fps                | 824      |\n",
      "|    iterations         | 36400    |\n",
      "|    time_elapsed       | 3533     |\n",
      "|    total_timesteps    | 2912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36399    |\n",
      "|    policy_loss        | 0.00475  |\n",
      "|    value_loss         | 0.0216   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2920000, episode_reward=9.20 +/- 4.83\n",
      "Episode length: 1731.40 +/- 492.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.73e+03 |\n",
      "|    mean_reward        | 9.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | 0.884    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36499    |\n",
      "|    policy_loss        | -0.0567  |\n",
      "|    value_loss         | 0.0211   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.41e+03 |\n",
      "|    ep_rew_mean     | 6.84     |\n",
      "| time/              |          |\n",
      "|    fps             | 823      |\n",
      "|    iterations      | 36500    |\n",
      "|    time_elapsed    | 3544     |\n",
      "|    total_timesteps | 2920000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.41e+03 |\n",
      "|    ep_rew_mean        | 6.75     |\n",
      "| time/                 |          |\n",
      "|    fps                | 824      |\n",
      "|    iterations         | 36600    |\n",
      "|    time_elapsed       | 3552     |\n",
      "|    total_timesteps    | 2928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36599    |\n",
      "|    policy_loss        | -0.00546 |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2930000, episode_reward=7.20 +/- 3.71\n",
      "Episode length: 1457.80 +/- 427.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.46e+03 |\n",
      "|    mean_reward        | 7.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2930000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36624    |\n",
      "|    policy_loss        | -0.0705  |\n",
      "|    value_loss         | 0.044    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.47e+03 |\n",
      "|    ep_rew_mean        | 7.15     |\n",
      "| time/                 |          |\n",
      "|    fps                | 824      |\n",
      "|    iterations         | 36700    |\n",
      "|    time_elapsed       | 3563     |\n",
      "|    total_timesteps    | 2936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36699    |\n",
      "|    policy_loss        | 0.0246   |\n",
      "|    value_loss         | 0.0194   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2940000, episode_reward=8.00 +/- 1.90\n",
      "Episode length: 1718.40 +/- 246.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.72e+03 |\n",
      "|    mean_reward        | 8        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.858    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36749    |\n",
      "|    policy_loss        | -0.0519  |\n",
      "|    value_loss         | 0.0257   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.5e+03  |\n",
      "|    ep_rew_mean        | 7.32     |\n",
      "| time/                 |          |\n",
      "|    fps                | 823      |\n",
      "|    iterations         | 36800    |\n",
      "|    time_elapsed       | 3574     |\n",
      "|    total_timesteps    | 2944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.82     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36799    |\n",
      "|    policy_loss        | -0.0436  |\n",
      "|    value_loss         | 0.0342   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2950000, episode_reward=7.60 +/- 2.33\n",
      "Episode length: 1498.60 +/- 269.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.5e+03  |\n",
      "|    mean_reward        | 7.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2950000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.897    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36874    |\n",
      "|    policy_loss        | 0.0637   |\n",
      "|    value_loss         | 0.016    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.53e+03 |\n",
      "|    ep_rew_mean        | 7.81     |\n",
      "| time/                 |          |\n",
      "|    fps                | 823      |\n",
      "|    iterations         | 36900    |\n",
      "|    time_elapsed       | 3586     |\n",
      "|    total_timesteps    | 2952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36899    |\n",
      "|    policy_loss        | 0.0302   |\n",
      "|    value_loss         | 0.0223   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2960000, episode_reward=8.60 +/- 3.61\n",
      "Episode length: 1666.40 +/- 458.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.67e+03 |\n",
      "|    mean_reward        | 8.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.835    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36999    |\n",
      "|    policy_loss        | 0.0482   |\n",
      "|    value_loss         | 0.0344   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | 7.87     |\n",
      "| time/              |          |\n",
      "|    fps             | 822      |\n",
      "|    iterations      | 37000    |\n",
      "|    time_elapsed    | 3597     |\n",
      "|    total_timesteps | 2960000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.54e+03 |\n",
      "|    ep_rew_mean        | 8.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 823      |\n",
      "|    iterations         | 37100    |\n",
      "|    time_elapsed       | 3605     |\n",
      "|    total_timesteps    | 2968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37099    |\n",
      "|    policy_loss        | -0.0318  |\n",
      "|    value_loss         | 0.0368   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2970000, episode_reward=7.20 +/- 2.48\n",
      "Episode length: 1587.80 +/- 283.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.59e+03 |\n",
      "|    mean_reward        | 7.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2970000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37124    |\n",
      "|    policy_loss        | -0.0586  |\n",
      "|    value_loss         | 0.0264   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.52e+03 |\n",
      "|    ep_rew_mean        | 7.86     |\n",
      "| time/                 |          |\n",
      "|    fps                | 822      |\n",
      "|    iterations         | 37200    |\n",
      "|    time_elapsed       | 3617     |\n",
      "|    total_timesteps    | 2976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.682    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37199    |\n",
      "|    policy_loss        | 0.128    |\n",
      "|    value_loss         | 0.0519   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2980000, episode_reward=7.20 +/- 1.33\n",
      "Episode length: 1475.20 +/- 395.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.48e+03 |\n",
      "|    mean_reward        | 7.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.827    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37249    |\n",
      "|    policy_loss        | 0.0365   |\n",
      "|    value_loss         | 0.0274   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.49e+03 |\n",
      "|    ep_rew_mean        | 7.61     |\n",
      "| time/                 |          |\n",
      "|    fps                | 822      |\n",
      "|    iterations         | 37300    |\n",
      "|    time_elapsed       | 3628     |\n",
      "|    total_timesteps    | 2984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.753    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37299    |\n",
      "|    policy_loss        | 0.00354  |\n",
      "|    value_loss         | 0.0431   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2990000, episode_reward=7.80 +/- 2.71\n",
      "Episode length: 1652.00 +/- 145.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.65e+03 |\n",
      "|    mean_reward        | 7.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2990000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.2     |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37374    |\n",
      "|    policy_loss        | -0.0142  |\n",
      "|    value_loss         | 0.0155   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.55e+03 |\n",
      "|    ep_rew_mean        | 7.63     |\n",
      "| time/                 |          |\n",
      "|    fps                | 821      |\n",
      "|    iterations         | 37400    |\n",
      "|    time_elapsed       | 3640     |\n",
      "|    total_timesteps    | 2992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37399    |\n",
      "|    policy_loss        | 0.0445   |\n",
      "|    value_loss         | 0.0279   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3000000, episode_reward=9.80 +/- 3.60\n",
      "Episode length: 1706.20 +/- 394.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.71e+03 |\n",
      "|    mean_reward        | 9.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.847    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37499    |\n",
      "|    policy_loss        | 0.0403   |\n",
      "|    value_loss         | 0.0524   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.61e+03 |\n",
      "|    ep_rew_mean     | 8.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 821      |\n",
      "|    iterations      | 37500    |\n",
      "|    time_elapsed    | 3652     |\n",
      "|    total_timesteps | 3000000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.61e+03 |\n",
      "|    ep_rew_mean        | 8.43     |\n",
      "| time/                 |          |\n",
      "|    fps                | 821      |\n",
      "|    iterations         | 37600    |\n",
      "|    time_elapsed       | 3660     |\n",
      "|    total_timesteps    | 3008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | 0.82     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37599    |\n",
      "|    policy_loss        | -0.0682  |\n",
      "|    value_loss         | 0.0286   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3010000, episode_reward=9.20 +/- 4.79\n",
      "Episode length: 1763.00 +/- 543.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.76e+03 |\n",
      "|    mean_reward        | 9.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3010000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37624    |\n",
      "|    policy_loss        | -0.0794  |\n",
      "|    value_loss         | 0.0332   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.55e+03 |\n",
      "|    ep_rew_mean        | 8.35     |\n",
      "| time/                 |          |\n",
      "|    fps                | 821      |\n",
      "|    iterations         | 37700    |\n",
      "|    time_elapsed       | 3673     |\n",
      "|    total_timesteps    | 3016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.748    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37699    |\n",
      "|    policy_loss        | -0.00816 |\n",
      "|    value_loss         | 0.0328   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3020000, episode_reward=9.00 +/- 4.65\n",
      "Episode length: 1720.60 +/- 302.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.72e+03 |\n",
      "|    mean_reward        | 9        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37749    |\n",
      "|    policy_loss        | -0.067   |\n",
      "|    value_loss         | 0.0172   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.57e+03 |\n",
      "|    ep_rew_mean        | 8.43     |\n",
      "| time/                 |          |\n",
      "|    fps                | 820      |\n",
      "|    iterations         | 37800    |\n",
      "|    time_elapsed       | 3685     |\n",
      "|    total_timesteps    | 3024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.897    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37799    |\n",
      "|    policy_loss        | 0.0226   |\n",
      "|    value_loss         | 0.0241   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3030000, episode_reward=8.80 +/- 4.66\n",
      "Episode length: 1622.20 +/- 276.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.62e+03 |\n",
      "|    mean_reward        | 8.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3030000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37874    |\n",
      "|    policy_loss        | 0.0377   |\n",
      "|    value_loss         | 0.0259   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.5e+03  |\n",
      "|    ep_rew_mean        | 7.99     |\n",
      "| time/                 |          |\n",
      "|    fps                | 820      |\n",
      "|    iterations         | 37900    |\n",
      "|    time_elapsed       | 3697     |\n",
      "|    total_timesteps    | 3032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.792    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37899    |\n",
      "|    policy_loss        | 0.0212   |\n",
      "|    value_loss         | 0.0483   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3040000, episode_reward=7.40 +/- 3.56\n",
      "Episode length: 1452.20 +/- 363.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.45e+03 |\n",
      "|    mean_reward        | 7.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.863    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37999    |\n",
      "|    policy_loss        | -0.119   |\n",
      "|    value_loss         | 0.0417   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.48e+03 |\n",
      "|    ep_rew_mean     | 7.81     |\n",
      "| time/              |          |\n",
      "|    fps             | 819      |\n",
      "|    iterations      | 38000    |\n",
      "|    time_elapsed    | 3708     |\n",
      "|    total_timesteps | 3040000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.46e+03 |\n",
      "|    ep_rew_mean        | 7.62     |\n",
      "| time/                 |          |\n",
      "|    fps                | 820      |\n",
      "|    iterations         | 38100    |\n",
      "|    time_elapsed       | 3716     |\n",
      "|    total_timesteps    | 3048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.758    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38099    |\n",
      "|    policy_loss        | -0.0273  |\n",
      "|    value_loss         | 0.0634   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3050000, episode_reward=7.80 +/- 4.75\n",
      "Episode length: 1378.20 +/- 558.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.38e+03 |\n",
      "|    mean_reward        | 7.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.805    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38124    |\n",
      "|    policy_loss        | 0.0966   |\n",
      "|    value_loss         | 0.0506   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.5e+03  |\n",
      "|    ep_rew_mean        | 7.86     |\n",
      "| time/                 |          |\n",
      "|    fps                | 819      |\n",
      "|    iterations         | 38200    |\n",
      "|    time_elapsed       | 3728     |\n",
      "|    total_timesteps    | 3056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.819    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38199    |\n",
      "|    policy_loss        | -0.013   |\n",
      "|    value_loss         | 0.0146   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3060000, episode_reward=6.20 +/- 2.14\n",
      "Episode length: 1444.80 +/- 305.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.44e+03 |\n",
      "|    mean_reward        | 6.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.824    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38249    |\n",
      "|    policy_loss        | 0.0771   |\n",
      "|    value_loss         | 0.0457   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.48e+03 |\n",
      "|    ep_rew_mean        | 7.73     |\n",
      "| time/                 |          |\n",
      "|    fps                | 819      |\n",
      "|    iterations         | 38300    |\n",
      "|    time_elapsed       | 3740     |\n",
      "|    total_timesteps    | 3064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38299    |\n",
      "|    policy_loss        | -0.0203  |\n",
      "|    value_loss         | 0.0223   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3070000, episode_reward=6.60 +/- 2.24\n",
      "Episode length: 1414.20 +/- 253.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.41e+03 |\n",
      "|    mean_reward        | 6.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3070000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38374    |\n",
      "|    policy_loss        | 0.0102   |\n",
      "|    value_loss         | 0.0328   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.53e+03 |\n",
      "|    ep_rew_mean        | 8.23     |\n",
      "| time/                 |          |\n",
      "|    fps                | 818      |\n",
      "|    iterations         | 38400    |\n",
      "|    time_elapsed       | 3751     |\n",
      "|    total_timesteps    | 3072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38399    |\n",
      "|    policy_loss        | -0.0684  |\n",
      "|    value_loss         | 0.0301   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3080000, episode_reward=9.20 +/- 4.31\n",
      "Episode length: 1475.60 +/- 253.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.48e+03 |\n",
      "|    mean_reward        | 9.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.783    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38499    |\n",
      "|    policy_loss        | -0.0629  |\n",
      "|    value_loss         | 0.0769   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.51e+03 |\n",
      "|    ep_rew_mean     | 7.74     |\n",
      "| time/              |          |\n",
      "|    fps             | 818      |\n",
      "|    iterations      | 38500    |\n",
      "|    time_elapsed    | 3762     |\n",
      "|    total_timesteps | 3080000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.51e+03 |\n",
      "|    ep_rew_mean        | 7.93     |\n",
      "| time/                 |          |\n",
      "|    fps                | 818      |\n",
      "|    iterations         | 38600    |\n",
      "|    time_elapsed       | 3770     |\n",
      "|    total_timesteps    | 3088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38599    |\n",
      "|    policy_loss        | -0.0333  |\n",
      "|    value_loss         | 0.0119   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3090000, episode_reward=9.80 +/- 1.47\n",
      "Episode length: 1853.20 +/- 286.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.85e+03 |\n",
      "|    mean_reward        | 9.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3090000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.898    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38624    |\n",
      "|    policy_loss        | 0.0232   |\n",
      "|    value_loss         | 0.0174   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.58e+03 |\n",
      "|    ep_rew_mean        | 8.27     |\n",
      "| time/                 |          |\n",
      "|    fps                | 818      |\n",
      "|    iterations         | 38700    |\n",
      "|    time_elapsed       | 3783     |\n",
      "|    total_timesteps    | 3096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38699    |\n",
      "|    policy_loss        | -0.072   |\n",
      "|    value_loss         | 0.0194   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3100000, episode_reward=11.20 +/- 5.64\n",
      "Episode length: 1949.40 +/- 530.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.95e+03 |\n",
      "|    mean_reward        | 11.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38749    |\n",
      "|    policy_loss        | 0.00592  |\n",
      "|    value_loss         | 0.0141   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.6e+03  |\n",
      "|    ep_rew_mean        | 8.53     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 38800    |\n",
      "|    time_elapsed       | 3796     |\n",
      "|    total_timesteps    | 3104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.843    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38799    |\n",
      "|    policy_loss        | 0.00746  |\n",
      "|    value_loss         | 0.0231   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3110000, episode_reward=8.00 +/- 2.19\n",
      "Episode length: 1764.20 +/- 383.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.76e+03 |\n",
      "|    mean_reward        | 8        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3110000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38874    |\n",
      "|    policy_loss        | 0.0232   |\n",
      "|    value_loss         | 0.0117   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.6e+03  |\n",
      "|    ep_rew_mean        | 8.42     |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 38900    |\n",
      "|    time_elapsed       | 3808     |\n",
      "|    total_timesteps    | 3112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.789    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38899    |\n",
      "|    policy_loss        | -0.0312  |\n",
      "|    value_loss         | 0.0501   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3120000, episode_reward=8.40 +/- 3.93\n",
      "Episode length: 1649.20 +/- 612.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.65e+03 |\n",
      "|    mean_reward        | 8.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38999    |\n",
      "|    policy_loss        | 0.0299   |\n",
      "|    value_loss         | 0.0199   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.69e+03 |\n",
      "|    ep_rew_mean     | 8.92     |\n",
      "| time/              |          |\n",
      "|    fps             | 816      |\n",
      "|    iterations      | 39000    |\n",
      "|    time_elapsed    | 3820     |\n",
      "|    total_timesteps | 3120000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.72e+03 |\n",
      "|    ep_rew_mean        | 9        |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 39100    |\n",
      "|    time_elapsed       | 3828     |\n",
      "|    total_timesteps    | 3128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.853    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39099    |\n",
      "|    policy_loss        | -0.0461  |\n",
      "|    value_loss         | 0.0227   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3130000, episode_reward=7.80 +/- 2.32\n",
      "Episode length: 1528.20 +/- 309.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.53e+03 |\n",
      "|    mean_reward        | 7.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3130000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.771    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39124    |\n",
      "|    policy_loss        | 0.0434   |\n",
      "|    value_loss         | 0.0416   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.75e+03 |\n",
      "|    ep_rew_mean        | 9.32     |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 39200    |\n",
      "|    time_elapsed       | 3839     |\n",
      "|    total_timesteps    | 3136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.807    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39199    |\n",
      "|    policy_loss        | 0.0686   |\n",
      "|    value_loss         | 0.0349   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3140000, episode_reward=10.80 +/- 2.04\n",
      "Episode length: 1933.80 +/- 332.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.93e+03 |\n",
      "|    mean_reward        | 10.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.841    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39249    |\n",
      "|    policy_loss        | 0.0551   |\n",
      "|    value_loss         | 0.0253   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.74e+03 |\n",
      "|    ep_rew_mean        | 9.22     |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 39300    |\n",
      "|    time_elapsed       | 3852     |\n",
      "|    total_timesteps    | 3144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.19    |\n",
      "|    explained_variance | 0.752    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39299    |\n",
      "|    policy_loss        | 0.0729   |\n",
      "|    value_loss         | 0.0287   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3150000, episode_reward=10.20 +/- 3.31\n",
      "Episode length: 1796.00 +/- 470.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.8e+03  |\n",
      "|    mean_reward        | 10.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.89     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39374    |\n",
      "|    policy_loss        | -0.00739 |\n",
      "|    value_loss         | 0.0102   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.73e+03 |\n",
      "|    ep_rew_mean        | 8.99     |\n",
      "| time/                 |          |\n",
      "|    fps                | 815      |\n",
      "|    iterations         | 39400    |\n",
      "|    time_elapsed       | 3864     |\n",
      "|    total_timesteps    | 3152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39399    |\n",
      "|    policy_loss        | -0.0375  |\n",
      "|    value_loss         | 0.0248   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3160000, episode_reward=7.20 +/- 2.04\n",
      "Episode length: 1600.20 +/- 343.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.6e+03  |\n",
      "|    mean_reward        | 7.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39499    |\n",
      "|    policy_loss        | -0.0766  |\n",
      "|    value_loss         | 0.0158   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.69e+03 |\n",
      "|    ep_rew_mean     | 9.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 815      |\n",
      "|    iterations      | 39500    |\n",
      "|    time_elapsed    | 3875     |\n",
      "|    total_timesteps | 3160000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.67e+03 |\n",
      "|    ep_rew_mean        | 9.06     |\n",
      "| time/                 |          |\n",
      "|    fps                | 815      |\n",
      "|    iterations         | 39600    |\n",
      "|    time_elapsed       | 3883     |\n",
      "|    total_timesteps    | 3168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.776    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39599    |\n",
      "|    policy_loss        | -0.0107  |\n",
      "|    value_loss         | 0.0445   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3170000, episode_reward=6.80 +/- 2.79\n",
      "Episode length: 1501.40 +/- 325.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.5e+03  |\n",
      "|    mean_reward        | 6.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3170000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.771    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39624    |\n",
      "|    policy_loss        | -0.052   |\n",
      "|    value_loss         | 0.056    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.66e+03 |\n",
      "|    ep_rew_mean        | 8.9      |\n",
      "| time/                 |          |\n",
      "|    fps                | 815      |\n",
      "|    iterations         | 39700    |\n",
      "|    time_elapsed       | 3895     |\n",
      "|    total_timesteps    | 3176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39699    |\n",
      "|    policy_loss        | -0.0336  |\n",
      "|    value_loss         | 0.0166   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3180000, episode_reward=6.80 +/- 2.14\n",
      "Episode length: 1555.60 +/- 295.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.56e+03 |\n",
      "|    mean_reward        | 6.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.775    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39749    |\n",
      "|    policy_loss        | 0.00565  |\n",
      "|    value_loss         | 0.0315   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.69e+03 |\n",
      "|    ep_rew_mean        | 9.1      |\n",
      "| time/                 |          |\n",
      "|    fps                | 815      |\n",
      "|    iterations         | 39800    |\n",
      "|    time_elapsed       | 3906     |\n",
      "|    total_timesteps    | 3184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.19    |\n",
      "|    explained_variance | 0.408    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39799    |\n",
      "|    policy_loss        | 0.0367   |\n",
      "|    value_loss         | 0.0802   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3190000, episode_reward=8.80 +/- 1.47\n",
      "Episode length: 1790.00 +/- 188.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.79e+03 |\n",
      "|    mean_reward        | 8.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3190000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.832    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39874    |\n",
      "|    policy_loss        | -0.0347  |\n",
      "|    value_loss         | 0.028    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.72e+03 |\n",
      "|    ep_rew_mean        | 9.25     |\n",
      "| time/                 |          |\n",
      "|    fps                | 814      |\n",
      "|    iterations         | 39900    |\n",
      "|    time_elapsed       | 3918     |\n",
      "|    total_timesteps    | 3192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.906    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39899    |\n",
      "|    policy_loss        | -0.0114  |\n",
      "|    value_loss         | 0.0164   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3200000, episode_reward=10.40 +/- 4.32\n",
      "Episode length: 1755.20 +/- 569.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.76e+03 |\n",
      "|    mean_reward        | 10.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.2     |\n",
      "|    explained_variance | 0.668    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39999    |\n",
      "|    policy_loss        | 0.0806   |\n",
      "|    value_loss         | 0.0691   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.75e+03 |\n",
      "|    ep_rew_mean     | 9.42     |\n",
      "| time/              |          |\n",
      "|    fps             | 814      |\n",
      "|    iterations      | 40000    |\n",
      "|    time_elapsed    | 3929     |\n",
      "|    total_timesteps | 3200000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.78e+03 |\n",
      "|    ep_rew_mean        | 9.55     |\n",
      "| time/                 |          |\n",
      "|    fps                | 814      |\n",
      "|    iterations         | 40100    |\n",
      "|    time_elapsed       | 3937     |\n",
      "|    total_timesteps    | 3208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40099    |\n",
      "|    policy_loss        | -0.0313  |\n",
      "|    value_loss         | 0.035    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3210000, episode_reward=9.60 +/- 2.87\n",
      "Episode length: 1973.80 +/- 396.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.97e+03 |\n",
      "|    mean_reward        | 9.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3210000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.19    |\n",
      "|    explained_variance | 0.85     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40124    |\n",
      "|    policy_loss        | 0.0383   |\n",
      "|    value_loss         | 0.0473   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.77e+03 |\n",
      "|    ep_rew_mean        | 9.61     |\n",
      "| time/                 |          |\n",
      "|    fps                | 814      |\n",
      "|    iterations         | 40200    |\n",
      "|    time_elapsed       | 3949     |\n",
      "|    total_timesteps    | 3216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.886    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40199    |\n",
      "|    policy_loss        | -0.00143 |\n",
      "|    value_loss         | 0.024    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3220000, episode_reward=11.40 +/- 5.16\n",
      "Episode length: 1769.60 +/- 452.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.77e+03 |\n",
      "|    mean_reward        | 11.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.705    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40249    |\n",
      "|    policy_loss        | -0.0353  |\n",
      "|    value_loss         | 0.0731   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.77e+03 |\n",
      "|    ep_rew_mean        | 9.58     |\n",
      "| time/                 |          |\n",
      "|    fps                | 814      |\n",
      "|    iterations         | 40300    |\n",
      "|    time_elapsed       | 3960     |\n",
      "|    total_timesteps    | 3224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40299    |\n",
      "|    policy_loss        | 0.0229   |\n",
      "|    value_loss         | 0.0273   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3230000, episode_reward=9.20 +/- 2.40\n",
      "Episode length: 1713.60 +/- 51.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.71e+03 |\n",
      "|    mean_reward        | 9.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3230000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40374    |\n",
      "|    policy_loss        | -0.0651  |\n",
      "|    value_loss         | 0.0492   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.8e+03  |\n",
      "|    ep_rew_mean        | 9.64     |\n",
      "| time/                 |          |\n",
      "|    fps                | 813      |\n",
      "|    iterations         | 40400    |\n",
      "|    time_elapsed       | 3971     |\n",
      "|    total_timesteps    | 3232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.518    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40399    |\n",
      "|    policy_loss        | 0.075    |\n",
      "|    value_loss         | 0.0586   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3240000, episode_reward=8.60 +/- 1.74\n",
      "Episode length: 1890.60 +/- 207.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.89e+03 |\n",
      "|    mean_reward        | 8.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40499    |\n",
      "|    policy_loss        | -0.0474  |\n",
      "|    value_loss         | 0.017    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.82e+03 |\n",
      "|    ep_rew_mean     | 10.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 813      |\n",
      "|    iterations      | 40500    |\n",
      "|    time_elapsed    | 3983     |\n",
      "|    total_timesteps | 3240000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.79e+03 |\n",
      "|    ep_rew_mean        | 9.92     |\n",
      "| time/                 |          |\n",
      "|    fps                | 813      |\n",
      "|    iterations         | 40600    |\n",
      "|    time_elapsed       | 3990     |\n",
      "|    total_timesteps    | 3248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.739    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40599    |\n",
      "|    policy_loss        | -0.0237  |\n",
      "|    value_loss         | 0.0413   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3250000, episode_reward=12.00 +/- 5.29\n",
      "Episode length: 1948.00 +/- 572.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.95e+03 |\n",
      "|    mean_reward        | 12       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.812    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40624    |\n",
      "|    policy_loss        | 0.0174   |\n",
      "|    value_loss         | 0.0325   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.8e+03  |\n",
      "|    ep_rew_mean        | 10.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 813      |\n",
      "|    iterations         | 40700    |\n",
      "|    time_elapsed       | 4002     |\n",
      "|    total_timesteps    | 3256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.766    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40699    |\n",
      "|    policy_loss        | 0.0259   |\n",
      "|    value_loss         | 0.0268   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3260000, episode_reward=11.60 +/- 3.50\n",
      "Episode length: 1997.60 +/- 360.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2e+03    |\n",
      "|    mean_reward        | 11.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.858    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40749    |\n",
      "|    policy_loss        | -0.0676  |\n",
      "|    value_loss         | 0.0288   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.83e+03 |\n",
      "|    ep_rew_mean        | 10.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 813      |\n",
      "|    iterations         | 40800    |\n",
      "|    time_elapsed       | 4014     |\n",
      "|    total_timesteps    | 3264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.764    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40799    |\n",
      "|    policy_loss        | -0.0638  |\n",
      "|    value_loss         | 0.0441   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3270000, episode_reward=9.00 +/- 3.74\n",
      "Episode length: 1757.20 +/- 471.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.76e+03 |\n",
      "|    mean_reward        | 9        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3270000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40874    |\n",
      "|    policy_loss        | -0.0519  |\n",
      "|    value_loss         | 0.0264   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.85e+03 |\n",
      "|    ep_rew_mean        | 10.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 812      |\n",
      "|    iterations         | 40900    |\n",
      "|    time_elapsed       | 4025     |\n",
      "|    total_timesteps    | 3272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.8      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40899    |\n",
      "|    policy_loss        | 0.00287  |\n",
      "|    value_loss         | 0.0422   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3280000, episode_reward=8.80 +/- 2.86\n",
      "Episode length: 1553.20 +/- 352.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.55e+03 |\n",
      "|    mean_reward        | 8.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40999    |\n",
      "|    policy_loss        | -0.00127 |\n",
      "|    value_loss         | 0.0379   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.84e+03 |\n",
      "|    ep_rew_mean     | 10.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 41000    |\n",
      "|    time_elapsed    | 4035     |\n",
      "|    total_timesteps | 3280000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.86e+03 |\n",
      "|    ep_rew_mean        | 11       |\n",
      "| time/                 |          |\n",
      "|    fps                | 813      |\n",
      "|    iterations         | 41100    |\n",
      "|    time_elapsed       | 4043     |\n",
      "|    total_timesteps    | 3288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.78     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41099    |\n",
      "|    policy_loss        | 0.00145  |\n",
      "|    value_loss         | 0.0188   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3290000, episode_reward=11.00 +/- 2.83\n",
      "Episode length: 1928.80 +/- 576.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.93e+03 |\n",
      "|    mean_reward        | 11       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3290000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.826    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41124    |\n",
      "|    policy_loss        | 0.00326  |\n",
      "|    value_loss         | 0.0435   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.93e+03 |\n",
      "|    ep_rew_mean        | 11.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 812      |\n",
      "|    iterations         | 41200    |\n",
      "|    time_elapsed       | 4055     |\n",
      "|    total_timesteps    | 3296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.772    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41199    |\n",
      "|    policy_loss        | -0.117   |\n",
      "|    value_loss         | 0.0744   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3300000, episode_reward=8.80 +/- 2.64\n",
      "Episode length: 1714.40 +/- 489.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.71e+03 |\n",
      "|    mean_reward        | 8.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.835    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41249    |\n",
      "|    policy_loss        | -0.0383  |\n",
      "|    value_loss         | 0.0321   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.97e+03 |\n",
      "|    ep_rew_mean        | 11.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 812      |\n",
      "|    iterations         | 41300    |\n",
      "|    time_elapsed       | 4066     |\n",
      "|    total_timesteps    | 3304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.833    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41299    |\n",
      "|    policy_loss        | 0.0266   |\n",
      "|    value_loss         | 0.0442   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3310000, episode_reward=12.00 +/- 3.90\n",
      "Episode length: 1899.00 +/- 324.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.9e+03  |\n",
      "|    mean_reward        | 12       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3310000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.69     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41374    |\n",
      "|    policy_loss        | -0.0708  |\n",
      "|    value_loss         | 0.0547   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.01e+03 |\n",
      "|    ep_rew_mean        | 11.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 812      |\n",
      "|    iterations         | 41400    |\n",
      "|    time_elapsed       | 4078     |\n",
      "|    total_timesteps    | 3312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41399    |\n",
      "|    policy_loss        | -0.0546  |\n",
      "|    value_loss         | 0.0254   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3320000, episode_reward=14.60 +/- 3.93\n",
      "Episode length: 2139.60 +/- 419.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.14e+03 |\n",
      "|    mean_reward        | 14.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.2     |\n",
      "|    explained_variance | 0.56     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41499    |\n",
      "|    policy_loss        | 0.0481   |\n",
      "|    value_loss         | 0.0664   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.01e+03 |\n",
      "|    ep_rew_mean     | 11.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 811      |\n",
      "|    iterations      | 41500    |\n",
      "|    time_elapsed    | 4090     |\n",
      "|    total_timesteps | 3320000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.05e+03 |\n",
      "|    ep_rew_mean        | 11.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 812      |\n",
      "|    iterations         | 41600    |\n",
      "|    time_elapsed       | 4098     |\n",
      "|    total_timesteps    | 3328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41599    |\n",
      "|    policy_loss        | -0.0647  |\n",
      "|    value_loss         | 0.0332   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3330000, episode_reward=10.40 +/- 1.36\n",
      "Episode length: 1848.00 +/- 291.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.85e+03 |\n",
      "|    mean_reward        | 10.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3330000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.2     |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41624    |\n",
      "|    policy_loss        | 0.00436  |\n",
      "|    value_loss         | 0.0165   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.05e+03 |\n",
      "|    ep_rew_mean        | 11.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 811      |\n",
      "|    iterations         | 41700    |\n",
      "|    time_elapsed       | 4109     |\n",
      "|    total_timesteps    | 3336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.733    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41699    |\n",
      "|    policy_loss        | 0.0302   |\n",
      "|    value_loss         | 0.033    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3340000, episode_reward=13.40 +/- 5.68\n",
      "Episode length: 2173.20 +/- 545.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.17e+03 |\n",
      "|    mean_reward        | 13.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.748    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41749    |\n",
      "|    policy_loss        | -0.0467  |\n",
      "|    value_loss         | 0.0429   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.03e+03 |\n",
      "|    ep_rew_mean        | 11.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 811      |\n",
      "|    iterations         | 41800    |\n",
      "|    time_elapsed       | 4121     |\n",
      "|    total_timesteps    | 3344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41799    |\n",
      "|    policy_loss        | 0.0549   |\n",
      "|    value_loss         | 0.022    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3350000, episode_reward=9.40 +/- 3.38\n",
      "Episode length: 1745.80 +/- 376.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.75e+03 |\n",
      "|    mean_reward        | 9.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | 0.784    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41874    |\n",
      "|    policy_loss        | -0.0196  |\n",
      "|    value_loss         | 0.0472   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2e+03    |\n",
      "|    ep_rew_mean        | 11.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 810      |\n",
      "|    iterations         | 41900    |\n",
      "|    time_elapsed       | 4133     |\n",
      "|    total_timesteps    | 3352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.67     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41899    |\n",
      "|    policy_loss        | 0.0114   |\n",
      "|    value_loss         | 0.0781   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3360000, episode_reward=8.40 +/- 4.08\n",
      "Episode length: 1791.80 +/- 517.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.79e+03 |\n",
      "|    mean_reward        | 8.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41999    |\n",
      "|    policy_loss        | -0.00496 |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.98e+03 |\n",
      "|    ep_rew_mean     | 11.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 810      |\n",
      "|    iterations      | 42000    |\n",
      "|    time_elapsed    | 4144     |\n",
      "|    total_timesteps | 3360000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2e+03    |\n",
      "|    ep_rew_mean        | 11.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 811      |\n",
      "|    iterations         | 42100    |\n",
      "|    time_elapsed       | 4151     |\n",
      "|    total_timesteps    | 3368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.749    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42099    |\n",
      "|    policy_loss        | -0.0224  |\n",
      "|    value_loss         | 0.0851   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3370000, episode_reward=15.20 +/- 6.62\n",
      "Episode length: 2307.40 +/- 390.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.31e+03 |\n",
      "|    mean_reward        | 15.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3370000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.2     |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42124    |\n",
      "|    policy_loss        | -0.0217  |\n",
      "|    value_loss         | 0.01     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.02e+03 |\n",
      "|    ep_rew_mean        | 12.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 810      |\n",
      "|    iterations         | 42200    |\n",
      "|    time_elapsed       | 4164     |\n",
      "|    total_timesteps    | 3376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42199    |\n",
      "|    policy_loss        | 0.000396 |\n",
      "|    value_loss         | 0.0244   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3380000, episode_reward=19.40 +/- 10.29\n",
      "Episode length: 2433.40 +/- 569.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.43e+03 |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.853    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42249    |\n",
      "|    policy_loss        | -0.0605  |\n",
      "|    value_loss         | 0.0337   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.01e+03 |\n",
      "|    ep_rew_mean        | 12.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 810      |\n",
      "|    iterations         | 42300    |\n",
      "|    time_elapsed       | 4177     |\n",
      "|    total_timesteps    | 3384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.2     |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42299    |\n",
      "|    policy_loss        | -0.082   |\n",
      "|    value_loss         | 0.0532   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3390000, episode_reward=14.00 +/- 5.40\n",
      "Episode length: 2261.80 +/- 649.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.26e+03 |\n",
      "|    mean_reward        | 14       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3390000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.827    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42374    |\n",
      "|    policy_loss        | 0.0179   |\n",
      "|    value_loss         | 0.0292   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.01e+03 |\n",
      "|    ep_rew_mean        | 12.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 809      |\n",
      "|    iterations         | 42400    |\n",
      "|    time_elapsed       | 4189     |\n",
      "|    total_timesteps    | 3392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.814    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42399    |\n",
      "|    policy_loss        | 0.0385   |\n",
      "|    value_loss         | 0.0308   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3400000, episode_reward=13.40 +/- 3.01\n",
      "Episode length: 1892.20 +/- 456.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.89e+03 |\n",
      "|    mean_reward        | 13.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.837    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42499    |\n",
      "|    policy_loss        | -0.0735  |\n",
      "|    value_loss         | 0.0404   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.02e+03 |\n",
      "|    ep_rew_mean     | 12.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 809      |\n",
      "|    iterations      | 42500    |\n",
      "|    time_elapsed    | 4200     |\n",
      "|    total_timesteps | 3400000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.01e+03 |\n",
      "|    ep_rew_mean        | 12.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 809      |\n",
      "|    iterations         | 42600    |\n",
      "|    time_elapsed       | 4208     |\n",
      "|    total_timesteps    | 3408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.805    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42599    |\n",
      "|    policy_loss        | 0.08     |\n",
      "|    value_loss         | 0.0469   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3410000, episode_reward=12.40 +/- 3.01\n",
      "Episode length: 1734.80 +/- 298.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.73e+03 |\n",
      "|    mean_reward        | 12.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3410000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42624    |\n",
      "|    policy_loss        | 0.01     |\n",
      "|    value_loss         | 0.0209   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2e+03    |\n",
      "|    ep_rew_mean        | 12.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 809      |\n",
      "|    iterations         | 42700    |\n",
      "|    time_elapsed       | 4219     |\n",
      "|    total_timesteps    | 3416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.726    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42699    |\n",
      "|    policy_loss        | 0.0173   |\n",
      "|    value_loss         | 0.051    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3420000, episode_reward=8.80 +/- 2.99\n",
      "Episode length: 1872.00 +/- 515.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.87e+03 |\n",
      "|    mean_reward        | 8.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.19    |\n",
      "|    explained_variance | 0.858    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42749    |\n",
      "|    policy_loss        | 0.00899  |\n",
      "|    value_loss         | 0.0404   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.02e+03 |\n",
      "|    ep_rew_mean        | 12.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 809      |\n",
      "|    iterations         | 42800    |\n",
      "|    time_elapsed       | 4231     |\n",
      "|    total_timesteps    | 3424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.763    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42799    |\n",
      "|    policy_loss        | 0.0211   |\n",
      "|    value_loss         | 0.0326   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3430000, episode_reward=8.80 +/- 3.19\n",
      "Episode length: 1711.60 +/- 536.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.71e+03 |\n",
      "|    mean_reward        | 8.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3430000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42874    |\n",
      "|    policy_loss        | -0.0301  |\n",
      "|    value_loss         | 0.0105   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.03e+03 |\n",
      "|    ep_rew_mean        | 12.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 809      |\n",
      "|    iterations         | 42900    |\n",
      "|    time_elapsed       | 4242     |\n",
      "|    total_timesteps    | 3432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42899    |\n",
      "|    policy_loss        | -0.0534  |\n",
      "|    value_loss         | 0.0568   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3440000, episode_reward=11.60 +/- 4.08\n",
      "Episode length: 2051.00 +/- 298.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.05e+03 |\n",
      "|    mean_reward        | 11.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.19    |\n",
      "|    explained_variance | 0.868    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42999    |\n",
      "|    policy_loss        | -0.0141  |\n",
      "|    value_loss         | 0.0394   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.04e+03 |\n",
      "|    ep_rew_mean     | 12.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 808      |\n",
      "|    iterations      | 43000    |\n",
      "|    time_elapsed    | 4253     |\n",
      "|    total_timesteps | 3440000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.06e+03 |\n",
      "|    ep_rew_mean        | 12.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 809      |\n",
      "|    iterations         | 43100    |\n",
      "|    time_elapsed       | 4261     |\n",
      "|    total_timesteps    | 3448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.831    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43099    |\n",
      "|    policy_loss        | -0.0458  |\n",
      "|    value_loss         | 0.0405   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3450000, episode_reward=12.60 +/- 4.96\n",
      "Episode length: 2239.80 +/- 388.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.24e+03 |\n",
      "|    mean_reward        | 12.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43124    |\n",
      "|    policy_loss        | 0.00312  |\n",
      "|    value_loss         | 0.0235   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.04e+03 |\n",
      "|    ep_rew_mean        | 12.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 808      |\n",
      "|    iterations         | 43200    |\n",
      "|    time_elapsed       | 4273     |\n",
      "|    total_timesteps    | 3456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43199    |\n",
      "|    policy_loss        | 0.025    |\n",
      "|    value_loss         | 0.0138   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3460000, episode_reward=11.20 +/- 3.54\n",
      "Episode length: 1985.80 +/- 336.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.99e+03 |\n",
      "|    mean_reward        | 11.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43249    |\n",
      "|    policy_loss        | -0.0596  |\n",
      "|    value_loss         | 0.0461   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.04e+03 |\n",
      "|    ep_rew_mean        | 12.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 808      |\n",
      "|    iterations         | 43300    |\n",
      "|    time_elapsed       | 4285     |\n",
      "|    total_timesteps    | 3464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.9      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43299    |\n",
      "|    policy_loss        | -0.0881  |\n",
      "|    value_loss         | 0.0287   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3470000, episode_reward=10.80 +/- 4.26\n",
      "Episode length: 1921.40 +/- 532.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.92e+03 |\n",
      "|    mean_reward        | 10.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3470000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43374    |\n",
      "|    policy_loss        | 0.00782  |\n",
      "|    value_loss         | 0.0256   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.05e+03 |\n",
      "|    ep_rew_mean        | 12.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 808      |\n",
      "|    iterations         | 43400    |\n",
      "|    time_elapsed       | 4296     |\n",
      "|    total_timesteps    | 3472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.855    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43399    |\n",
      "|    policy_loss        | 0.094    |\n",
      "|    value_loss         | 0.0435   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3480000, episode_reward=20.60 +/- 6.92\n",
      "Episode length: 2461.80 +/- 463.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.46e+03 |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43499    |\n",
      "|    policy_loss        | -0.065   |\n",
      "|    value_loss         | 0.0229   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.08e+03 |\n",
      "|    ep_rew_mean     | 12.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 807      |\n",
      "|    iterations      | 43500    |\n",
      "|    time_elapsed    | 4309     |\n",
      "|    total_timesteps | 3480000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.14e+03 |\n",
      "|    ep_rew_mean        | 12.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 807      |\n",
      "|    iterations         | 43600    |\n",
      "|    time_elapsed       | 4317     |\n",
      "|    total_timesteps    | 3488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.679    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43599    |\n",
      "|    policy_loss        | 0.0245   |\n",
      "|    value_loss         | 0.0423   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3490000, episode_reward=14.40 +/- 4.92\n",
      "Episode length: 2176.40 +/- 601.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.18e+03 |\n",
      "|    mean_reward        | 14.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3490000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.679    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43624    |\n",
      "|    policy_loss        | 0.00132  |\n",
      "|    value_loss         | 0.0323   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.15e+03 |\n",
      "|    ep_rew_mean        | 12.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 807      |\n",
      "|    iterations         | 43700    |\n",
      "|    time_elapsed       | 4329     |\n",
      "|    total_timesteps    | 3496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.834    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43699    |\n",
      "|    policy_loss        | 0.0951   |\n",
      "|    value_loss         | 0.0352   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3500000, episode_reward=16.20 +/- 2.04\n",
      "Episode length: 2041.40 +/- 254.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.04e+03 |\n",
      "|    mean_reward        | 16.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.643    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43749    |\n",
      "|    policy_loss        | 0.0191   |\n",
      "|    value_loss         | 0.0293   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.15e+03 |\n",
      "|    ep_rew_mean        | 13       |\n",
      "| time/                 |          |\n",
      "|    fps                | 807      |\n",
      "|    iterations         | 43800    |\n",
      "|    time_elapsed       | 4341     |\n",
      "|    total_timesteps    | 3504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | 0.716    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43799    |\n",
      "|    policy_loss        | -0.0262  |\n",
      "|    value_loss         | 0.0402   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3510000, episode_reward=11.60 +/- 4.08\n",
      "Episode length: 2199.80 +/- 400.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.2e+03  |\n",
      "|    mean_reward        | 11.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3510000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.65     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43874    |\n",
      "|    policy_loss        | -0.0753  |\n",
      "|    value_loss         | 0.0988   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.18e+03 |\n",
      "|    ep_rew_mean        | 13.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 806      |\n",
      "|    iterations         | 43900    |\n",
      "|    time_elapsed       | 4353     |\n",
      "|    total_timesteps    | 3512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43899    |\n",
      "|    policy_loss        | -0.0298  |\n",
      "|    value_loss         | 0.026    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3520000, episode_reward=15.60 +/- 4.76\n",
      "Episode length: 2348.20 +/- 399.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.35e+03 |\n",
      "|    mean_reward        | 15.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43999    |\n",
      "|    policy_loss        | -0.0965  |\n",
      "|    value_loss         | 0.0285   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.17e+03 |\n",
      "|    ep_rew_mean     | 13.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 806      |\n",
      "|    iterations      | 44000    |\n",
      "|    time_elapsed    | 4366     |\n",
      "|    total_timesteps | 3520000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.15e+03 |\n",
      "|    ep_rew_mean        | 12.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 806      |\n",
      "|    iterations         | 44100    |\n",
      "|    time_elapsed       | 4373     |\n",
      "|    total_timesteps    | 3528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44099    |\n",
      "|    policy_loss        | -0.0226  |\n",
      "|    value_loss         | 0.015    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3530000, episode_reward=12.20 +/- 1.94\n",
      "Episode length: 2275.40 +/- 196.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.28e+03 |\n",
      "|    mean_reward        | 12.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3530000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44124    |\n",
      "|    policy_loss        | -0.0157  |\n",
      "|    value_loss         | 0.0183   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.16e+03 |\n",
      "|    ep_rew_mean        | 13.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 806      |\n",
      "|    iterations         | 44200    |\n",
      "|    time_elapsed       | 4385     |\n",
      "|    total_timesteps    | 3536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.788    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44199    |\n",
      "|    policy_loss        | 0.0173   |\n",
      "|    value_loss         | 0.0256   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3540000, episode_reward=12.20 +/- 4.53\n",
      "Episode length: 2240.20 +/- 657.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.24e+03 |\n",
      "|    mean_reward        | 12.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.826    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44249    |\n",
      "|    policy_loss        | 0.0191   |\n",
      "|    value_loss         | 0.0352   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.11e+03 |\n",
      "|    ep_rew_mean        | 12.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 805      |\n",
      "|    iterations         | 44300    |\n",
      "|    time_elapsed       | 4397     |\n",
      "|    total_timesteps    | 3544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.716    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44299    |\n",
      "|    policy_loss        | -0.0435  |\n",
      "|    value_loss         | 0.0342   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3550000, episode_reward=15.60 +/- 7.96\n",
      "Episode length: 2234.40 +/- 774.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.23e+03 |\n",
      "|    mean_reward        | 15.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.717    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44374    |\n",
      "|    policy_loss        | 0.075    |\n",
      "|    value_loss         | 0.0687   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.15e+03 |\n",
      "|    ep_rew_mean        | 13.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 805      |\n",
      "|    iterations         | 44400    |\n",
      "|    time_elapsed       | 4409     |\n",
      "|    total_timesteps    | 3552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.837    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44399    |\n",
      "|    policy_loss        | -0.0498  |\n",
      "|    value_loss         | 0.0535   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3560000, episode_reward=13.20 +/- 3.19\n",
      "Episode length: 2280.80 +/- 293.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.28e+03 |\n",
      "|    mean_reward        | 13.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.604    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44499    |\n",
      "|    policy_loss        | -0.0292  |\n",
      "|    value_loss         | 0.0789   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.15e+03 |\n",
      "|    ep_rew_mean     | 13.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 805      |\n",
      "|    iterations      | 44500    |\n",
      "|    time_elapsed    | 4422     |\n",
      "|    total_timesteps | 3560000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.19e+03 |\n",
      "|    ep_rew_mean        | 13.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 805      |\n",
      "|    iterations         | 44600    |\n",
      "|    time_elapsed       | 4429     |\n",
      "|    total_timesteps    | 3568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.729    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44599    |\n",
      "|    policy_loss        | -0.0425  |\n",
      "|    value_loss         | 0.0561   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3570000, episode_reward=7.00 +/- 1.90\n",
      "Episode length: 1572.20 +/- 296.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.57e+03 |\n",
      "|    mean_reward        | 7        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3570000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.762    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44624    |\n",
      "|    policy_loss        | -0.0245  |\n",
      "|    value_loss         | 0.04     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.19e+03 |\n",
      "|    ep_rew_mean        | 13.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 805      |\n",
      "|    iterations         | 44700    |\n",
      "|    time_elapsed       | 4440     |\n",
      "|    total_timesteps    | 3576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44699    |\n",
      "|    policy_loss        | -0.0195  |\n",
      "|    value_loss         | 0.0401   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3580000, episode_reward=16.60 +/- 5.08\n",
      "Episode length: 2552.00 +/- 379.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.55e+03 |\n",
      "|    mean_reward        | 16.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.798    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44749    |\n",
      "|    policy_loss        | -0.0469  |\n",
      "|    value_loss         | 0.0633   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.13e+03 |\n",
      "|    ep_rew_mean        | 13       |\n",
      "| time/                 |          |\n",
      "|    fps                | 804      |\n",
      "|    iterations         | 44800    |\n",
      "|    time_elapsed       | 4453     |\n",
      "|    total_timesteps    | 3584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.877    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44799    |\n",
      "|    policy_loss        | 0.00411  |\n",
      "|    value_loss         | 0.0349   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3590000, episode_reward=15.60 +/- 6.12\n",
      "Episode length: 2332.20 +/- 507.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.33e+03 |\n",
      "|    mean_reward        | 15.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3590000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.7      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44874    |\n",
      "|    policy_loss        | -0.00395 |\n",
      "|    value_loss         | 0.0481   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.15e+03 |\n",
      "|    ep_rew_mean        | 13.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 804      |\n",
      "|    iterations         | 44900    |\n",
      "|    time_elapsed       | 4465     |\n",
      "|    total_timesteps    | 3592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44899    |\n",
      "|    policy_loss        | -0.0682  |\n",
      "|    value_loss         | 0.0277   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3600000, episode_reward=7.20 +/- 3.31\n",
      "Episode length: 1756.20 +/- 497.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.76e+03 |\n",
      "|    mean_reward        | 7.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.808    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44999    |\n",
      "|    policy_loss        | 0.0734   |\n",
      "|    value_loss         | 0.0362   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.13e+03 |\n",
      "|    ep_rew_mean     | 13.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 804      |\n",
      "|    iterations      | 45000    |\n",
      "|    time_elapsed    | 4476     |\n",
      "|    total_timesteps | 3600000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.16e+03 |\n",
      "|    ep_rew_mean        | 13.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 804      |\n",
      "|    iterations         | 45100    |\n",
      "|    time_elapsed       | 4484     |\n",
      "|    total_timesteps    | 3608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.723    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45099    |\n",
      "|    policy_loss        | 0.0573   |\n",
      "|    value_loss         | 0.03     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3610000, episode_reward=14.60 +/- 2.58\n",
      "Episode length: 2565.60 +/- 263.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.57e+03 |\n",
      "|    mean_reward        | 14.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3610000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.823    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45124    |\n",
      "|    policy_loss        | 0.0846   |\n",
      "|    value_loss         | 0.0496   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.18e+03 |\n",
      "|    ep_rew_mean        | 13.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 804      |\n",
      "|    iterations         | 45200    |\n",
      "|    time_elapsed       | 4497     |\n",
      "|    total_timesteps    | 3616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.783    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45199    |\n",
      "|    policy_loss        | 0.0196   |\n",
      "|    value_loss         | 0.0319   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3620000, episode_reward=20.00 +/- 8.99\n",
      "Episode length: 2612.40 +/- 273.47\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.61e+03  |\n",
      "|    mean_reward        | 20        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 3620000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.12     |\n",
      "|    explained_variance | 0.86      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 45249     |\n",
      "|    policy_loss        | -0.000537 |\n",
      "|    value_loss         | 0.029     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.17e+03 |\n",
      "|    ep_rew_mean        | 13.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 803      |\n",
      "|    iterations         | 45300    |\n",
      "|    time_elapsed       | 4509     |\n",
      "|    total_timesteps    | 3624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.814    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45299    |\n",
      "|    policy_loss        | -0.0424  |\n",
      "|    value_loss         | 0.0322   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3630000, episode_reward=11.60 +/- 5.99\n",
      "Episode length: 2043.80 +/- 601.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.04e+03 |\n",
      "|    mean_reward        | 11.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3630000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.755    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45374    |\n",
      "|    policy_loss        | -0.0912  |\n",
      "|    value_loss         | 0.0433   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.24e+03 |\n",
      "|    ep_rew_mean        | 14.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 803      |\n",
      "|    iterations         | 45400    |\n",
      "|    time_elapsed       | 4521     |\n",
      "|    total_timesteps    | 3632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.813    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45399    |\n",
      "|    policy_loss        | -0.0765  |\n",
      "|    value_loss         | 0.0333   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3640000, episode_reward=11.80 +/- 4.58\n",
      "Episode length: 2088.20 +/- 347.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.09e+03 |\n",
      "|    mean_reward        | 11.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.804    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45499    |\n",
      "|    policy_loss        | -0.102   |\n",
      "|    value_loss         | 0.0587   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.29e+03 |\n",
      "|    ep_rew_mean     | 14.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 802      |\n",
      "|    iterations      | 45500    |\n",
      "|    time_elapsed    | 4533     |\n",
      "|    total_timesteps | 3640000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.32e+03 |\n",
      "|    ep_rew_mean        | 14.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 803      |\n",
      "|    iterations         | 45600    |\n",
      "|    time_elapsed       | 4541     |\n",
      "|    total_timesteps    | 3648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45599    |\n",
      "|    policy_loss        | -0.0523  |\n",
      "|    value_loss         | 0.0294   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3650000, episode_reward=12.40 +/- 4.84\n",
      "Episode length: 2105.40 +/- 413.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.11e+03 |\n",
      "|    mean_reward        | 12.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45624    |\n",
      "|    policy_loss        | 0.0968   |\n",
      "|    value_loss         | 0.0461   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.35e+03 |\n",
      "|    ep_rew_mean        | 15.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 802      |\n",
      "|    iterations         | 45700    |\n",
      "|    time_elapsed       | 4553     |\n",
      "|    total_timesteps    | 3656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45699    |\n",
      "|    policy_loss        | 0.00481  |\n",
      "|    value_loss         | 0.0193   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3660000, episode_reward=16.20 +/- 7.93\n",
      "Episode length: 2506.00 +/- 664.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.51e+03 |\n",
      "|    mean_reward        | 16.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.805    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45749    |\n",
      "|    policy_loss        | 0.0878   |\n",
      "|    value_loss         | 0.0321   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.38e+03 |\n",
      "|    ep_rew_mean        | 15.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 802      |\n",
      "|    iterations         | 45800    |\n",
      "|    time_elapsed       | 4566     |\n",
      "|    total_timesteps    | 3664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.749    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45799    |\n",
      "|    policy_loss        | -0.0359  |\n",
      "|    value_loss         | 0.0363   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3670000, episode_reward=19.20 +/- 5.60\n",
      "Episode length: 2495.40 +/- 255.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.5e+03  |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3670000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.622    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45874    |\n",
      "|    policy_loss        | 0.00561  |\n",
      "|    value_loss         | 0.0775   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.4e+03  |\n",
      "|    ep_rew_mean        | 16       |\n",
      "| time/                 |          |\n",
      "|    fps                | 801      |\n",
      "|    iterations         | 45900    |\n",
      "|    time_elapsed       | 4579     |\n",
      "|    total_timesteps    | 3672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45899    |\n",
      "|    policy_loss        | -0.0697  |\n",
      "|    value_loss         | 0.0298   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3680000, episode_reward=15.60 +/- 4.50\n",
      "Episode length: 2477.40 +/- 411.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.48e+03 |\n",
      "|    mean_reward        | 15.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.39     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45999    |\n",
      "|    policy_loss        | 0.162    |\n",
      "|    value_loss         | 0.111    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.42e+03 |\n",
      "|    ep_rew_mean     | 16.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 801      |\n",
      "|    iterations      | 46000    |\n",
      "|    time_elapsed    | 4591     |\n",
      "|    total_timesteps | 3680000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.46e+03 |\n",
      "|    ep_rew_mean        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 801      |\n",
      "|    iterations         | 46100    |\n",
      "|    time_elapsed       | 4599     |\n",
      "|    total_timesteps    | 3688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.647    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46099    |\n",
      "|    policy_loss        | -0.035   |\n",
      "|    value_loss         | 0.0802   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3690000, episode_reward=16.40 +/- 5.43\n",
      "Episode length: 2302.20 +/- 532.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.3e+03  |\n",
      "|    mean_reward        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3690000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.971   |\n",
      "|    explained_variance | 0.535    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46124    |\n",
      "|    policy_loss        | -0.0776  |\n",
      "|    value_loss         | 0.092    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.48e+03 |\n",
      "|    ep_rew_mean        | 17       |\n",
      "| time/                 |          |\n",
      "|    fps                | 801      |\n",
      "|    iterations         | 46200    |\n",
      "|    time_elapsed       | 4611     |\n",
      "|    total_timesteps    | 3696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.652    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46199    |\n",
      "|    policy_loss        | -0.0151  |\n",
      "|    value_loss         | 0.116    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3700000, episode_reward=18.40 +/- 5.50\n",
      "Episode length: 2754.40 +/- 297.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.75e+03 |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.782    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46249    |\n",
      "|    policy_loss        | 0.00673  |\n",
      "|    value_loss         | 0.0451   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.48e+03 |\n",
      "|    ep_rew_mean        | 16.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 800      |\n",
      "|    iterations         | 46300    |\n",
      "|    time_elapsed       | 4624     |\n",
      "|    total_timesteps    | 3704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.786    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46299    |\n",
      "|    policy_loss        | 0.0574   |\n",
      "|    value_loss         | 0.0542   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3710000, episode_reward=20.60 +/- 6.50\n",
      "Episode length: 2734.80 +/- 535.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.73e+03 |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3710000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.75     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46374    |\n",
      "|    policy_loss        | -0.0573  |\n",
      "|    value_loss         | 0.0331   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.48e+03 |\n",
      "|    ep_rew_mean        | 16.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 800      |\n",
      "|    iterations         | 46400    |\n",
      "|    time_elapsed       | 4638     |\n",
      "|    total_timesteps    | 3712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.728    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46399    |\n",
      "|    policy_loss        | -0.0728  |\n",
      "|    value_loss         | 0.0488   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3720000, episode_reward=15.20 +/- 4.17\n",
      "Episode length: 2559.00 +/- 505.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.56e+03 |\n",
      "|    mean_reward        | 15.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.764    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46499    |\n",
      "|    policy_loss        | 0.0133   |\n",
      "|    value_loss         | 0.0553   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.54e+03 |\n",
      "|    ep_rew_mean     | 17.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 799      |\n",
      "|    iterations      | 46500    |\n",
      "|    time_elapsed    | 4651     |\n",
      "|    total_timesteps | 3720000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.55e+03 |\n",
      "|    ep_rew_mean        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 800      |\n",
      "|    iterations         | 46600    |\n",
      "|    time_elapsed       | 4658     |\n",
      "|    total_timesteps    | 3728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46599    |\n",
      "|    policy_loss        | 0.023    |\n",
      "|    value_loss         | 0.0331   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3730000, episode_reward=13.40 +/- 4.22\n",
      "Episode length: 2270.20 +/- 384.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.27e+03 |\n",
      "|    mean_reward        | 13.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3730000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.522    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46624    |\n",
      "|    policy_loss        | 0.0448   |\n",
      "|    value_loss         | 0.0527   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.52e+03 |\n",
      "|    ep_rew_mean        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 799      |\n",
      "|    iterations         | 46700    |\n",
      "|    time_elapsed       | 4670     |\n",
      "|    total_timesteps    | 3736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.957   |\n",
      "|    explained_variance | 0.753    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46699    |\n",
      "|    policy_loss        | 0.066    |\n",
      "|    value_loss         | 0.067    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3740000, episode_reward=18.60 +/- 6.71\n",
      "Episode length: 2582.40 +/- 601.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.58e+03 |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.797    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46749    |\n",
      "|    policy_loss        | -0.0246  |\n",
      "|    value_loss         | 0.0468   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.56e+03 |\n",
      "|    ep_rew_mean        | 17.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 799      |\n",
      "|    iterations         | 46800    |\n",
      "|    time_elapsed       | 4683     |\n",
      "|    total_timesteps    | 3744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.845    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46799    |\n",
      "|    policy_loss        | 0.0281   |\n",
      "|    value_loss         | 0.0354   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3750000, episode_reward=21.20 +/- 6.43\n",
      "Episode length: 2755.00 +/- 260.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.753    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46874    |\n",
      "|    policy_loss        | -0.0508  |\n",
      "|    value_loss         | 0.0426   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.51e+03 |\n",
      "|    ep_rew_mean        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 798      |\n",
      "|    iterations         | 46900    |\n",
      "|    time_elapsed       | 4697     |\n",
      "|    total_timesteps    | 3752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.2     |\n",
      "|    explained_variance | 0.794    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46899    |\n",
      "|    policy_loss        | 0.0713   |\n",
      "|    value_loss         | 0.0482   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3760000, episode_reward=14.40 +/- 5.24\n",
      "Episode length: 2528.40 +/- 553.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.53e+03 |\n",
      "|    mean_reward        | 14.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.729    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46999    |\n",
      "|    policy_loss        | -0.0259  |\n",
      "|    value_loss         | 0.0568   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.59e+03 |\n",
      "|    ep_rew_mean     | 17.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 798      |\n",
      "|    iterations      | 47000    |\n",
      "|    time_elapsed    | 4709     |\n",
      "|    total_timesteps | 3760000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.63e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 798      |\n",
      "|    iterations         | 47100    |\n",
      "|    time_elapsed       | 4717     |\n",
      "|    total_timesteps    | 3768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.798    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47099    |\n",
      "|    policy_loss        | -0.0158  |\n",
      "|    value_loss         | 0.0398   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3770000, episode_reward=22.00 +/- 3.95\n",
      "Episode length: 2979.60 +/- 371.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.98e+03 |\n",
      "|    mean_reward        | 22       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3770000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47124    |\n",
      "|    policy_loss        | -0.049   |\n",
      "|    value_loss         | 0.0149   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.63e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 798      |\n",
      "|    iterations         | 47200    |\n",
      "|    time_elapsed       | 4731     |\n",
      "|    total_timesteps    | 3776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.725    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47199    |\n",
      "|    policy_loss        | -0.0265  |\n",
      "|    value_loss         | 0.0766   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3780000, episode_reward=17.40 +/- 5.64\n",
      "Episode length: 2775.80 +/- 367.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.78e+03 |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.863    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47249    |\n",
      "|    policy_loss        | -0.0364  |\n",
      "|    value_loss         | 0.036    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.65e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 797      |\n",
      "|    iterations         | 47300    |\n",
      "|    time_elapsed       | 4744     |\n",
      "|    total_timesteps    | 3784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.772    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47299    |\n",
      "|    policy_loss        | 0.0942   |\n",
      "|    value_loss         | 0.0701   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3790000, episode_reward=23.00 +/- 6.10\n",
      "Episode length: 3195.20 +/- 326.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.2e+03  |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3790000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.593    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47374    |\n",
      "|    policy_loss        | 0.000757 |\n",
      "|    value_loss         | 0.0569   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.65e+03 |\n",
      "|    ep_rew_mean        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 796      |\n",
      "|    iterations         | 47400    |\n",
      "|    time_elapsed       | 4758     |\n",
      "|    total_timesteps    | 3792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.722    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47399    |\n",
      "|    policy_loss        | -0.103   |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3800000, episode_reward=17.20 +/- 4.07\n",
      "Episode length: 2479.40 +/- 530.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.48e+03 |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.811    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47499    |\n",
      "|    policy_loss        | -0.0466  |\n",
      "|    value_loss         | 0.0486   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.7e+03  |\n",
      "|    ep_rew_mean     | 19.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 796      |\n",
      "|    iterations      | 47500    |\n",
      "|    time_elapsed    | 4771     |\n",
      "|    total_timesteps | 3800000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.72e+03 |\n",
      "|    ep_rew_mean        | 19.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 796      |\n",
      "|    iterations         | 47600    |\n",
      "|    time_elapsed       | 4778     |\n",
      "|    total_timesteps    | 3808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.749    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47599    |\n",
      "|    policy_loss        | 0.0188   |\n",
      "|    value_loss         | 0.0186   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3810000, episode_reward=15.20 +/- 4.35\n",
      "Episode length: 2311.60 +/- 316.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.31e+03 |\n",
      "|    mean_reward        | 15.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3810000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.795    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47624    |\n",
      "|    policy_loss        | 0.0333   |\n",
      "|    value_loss         | 0.0325   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.76e+03 |\n",
      "|    ep_rew_mean        | 20.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 796      |\n",
      "|    iterations         | 47700    |\n",
      "|    time_elapsed       | 4790     |\n",
      "|    total_timesteps    | 3816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.781    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47699    |\n",
      "|    policy_loss        | 0.0497   |\n",
      "|    value_loss         | 0.0315   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3820000, episode_reward=20.20 +/- 3.82\n",
      "Episode length: 2763.80 +/- 661.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.485    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47749    |\n",
      "|    policy_loss        | 0.0475   |\n",
      "|    value_loss         | 0.126    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.79e+03 |\n",
      "|    ep_rew_mean        | 20.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 796      |\n",
      "|    iterations         | 47800    |\n",
      "|    time_elapsed       | 4803     |\n",
      "|    total_timesteps    | 3824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.77     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47799    |\n",
      "|    policy_loss        | -0.1     |\n",
      "|    value_loss         | 0.0566   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3830000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 528.00 +/- 6.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 528      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3830000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.857   |\n",
      "|    explained_variance | 0.672    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47874    |\n",
      "|    policy_loss        | 0.0459   |\n",
      "|    value_loss         | 0.0494   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.65e+03 |\n",
      "|    ep_rew_mean        | 18.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 796      |\n",
      "|    iterations         | 47900    |\n",
      "|    time_elapsed       | 4812     |\n",
      "|    total_timesteps    | 3832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47899    |\n",
      "|    policy_loss        | 0.049    |\n",
      "|    value_loss         | 0.0243   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3840000, episode_reward=14.40 +/- 6.37\n",
      "Episode length: 2364.00 +/- 650.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.36e+03 |\n",
      "|    mean_reward        | 14.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.808    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47999    |\n",
      "|    policy_loss        | -0.134   |\n",
      "|    value_loss         | 0.0552   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.54e+03 |\n",
      "|    ep_rew_mean     | 17.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 795      |\n",
      "|    iterations      | 48000    |\n",
      "|    time_elapsed    | 4824     |\n",
      "|    total_timesteps | 3840000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.52e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 796      |\n",
      "|    iterations         | 48100    |\n",
      "|    time_elapsed       | 4832     |\n",
      "|    total_timesteps    | 3848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.858    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48099    |\n",
      "|    policy_loss        | -0.0397  |\n",
      "|    value_loss         | 0.0355   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3850000, episode_reward=21.20 +/- 4.87\n",
      "Episode length: 2765.40 +/- 494.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.77e+03 |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3850000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48124    |\n",
      "|    policy_loss        | 0.00627  |\n",
      "|    value_loss         | 0.0188   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.56e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 795      |\n",
      "|    iterations         | 48200    |\n",
      "|    time_elapsed       | 4845     |\n",
      "|    total_timesteps    | 3856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.635    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48199    |\n",
      "|    policy_loss        | 0.0167   |\n",
      "|    value_loss         | 0.0675   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3860000, episode_reward=19.40 +/- 7.61\n",
      "Episode length: 2662.80 +/- 391.05\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.66e+03  |\n",
      "|    mean_reward        | 19.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 3860000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.07     |\n",
      "|    explained_variance | 0.812     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 48249     |\n",
      "|    policy_loss        | -0.000781 |\n",
      "|    value_loss         | 0.0251    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.56e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 795      |\n",
      "|    iterations         | 48300    |\n",
      "|    time_elapsed       | 4858     |\n",
      "|    total_timesteps    | 3864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.956   |\n",
      "|    explained_variance | 0.658    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48299    |\n",
      "|    policy_loss        | -0.0177  |\n",
      "|    value_loss         | 0.0425   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3870000, episode_reward=20.40 +/- 7.61\n",
      "Episode length: 2838.00 +/- 507.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.84e+03 |\n",
      "|    mean_reward        | 20.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3870000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.832    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48374    |\n",
      "|    policy_loss        | 0.0125   |\n",
      "|    value_loss         | 0.0541   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.57e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 794      |\n",
      "|    iterations         | 48400    |\n",
      "|    time_elapsed       | 4871     |\n",
      "|    total_timesteps    | 3872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.731    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48399    |\n",
      "|    policy_loss        | -0.0148  |\n",
      "|    value_loss         | 0.0734   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3880000, episode_reward=21.40 +/- 8.96\n",
      "Episode length: 2975.20 +/- 547.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.98e+03 |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.671    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48499    |\n",
      "|    policy_loss        | 0.0691   |\n",
      "|    value_loss         | 0.0704   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.63e+03 |\n",
      "|    ep_rew_mean     | 18.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 794      |\n",
      "|    iterations      | 48500    |\n",
      "|    time_elapsed    | 4884     |\n",
      "|    total_timesteps | 3880000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.64e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 794      |\n",
      "|    iterations         | 48600    |\n",
      "|    time_elapsed       | 4892     |\n",
      "|    total_timesteps    | 3888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.821    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48599    |\n",
      "|    policy_loss        | -0.0288  |\n",
      "|    value_loss         | 0.0698   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3890000, episode_reward=25.20 +/- 4.71\n",
      "Episode length: 2963.40 +/- 574.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.96e+03 |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3890000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48624    |\n",
      "|    policy_loss        | 0.0605   |\n",
      "|    value_loss         | 0.0342   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.71e+03 |\n",
      "|    ep_rew_mean        | 19       |\n",
      "| time/                 |          |\n",
      "|    fps                | 794      |\n",
      "|    iterations         | 48700    |\n",
      "|    time_elapsed       | 4905     |\n",
      "|    total_timesteps    | 3896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.967   |\n",
      "|    explained_variance | 0.837    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48699    |\n",
      "|    policy_loss        | 0.0164   |\n",
      "|    value_loss         | 0.0237   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3900000, episode_reward=25.20 +/- 6.08\n",
      "Episode length: 3392.80 +/- 517.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.39e+03 |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3900000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.794    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48749    |\n",
      "|    policy_loss        | -0.0338  |\n",
      "|    value_loss         | 0.0521   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.8e+03  |\n",
      "|    ep_rew_mean        | 19.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 793      |\n",
      "|    iterations         | 48800    |\n",
      "|    time_elapsed       | 4919     |\n",
      "|    total_timesteps    | 3904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.805    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48799    |\n",
      "|    policy_loss        | 0.0326   |\n",
      "|    value_loss         | 0.0193   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3910000, episode_reward=27.60 +/- 8.75\n",
      "Episode length: 3264.20 +/- 562.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.26e+03 |\n",
      "|    mean_reward        | 27.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3910000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.793    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48874    |\n",
      "|    policy_loss        | -0.0454  |\n",
      "|    value_loss         | 0.025    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.99e+03 |\n",
      "|    ep_rew_mean        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 792      |\n",
      "|    iterations         | 48900    |\n",
      "|    time_elapsed       | 4934     |\n",
      "|    total_timesteps    | 3912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.762    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48899    |\n",
      "|    policy_loss        | -0.0692  |\n",
      "|    value_loss         | 0.076    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3920000, episode_reward=22.60 +/- 3.50\n",
      "Episode length: 2909.00 +/- 582.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.91e+03 |\n",
      "|    mean_reward        | 22.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.764    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48999    |\n",
      "|    policy_loss        | -0.0365  |\n",
      "|    value_loss         | 0.0319   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.08e+03 |\n",
      "|    ep_rew_mean     | 23       |\n",
      "| time/              |          |\n",
      "|    fps             | 792      |\n",
      "|    iterations      | 49000    |\n",
      "|    time_elapsed    | 4947     |\n",
      "|    total_timesteps | 3920000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.07e+03 |\n",
      "|    ep_rew_mean        | 22.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 792      |\n",
      "|    iterations         | 49100    |\n",
      "|    time_elapsed       | 4955     |\n",
      "|    total_timesteps    | 3928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.985   |\n",
      "|    explained_variance | 0.628    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49099    |\n",
      "|    policy_loss        | 0.0652   |\n",
      "|    value_loss         | 0.0374   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3930000, episode_reward=21.20 +/- 9.41\n",
      "Episode length: 2820.60 +/- 725.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.82e+03 |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3930000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.766    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49124    |\n",
      "|    policy_loss        | 0.00315  |\n",
      "|    value_loss         | 0.05     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.13e+03 |\n",
      "|    ep_rew_mean        | 23.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 792      |\n",
      "|    iterations         | 49200    |\n",
      "|    time_elapsed       | 4968     |\n",
      "|    total_timesteps    | 3936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.636    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49199    |\n",
      "|    policy_loss        | 0.0355   |\n",
      "|    value_loss         | 0.0427   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3940000, episode_reward=21.40 +/- 4.13\n",
      "Episode length: 2682.00 +/- 472.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.68e+03 |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.989   |\n",
      "|    explained_variance | 0.681    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49249    |\n",
      "|    policy_loss        | 0.0863   |\n",
      "|    value_loss         | 0.0511   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.15e+03 |\n",
      "|    ep_rew_mean        | 23.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 791      |\n",
      "|    iterations         | 49300    |\n",
      "|    time_elapsed       | 4981     |\n",
      "|    total_timesteps    | 3944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.976   |\n",
      "|    explained_variance | 0.543    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49299    |\n",
      "|    policy_loss        | -0.0194  |\n",
      "|    value_loss         | 0.0823   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3950000, episode_reward=22.40 +/- 12.03\n",
      "Episode length: 3224.00 +/- 929.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.22e+03 |\n",
      "|    mean_reward        | 22.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3950000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49374    |\n",
      "|    policy_loss        | 0.0182   |\n",
      "|    value_loss         | 0.0344   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.17e+03 |\n",
      "|    ep_rew_mean        | 24.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 790      |\n",
      "|    iterations         | 49400    |\n",
      "|    time_elapsed       | 4996     |\n",
      "|    total_timesteps    | 3952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.731    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49399    |\n",
      "|    policy_loss        | -0.0328  |\n",
      "|    value_loss         | 0.0433   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3960000, episode_reward=33.20 +/- 10.26\n",
      "Episode length: 3993.80 +/- 877.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.99e+03 |\n",
      "|    mean_reward        | 33.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.826    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49499    |\n",
      "|    policy_loss        | -0.0439  |\n",
      "|    value_loss         | 0.0815   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.2e+03  |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 790      |\n",
      "|    iterations      | 49500    |\n",
      "|    time_elapsed    | 5011     |\n",
      "|    total_timesteps | 3960000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.18e+03 |\n",
      "|    ep_rew_mean        | 24.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 790      |\n",
      "|    iterations         | 49600    |\n",
      "|    time_elapsed       | 5019     |\n",
      "|    total_timesteps    | 3968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.655    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49599    |\n",
      "|    policy_loss        | 0.0618   |\n",
      "|    value_loss         | 0.0259   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3970000, episode_reward=26.20 +/- 6.68\n",
      "Episode length: 3785.20 +/- 553.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.79e+03 |\n",
      "|    mean_reward        | 26.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3970000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.535    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49624    |\n",
      "|    policy_loss        | 0.0106   |\n",
      "|    value_loss         | 0.0297   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.16e+03 |\n",
      "|    ep_rew_mean        | 24.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 789      |\n",
      "|    iterations         | 49700    |\n",
      "|    time_elapsed       | 5034     |\n",
      "|    total_timesteps    | 3976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49699    |\n",
      "|    policy_loss        | -0.019   |\n",
      "|    value_loss         | 0.0242   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3980000, episode_reward=24.20 +/- 2.56\n",
      "Episode length: 3512.00 +/- 440.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.51e+03 |\n",
      "|    mean_reward        | 24.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.798    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49749    |\n",
      "|    policy_loss        | 0.049    |\n",
      "|    value_loss         | 0.0264   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.13e+03 |\n",
      "|    ep_rew_mean        | 24       |\n",
      "| time/                 |          |\n",
      "|    fps                | 789      |\n",
      "|    iterations         | 49800    |\n",
      "|    time_elapsed       | 5049     |\n",
      "|    total_timesteps    | 3984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49799    |\n",
      "|    policy_loss        | -0.0317  |\n",
      "|    value_loss         | 0.0146   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3990000, episode_reward=12.20 +/- 2.99\n",
      "Episode length: 2258.20 +/- 374.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.26e+03 |\n",
      "|    mean_reward        | 12.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3990000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.89     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49874    |\n",
      "|    policy_loss        | 0.00329  |\n",
      "|    value_loss         | 0.0146   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.1e+03  |\n",
      "|    ep_rew_mean        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 788      |\n",
      "|    iterations         | 49900    |\n",
      "|    time_elapsed       | 5061     |\n",
      "|    total_timesteps    | 3992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.766    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49899    |\n",
      "|    policy_loss        | 0.00384  |\n",
      "|    value_loss         | 0.0191   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4000000, episode_reward=18.00 +/- 5.44\n",
      "Episode length: 2621.20 +/- 540.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.62e+03 |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.781    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49999    |\n",
      "|    policy_loss        | 0.0172   |\n",
      "|    value_loss         | 0.0314   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.06e+03 |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 788      |\n",
      "|    iterations      | 50000    |\n",
      "|    time_elapsed    | 5073     |\n",
      "|    total_timesteps | 4000000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.98e+03 |\n",
      "|    ep_rew_mean        | 22.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 788      |\n",
      "|    iterations         | 50100    |\n",
      "|    time_elapsed       | 5081     |\n",
      "|    total_timesteps    | 4008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50099    |\n",
      "|    policy_loss        | 0.0224   |\n",
      "|    value_loss         | 0.0308   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4010000, episode_reward=23.60 +/- 4.45\n",
      "Episode length: 2428.80 +/- 335.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.43e+03 |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4010000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.857    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50124    |\n",
      "|    policy_loss        | 0.0201   |\n",
      "|    value_loss         | 0.019    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3e+03    |\n",
      "|    ep_rew_mean        | 22.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 788      |\n",
      "|    iterations         | 50200    |\n",
      "|    time_elapsed       | 5093     |\n",
      "|    total_timesteps    | 4016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.779    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50199    |\n",
      "|    policy_loss        | 0.0271   |\n",
      "|    value_loss         | 0.0376   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4020000, episode_reward=25.80 +/- 8.57\n",
      "Episode length: 3387.80 +/- 834.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.39e+03 |\n",
      "|    mean_reward        | 25.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.545    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50249    |\n",
      "|    policy_loss        | 0.107    |\n",
      "|    value_loss         | 0.0903   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.99e+03 |\n",
      "|    ep_rew_mean        | 22.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 787      |\n",
      "|    iterations         | 50300    |\n",
      "|    time_elapsed       | 5108     |\n",
      "|    total_timesteps    | 4024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.837    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50299    |\n",
      "|    policy_loss        | 0.0225   |\n",
      "|    value_loss         | 0.0282   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4030000, episode_reward=22.80 +/- 4.26\n",
      "Episode length: 2798.40 +/- 327.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.8e+03  |\n",
      "|    mean_reward        | 22.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4030000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.772    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50374    |\n",
      "|    policy_loss        | -0.0278  |\n",
      "|    value_loss         | 0.0544   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.01e+03 |\n",
      "|    ep_rew_mean        | 22.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 787      |\n",
      "|    iterations         | 50400    |\n",
      "|    time_elapsed       | 5121     |\n",
      "|    total_timesteps    | 4032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.712    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50399    |\n",
      "|    policy_loss        | 0.0727   |\n",
      "|    value_loss         | 0.0482   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4040000, episode_reward=24.80 +/- 8.84\n",
      "Episode length: 3027.60 +/- 490.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.03e+03 |\n",
      "|    mean_reward        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.845    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50499    |\n",
      "|    policy_loss        | 0.0688   |\n",
      "|    value_loss         | 0.0362   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.01e+03 |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 786      |\n",
      "|    iterations      | 50500    |\n",
      "|    time_elapsed    | 5135     |\n",
      "|    total_timesteps | 4040000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.02e+03 |\n",
      "|    ep_rew_mean        | 22.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 787      |\n",
      "|    iterations         | 50600    |\n",
      "|    time_elapsed       | 5142     |\n",
      "|    total_timesteps    | 4048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.842    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50599    |\n",
      "|    policy_loss        | -0.0247  |\n",
      "|    value_loss         | 0.0371   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4050000, episode_reward=25.00 +/- 8.72\n",
      "Episode length: 3031.20 +/- 668.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.03e+03 |\n",
      "|    mean_reward        | 25       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.755    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50624    |\n",
      "|    policy_loss        | -0.0427  |\n",
      "|    value_loss         | 0.105    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.06e+03 |\n",
      "|    ep_rew_mean        | 23.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 786      |\n",
      "|    iterations         | 50700    |\n",
      "|    time_elapsed       | 5156     |\n",
      "|    total_timesteps    | 4056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.764    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50699    |\n",
      "|    policy_loss        | -0.00191 |\n",
      "|    value_loss         | 0.027    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4060000, episode_reward=20.80 +/- 6.55\n",
      "Episode length: 2627.40 +/- 655.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.63e+03 |\n",
      "|    mean_reward        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.833    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50749    |\n",
      "|    policy_loss        | 0.0913   |\n",
      "|    value_loss         | 0.0487   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.03e+03 |\n",
      "|    ep_rew_mean        | 24       |\n",
      "| time/                 |          |\n",
      "|    fps                | 786      |\n",
      "|    iterations         | 50800    |\n",
      "|    time_elapsed       | 5169     |\n",
      "|    total_timesteps    | 4064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.999   |\n",
      "|    explained_variance | 0.515    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50799    |\n",
      "|    policy_loss        | -0.0116  |\n",
      "|    value_loss         | 0.101    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4070000, episode_reward=19.80 +/- 6.24\n",
      "Episode length: 2851.20 +/- 596.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.85e+03 |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4070000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50874    |\n",
      "|    policy_loss        | -0.00468 |\n",
      "|    value_loss         | 0.0191   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.09e+03 |\n",
      "|    ep_rew_mean        | 24.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 785      |\n",
      "|    iterations         | 50900    |\n",
      "|    time_elapsed       | 5182     |\n",
      "|    total_timesteps    | 4072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.974   |\n",
      "|    explained_variance | 0.831    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50899    |\n",
      "|    policy_loss        | -0.0385  |\n",
      "|    value_loss         | 0.0171   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4080000, episode_reward=21.20 +/- 8.23\n",
      "Episode length: 2868.40 +/- 769.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.87e+03 |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.83     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50999    |\n",
      "|    policy_loss        | 0.0214   |\n",
      "|    value_loss         | 0.0163   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.11e+03 |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 785      |\n",
      "|    iterations      | 51000    |\n",
      "|    time_elapsed    | 5195     |\n",
      "|    total_timesteps | 4080000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.1e+03  |\n",
      "|    ep_rew_mean        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 785      |\n",
      "|    iterations         | 51100    |\n",
      "|    time_elapsed       | 5203     |\n",
      "|    total_timesteps    | 4088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.989   |\n",
      "|    explained_variance | 0.536    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51099    |\n",
      "|    policy_loss        | 0.0402   |\n",
      "|    value_loss         | 0.0636   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4090000, episode_reward=26.40 +/- 10.33\n",
      "Episode length: 2828.80 +/- 792.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.83e+03 |\n",
      "|    mean_reward        | 26.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4090000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.724    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51124    |\n",
      "|    policy_loss        | 0.00956  |\n",
      "|    value_loss         | 0.0376   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.08e+03 |\n",
      "|    ep_rew_mean        | 25.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 785      |\n",
      "|    iterations         | 51200    |\n",
      "|    time_elapsed       | 5216     |\n",
      "|    total_timesteps    | 4096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.507    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51199    |\n",
      "|    policy_loss        | -0.102   |\n",
      "|    value_loss         | 0.0698   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4100000, episode_reward=27.00 +/- 13.11\n",
      "Episode length: 3109.80 +/- 1224.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.11e+03 |\n",
      "|    mean_reward        | 27       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.543    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51249    |\n",
      "|    policy_loss        | 0.157    |\n",
      "|    value_loss         | 0.064    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.09e+03 |\n",
      "|    ep_rew_mean        | 25.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 784      |\n",
      "|    iterations         | 51300    |\n",
      "|    time_elapsed       | 5230     |\n",
      "|    total_timesteps    | 4104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.643    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51299    |\n",
      "|    policy_loss        | -0.0226  |\n",
      "|    value_loss         | 0.035    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4110000, episode_reward=29.60 +/- 9.69\n",
      "Episode length: 3681.00 +/- 857.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.68e+03 |\n",
      "|    mean_reward        | 29.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4110000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51374    |\n",
      "|    policy_loss        | 0.0131   |\n",
      "|    value_loss         | 0.0259   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.08e+03 |\n",
      "|    ep_rew_mean        | 25.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 783      |\n",
      "|    iterations         | 51400    |\n",
      "|    time_elapsed       | 5245     |\n",
      "|    total_timesteps    | 4112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.641    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51399    |\n",
      "|    policy_loss        | -0.0137  |\n",
      "|    value_loss         | 0.0735   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4120000, episode_reward=26.60 +/- 5.68\n",
      "Episode length: 3427.60 +/- 751.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.43e+03 |\n",
      "|    mean_reward        | 26.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.661    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51499    |\n",
      "|    policy_loss        | 0.0228   |\n",
      "|    value_loss         | 0.0503   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.09e+03 |\n",
      "|    ep_rew_mean     | 25.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 783      |\n",
      "|    iterations      | 51500    |\n",
      "|    time_elapsed    | 5259     |\n",
      "|    total_timesteps | 4120000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.05e+03 |\n",
      "|    ep_rew_mean        | 24.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 783      |\n",
      "|    iterations         | 51600    |\n",
      "|    time_elapsed       | 5267     |\n",
      "|    total_timesteps    | 4128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.603    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51599    |\n",
      "|    policy_loss        | 0.00142  |\n",
      "|    value_loss         | 0.0868   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4130000, episode_reward=39.60 +/- 18.43\n",
      "Episode length: 4190.80 +/- 1179.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.19e+03 |\n",
      "|    mean_reward        | 39.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4130000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.683    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51624    |\n",
      "|    policy_loss        | 0.00991  |\n",
      "|    value_loss         | 0.036    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.06e+03 |\n",
      "|    ep_rew_mean        | 24.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 782      |\n",
      "|    iterations         | 51700    |\n",
      "|    time_elapsed       | 5283     |\n",
      "|    total_timesteps    | 4136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.72     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51699    |\n",
      "|    policy_loss        | -0.124   |\n",
      "|    value_loss         | 0.126    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4140000, episode_reward=42.20 +/- 14.02\n",
      "Episode length: 4308.60 +/- 1103.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.31e+03 |\n",
      "|    mean_reward        | 42.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51749    |\n",
      "|    policy_loss        | 0.0381   |\n",
      "|    value_loss         | 0.0178   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.1e+03  |\n",
      "|    ep_rew_mean        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 781      |\n",
      "|    iterations         | 51800    |\n",
      "|    time_elapsed       | 5299     |\n",
      "|    total_timesteps    | 4144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.689    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51799    |\n",
      "|    policy_loss        | -0.0406  |\n",
      "|    value_loss         | 0.104    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4150000, episode_reward=25.60 +/- 5.00\n",
      "Episode length: 3222.20 +/- 584.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.22e+03 |\n",
      "|    mean_reward        | 25.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.714    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51874    |\n",
      "|    policy_loss        | -0.00765 |\n",
      "|    value_loss         | 0.0596   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.12e+03 |\n",
      "|    ep_rew_mean        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 781      |\n",
      "|    iterations         | 51900    |\n",
      "|    time_elapsed       | 5313     |\n",
      "|    total_timesteps    | 4152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.729    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51899    |\n",
      "|    policy_loss        | -0.0538  |\n",
      "|    value_loss         | 0.0612   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4160000, episode_reward=31.20 +/- 6.08\n",
      "Episode length: 3898.40 +/- 628.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.9e+03  |\n",
      "|    mean_reward        | 31.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.582    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51999    |\n",
      "|    policy_loss        | -0.00832 |\n",
      "|    value_loss         | 0.0862   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.19e+03 |\n",
      "|    ep_rew_mean     | 25.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 780      |\n",
      "|    iterations      | 52000    |\n",
      "|    time_elapsed    | 5329     |\n",
      "|    total_timesteps | 4160000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.23e+03 |\n",
      "|    ep_rew_mean        | 25.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 780      |\n",
      "|    iterations         | 52100    |\n",
      "|    time_elapsed       | 5336     |\n",
      "|    total_timesteps    | 4168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.957   |\n",
      "|    explained_variance | 0.642    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52099    |\n",
      "|    policy_loss        | 0.0657   |\n",
      "|    value_loss         | 0.0541   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4170000, episode_reward=19.00 +/- 8.49\n",
      "Episode length: 2704.60 +/- 419.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.7e+03  |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4170000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.64     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52124    |\n",
      "|    policy_loss        | -0.0266  |\n",
      "|    value_loss         | 0.0277   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.19e+03 |\n",
      "|    ep_rew_mean        | 25       |\n",
      "| time/                 |          |\n",
      "|    fps                | 780      |\n",
      "|    iterations         | 52200    |\n",
      "|    time_elapsed       | 5349     |\n",
      "|    total_timesteps    | 4176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.638    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52199    |\n",
      "|    policy_loss        | 0.0571   |\n",
      "|    value_loss         | 0.0624   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4180000, episode_reward=26.00 +/- 10.71\n",
      "Episode length: 3191.40 +/- 811.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.19e+03 |\n",
      "|    mean_reward        | 26       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.723    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52249    |\n",
      "|    policy_loss        | 0.0431   |\n",
      "|    value_loss         | 0.0335   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.26e+03 |\n",
      "|    ep_rew_mean        | 26.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 780      |\n",
      "|    iterations         | 52300    |\n",
      "|    time_elapsed       | 5364     |\n",
      "|    total_timesteps    | 4184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.721    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52299    |\n",
      "|    policy_loss        | -0.0956  |\n",
      "|    value_loss         | 0.0663   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4190000, episode_reward=32.80 +/- 7.03\n",
      "Episode length: 3576.20 +/- 888.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.58e+03 |\n",
      "|    mean_reward        | 32.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4190000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.678    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52374    |\n",
      "|    policy_loss        | -0.0841  |\n",
      "|    value_loss         | 0.133    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.25e+03 |\n",
      "|    ep_rew_mean        | 26       |\n",
      "| time/                 |          |\n",
      "|    fps                | 779      |\n",
      "|    iterations         | 52400    |\n",
      "|    time_elapsed       | 5378     |\n",
      "|    total_timesteps    | 4192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.973   |\n",
      "|    explained_variance | 0.631    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52399    |\n",
      "|    policy_loss        | -0.0428  |\n",
      "|    value_loss         | 0.088    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4200000, episode_reward=27.40 +/- 13.12\n",
      "Episode length: 3098.80 +/- 1091.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.1e+03  |\n",
      "|    mean_reward        | 27.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.991   |\n",
      "|    explained_variance | 0.653    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52499    |\n",
      "|    policy_loss        | -0.0442  |\n",
      "|    value_loss         | 0.0825   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.27e+03 |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 52500    |\n",
      "|    time_elapsed    | 5392     |\n",
      "|    total_timesteps | 4200000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.25e+03 |\n",
      "|    ep_rew_mean        | 26       |\n",
      "| time/                 |          |\n",
      "|    fps                | 779      |\n",
      "|    iterations         | 52600    |\n",
      "|    time_elapsed       | 5400     |\n",
      "|    total_timesteps    | 4208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.694    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52599    |\n",
      "|    policy_loss        | -0.0648  |\n",
      "|    value_loss         | 0.109    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4210000, episode_reward=34.80 +/- 11.23\n",
      "Episode length: 3894.60 +/- 981.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.89e+03 |\n",
      "|    mean_reward        | 34.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4210000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.766    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52624    |\n",
      "|    policy_loss        | 0.0365   |\n",
      "|    value_loss         | 0.0423   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.29e+03 |\n",
      "|    ep_rew_mean        | 26.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 778      |\n",
      "|    iterations         | 52700    |\n",
      "|    time_elapsed       | 5415     |\n",
      "|    total_timesteps    | 4216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.626    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52699    |\n",
      "|    policy_loss        | -0.0818  |\n",
      "|    value_loss         | 0.0646   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4220000, episode_reward=44.40 +/- 16.27\n",
      "Episode length: 4477.20 +/- 1011.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.48e+03 |\n",
      "|    mean_reward        | 44.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.683    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52749    |\n",
      "|    policy_loss        | 0.00883  |\n",
      "|    value_loss         | 0.0969   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.34e+03 |\n",
      "|    ep_rew_mean        | 27.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 777      |\n",
      "|    iterations         | 52800    |\n",
      "|    time_elapsed       | 5432     |\n",
      "|    total_timesteps    | 4224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.613    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52799    |\n",
      "|    policy_loss        | 0.134    |\n",
      "|    value_loss         | 0.0792   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4230000, episode_reward=36.20 +/- 11.34\n",
      "Episode length: 4202.40 +/- 1145.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.2e+03  |\n",
      "|    mean_reward        | 36.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4230000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.799    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52874    |\n",
      "|    policy_loss        | 0.0915   |\n",
      "|    value_loss         | 0.0376   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.36e+03 |\n",
      "|    ep_rew_mean        | 27.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 776      |\n",
      "|    iterations         | 52900    |\n",
      "|    time_elapsed       | 5448     |\n",
      "|    total_timesteps    | 4232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.597    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52899    |\n",
      "|    policy_loss        | -0.0464  |\n",
      "|    value_loss         | 0.0479   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4240000, episode_reward=24.40 +/- 9.33\n",
      "Episode length: 3207.80 +/- 682.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.21e+03 |\n",
      "|    mean_reward        | 24.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.865    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52999    |\n",
      "|    policy_loss        | 0.0437   |\n",
      "|    value_loss         | 0.0475   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.36e+03 |\n",
      "|    ep_rew_mean     | 27.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 53000    |\n",
      "|    time_elapsed    | 5462     |\n",
      "|    total_timesteps | 4240000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.39e+03 |\n",
      "|    ep_rew_mean        | 28.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 776      |\n",
      "|    iterations         | 53100    |\n",
      "|    time_elapsed       | 5470     |\n",
      "|    total_timesteps    | 4248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.977   |\n",
      "|    explained_variance | 0.664    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53099    |\n",
      "|    policy_loss        | -0.0136  |\n",
      "|    value_loss         | 0.0394   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4250000, episode_reward=29.80 +/- 8.49\n",
      "Episode length: 3537.80 +/- 429.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.54e+03 |\n",
      "|    mean_reward        | 29.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.861    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53124    |\n",
      "|    policy_loss        | 0.000965 |\n",
      "|    value_loss         | 0.0208   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.4e+03  |\n",
      "|    ep_rew_mean        | 28.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 775      |\n",
      "|    iterations         | 53200    |\n",
      "|    time_elapsed       | 5484     |\n",
      "|    total_timesteps    | 4256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.754    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53199    |\n",
      "|    policy_loss        | -0.0524  |\n",
      "|    value_loss         | 0.0669   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4260000, episode_reward=22.20 +/- 7.03\n",
      "Episode length: 2700.80 +/- 734.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.7e+03  |\n",
      "|    mean_reward        | 22.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.733    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53249    |\n",
      "|    policy_loss        | 0.0203   |\n",
      "|    value_loss         | 0.0779   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.51e+03 |\n",
      "|    ep_rew_mean        | 29.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 775      |\n",
      "|    iterations         | 53300    |\n",
      "|    time_elapsed       | 5497     |\n",
      "|    total_timesteps    | 4264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.863    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53299    |\n",
      "|    policy_loss        | 0.00533  |\n",
      "|    value_loss         | 0.0251   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4270000, episode_reward=36.60 +/- 18.56\n",
      "Episode length: 3806.60 +/- 621.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.81e+03 |\n",
      "|    mean_reward        | 36.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4270000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.618    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53374    |\n",
      "|    policy_loss        | 0.0797   |\n",
      "|    value_loss         | 0.0683   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.51e+03 |\n",
      "|    ep_rew_mean        | 29.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 774      |\n",
      "|    iterations         | 53400    |\n",
      "|    time_elapsed       | 5512     |\n",
      "|    total_timesteps    | 4272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.861    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53399    |\n",
      "|    policy_loss        | -0.0274  |\n",
      "|    value_loss         | 0.0369   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4280000, episode_reward=19.60 +/- 2.15\n",
      "Episode length: 2867.00 +/- 360.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.87e+03 |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.7      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53499    |\n",
      "|    policy_loss        | 0.0287   |\n",
      "|    value_loss         | 0.0535   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.59e+03 |\n",
      "|    ep_rew_mean     | 30.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 53500    |\n",
      "|    time_elapsed    | 5526     |\n",
      "|    total_timesteps | 4280000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.63e+03 |\n",
      "|    ep_rew_mean        | 31.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 774      |\n",
      "|    iterations         | 53600    |\n",
      "|    time_elapsed       | 5533     |\n",
      "|    total_timesteps    | 4288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53599    |\n",
      "|    policy_loss        | 0.0532   |\n",
      "|    value_loss         | 0.037    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4290000, episode_reward=35.20 +/- 10.01\n",
      "Episode length: 3858.80 +/- 666.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 35.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4290000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.694    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53624    |\n",
      "|    policy_loss        | 0.0644   |\n",
      "|    value_loss         | 0.0436   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.63e+03 |\n",
      "|    ep_rew_mean        | 31.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 774      |\n",
      "|    iterations         | 53700    |\n",
      "|    time_elapsed       | 5548     |\n",
      "|    total_timesteps    | 4296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.842    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53699    |\n",
      "|    policy_loss        | -0.0546  |\n",
      "|    value_loss         | 0.0283   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4300000, episode_reward=28.80 +/- 9.68\n",
      "Episode length: 3306.80 +/- 909.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.31e+03 |\n",
      "|    mean_reward        | 28.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.786    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53749    |\n",
      "|    policy_loss        | 0.0916   |\n",
      "|    value_loss         | 0.0428   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.63e+03 |\n",
      "|    ep_rew_mean        | 31.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 773      |\n",
      "|    iterations         | 53800    |\n",
      "|    time_elapsed       | 5563     |\n",
      "|    total_timesteps    | 4304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.768    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53799    |\n",
      "|    policy_loss        | 0.0414   |\n",
      "|    value_loss         | 0.0239   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4310000, episode_reward=44.20 +/- 14.02\n",
      "Episode length: 4287.60 +/- 627.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.29e+03 |\n",
      "|    mean_reward        | 44.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4310000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.773    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53874    |\n",
      "|    policy_loss        | 0.0392   |\n",
      "|    value_loss         | 0.043    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.59e+03 |\n",
      "|    ep_rew_mean        | 31.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 772      |\n",
      "|    iterations         | 53900    |\n",
      "|    time_elapsed       | 5579     |\n",
      "|    total_timesteps    | 4312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.988   |\n",
      "|    explained_variance | 0.641    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53899    |\n",
      "|    policy_loss        | -0.135   |\n",
      "|    value_loss         | 0.213    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4320000, episode_reward=45.40 +/- 17.23\n",
      "Episode length: 4434.40 +/- 1145.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.43e+03 |\n",
      "|    mean_reward        | 45.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.796    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53999    |\n",
      "|    policy_loss        | -0.075   |\n",
      "|    value_loss         | 0.0237   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.63e+03 |\n",
      "|    ep_rew_mean     | 31.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 771      |\n",
      "|    iterations      | 54000    |\n",
      "|    time_elapsed    | 5596     |\n",
      "|    total_timesteps | 4320000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.63e+03 |\n",
      "|    ep_rew_mean        | 31.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 772      |\n",
      "|    iterations         | 54100    |\n",
      "|    time_elapsed       | 5603     |\n",
      "|    total_timesteps    | 4328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.719    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54099    |\n",
      "|    policy_loss        | -0.055   |\n",
      "|    value_loss         | 0.0337   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4330000, episode_reward=29.40 +/- 18.25\n",
      "Episode length: 3533.40 +/- 1281.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.53e+03 |\n",
      "|    mean_reward        | 29.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4330000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.755    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54124    |\n",
      "|    policy_loss        | -0.0397  |\n",
      "|    value_loss         | 0.0862   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.65e+03 |\n",
      "|    ep_rew_mean        | 32       |\n",
      "| time/                 |          |\n",
      "|    fps                | 771      |\n",
      "|    iterations         | 54200    |\n",
      "|    time_elapsed       | 5618     |\n",
      "|    total_timesteps    | 4336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.697    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54199    |\n",
      "|    policy_loss        | 0.0767   |\n",
      "|    value_loss         | 0.0969   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4340000, episode_reward=32.40 +/- 17.14\n",
      "Episode length: 3349.60 +/- 1101.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.35e+03 |\n",
      "|    mean_reward        | 32.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.58     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54249    |\n",
      "|    policy_loss        | 0.0279   |\n",
      "|    value_loss         | 0.0451   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.67e+03 |\n",
      "|    ep_rew_mean        | 32.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 771      |\n",
      "|    iterations         | 54300    |\n",
      "|    time_elapsed       | 5632     |\n",
      "|    total_timesteps    | 4344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54299    |\n",
      "|    policy_loss        | -0.014   |\n",
      "|    value_loss         | 0.0397   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4350000, episode_reward=35.60 +/- 5.75\n",
      "Episode length: 4153.40 +/- 493.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.15e+03 |\n",
      "|    mean_reward        | 35.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.643    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54374    |\n",
      "|    policy_loss        | -0.105   |\n",
      "|    value_loss         | 0.199    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.69e+03 |\n",
      "|    ep_rew_mean        | 32.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 770      |\n",
      "|    iterations         | 54400    |\n",
      "|    time_elapsed       | 5648     |\n",
      "|    total_timesteps    | 4352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.92    |\n",
      "|    explained_variance | 0.812    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54399    |\n",
      "|    policy_loss        | 0.00328  |\n",
      "|    value_loss         | 0.0246   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4360000, episode_reward=35.20 +/- 14.46\n",
      "Episode length: 3662.20 +/- 740.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.66e+03 |\n",
      "|    mean_reward        | 35.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.967   |\n",
      "|    explained_variance | 0.528    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54499    |\n",
      "|    policy_loss        | 0.0038   |\n",
      "|    value_loss         | 0.14     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.72e+03 |\n",
      "|    ep_rew_mean     | 33.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 769      |\n",
      "|    iterations      | 54500    |\n",
      "|    time_elapsed    | 5663     |\n",
      "|    total_timesteps | 4360000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | 34.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 770      |\n",
      "|    iterations         | 54600    |\n",
      "|    time_elapsed       | 5671     |\n",
      "|    total_timesteps    | 4368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.946   |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54599    |\n",
      "|    policy_loss        | 0.00249  |\n",
      "|    value_loss         | 0.0262   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4370000, episode_reward=23.00 +/- 12.41\n",
      "Episode length: 3113.60 +/- 1038.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.11e+03 |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4370000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.665    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54624    |\n",
      "|    policy_loss        | 0.0218   |\n",
      "|    value_loss         | 0.0443   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.85e+03 |\n",
      "|    ep_rew_mean        | 35.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 769      |\n",
      "|    iterations         | 54700    |\n",
      "|    time_elapsed       | 5685     |\n",
      "|    total_timesteps    | 4376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.957   |\n",
      "|    explained_variance | 0.766    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54699    |\n",
      "|    policy_loss        | -0.0268  |\n",
      "|    value_loss         | 0.0478   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4380000, episode_reward=31.80 +/- 9.52\n",
      "Episode length: 3706.80 +/- 915.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.71e+03 |\n",
      "|    mean_reward        | 31.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54749    |\n",
      "|    policy_loss        | 0.015    |\n",
      "|    value_loss         | 0.0407   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.85e+03 |\n",
      "|    ep_rew_mean        | 35.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 769      |\n",
      "|    iterations         | 54800    |\n",
      "|    time_elapsed       | 5700     |\n",
      "|    total_timesteps    | 4384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.913   |\n",
      "|    explained_variance | 0.554    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54799    |\n",
      "|    policy_loss        | 0.0248   |\n",
      "|    value_loss         | 0.0335   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4390000, episode_reward=23.60 +/- 4.32\n",
      "Episode length: 3288.00 +/- 422.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.29e+03 |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4390000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.843    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54874    |\n",
      "|    policy_loss        | 0.0142   |\n",
      "|    value_loss         | 0.0199   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.89e+03 |\n",
      "|    ep_rew_mean        | 36.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 768      |\n",
      "|    iterations         | 54900    |\n",
      "|    time_elapsed       | 5714     |\n",
      "|    total_timesteps    | 4392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.985   |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54899    |\n",
      "|    policy_loss        | 0.0518   |\n",
      "|    value_loss         | 0.0329   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4400000, episode_reward=50.40 +/- 11.13\n",
      "Episode length: 5051.00 +/- 435.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.05e+03 |\n",
      "|    mean_reward        | 50.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.73     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54999    |\n",
      "|    policy_loss        | -0.121   |\n",
      "|    value_loss         | 0.0755   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.97e+03 |\n",
      "|    ep_rew_mean     | 37.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 767      |\n",
      "|    iterations      | 55000    |\n",
      "|    time_elapsed    | 5732     |\n",
      "|    total_timesteps | 4400000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4e+03    |\n",
      "|    ep_rew_mean        | 37.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 768      |\n",
      "|    iterations         | 55100    |\n",
      "|    time_elapsed       | 5739     |\n",
      "|    total_timesteps    | 4408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.742    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55099    |\n",
      "|    policy_loss        | -0.035   |\n",
      "|    value_loss         | 0.152    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4410000, episode_reward=38.00 +/- 18.75\n",
      "Episode length: 3887.20 +/- 862.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.89e+03 |\n",
      "|    mean_reward        | 38       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4410000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.565    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55124    |\n",
      "|    policy_loss        | 0.0402   |\n",
      "|    value_loss         | 0.0353   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.99e+03 |\n",
      "|    ep_rew_mean        | 37.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 767      |\n",
      "|    iterations         | 55200    |\n",
      "|    time_elapsed       | 5754     |\n",
      "|    total_timesteps    | 4416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.579    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55199    |\n",
      "|    policy_loss        | -0.0804  |\n",
      "|    value_loss         | 0.0849   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4420000, episode_reward=48.40 +/- 22.74\n",
      "Episode length: 4577.20 +/- 1224.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.58e+03 |\n",
      "|    mean_reward        | 48.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.507    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55249    |\n",
      "|    policy_loss        | -0.0546  |\n",
      "|    value_loss         | 0.0959   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.05e+03 |\n",
      "|    ep_rew_mean        | 38.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 766      |\n",
      "|    iterations         | 55300    |\n",
      "|    time_elapsed       | 5771     |\n",
      "|    total_timesteps    | 4424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.835    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55299    |\n",
      "|    policy_loss        | 0.0524   |\n",
      "|    value_loss         | 0.0279   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4430000, episode_reward=41.80 +/- 10.76\n",
      "Episode length: 4302.80 +/- 1105.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.3e+03  |\n",
      "|    mean_reward        | 41.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4430000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.631    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55374    |\n",
      "|    policy_loss        | 0.0119   |\n",
      "|    value_loss         | 0.0345   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.05e+03 |\n",
      "|    ep_rew_mean        | 38.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 765      |\n",
      "|    iterations         | 55400    |\n",
      "|    time_elapsed       | 5787     |\n",
      "|    total_timesteps    | 4432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.761    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55399    |\n",
      "|    policy_loss        | 0.0575   |\n",
      "|    value_loss         | 0.0373   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4440000, episode_reward=46.00 +/- 18.59\n",
      "Episode length: 4420.00 +/- 1368.43\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 4.42e+03  |\n",
      "|    mean_reward        | 46        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 4440000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.05     |\n",
      "|    explained_variance | 0.849     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 55499     |\n",
      "|    policy_loss        | -6.68e-05 |\n",
      "|    value_loss         | 0.0135    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.02e+03 |\n",
      "|    ep_rew_mean     | 37.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 764      |\n",
      "|    iterations      | 55500    |\n",
      "|    time_elapsed    | 5804     |\n",
      "|    total_timesteps | 4440000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.93e+03 |\n",
      "|    ep_rew_mean        | 37.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 765      |\n",
      "|    iterations         | 55600    |\n",
      "|    time_elapsed       | 5811     |\n",
      "|    total_timesteps    | 4448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.583    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55599    |\n",
      "|    policy_loss        | -0.0536  |\n",
      "|    value_loss         | 0.125    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4450000, episode_reward=43.80 +/- 15.50\n",
      "Episode length: 4149.00 +/- 848.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.15e+03 |\n",
      "|    mean_reward        | 43.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.54     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55624    |\n",
      "|    policy_loss        | 0.0631   |\n",
      "|    value_loss         | 0.0475   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.95e+03 |\n",
      "|    ep_rew_mean        | 37.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 764      |\n",
      "|    iterations         | 55700    |\n",
      "|    time_elapsed       | 5827     |\n",
      "|    total_timesteps    | 4456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.711    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55699    |\n",
      "|    policy_loss        | 0.0442   |\n",
      "|    value_loss         | 0.0472   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4460000, episode_reward=38.60 +/- 4.27\n",
      "Episode length: 3836.40 +/- 527.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.84e+03 |\n",
      "|    mean_reward        | 38.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.77     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55749    |\n",
      "|    policy_loss        | 0.011    |\n",
      "|    value_loss         | 0.0252   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.94e+03 |\n",
      "|    ep_rew_mean        | 37.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 763      |\n",
      "|    iterations         | 55800    |\n",
      "|    time_elapsed       | 5843     |\n",
      "|    total_timesteps    | 4464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.963   |\n",
      "|    explained_variance | 0.363    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55799    |\n",
      "|    policy_loss        | -0.117   |\n",
      "|    value_loss         | 0.256    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4470000, episode_reward=31.80 +/- 12.29\n",
      "Episode length: 3750.80 +/- 910.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.75e+03 |\n",
      "|    mean_reward        | 31.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4470000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.712    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55874    |\n",
      "|    policy_loss        | -0.0792  |\n",
      "|    value_loss         | 0.128    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.93e+03 |\n",
      "|    ep_rew_mean        | 37.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 763      |\n",
      "|    iterations         | 55900    |\n",
      "|    time_elapsed       | 5858     |\n",
      "|    total_timesteps    | 4472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.809    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55899    |\n",
      "|    policy_loss        | 0.0826   |\n",
      "|    value_loss         | 0.0913   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4480000, episode_reward=28.60 +/- 11.91\n",
      "Episode length: 3406.00 +/- 1032.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.41e+03 |\n",
      "|    mean_reward        | 28.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.711    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55999    |\n",
      "|    policy_loss        | -0.092   |\n",
      "|    value_loss         | 0.0842   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.91e+03 |\n",
      "|    ep_rew_mean     | 36.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 762      |\n",
      "|    iterations      | 56000    |\n",
      "|    time_elapsed    | 5873     |\n",
      "|    total_timesteps | 4480000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.86e+03 |\n",
      "|    ep_rew_mean        | 36.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 763      |\n",
      "|    iterations         | 56100    |\n",
      "|    time_elapsed       | 5880     |\n",
      "|    total_timesteps    | 4488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.7      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56099    |\n",
      "|    policy_loss        | -0.141   |\n",
      "|    value_loss         | 0.131    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4490000, episode_reward=34.60 +/- 12.34\n",
      "Episode length: 3613.80 +/- 956.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.61e+03 |\n",
      "|    mean_reward        | 34.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4490000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.799    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56124    |\n",
      "|    policy_loss        | -0.119   |\n",
      "|    value_loss         | 0.0625   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.81e+03 |\n",
      "|    ep_rew_mean        | 36       |\n",
      "| time/                 |          |\n",
      "|    fps                | 762      |\n",
      "|    iterations         | 56200    |\n",
      "|    time_elapsed       | 5895     |\n",
      "|    total_timesteps    | 4496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.771    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56199    |\n",
      "|    policy_loss        | 0.0813   |\n",
      "|    value_loss         | 0.0412   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4500000, episode_reward=50.80 +/- 15.82\n",
      "Episode length: 4676.20 +/- 800.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.68e+03 |\n",
      "|    mean_reward        | 50.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.714    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56249    |\n",
      "|    policy_loss        | -0.0445  |\n",
      "|    value_loss         | 0.0331   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.79e+03 |\n",
      "|    ep_rew_mean        | 35.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 761      |\n",
      "|    iterations         | 56300    |\n",
      "|    time_elapsed       | 5912     |\n",
      "|    total_timesteps    | 4504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56299    |\n",
      "|    policy_loss        | -0.00234 |\n",
      "|    value_loss         | 0.0162   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4510000, episode_reward=41.00 +/- 15.71\n",
      "Episode length: 3764.60 +/- 986.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.76e+03 |\n",
      "|    mean_reward        | 41       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4510000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.592    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56374    |\n",
      "|    policy_loss        | -0.0669  |\n",
      "|    value_loss         | 0.0649   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.73e+03 |\n",
      "|    ep_rew_mean        | 34.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 761      |\n",
      "|    iterations         | 56400    |\n",
      "|    time_elapsed       | 5927     |\n",
      "|    total_timesteps    | 4512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56399    |\n",
      "|    policy_loss        | -0.00308 |\n",
      "|    value_loss         | 0.0174   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4520000, episode_reward=50.00 +/- 18.19\n",
      "Episode length: 4458.20 +/- 1391.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.46e+03 |\n",
      "|    mean_reward        | 50       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.888   |\n",
      "|    explained_variance | 0.765    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56499    |\n",
      "|    policy_loss        | 0.042    |\n",
      "|    value_loss         | 0.0248   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.74e+03 |\n",
      "|    ep_rew_mean     | 34.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 760      |\n",
      "|    iterations      | 56500    |\n",
      "|    time_elapsed    | 5943     |\n",
      "|    total_timesteps | 4520000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.78e+03 |\n",
      "|    ep_rew_mean        | 35.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 760      |\n",
      "|    iterations         | 56600    |\n",
      "|    time_elapsed       | 5951     |\n",
      "|    total_timesteps    | 4528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.669    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56599    |\n",
      "|    policy_loss        | 0.0028   |\n",
      "|    value_loss         | 0.0228   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4530000, episode_reward=38.60 +/- 12.03\n",
      "Episode length: 3886.00 +/- 1192.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.89e+03 |\n",
      "|    mean_reward        | 38.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4530000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.988   |\n",
      "|    explained_variance | 0.87     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56624    |\n",
      "|    policy_loss        | -0.0359  |\n",
      "|    value_loss         | 0.0151   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.89e+03 |\n",
      "|    ep_rew_mean        | 36.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 760      |\n",
      "|    iterations         | 56700    |\n",
      "|    time_elapsed       | 5967     |\n",
      "|    total_timesteps    | 4536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.815    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56699    |\n",
      "|    policy_loss        | 0.0295   |\n",
      "|    value_loss         | 0.0362   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4540000, episode_reward=50.20 +/- 2.64\n",
      "Episode length: 5060.20 +/- 485.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.06e+03 |\n",
      "|    mean_reward        | 50.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.985   |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56749    |\n",
      "|    policy_loss        | -0.00431 |\n",
      "|    value_loss         | 0.0377   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.95e+03 |\n",
      "|    ep_rew_mean        | 37.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 759      |\n",
      "|    iterations         | 56800    |\n",
      "|    time_elapsed       | 5984     |\n",
      "|    total_timesteps    | 4544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.986   |\n",
      "|    explained_variance | 0.784    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56799    |\n",
      "|    policy_loss        | -0.0639  |\n",
      "|    value_loss         | 0.0728   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4550000, episode_reward=41.20 +/- 10.98\n",
      "Episode length: 4322.60 +/- 688.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.32e+03 |\n",
      "|    mean_reward        | 41.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.998   |\n",
      "|    explained_variance | 0.735    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56874    |\n",
      "|    policy_loss        | -0.071   |\n",
      "|    value_loss         | 0.105    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.97e+03 |\n",
      "|    ep_rew_mean        | 37.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 758      |\n",
      "|    iterations         | 56900    |\n",
      "|    time_elapsed       | 6001     |\n",
      "|    total_timesteps    | 4552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56899    |\n",
      "|    policy_loss        | -0.0379  |\n",
      "|    value_loss         | 0.0265   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4560000, episode_reward=32.00 +/- 9.92\n",
      "Episode length: 3562.00 +/- 490.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.56e+03 |\n",
      "|    mean_reward        | 32       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.799   |\n",
      "|    explained_variance | 0.574    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56999    |\n",
      "|    policy_loss        | 0.0178   |\n",
      "|    value_loss         | 0.0479   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.93e+03 |\n",
      "|    ep_rew_mean     | 36.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 757      |\n",
      "|    iterations      | 57000    |\n",
      "|    time_elapsed    | 6015     |\n",
      "|    total_timesteps | 4560000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.9e+03  |\n",
      "|    ep_rew_mean        | 36       |\n",
      "| time/                 |          |\n",
      "|    fps                | 758      |\n",
      "|    iterations         | 57100    |\n",
      "|    time_elapsed       | 6023     |\n",
      "|    total_timesteps    | 4568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.64     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57099    |\n",
      "|    policy_loss        | -0.0024  |\n",
      "|    value_loss         | 0.0468   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4570000, episode_reward=52.40 +/- 7.94\n",
      "Episode length: 4777.80 +/- 648.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.78e+03 |\n",
      "|    mean_reward        | 52.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4570000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.81     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57124    |\n",
      "|    policy_loss        | -0.0174  |\n",
      "|    value_loss         | 0.0209   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.91e+03 |\n",
      "|    ep_rew_mean        | 36.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 757      |\n",
      "|    iterations         | 57200    |\n",
      "|    time_elapsed       | 6040     |\n",
      "|    total_timesteps    | 4576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.974   |\n",
      "|    explained_variance | 0.783    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57199    |\n",
      "|    policy_loss        | -0.158   |\n",
      "|    value_loss         | 0.083    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4580000, episode_reward=26.60 +/- 5.39\n",
      "Episode length: 3082.20 +/- 677.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.08e+03 |\n",
      "|    mean_reward        | 26.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.618    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57249    |\n",
      "|    policy_loss        | 0.0549   |\n",
      "|    value_loss         | 0.1      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4e+03    |\n",
      "|    ep_rew_mean        | 37.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 757      |\n",
      "|    iterations         | 57300    |\n",
      "|    time_elapsed       | 6054     |\n",
      "|    total_timesteps    | 4584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.766    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57299    |\n",
      "|    policy_loss        | -0.0261  |\n",
      "|    value_loss         | 0.0267   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4590000, episode_reward=36.60 +/- 15.19\n",
      "Episode length: 3646.60 +/- 1038.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.65e+03 |\n",
      "|    mean_reward        | 36.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4590000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.962   |\n",
      "|    explained_variance | 0.557    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57374    |\n",
      "|    policy_loss        | -0.0348  |\n",
      "|    value_loss         | 0.172    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.07e+03 |\n",
      "|    ep_rew_mean        | 38.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 756      |\n",
      "|    iterations         | 57400    |\n",
      "|    time_elapsed       | 6069     |\n",
      "|    total_timesteps    | 4592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.697    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57399    |\n",
      "|    policy_loss        | 0.0774   |\n",
      "|    value_loss         | 0.0459   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4600000, episode_reward=68.20 +/- 46.95\n",
      "Episode length: 4146.80 +/- 1066.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.15e+03 |\n",
      "|    mean_reward        | 68.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.683    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57499    |\n",
      "|    policy_loss        | -0.0429  |\n",
      "|    value_loss         | 0.0879   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.14e+03 |\n",
      "|    ep_rew_mean     | 39.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 755      |\n",
      "|    iterations      | 57500    |\n",
      "|    time_elapsed    | 6085     |\n",
      "|    total_timesteps | 4600000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.26e+03 |\n",
      "|    ep_rew_mean        | 41.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 756      |\n",
      "|    iterations         | 57600    |\n",
      "|    time_elapsed       | 6092     |\n",
      "|    total_timesteps    | 4608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.945   |\n",
      "|    explained_variance | 0.844    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57599    |\n",
      "|    policy_loss        | -0.0332  |\n",
      "|    value_loss         | 0.0327   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4610000, episode_reward=65.40 +/- 23.16\n",
      "Episode length: 5478.40 +/- 1145.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.48e+03 |\n",
      "|    mean_reward        | 65.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4610000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.83     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57624    |\n",
      "|    policy_loss        | 0.0411   |\n",
      "|    value_loss         | 0.0186   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.37e+03 |\n",
      "|    ep_rew_mean        | 43       |\n",
      "| time/                 |          |\n",
      "|    fps                | 755      |\n",
      "|    iterations         | 57700    |\n",
      "|    time_elapsed       | 6111     |\n",
      "|    total_timesteps    | 4616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.894   |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57699    |\n",
      "|    policy_loss        | 0.0208   |\n",
      "|    value_loss         | 0.0295   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4620000, episode_reward=37.20 +/- 7.30\n",
      "Episode length: 3991.20 +/- 256.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.99e+03 |\n",
      "|    mean_reward        | 37.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4620000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.925   |\n",
      "|    explained_variance | 0.549    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57749    |\n",
      "|    policy_loss        | -0.0358  |\n",
      "|    value_loss         | 0.0706   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.39e+03 |\n",
      "|    ep_rew_mean        | 44.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 754      |\n",
      "|    iterations         | 57800    |\n",
      "|    time_elapsed       | 6126     |\n",
      "|    total_timesteps    | 4624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.882   |\n",
      "|    explained_variance | 0.789    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57799    |\n",
      "|    policy_loss        | 0.000445 |\n",
      "|    value_loss         | 0.0305   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4630000, episode_reward=51.80 +/- 22.00\n",
      "Episode length: 4684.40 +/- 1415.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.68e+03 |\n",
      "|    mean_reward        | 51.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4630000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.883   |\n",
      "|    explained_variance | 0.456    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57874    |\n",
      "|    policy_loss        | -0.0392  |\n",
      "|    value_loss         | 0.211    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.32e+03 |\n",
      "|    ep_rew_mean        | 43.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 753      |\n",
      "|    iterations         | 57900    |\n",
      "|    time_elapsed       | 6143     |\n",
      "|    total_timesteps    | 4632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.847    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57899    |\n",
      "|    policy_loss        | -0.0503  |\n",
      "|    value_loss         | 0.0217   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4640000, episode_reward=35.00 +/- 7.35\n",
      "Episode length: 3994.80 +/- 412.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.99e+03 |\n",
      "|    mean_reward        | 35       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.725    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57999    |\n",
      "|    policy_loss        | -0.0743  |\n",
      "|    value_loss         | 0.0261   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.31e+03 |\n",
      "|    ep_rew_mean     | 43.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 753      |\n",
      "|    iterations      | 58000    |\n",
      "|    time_elapsed    | 6159     |\n",
      "|    total_timesteps | 4640000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.23e+03 |\n",
      "|    ep_rew_mean        | 42.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 753      |\n",
      "|    iterations         | 58100    |\n",
      "|    time_elapsed       | 6166     |\n",
      "|    total_timesteps    | 4648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.747    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58099    |\n",
      "|    policy_loss        | -0.0211  |\n",
      "|    value_loss         | 0.0619   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4650000, episode_reward=43.40 +/- 18.74\n",
      "Episode length: 4737.00 +/- 1033.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.74e+03 |\n",
      "|    mean_reward        | 43.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.73     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58124    |\n",
      "|    policy_loss        | 0.0644   |\n",
      "|    value_loss         | 0.0417   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.32e+03 |\n",
      "|    ep_rew_mean        | 45.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 752      |\n",
      "|    iterations         | 58200    |\n",
      "|    time_elapsed       | 6184     |\n",
      "|    total_timesteps    | 4656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.573    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58199    |\n",
      "|    policy_loss        | 0.0555   |\n",
      "|    value_loss         | 0.0578   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4660000, episode_reward=48.20 +/- 31.98\n",
      "Episode length: 4434.60 +/- 1169.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.43e+03 |\n",
      "|    mean_reward        | 48.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.8      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58249    |\n",
      "|    policy_loss        | 0.0253   |\n",
      "|    value_loss         | 0.0498   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.3e+03  |\n",
      "|    ep_rew_mean        | 45.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 752      |\n",
      "|    iterations         | 58300    |\n",
      "|    time_elapsed       | 6200     |\n",
      "|    total_timesteps    | 4664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.677    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58299    |\n",
      "|    policy_loss        | -0.0266  |\n",
      "|    value_loss         | 0.0264   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4670000, episode_reward=35.20 +/- 16.55\n",
      "Episode length: 3557.60 +/- 1249.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.56e+03 |\n",
      "|    mean_reward        | 35.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4670000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.624    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58374    |\n",
      "|    policy_loss        | 0.0384   |\n",
      "|    value_loss         | 0.0461   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.39e+03 |\n",
      "|    ep_rew_mean        | 48.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 751      |\n",
      "|    iterations         | 58400    |\n",
      "|    time_elapsed       | 6215     |\n",
      "|    total_timesteps    | 4672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.794    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58399    |\n",
      "|    policy_loss        | -0.00603 |\n",
      "|    value_loss         | 0.105    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4680000, episode_reward=54.80 +/- 7.00\n",
      "Episode length: 4817.40 +/- 690.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.82e+03 |\n",
      "|    mean_reward        | 54.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58499    |\n",
      "|    policy_loss        | -0.0105  |\n",
      "|    value_loss         | 0.0155   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.33e+03 |\n",
      "|    ep_rew_mean     | 47       |\n",
      "| time/              |          |\n",
      "|    fps             | 750      |\n",
      "|    iterations      | 58500    |\n",
      "|    time_elapsed    | 6233     |\n",
      "|    total_timesteps | 4680000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.33e+03 |\n",
      "|    ep_rew_mean        | 47       |\n",
      "| time/                 |          |\n",
      "|    fps                | 751      |\n",
      "|    iterations         | 58600    |\n",
      "|    time_elapsed       | 6240     |\n",
      "|    total_timesteps    | 4688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.611    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58599    |\n",
      "|    policy_loss        | -0.0332  |\n",
      "|    value_loss         | 0.0261   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4690000, episode_reward=45.80 +/- 20.00\n",
      "Episode length: 4423.60 +/- 1437.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.42e+03 |\n",
      "|    mean_reward        | 45.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4690000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.988   |\n",
      "|    explained_variance | 0.518    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58624    |\n",
      "|    policy_loss        | 0.0609   |\n",
      "|    value_loss         | 0.19     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.34e+03 |\n",
      "|    ep_rew_mean        | 47.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 750      |\n",
      "|    iterations         | 58700    |\n",
      "|    time_elapsed       | 6257     |\n",
      "|    total_timesteps    | 4696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.813    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58699    |\n",
      "|    policy_loss        | 0.0108   |\n",
      "|    value_loss         | 0.0231   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4700000, episode_reward=55.80 +/- 28.39\n",
      "Episode length: 4753.00 +/- 1531.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.75e+03 |\n",
      "|    mean_reward        | 55.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.939   |\n",
      "|    explained_variance | 0.702    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58749    |\n",
      "|    policy_loss        | 0.0259   |\n",
      "|    value_loss         | 0.0406   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.28e+03 |\n",
      "|    ep_rew_mean        | 46.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 749      |\n",
      "|    iterations         | 58800    |\n",
      "|    time_elapsed       | 6274     |\n",
      "|    total_timesteps    | 4704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.831    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58799    |\n",
      "|    policy_loss        | -0.0364  |\n",
      "|    value_loss         | 0.022    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4710000, episode_reward=33.80 +/- 10.23\n",
      "Episode length: 3673.40 +/- 1017.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.67e+03 |\n",
      "|    mean_reward        | 33.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4710000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.68     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58874    |\n",
      "|    policy_loss        | -0.00188 |\n",
      "|    value_loss         | 0.0412   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.32e+03 |\n",
      "|    ep_rew_mean        | 47.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 749      |\n",
      "|    iterations         | 58900    |\n",
      "|    time_elapsed       | 6289     |\n",
      "|    total_timesteps    | 4712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.735    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58899    |\n",
      "|    policy_loss        | 0.0799   |\n",
      "|    value_loss         | 0.0564   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4720000, episode_reward=44.00 +/- 16.53\n",
      "Episode length: 4511.20 +/- 786.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.51e+03 |\n",
      "|    mean_reward        | 44       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.866    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58999    |\n",
      "|    policy_loss        | 0.0807   |\n",
      "|    value_loss         | 0.0339   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.27e+03 |\n",
      "|    ep_rew_mean     | 46.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 748      |\n",
      "|    iterations      | 59000    |\n",
      "|    time_elapsed    | 6305     |\n",
      "|    total_timesteps | 4720000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.33e+03 |\n",
      "|    ep_rew_mean        | 47.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 748      |\n",
      "|    iterations         | 59100    |\n",
      "|    time_elapsed       | 6312     |\n",
      "|    total_timesteps    | 4728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.987   |\n",
      "|    explained_variance | 0.802    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59099    |\n",
      "|    policy_loss        | -0.0155  |\n",
      "|    value_loss         | 0.0395   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4730000, episode_reward=97.80 +/- 94.48\n",
      "Episode length: 4719.80 +/- 790.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.72e+03 |\n",
      "|    mean_reward        | 97.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4730000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59124    |\n",
      "|    policy_loss        | 0.0553   |\n",
      "|    value_loss         | 0.0142   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.38e+03 |\n",
      "|    ep_rew_mean        | 47.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 748      |\n",
      "|    iterations         | 59200    |\n",
      "|    time_elapsed       | 6329     |\n",
      "|    total_timesteps    | 4736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.746    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59199    |\n",
      "|    policy_loss        | 0.0118   |\n",
      "|    value_loss         | 0.027    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4740000, episode_reward=27.40 +/- 9.73\n",
      "Episode length: 3015.60 +/- 856.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.02e+03 |\n",
      "|    mean_reward        | 27.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.755    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59249    |\n",
      "|    policy_loss        | 0.0866   |\n",
      "|    value_loss         | 0.0343   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.38e+03 |\n",
      "|    ep_rew_mean        | 47.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 747      |\n",
      "|    iterations         | 59300    |\n",
      "|    time_elapsed       | 6343     |\n",
      "|    total_timesteps    | 4744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.94    |\n",
      "|    explained_variance | 0.74     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59299    |\n",
      "|    policy_loss        | 0.0277   |\n",
      "|    value_loss         | 0.0298   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4750000, episode_reward=39.20 +/- 15.97\n",
      "Episode length: 4165.80 +/- 857.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.17e+03 |\n",
      "|    mean_reward        | 39.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.6      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59374    |\n",
      "|    policy_loss        | 0.15     |\n",
      "|    value_loss         | 0.124    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.44e+03 |\n",
      "|    ep_rew_mean        | 48.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 747      |\n",
      "|    iterations         | 59400    |\n",
      "|    time_elapsed       | 6359     |\n",
      "|    total_timesteps    | 4752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.654    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59399    |\n",
      "|    policy_loss        | -0.0159  |\n",
      "|    value_loss         | 0.0415   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4760000, episode_reward=52.20 +/- 12.30\n",
      "Episode length: 4581.60 +/- 424.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.58e+03 |\n",
      "|    mean_reward        | 52.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.911   |\n",
      "|    explained_variance | 0.396    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59499    |\n",
      "|    policy_loss        | 0.0849   |\n",
      "|    value_loss         | 0.0936   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.52e+03 |\n",
      "|    ep_rew_mean     | 49.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 746      |\n",
      "|    iterations      | 59500    |\n",
      "|    time_elapsed    | 6375     |\n",
      "|    total_timesteps | 4760000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.55e+03 |\n",
      "|    ep_rew_mean        | 49.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 746      |\n",
      "|    iterations         | 59600    |\n",
      "|    time_elapsed       | 6383     |\n",
      "|    total_timesteps    | 4768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.757    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59599    |\n",
      "|    policy_loss        | 0.0632   |\n",
      "|    value_loss         | 0.0389   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4770000, episode_reward=38.80 +/- 21.64\n",
      "Episode length: 3658.40 +/- 1308.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.66e+03 |\n",
      "|    mean_reward        | 38.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4770000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59624    |\n",
      "|    policy_loss        | -0.00486 |\n",
      "|    value_loss         | 0.0142   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.61e+03 |\n",
      "|    ep_rew_mean        | 50.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 746      |\n",
      "|    iterations         | 59700    |\n",
      "|    time_elapsed       | 6397     |\n",
      "|    total_timesteps    | 4776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.793    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59699    |\n",
      "|    policy_loss        | -0.204   |\n",
      "|    value_loss         | 0.114    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4780000, episode_reward=66.40 +/- 12.35\n",
      "Episode length: 5348.00 +/- 378.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.35e+03 |\n",
      "|    mean_reward        | 66.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.789    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59749    |\n",
      "|    policy_loss        | 0.0256   |\n",
      "|    value_loss         | 0.0527   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.59e+03 |\n",
      "|    ep_rew_mean        | 49.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 745      |\n",
      "|    iterations         | 59800    |\n",
      "|    time_elapsed       | 6416     |\n",
      "|    total_timesteps    | 4784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.578    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59799    |\n",
      "|    policy_loss        | 0.0871   |\n",
      "|    value_loss         | 0.0681   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4790000, episode_reward=69.80 +/- 20.87\n",
      "Episode length: 5465.20 +/- 1354.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.47e+03 |\n",
      "|    mean_reward        | 69.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4790000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.797    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59874    |\n",
      "|    policy_loss        | -0.023   |\n",
      "|    value_loss         | 0.0715   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.61e+03 |\n",
      "|    ep_rew_mean        | 48.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 744      |\n",
      "|    iterations         | 59900    |\n",
      "|    time_elapsed       | 6435     |\n",
      "|    total_timesteps    | 4792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.51     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59899    |\n",
      "|    policy_loss        | -0.0334  |\n",
      "|    value_loss         | 0.142    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4800000, episode_reward=37.40 +/- 11.22\n",
      "Episode length: 3607.20 +/- 667.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.61e+03 |\n",
      "|    mean_reward        | 37.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.47     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59999    |\n",
      "|    policy_loss        | -0.0522  |\n",
      "|    value_loss         | 0.116    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.67e+03 |\n",
      "|    ep_rew_mean     | 49.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 744      |\n",
      "|    iterations      | 60000    |\n",
      "|    time_elapsed    | 6450     |\n",
      "|    total_timesteps | 4800000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.71e+03 |\n",
      "|    ep_rew_mean        | 50.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 744      |\n",
      "|    iterations         | 60100    |\n",
      "|    time_elapsed       | 6457     |\n",
      "|    total_timesteps    | 4808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.945   |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60099    |\n",
      "|    policy_loss        | -0.0445  |\n",
      "|    value_loss         | 0.0225   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4810000, episode_reward=61.60 +/- 13.37\n",
      "Episode length: 5255.20 +/- 461.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.26e+03 |\n",
      "|    mean_reward        | 61.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4810000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.844    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60124    |\n",
      "|    policy_loss        | -0.0405  |\n",
      "|    value_loss         | 0.0578   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.69e+03 |\n",
      "|    ep_rew_mean        | 50.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 743      |\n",
      "|    iterations         | 60200    |\n",
      "|    time_elapsed       | 6476     |\n",
      "|    total_timesteps    | 4816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.505    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60199    |\n",
      "|    policy_loss        | -0.0386  |\n",
      "|    value_loss         | 0.0846   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4820000, episode_reward=41.40 +/- 12.58\n",
      "Episode length: 4413.80 +/- 788.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.41e+03 |\n",
      "|    mean_reward        | 41.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.739    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60249    |\n",
      "|    policy_loss        | -0.0401  |\n",
      "|    value_loss         | 0.0961   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.8e+03  |\n",
      "|    ep_rew_mean        | 52.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 742      |\n",
      "|    iterations         | 60300    |\n",
      "|    time_elapsed       | 6492     |\n",
      "|    total_timesteps    | 4824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.957   |\n",
      "|    explained_variance | 0.751    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60299    |\n",
      "|    policy_loss        | 0.0353   |\n",
      "|    value_loss         | 0.0522   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4830000, episode_reward=51.60 +/- 30.28\n",
      "Episode length: 4306.60 +/- 1464.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.31e+03 |\n",
      "|    mean_reward        | 51.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4830000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.941   |\n",
      "|    explained_variance | 0.676    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60374    |\n",
      "|    policy_loss        | 0.00758  |\n",
      "|    value_loss         | 0.123    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.8e+03  |\n",
      "|    ep_rew_mean        | 52.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 742      |\n",
      "|    iterations         | 60400    |\n",
      "|    time_elapsed       | 6508     |\n",
      "|    total_timesteps    | 4832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.974   |\n",
      "|    explained_variance | 0.767    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60399    |\n",
      "|    policy_loss        | -0.0544  |\n",
      "|    value_loss         | 0.0844   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4840000, episode_reward=47.40 +/- 22.45\n",
      "Episode length: 4167.40 +/- 873.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.17e+03 |\n",
      "|    mean_reward        | 47.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.991   |\n",
      "|    explained_variance | 0.604    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60499    |\n",
      "|    policy_loss        | 0.0161   |\n",
      "|    value_loss         | 0.0574   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.74e+03 |\n",
      "|    ep_rew_mean     | 51.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 741      |\n",
      "|    iterations      | 60500    |\n",
      "|    time_elapsed    | 6524     |\n",
      "|    total_timesteps | 4840000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.77e+03 |\n",
      "|    ep_rew_mean        | 52.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 742      |\n",
      "|    iterations         | 60600    |\n",
      "|    time_elapsed       | 6532     |\n",
      "|    total_timesteps    | 4848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.972   |\n",
      "|    explained_variance | 0.834    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60599    |\n",
      "|    policy_loss        | -0.0952  |\n",
      "|    value_loss         | 0.0491   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4850000, episode_reward=47.00 +/- 15.65\n",
      "Episode length: 4913.40 +/- 1116.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.91e+03 |\n",
      "|    mean_reward        | 47       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4850000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.584    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60624    |\n",
      "|    policy_loss        | -0.0505  |\n",
      "|    value_loss         | 0.0517   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.76e+03 |\n",
      "|    ep_rew_mean        | 51.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 60700    |\n",
      "|    time_elapsed       | 6549     |\n",
      "|    total_timesteps    | 4856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.987   |\n",
      "|    explained_variance | 0.747    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60699    |\n",
      "|    policy_loss        | -0.0633  |\n",
      "|    value_loss         | 0.0569   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4860000, episode_reward=54.00 +/- 15.23\n",
      "Episode length: 4968.60 +/- 822.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.97e+03 |\n",
      "|    mean_reward        | 54       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4860000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.645    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60749    |\n",
      "|    policy_loss        | 0.0294   |\n",
      "|    value_loss         | 0.0429   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.8e+03  |\n",
      "|    ep_rew_mean        | 52.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 60800    |\n",
      "|    time_elapsed       | 6567     |\n",
      "|    total_timesteps    | 4864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.748    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60799    |\n",
      "|    policy_loss        | 0.0205   |\n",
      "|    value_loss         | 0.0346   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4870000, episode_reward=31.80 +/- 8.98\n",
      "Episode length: 4054.40 +/- 649.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.05e+03 |\n",
      "|    mean_reward        | 31.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4870000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.405    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60874    |\n",
      "|    policy_loss        | -0.0167  |\n",
      "|    value_loss         | 0.0841   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.79e+03 |\n",
      "|    ep_rew_mean        | 52.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 60900    |\n",
      "|    time_elapsed       | 6582     |\n",
      "|    total_timesteps    | 4872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60899    |\n",
      "|    policy_loss        | 0.0465   |\n",
      "|    value_loss         | 0.0177   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4880000, episode_reward=52.40 +/- 19.24\n",
      "Episode length: 4799.40 +/- 997.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.8e+03  |\n",
      "|    mean_reward        | 52.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.9     |\n",
      "|    explained_variance | 0.636    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60999    |\n",
      "|    policy_loss        | 0.0116   |\n",
      "|    value_loss         | 0.134    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.82e+03 |\n",
      "|    ep_rew_mean     | 52.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 739      |\n",
      "|    iterations      | 61000    |\n",
      "|    time_elapsed    | 6600     |\n",
      "|    total_timesteps | 4880000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.79e+03 |\n",
      "|    ep_rew_mean        | 53.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 61100    |\n",
      "|    time_elapsed       | 6608     |\n",
      "|    total_timesteps    | 4888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.299   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61099    |\n",
      "|    policy_loss        | -0.0109  |\n",
      "|    value_loss         | 0.247    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4890000, episode_reward=53.60 +/- 15.04\n",
      "Episode length: 4957.20 +/- 471.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.96e+03 |\n",
      "|    mean_reward        | 53.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4890000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.939   |\n",
      "|    explained_variance | 0.825    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61124    |\n",
      "|    policy_loss        | -0.0455  |\n",
      "|    value_loss         | 0.0855   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.76e+03 |\n",
      "|    ep_rew_mean        | 55.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 61200    |\n",
      "|    time_elapsed       | 6625     |\n",
      "|    total_timesteps    | 4896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.724    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61199    |\n",
      "|    policy_loss        | -0.00722 |\n",
      "|    value_loss         | 0.0227   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4900000, episode_reward=73.40 +/- 39.95\n",
      "Episode length: 5286.00 +/- 985.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.29e+03 |\n",
      "|    mean_reward        | 73.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4900000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.735    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61249    |\n",
      "|    policy_loss        | 0.0729   |\n",
      "|    value_loss         | 0.0582   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.71e+03 |\n",
      "|    ep_rew_mean        | 54.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 61300    |\n",
      "|    time_elapsed       | 6643     |\n",
      "|    total_timesteps    | 4904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.726    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61299    |\n",
      "|    policy_loss        | 0.00809  |\n",
      "|    value_loss         | 0.0363   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4910000, episode_reward=61.20 +/- 26.59\n",
      "Episode length: 4809.20 +/- 858.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.81e+03 |\n",
      "|    mean_reward        | 61.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4910000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.722    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61374    |\n",
      "|    policy_loss        | 0.0216   |\n",
      "|    value_loss         | 0.101    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.7e+03  |\n",
      "|    ep_rew_mean        | 54.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 61400    |\n",
      "|    time_elapsed       | 6661     |\n",
      "|    total_timesteps    | 4912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.772    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61399    |\n",
      "|    policy_loss        | -0.00805 |\n",
      "|    value_loss         | 0.0982   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4920000, episode_reward=57.60 +/- 18.75\n",
      "Episode length: 5034.40 +/- 1011.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.03e+03 |\n",
      "|    mean_reward        | 57.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61499    |\n",
      "|    policy_loss        | -0.12    |\n",
      "|    value_loss         | 0.0919   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.76e+03 |\n",
      "|    ep_rew_mean     | 55.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 736      |\n",
      "|    iterations      | 61500    |\n",
      "|    time_elapsed    | 6678     |\n",
      "|    total_timesteps | 4920000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.72e+03 |\n",
      "|    ep_rew_mean        | 55.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 61600    |\n",
      "|    time_elapsed       | 6686     |\n",
      "|    total_timesteps    | 4928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.842    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61599    |\n",
      "|    policy_loss        | -0.0152  |\n",
      "|    value_loss         | 0.027    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4930000, episode_reward=49.40 +/- 16.46\n",
      "Episode length: 4474.40 +/- 685.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.47e+03 |\n",
      "|    mean_reward        | 49.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4930000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.823   |\n",
      "|    explained_variance | 0.593    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61624    |\n",
      "|    policy_loss        | 0.0944   |\n",
      "|    value_loss         | 0.0617   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.78e+03 |\n",
      "|    ep_rew_mean        | 56.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 61700    |\n",
      "|    time_elapsed       | 6702     |\n",
      "|    total_timesteps    | 4936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.85    |\n",
      "|    explained_variance | 0.677    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61699    |\n",
      "|    policy_loss        | -0.0423  |\n",
      "|    value_loss         | 0.151    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4940000, episode_reward=56.20 +/- 9.13\n",
      "Episode length: 4966.20 +/- 466.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.97e+03 |\n",
      "|    mean_reward        | 56.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.793    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61749    |\n",
      "|    policy_loss        | -0.00986 |\n",
      "|    value_loss         | 0.0263   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.75e+03 |\n",
      "|    ep_rew_mean        | 55.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 61800    |\n",
      "|    time_elapsed       | 6719     |\n",
      "|    total_timesteps    | 4944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.836    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61799    |\n",
      "|    policy_loss        | 0.0368   |\n",
      "|    value_loss         | 0.0183   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4950000, episode_reward=58.00 +/- 33.26\n",
      "Episode length: 4291.20 +/- 1383.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.29e+03 |\n",
      "|    mean_reward        | 58       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4950000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.783    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61874    |\n",
      "|    policy_loss        | -5.1e-06 |\n",
      "|    value_loss         | 0.0299   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.74e+03 |\n",
      "|    ep_rew_mean        | 55.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 61900    |\n",
      "|    time_elapsed       | 6736     |\n",
      "|    total_timesteps    | 4952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.836    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61899    |\n",
      "|    policy_loss        | -0.048   |\n",
      "|    value_loss         | 0.0766   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4960000, episode_reward=50.00 +/- 17.81\n",
      "Episode length: 4695.40 +/- 1044.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.7e+03  |\n",
      "|    mean_reward        | 50       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.301    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61999    |\n",
      "|    policy_loss        | 0.115    |\n",
      "|    value_loss         | 0.311    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.82e+03 |\n",
      "|    ep_rew_mean     | 57.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 734      |\n",
      "|    iterations      | 62000    |\n",
      "|    time_elapsed    | 6752     |\n",
      "|    total_timesteps | 4960000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.86e+03 |\n",
      "|    ep_rew_mean        | 59.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 62100    |\n",
      "|    time_elapsed       | 6760     |\n",
      "|    total_timesteps    | 4968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.718    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62099    |\n",
      "|    policy_loss        | 0.0659   |\n",
      "|    value_loss         | 0.0472   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4970000, episode_reward=56.20 +/- 15.04\n",
      "Episode length: 5130.80 +/- 543.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.13e+03 |\n",
      "|    mean_reward        | 56.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4970000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.976   |\n",
      "|    explained_variance | 0.636    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62124    |\n",
      "|    policy_loss        | -0.145   |\n",
      "|    value_loss         | 0.235    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.88e+03 |\n",
      "|    ep_rew_mean        | 60       |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 62200    |\n",
      "|    time_elapsed       | 6777     |\n",
      "|    total_timesteps    | 4976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.921   |\n",
      "|    explained_variance | 0.731    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62199    |\n",
      "|    policy_loss        | 0.0322   |\n",
      "|    value_loss         | 0.0282   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4980000, episode_reward=34.20 +/- 10.17\n",
      "Episode length: 3793.80 +/- 993.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.79e+03 |\n",
      "|    mean_reward        | 34.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.985   |\n",
      "|    explained_variance | 0.607    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62249    |\n",
      "|    policy_loss        | -0.0208  |\n",
      "|    value_loss         | 0.0574   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.87e+03 |\n",
      "|    ep_rew_mean        | 60.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 62300    |\n",
      "|    time_elapsed       | 6792     |\n",
      "|    total_timesteps    | 4984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.96    |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62299    |\n",
      "|    policy_loss        | -0.0507  |\n",
      "|    value_loss         | 0.0583   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4990000, episode_reward=38.80 +/- 18.70\n",
      "Episode length: 3674.00 +/- 819.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.67e+03 |\n",
      "|    mean_reward        | 38.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4990000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.966   |\n",
      "|    explained_variance | 0.868    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62374    |\n",
      "|    policy_loss        | -0.0204  |\n",
      "|    value_loss         | 0.0341   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.87e+03 |\n",
      "|    ep_rew_mean        | 60.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 62400    |\n",
      "|    time_elapsed       | 6807     |\n",
      "|    total_timesteps    | 4992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62399    |\n",
      "|    policy_loss        | 0.00325  |\n",
      "|    value_loss         | 0.0127   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5000000, episode_reward=61.00 +/- 19.27\n",
      "Episode length: 4924.40 +/- 995.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.92e+03 |\n",
      "|    mean_reward        | 61       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.773    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62499    |\n",
      "|    policy_loss        | 0.0209   |\n",
      "|    value_loss         | 0.123    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.87e+03 |\n",
      "|    ep_rew_mean     | 60.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 732      |\n",
      "|    iterations      | 62500    |\n",
      "|    time_elapsed    | 6825     |\n",
      "|    total_timesteps | 5000000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.87e+03 |\n",
      "|    ep_rew_mean        | 60.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 62600    |\n",
      "|    time_elapsed       | 6832     |\n",
      "|    total_timesteps    | 5008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.863    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62599    |\n",
      "|    policy_loss        | -0.0168  |\n",
      "|    value_loss         | 0.0404   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5010000, episode_reward=60.20 +/- 17.05\n",
      "Episode length: 5130.40 +/- 660.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.13e+03 |\n",
      "|    mean_reward        | 60.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5010000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.755    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62624    |\n",
      "|    policy_loss        | 0.0383   |\n",
      "|    value_loss         | 0.0281   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.92e+03 |\n",
      "|    ep_rew_mean        | 58.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 62700    |\n",
      "|    time_elapsed       | 6850     |\n",
      "|    total_timesteps    | 5016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.982   |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62699    |\n",
      "|    policy_loss        | -0.0496  |\n",
      "|    value_loss         | 0.0456   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5020000, episode_reward=68.80 +/- 13.89\n",
      "Episode length: 5367.40 +/- 640.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.37e+03 |\n",
      "|    mean_reward        | 68.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.814    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62749    |\n",
      "|    policy_loss        | -0.00984 |\n",
      "|    value_loss         | 0.0268   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 4.98e+03  |\n",
      "|    ep_rew_mean        | 61.8      |\n",
      "| time/                 |           |\n",
      "|    fps                | 731       |\n",
      "|    iterations         | 62800     |\n",
      "|    time_elapsed       | 6868      |\n",
      "|    total_timesteps    | 5024000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.05     |\n",
      "|    explained_variance | 0.702     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 62799     |\n",
      "|    policy_loss        | -0.000538 |\n",
      "|    value_loss         | 0.0719    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=5030000, episode_reward=42.20 +/- 13.44\n",
      "Episode length: 3829.40 +/- 957.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.83e+03 |\n",
      "|    mean_reward        | 42.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5030000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.629    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62874    |\n",
      "|    policy_loss        | -0.0216  |\n",
      "|    value_loss         | 0.0296   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.05e+03 |\n",
      "|    ep_rew_mean        | 63.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 62900    |\n",
      "|    time_elapsed       | 6883     |\n",
      "|    total_timesteps    | 5032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62899    |\n",
      "|    policy_loss        | 0.0474   |\n",
      "|    value_loss         | 0.0368   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5040000, episode_reward=54.20 +/- 20.04\n",
      "Episode length: 4468.20 +/- 684.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.47e+03 |\n",
      "|    mean_reward        | 54.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.982   |\n",
      "|    explained_variance | 0.765    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62999    |\n",
      "|    policy_loss        | 0.0257   |\n",
      "|    value_loss         | 0.0613   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.05e+03 |\n",
      "|    ep_rew_mean     | 63.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 730      |\n",
      "|    iterations      | 63000    |\n",
      "|    time_elapsed    | 6900     |\n",
      "|    total_timesteps | 5040000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5e+03    |\n",
      "|    ep_rew_mean        | 62.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 63100    |\n",
      "|    time_elapsed       | 6907     |\n",
      "|    total_timesteps    | 5048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.459    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63099    |\n",
      "|    policy_loss        | -0.0342  |\n",
      "|    value_loss         | 0.282    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5050000, episode_reward=61.40 +/- 19.95\n",
      "Episode length: 4815.00 +/- 968.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.82e+03 |\n",
      "|    mean_reward        | 61.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.958   |\n",
      "|    explained_variance | 0.406    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63124    |\n",
      "|    policy_loss        | 0.0354   |\n",
      "|    value_loss         | 0.0391   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.98e+03 |\n",
      "|    ep_rew_mean        | 62.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 63200    |\n",
      "|    time_elapsed       | 6924     |\n",
      "|    total_timesteps    | 5056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.962   |\n",
      "|    explained_variance | 0.692    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63199    |\n",
      "|    policy_loss        | -0.0563  |\n",
      "|    value_loss         | 0.0525   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5060000, episode_reward=60.80 +/- 16.49\n",
      "Episode length: 5012.40 +/- 1263.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.01e+03 |\n",
      "|    mean_reward        | 60.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.833   |\n",
      "|    explained_variance | 0.514    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63249    |\n",
      "|    policy_loss        | 0.0437   |\n",
      "|    value_loss         | 0.0903   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.94e+03 |\n",
      "|    ep_rew_mean        | 61.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 63300    |\n",
      "|    time_elapsed       | 6941     |\n",
      "|    total_timesteps    | 5064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.947   |\n",
      "|    explained_variance | 0.651    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63299    |\n",
      "|    policy_loss        | 0.035    |\n",
      "|    value_loss         | 0.0306   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5070000, episode_reward=68.60 +/- 19.68\n",
      "Episode length: 4825.00 +/- 1134.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.82e+03 |\n",
      "|    mean_reward        | 68.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5070000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63374    |\n",
      "|    policy_loss        | 0.00987  |\n",
      "|    value_loss         | 0.0138   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5e+03    |\n",
      "|    ep_rew_mean        | 63.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 63400    |\n",
      "|    time_elapsed       | 6959     |\n",
      "|    total_timesteps    | 5072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63399    |\n",
      "|    policy_loss        | 0.0164   |\n",
      "|    value_loss         | 0.0272   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5080000, episode_reward=59.80 +/- 15.43\n",
      "Episode length: 4435.40 +/- 775.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.44e+03 |\n",
      "|    mean_reward        | 59.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.995   |\n",
      "|    explained_variance | 0.197    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63499    |\n",
      "|    policy_loss        | 0.0605   |\n",
      "|    value_loss         | 0.0622   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | 64       |\n",
      "| time/              |          |\n",
      "|    fps             | 728      |\n",
      "|    iterations      | 63500    |\n",
      "|    time_elapsed    | 6975     |\n",
      "|    total_timesteps | 5080000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.07e+03 |\n",
      "|    ep_rew_mean        | 61.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 63600    |\n",
      "|    time_elapsed       | 6982     |\n",
      "|    total_timesteps    | 5088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.676    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63599    |\n",
      "|    policy_loss        | -0.128   |\n",
      "|    value_loss         | 0.113    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5090000, episode_reward=63.60 +/- 30.37\n",
      "Episode length: 4927.40 +/- 1597.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.93e+03 |\n",
      "|    mean_reward        | 63.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5090000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.916   |\n",
      "|    explained_variance | 0.767    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63624    |\n",
      "|    policy_loss        | -0.00691 |\n",
      "|    value_loss         | 0.0307   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.05e+03 |\n",
      "|    ep_rew_mean        | 61.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 63700    |\n",
      "|    time_elapsed       | 6999     |\n",
      "|    total_timesteps    | 5096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.926   |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63699    |\n",
      "|    policy_loss        | -0.01    |\n",
      "|    value_loss         | 0.0309   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5100000, episode_reward=64.80 +/- 15.55\n",
      "Episode length: 5089.40 +/- 477.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.09e+03 |\n",
      "|    mean_reward        | 64.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63749    |\n",
      "|    policy_loss        | -0.0398  |\n",
      "|    value_loss         | 0.0348   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.04e+03 |\n",
      "|    ep_rew_mean        | 61.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 63800    |\n",
      "|    time_elapsed       | 7017     |\n",
      "|    total_timesteps    | 5104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.34     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63799    |\n",
      "|    policy_loss        | 0.0101   |\n",
      "|    value_loss         | 0.0987   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5110000, episode_reward=61.00 +/- 22.92\n",
      "Episode length: 4785.00 +/- 1406.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.78e+03 |\n",
      "|    mean_reward        | 61       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5110000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.985   |\n",
      "|    explained_variance | 0.672    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63874    |\n",
      "|    policy_loss        | -0.00342 |\n",
      "|    value_loss         | 0.125    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 61.1      |\n",
      "| time/                 |           |\n",
      "|    fps                | 726       |\n",
      "|    iterations         | 63900     |\n",
      "|    time_elapsed       | 7034      |\n",
      "|    total_timesteps    | 5112000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.941    |\n",
      "|    explained_variance | 0.911     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 63899     |\n",
      "|    policy_loss        | -0.000623 |\n",
      "|    value_loss         | 0.0225    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=5120000, episode_reward=67.40 +/- 18.29\n",
      "Episode length: 5591.00 +/- 541.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.59e+03 |\n",
      "|    mean_reward        | 67.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.41     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63999    |\n",
      "|    policy_loss        | 0.00751  |\n",
      "|    value_loss         | 0.0576   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.07e+03 |\n",
      "|    ep_rew_mean     | 61.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 725      |\n",
      "|    iterations      | 64000    |\n",
      "|    time_elapsed    | 7053     |\n",
      "|    total_timesteps | 5120000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.07e+03 |\n",
      "|    ep_rew_mean        | 62.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 726      |\n",
      "|    iterations         | 64100    |\n",
      "|    time_elapsed       | 7060     |\n",
      "|    total_timesteps    | 5128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.915   |\n",
      "|    explained_variance | 0.629    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64099    |\n",
      "|    policy_loss        | -0.051   |\n",
      "|    value_loss         | 0.184    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5130000, episode_reward=56.80 +/- 36.79\n",
      "Episode length: 4735.20 +/- 1418.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.74e+03 |\n",
      "|    mean_reward        | 56.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5130000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.612    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64124    |\n",
      "|    policy_loss        | 0.0207   |\n",
      "|    value_loss         | 0.111    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.15e+03 |\n",
      "|    ep_rew_mean        | 64.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 725      |\n",
      "|    iterations         | 64200    |\n",
      "|    time_elapsed       | 7077     |\n",
      "|    total_timesteps    | 5136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.868   |\n",
      "|    explained_variance | 0.732    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64199    |\n",
      "|    policy_loss        | -0.0269  |\n",
      "|    value_loss         | 0.0679   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5140000, episode_reward=37.40 +/- 23.05\n",
      "Episode length: 4053.40 +/- 1106.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.05e+03 |\n",
      "|    mean_reward        | 37.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.831    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64249    |\n",
      "|    policy_loss        | -0.0221  |\n",
      "|    value_loss         | 0.0216   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.09e+03 |\n",
      "|    ep_rew_mean        | 61.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 725      |\n",
      "|    iterations         | 64300    |\n",
      "|    time_elapsed       | 7093     |\n",
      "|    total_timesteps    | 5144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.487    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64299    |\n",
      "|    policy_loss        | -0.035   |\n",
      "|    value_loss         | 0.0929   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5150000, episode_reward=73.80 +/- 17.08\n",
      "Episode length: 5389.80 +/- 555.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.39e+03 |\n",
      "|    mean_reward        | 73.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.787   |\n",
      "|    explained_variance | 0.739    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64374    |\n",
      "|    policy_loss        | 0.0171   |\n",
      "|    value_loss         | 0.0206   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.01e+03 |\n",
      "|    ep_rew_mean        | 60       |\n",
      "| time/                 |          |\n",
      "|    fps                | 724      |\n",
      "|    iterations         | 64400    |\n",
      "|    time_elapsed       | 7112     |\n",
      "|    total_timesteps    | 5152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.972   |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64399    |\n",
      "|    policy_loss        | 0.0124   |\n",
      "|    value_loss         | 0.0194   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5160000, episode_reward=41.80 +/- 12.80\n",
      "Episode length: 4324.60 +/- 840.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.32e+03 |\n",
      "|    mean_reward        | 41.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.949   |\n",
      "|    explained_variance | 0.786    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64499    |\n",
      "|    policy_loss        | -0.01    |\n",
      "|    value_loss         | 0.0227   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.92e+03 |\n",
      "|    ep_rew_mean     | 58.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 723      |\n",
      "|    iterations      | 64500    |\n",
      "|    time_elapsed    | 7128     |\n",
      "|    total_timesteps | 5160000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5e+03    |\n",
      "|    ep_rew_mean        | 60.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 724      |\n",
      "|    iterations         | 64600    |\n",
      "|    time_elapsed       | 7135     |\n",
      "|    total_timesteps    | 5168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.982   |\n",
      "|    explained_variance | 0.69     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64599    |\n",
      "|    policy_loss        | -0.0505  |\n",
      "|    value_loss         | 0.199    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5170000, episode_reward=41.00 +/- 14.18\n",
      "Episode length: 4143.00 +/- 948.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.14e+03 |\n",
      "|    mean_reward        | 41       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5170000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.893   |\n",
      "|    explained_variance | 0.78     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64624    |\n",
      "|    policy_loss        | 0.0158   |\n",
      "|    value_loss         | 0.0325   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.01e+03 |\n",
      "|    ep_rew_mean        | 61.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 723      |\n",
      "|    iterations         | 64700    |\n",
      "|    time_elapsed       | 7151     |\n",
      "|    total_timesteps    | 5176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.898   |\n",
      "|    explained_variance | 0.752    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64699    |\n",
      "|    policy_loss        | -0.0355  |\n",
      "|    value_loss         | 0.178    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5180000, episode_reward=20.80 +/- 9.20\n",
      "Episode length: 2339.40 +/- 777.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.34e+03 |\n",
      "|    mean_reward        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.865   |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64749    |\n",
      "|    policy_loss        | 0.134    |\n",
      "|    value_loss         | 0.257    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.03e+03 |\n",
      "|    ep_rew_mean        | 63.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 723      |\n",
      "|    iterations         | 64800    |\n",
      "|    time_elapsed       | 7163     |\n",
      "|    total_timesteps    | 5184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.914   |\n",
      "|    explained_variance | 0.898    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64799    |\n",
      "|    policy_loss        | -0.057   |\n",
      "|    value_loss         | 0.033    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5190000, episode_reward=70.40 +/- 34.80\n",
      "Episode length: 5422.20 +/- 603.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.42e+03 |\n",
      "|    mean_reward        | 70.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5190000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.661    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64874    |\n",
      "|    policy_loss        | 0.00314  |\n",
      "|    value_loss         | 0.0626   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.03e+03 |\n",
      "|    ep_rew_mean        | 62.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 722      |\n",
      "|    iterations         | 64900    |\n",
      "|    time_elapsed       | 7182     |\n",
      "|    total_timesteps    | 5192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.996   |\n",
      "|    explained_variance | 0.315    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64899    |\n",
      "|    policy_loss        | -0.0448  |\n",
      "|    value_loss         | 0.0634   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5200000, episode_reward=64.80 +/- 36.34\n",
      "Episode length: 5188.40 +/- 1128.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.19e+03 |\n",
      "|    mean_reward        | 64.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.378    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64999    |\n",
      "|    policy_loss        | -0.0489  |\n",
      "|    value_loss         | 0.0788   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.97e+03 |\n",
      "|    ep_rew_mean     | 62       |\n",
      "| time/              |          |\n",
      "|    fps             | 722      |\n",
      "|    iterations      | 65000    |\n",
      "|    time_elapsed    | 7200     |\n",
      "|    total_timesteps | 5200000  |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 4.93e+03  |\n",
      "|    ep_rew_mean        | 62.4      |\n",
      "| time/                 |           |\n",
      "|    fps                | 722       |\n",
      "|    iterations         | 65100     |\n",
      "|    time_elapsed       | 7207      |\n",
      "|    total_timesteps    | 5208000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.04     |\n",
      "|    explained_variance | 0.843     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 65099     |\n",
      "|    policy_loss        | -0.000319 |\n",
      "|    value_loss         | 0.0519    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=5210000, episode_reward=43.00 +/- 13.19\n",
      "Episode length: 4445.00 +/- 877.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.44e+03 |\n",
      "|    mean_reward        | 43       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5210000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.569    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65124    |\n",
      "|    policy_loss        | -0.0257  |\n",
      "|    value_loss         | 0.0253   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.92e+03 |\n",
      "|    ep_rew_mean        | 66.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 722      |\n",
      "|    iterations         | 65200    |\n",
      "|    time_elapsed       | 7224     |\n",
      "|    total_timesteps    | 5216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.863   |\n",
      "|    explained_variance | 0.686    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65199    |\n",
      "|    policy_loss        | 0.0299   |\n",
      "|    value_loss         | 0.0607   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5220000, episode_reward=51.00 +/- 17.64\n",
      "Episode length: 4458.20 +/- 770.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.46e+03 |\n",
      "|    mean_reward        | 51       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.715    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65249    |\n",
      "|    policy_loss        | 0.0407   |\n",
      "|    value_loss         | 0.0359   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.9e+03  |\n",
      "|    ep_rew_mean        | 65.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 721      |\n",
      "|    iterations         | 65300    |\n",
      "|    time_elapsed       | 7240     |\n",
      "|    total_timesteps    | 5224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.99    |\n",
      "|    explained_variance | 0.0615   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65299    |\n",
      "|    policy_loss        | -0.0286  |\n",
      "|    value_loss         | 0.0898   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5230000, episode_reward=61.60 +/- 13.12\n",
      "Episode length: 5238.00 +/- 1018.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.24e+03 |\n",
      "|    mean_reward        | 61.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5230000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.457    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65374    |\n",
      "|    policy_loss        | -0.0302  |\n",
      "|    value_loss         | 0.149    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.87e+03 |\n",
      "|    ep_rew_mean        | 65.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 720      |\n",
      "|    iterations         | 65400    |\n",
      "|    time_elapsed       | 7258     |\n",
      "|    total_timesteps    | 5232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.965   |\n",
      "|    explained_variance | 0.814    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65399    |\n",
      "|    policy_loss        | 0.0388   |\n",
      "|    value_loss         | 0.0241   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5240000, episode_reward=64.20 +/- 34.97\n",
      "Episode length: 4837.60 +/- 1462.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.84e+03 |\n",
      "|    mean_reward        | 64.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.996   |\n",
      "|    explained_variance | 0.818    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65499    |\n",
      "|    policy_loss        | -0.0408  |\n",
      "|    value_loss         | 0.0258   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.89e+03 |\n",
      "|    ep_rew_mean     | 65.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 720      |\n",
      "|    iterations      | 65500    |\n",
      "|    time_elapsed    | 7276     |\n",
      "|    total_timesteps | 5240000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.85e+03 |\n",
      "|    ep_rew_mean        | 65.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 720      |\n",
      "|    iterations         | 65600    |\n",
      "|    time_elapsed       | 7283     |\n",
      "|    total_timesteps    | 5248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.963   |\n",
      "|    explained_variance | 0.586    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65599    |\n",
      "|    policy_loss        | 0.0506   |\n",
      "|    value_loss         | 0.0482   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5250000, episode_reward=41.80 +/- 13.54\n",
      "Episode length: 3841.20 +/- 980.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.84e+03 |\n",
      "|    mean_reward        | 41.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.463    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65624    |\n",
      "|    policy_loss        | -0.0605  |\n",
      "|    value_loss         | 0.316    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.83e+03 |\n",
      "|    ep_rew_mean        | 66.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 720      |\n",
      "|    iterations         | 65700    |\n",
      "|    time_elapsed       | 7298     |\n",
      "|    total_timesteps    | 5256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65699    |\n",
      "|    policy_loss        | -0.0436  |\n",
      "|    value_loss         | 0.0269   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5260000, episode_reward=64.40 +/- 18.24\n",
      "Episode length: 4869.00 +/- 1258.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.87e+03 |\n",
      "|    mean_reward        | 64.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.963   |\n",
      "|    explained_variance | 0.655    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65749    |\n",
      "|    policy_loss        | 0.0519   |\n",
      "|    value_loss         | 0.0279   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.84e+03 |\n",
      "|    ep_rew_mean        | 65.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 719      |\n",
      "|    iterations         | 65800    |\n",
      "|    time_elapsed       | 7315     |\n",
      "|    total_timesteps    | 5264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.949   |\n",
      "|    explained_variance | 0.838    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65799    |\n",
      "|    policy_loss        | 0.0283   |\n",
      "|    value_loss         | 0.0613   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5270000, episode_reward=33.00 +/- 15.62\n",
      "Episode length: 3801.80 +/- 1178.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.8e+03  |\n",
      "|    mean_reward        | 33       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5270000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.986   |\n",
      "|    explained_variance | 0.611    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65874    |\n",
      "|    policy_loss        | -0.0296  |\n",
      "|    value_loss         | 0.0397   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.91e+03 |\n",
      "|    ep_rew_mean        | 68.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 719      |\n",
      "|    iterations         | 65900    |\n",
      "|    time_elapsed       | 7330     |\n",
      "|    total_timesteps    | 5272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.933   |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65899    |\n",
      "|    policy_loss        | -0.0104  |\n",
      "|    value_loss         | 0.0312   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5280000, episode_reward=85.60 +/- 24.78\n",
      "Episode length: 5942.20 +/- 654.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.94e+03 |\n",
      "|    mean_reward        | 85.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.887   |\n",
      "|    explained_variance | 0.773    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65999    |\n",
      "|    policy_loss        | -0.0384  |\n",
      "|    value_loss         | 0.0535   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | 71.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 718      |\n",
      "|    iterations      | 66000    |\n",
      "|    time_elapsed    | 7350     |\n",
      "|    total_timesteps | 5280000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.98e+03 |\n",
      "|    ep_rew_mean        | 70.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 718      |\n",
      "|    iterations         | 66100    |\n",
      "|    time_elapsed       | 7357     |\n",
      "|    total_timesteps    | 5288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.943   |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66099    |\n",
      "|    policy_loss        | -0.0681  |\n",
      "|    value_loss         | 0.0502   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5290000, episode_reward=80.00 +/- 20.82\n",
      "Episode length: 5827.40 +/- 834.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.83e+03 |\n",
      "|    mean_reward        | 80       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5290000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.764    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66124    |\n",
      "|    policy_loss        | -0.0193  |\n",
      "|    value_loss         | 0.0305   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.98e+03 |\n",
      "|    ep_rew_mean        | 70.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 717      |\n",
      "|    iterations         | 66200    |\n",
      "|    time_elapsed       | 7376     |\n",
      "|    total_timesteps    | 5296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66199    |\n",
      "|    policy_loss        | 0.0331   |\n",
      "|    value_loss         | 0.0142   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5300000, episode_reward=72.60 +/- 20.67\n",
      "Episode length: 5135.20 +/- 802.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.14e+03 |\n",
      "|    mean_reward        | 72.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.685    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66249    |\n",
      "|    policy_loss        | -0.0152  |\n",
      "|    value_loss         | 0.0845   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.08e+03 |\n",
      "|    ep_rew_mean        | 72       |\n",
      "| time/                 |          |\n",
      "|    fps                | 717      |\n",
      "|    iterations         | 66300    |\n",
      "|    time_elapsed       | 7394     |\n",
      "|    total_timesteps    | 5304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66299    |\n",
      "|    policy_loss        | -0.028   |\n",
      "|    value_loss         | 0.0362   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5310000, episode_reward=61.80 +/- 13.61\n",
      "Episode length: 4788.00 +/- 1084.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.79e+03 |\n",
      "|    mean_reward        | 61.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5310000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.608    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66374    |\n",
      "|    policy_loss        | 0.0135   |\n",
      "|    value_loss         | 0.0457   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.06e+03 |\n",
      "|    ep_rew_mean        | 71.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 716      |\n",
      "|    iterations         | 66400    |\n",
      "|    time_elapsed       | 7412     |\n",
      "|    total_timesteps    | 5312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.727    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66399    |\n",
      "|    policy_loss        | 0.0172   |\n",
      "|    value_loss         | 0.0295   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5320000, episode_reward=90.00 +/- 23.13\n",
      "Episode length: 5882.40 +/- 788.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.88e+03 |\n",
      "|    mean_reward        | 90       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.781    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66499    |\n",
      "|    policy_loss        | -0.0349  |\n",
      "|    value_loss         | 0.0305   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.08e+03 |\n",
      "|    ep_rew_mean     | 72       |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 66500    |\n",
      "|    time_elapsed    | 7431     |\n",
      "|    total_timesteps | 5320000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.15e+03 |\n",
      "|    ep_rew_mean        | 73.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 716      |\n",
      "|    iterations         | 66600    |\n",
      "|    time_elapsed       | 7438     |\n",
      "|    total_timesteps    | 5328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.969   |\n",
      "|    explained_variance | 0.467    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66599    |\n",
      "|    policy_loss        | -0.096   |\n",
      "|    value_loss         | 0.19     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5330000, episode_reward=92.40 +/- 12.56\n",
      "Episode length: 5917.20 +/- 662.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.92e+03 |\n",
      "|    mean_reward        | 92.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5330000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.955   |\n",
      "|    explained_variance | 0.445    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66624    |\n",
      "|    policy_loss        | -0.156   |\n",
      "|    value_loss         | 0.175    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.17e+03 |\n",
      "|    ep_rew_mean        | 74.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 715      |\n",
      "|    iterations         | 66700    |\n",
      "|    time_elapsed       | 7458     |\n",
      "|    total_timesteps    | 5336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.997   |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66699    |\n",
      "|    policy_loss        | -0.0127  |\n",
      "|    value_loss         | 0.039    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5340000, episode_reward=80.20 +/- 17.81\n",
      "Episode length: 5771.00 +/- 761.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.77e+03 |\n",
      "|    mean_reward        | 80.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.989   |\n",
      "|    explained_variance | 0.451    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66749    |\n",
      "|    policy_loss        | -0.0919  |\n",
      "|    value_loss         | 0.152    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.17e+03 |\n",
      "|    ep_rew_mean        | 76       |\n",
      "| time/                 |          |\n",
      "|    fps                | 714      |\n",
      "|    iterations         | 66800    |\n",
      "|    time_elapsed       | 7477     |\n",
      "|    total_timesteps    | 5344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.915   |\n",
      "|    explained_variance | 0.466    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66799    |\n",
      "|    policy_loss        | -0.0327  |\n",
      "|    value_loss         | 0.0337   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5350000, episode_reward=95.20 +/- 28.96\n",
      "Episode length: 5645.60 +/- 1107.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.65e+03 |\n",
      "|    mean_reward        | 95.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.863   |\n",
      "|    explained_variance | 0.469    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66874    |\n",
      "|    policy_loss        | 0.0928   |\n",
      "|    value_loss         | 0.0595   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.24e+03 |\n",
      "|    ep_rew_mean        | 74.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 713      |\n",
      "|    iterations         | 66900    |\n",
      "|    time_elapsed       | 7496     |\n",
      "|    total_timesteps    | 5352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.964   |\n",
      "|    explained_variance | 0.711    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66899    |\n",
      "|    policy_loss        | -0.00146 |\n",
      "|    value_loss         | 0.0196   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5360000, episode_reward=135.80 +/- 101.82\n",
      "Episode length: 6471.00 +/- 780.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.47e+03 |\n",
      "|    mean_reward        | 136      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.955   |\n",
      "|    explained_variance | 0.799    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66999    |\n",
      "|    policy_loss        | -0.0325  |\n",
      "|    value_loss         | 0.0244   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.32e+03 |\n",
      "|    ep_rew_mean     | 76.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 67000    |\n",
      "|    time_elapsed    | 7517     |\n",
      "|    total_timesteps | 5360000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.33e+03 |\n",
      "|    ep_rew_mean        | 76.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 713      |\n",
      "|    iterations         | 67100    |\n",
      "|    time_elapsed       | 7524     |\n",
      "|    total_timesteps    | 5368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.572    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67099    |\n",
      "|    policy_loss        | -0.0416  |\n",
      "|    value_loss         | 0.0625   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5370000, episode_reward=59.80 +/- 16.44\n",
      "Episode length: 4940.20 +/- 854.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.94e+03 |\n",
      "|    mean_reward        | 59.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5370000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.881   |\n",
      "|    explained_variance | 0.542    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67124    |\n",
      "|    policy_loss        | -0.0834  |\n",
      "|    value_loss         | 0.188    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.3e+03  |\n",
      "|    ep_rew_mean        | 75.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 712      |\n",
      "|    iterations         | 67200    |\n",
      "|    time_elapsed       | 7542     |\n",
      "|    total_timesteps    | 5376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.731    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67199    |\n",
      "|    policy_loss        | 0.0848   |\n",
      "|    value_loss         | 0.0432   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5380000, episode_reward=88.20 +/- 32.32\n",
      "Episode length: 5768.00 +/- 1068.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.77e+03 |\n",
      "|    mean_reward        | 88.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.741    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67249    |\n",
      "|    policy_loss        | -0.019   |\n",
      "|    value_loss         | 0.0273   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.34e+03 |\n",
      "|    ep_rew_mean        | 79.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 711      |\n",
      "|    iterations         | 67300    |\n",
      "|    time_elapsed       | 7562     |\n",
      "|    total_timesteps    | 5384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.941   |\n",
      "|    explained_variance | 0.602    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67299    |\n",
      "|    policy_loss        | 0.0632   |\n",
      "|    value_loss         | 0.0508   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5390000, episode_reward=115.00 +/- 110.50\n",
      "Episode length: 5529.00 +/- 1624.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.53e+03 |\n",
      "|    mean_reward        | 115      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5390000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.989   |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67374    |\n",
      "|    policy_loss        | 0.0127   |\n",
      "|    value_loss         | 0.0415   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.42e+03 |\n",
      "|    ep_rew_mean        | 80.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 711      |\n",
      "|    iterations         | 67400    |\n",
      "|    time_elapsed       | 7580     |\n",
      "|    total_timesteps    | 5392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.962   |\n",
      "|    explained_variance | 0.841    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67399    |\n",
      "|    policy_loss        | 0.0366   |\n",
      "|    value_loss         | 0.0244   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5400000, episode_reward=40.40 +/- 13.53\n",
      "Episode length: 4158.40 +/- 996.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.16e+03 |\n",
      "|    mean_reward        | 40.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.757    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67499    |\n",
      "|    policy_loss        | -0.0323  |\n",
      "|    value_loss         | 0.0296   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.39e+03 |\n",
      "|    ep_rew_mean     | 82.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 67500    |\n",
      "|    time_elapsed    | 7596     |\n",
      "|    total_timesteps | 5400000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.26e+03 |\n",
      "|    ep_rew_mean        | 79.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 711      |\n",
      "|    iterations         | 67600    |\n",
      "|    time_elapsed       | 7603     |\n",
      "|    total_timesteps    | 5408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.708    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67599    |\n",
      "|    policy_loss        | 0.112    |\n",
      "|    value_loss         | 0.0503   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5410000, episode_reward=111.60 +/- 52.29\n",
      "Episode length: 5644.40 +/- 1192.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.64e+03 |\n",
      "|    mean_reward        | 112      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5410000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.96    |\n",
      "|    explained_variance | 0.857    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67624    |\n",
      "|    policy_loss        | 0.0256   |\n",
      "|    value_loss         | 0.021    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.92e+03 |\n",
      "|    ep_rew_mean        | 72.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 710      |\n",
      "|    iterations         | 67700    |\n",
      "|    time_elapsed       | 7623     |\n",
      "|    total_timesteps    | 5416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.525    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67699    |\n",
      "|    policy_loss        | -0.0145  |\n",
      "|    value_loss         | 0.0301   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5420000, episode_reward=59.80 +/- 19.46\n",
      "Episode length: 5178.60 +/- 677.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.18e+03 |\n",
      "|    mean_reward        | 59.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.979   |\n",
      "|    explained_variance | 0.436    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67749    |\n",
      "|    policy_loss        | 0.0465   |\n",
      "|    value_loss         | 0.0434   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.8e+03  |\n",
      "|    ep_rew_mean        | 69.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 709      |\n",
      "|    iterations         | 67800    |\n",
      "|    time_elapsed       | 7641     |\n",
      "|    total_timesteps    | 5424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67799    |\n",
      "|    policy_loss        | -0.0249  |\n",
      "|    value_loss         | 0.0349   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5430000, episode_reward=63.00 +/- 22.78\n",
      "Episode length: 4944.00 +/- 741.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.94e+03 |\n",
      "|    mean_reward        | 63       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5430000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.524    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67874    |\n",
      "|    policy_loss        | -0.0666  |\n",
      "|    value_loss         | 0.0507   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.63e+03 |\n",
      "|    ep_rew_mean        | 66.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 709      |\n",
      "|    iterations         | 67900    |\n",
      "|    time_elapsed       | 7658     |\n",
      "|    total_timesteps    | 5432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.959   |\n",
      "|    explained_variance | 0.767    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67899    |\n",
      "|    policy_loss        | 0.0118   |\n",
      "|    value_loss         | 0.0284   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5440000, episode_reward=85.00 +/- 16.02\n",
      "Episode length: 5479.60 +/- 542.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.48e+03 |\n",
      "|    mean_reward        | 85       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.777    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67999    |\n",
      "|    policy_loss        | -0.0233  |\n",
      "|    value_loss         | 0.0238   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.62e+03 |\n",
      "|    ep_rew_mean     | 65       |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 68000    |\n",
      "|    time_elapsed    | 7677     |\n",
      "|    total_timesteps | 5440000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.62e+03 |\n",
      "|    ep_rew_mean        | 65.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 708      |\n",
      "|    iterations         | 68100    |\n",
      "|    time_elapsed       | 7685     |\n",
      "|    total_timesteps    | 5448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.942   |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68099    |\n",
      "|    policy_loss        | 0.0437   |\n",
      "|    value_loss         | 0.0386   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5450000, episode_reward=96.20 +/- 72.03\n",
      "Episode length: 4892.00 +/- 1394.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.89e+03 |\n",
      "|    mean_reward        | 96.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.968   |\n",
      "|    explained_variance | 0.695    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68124    |\n",
      "|    policy_loss        | -0.0449  |\n",
      "|    value_loss         | 0.0238   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.64e+03 |\n",
      "|    ep_rew_mean        | 63.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 708      |\n",
      "|    iterations         | 68200    |\n",
      "|    time_elapsed       | 7702     |\n",
      "|    total_timesteps    | 5456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.743    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68199    |\n",
      "|    policy_loss        | 0.0243   |\n",
      "|    value_loss         | 0.0464   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5460000, episode_reward=54.80 +/- 38.66\n",
      "Episode length: 4326.20 +/- 1034.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.33e+03 |\n",
      "|    mean_reward        | 54.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.675    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68249    |\n",
      "|    policy_loss        | 0.038    |\n",
      "|    value_loss         | 0.045    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.64e+03 |\n",
      "|    ep_rew_mean        | 63.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 707      |\n",
      "|    iterations         | 68300    |\n",
      "|    time_elapsed       | 7718     |\n",
      "|    total_timesteps    | 5464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.929   |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68299    |\n",
      "|    policy_loss        | 0.0494   |\n",
      "|    value_loss         | 0.0272   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5470000, episode_reward=80.40 +/- 21.85\n",
      "Episode length: 5480.60 +/- 952.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.48e+03 |\n",
      "|    mean_reward        | 80.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5470000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.885    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68374    |\n",
      "|    policy_loss        | -0.0155  |\n",
      "|    value_loss         | 0.152    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.6e+03  |\n",
      "|    ep_rew_mean        | 62.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 707      |\n",
      "|    iterations         | 68400    |\n",
      "|    time_elapsed       | 7737     |\n",
      "|    total_timesteps    | 5472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.991   |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68399    |\n",
      "|    policy_loss        | 0.0247   |\n",
      "|    value_loss         | 0.0828   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5480000, episode_reward=66.80 +/- 10.46\n",
      "Episode length: 4897.80 +/- 524.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.9e+03  |\n",
      "|    mean_reward        | 66.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.82     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68499    |\n",
      "|    policy_loss        | 0.0251   |\n",
      "|    value_loss         | 0.0253   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.65e+03 |\n",
      "|    ep_rew_mean     | 67.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 68500    |\n",
      "|    time_elapsed    | 7754     |\n",
      "|    total_timesteps | 5480000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.63e+03 |\n",
      "|    ep_rew_mean        | 67.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 707      |\n",
      "|    iterations         | 68600    |\n",
      "|    time_elapsed       | 7762     |\n",
      "|    total_timesteps    | 5488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.861    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68599    |\n",
      "|    policy_loss        | 0.0118   |\n",
      "|    value_loss         | 0.0327   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5490000, episode_reward=77.00 +/- 21.34\n",
      "Episode length: 5572.20 +/- 719.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.57e+03 |\n",
      "|    mean_reward        | 77       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5490000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.868   |\n",
      "|    explained_variance | 0.275    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68624    |\n",
      "|    policy_loss        | -0.236   |\n",
      "|    value_loss         | 0.584    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.58e+03 |\n",
      "|    ep_rew_mean        | 64.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 706      |\n",
      "|    iterations         | 68700    |\n",
      "|    time_elapsed       | 7781     |\n",
      "|    total_timesteps    | 5496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.95    |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68699    |\n",
      "|    policy_loss        | -0.0406  |\n",
      "|    value_loss         | 0.0235   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5500000, episode_reward=80.20 +/- 23.09\n",
      "Episode length: 5689.60 +/- 616.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.69e+03 |\n",
      "|    mean_reward        | 80.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.922   |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68749    |\n",
      "|    policy_loss        | 0.0204   |\n",
      "|    value_loss         | 0.0171   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.55e+03 |\n",
      "|    ep_rew_mean        | 64.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 705      |\n",
      "|    iterations         | 68800    |\n",
      "|    time_elapsed       | 7800     |\n",
      "|    total_timesteps    | 5504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.963   |\n",
      "|    explained_variance | 0.537    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68799    |\n",
      "|    policy_loss        | -0.113   |\n",
      "|    value_loss         | 0.192    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5510000, episode_reward=121.60 +/- 112.38\n",
      "Episode length: 5708.20 +/- 1968.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.71e+03 |\n",
      "|    mean_reward        | 122      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5510000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.988   |\n",
      "|    explained_variance | 0.705    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68874    |\n",
      "|    policy_loss        | 0.0145   |\n",
      "|    value_loss         | 0.021    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.56e+03 |\n",
      "|    ep_rew_mean        | 62.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 704      |\n",
      "|    iterations         | 68900    |\n",
      "|    time_elapsed       | 7819     |\n",
      "|    total_timesteps    | 5512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.895   |\n",
      "|    explained_variance | 0.717    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68899    |\n",
      "|    policy_loss        | 0.0126   |\n",
      "|    value_loss         | 0.0283   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5520000, episode_reward=88.80 +/- 50.65\n",
      "Episode length: 5131.40 +/- 967.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.13e+03 |\n",
      "|    mean_reward        | 88.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.785    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68999    |\n",
      "|    policy_loss        | 0.00156  |\n",
      "|    value_loss         | 0.016    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.61e+03 |\n",
      "|    ep_rew_mean     | 63       |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 69000    |\n",
      "|    time_elapsed    | 7836     |\n",
      "|    total_timesteps | 5520000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.69e+03 |\n",
      "|    ep_rew_mean        | 65.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 704      |\n",
      "|    iterations         | 69100    |\n",
      "|    time_elapsed       | 7844     |\n",
      "|    total_timesteps    | 5528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69099    |\n",
      "|    policy_loss        | -0.0731  |\n",
      "|    value_loss         | 0.0439   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5530000, episode_reward=72.80 +/- 25.88\n",
      "Episode length: 5108.80 +/- 594.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.11e+03 |\n",
      "|    mean_reward        | 72.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5530000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.843    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69124    |\n",
      "|    policy_loss        | 0.0324   |\n",
      "|    value_loss         | 0.0353   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.88e+03 |\n",
      "|    ep_rew_mean        | 70.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 704      |\n",
      "|    iterations         | 69200    |\n",
      "|    time_elapsed       | 7862     |\n",
      "|    total_timesteps    | 5536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.49     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69199    |\n",
      "|    policy_loss        | -0.0506  |\n",
      "|    value_loss         | 0.0359   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5540000, episode_reward=94.00 +/- 24.33\n",
      "Episode length: 6137.60 +/- 872.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.14e+03 |\n",
      "|    mean_reward        | 94       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.956   |\n",
      "|    explained_variance | 0.455    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69249    |\n",
      "|    policy_loss        | -0.1     |\n",
      "|    value_loss         | 0.224    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.96e+03 |\n",
      "|    ep_rew_mean        | 71.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 703      |\n",
      "|    iterations         | 69300    |\n",
      "|    time_elapsed       | 7882     |\n",
      "|    total_timesteps    | 5544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.896   |\n",
      "|    explained_variance | 0.788    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69299    |\n",
      "|    policy_loss        | 0.00163  |\n",
      "|    value_loss         | 0.0252   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5550000, episode_reward=126.60 +/- 37.58\n",
      "Episode length: 5935.60 +/- 487.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.94e+03 |\n",
      "|    mean_reward        | 127      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.659    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69374    |\n",
      "|    policy_loss        | -0.0399  |\n",
      "|    value_loss         | 0.159    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.14e+03 |\n",
      "|    ep_rew_mean        | 75.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 702      |\n",
      "|    iterations         | 69400    |\n",
      "|    time_elapsed       | 7902     |\n",
      "|    total_timesteps    | 5552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.988   |\n",
      "|    explained_variance | 0.561    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69399    |\n",
      "|    policy_loss        | -0.0944  |\n",
      "|    value_loss         | 0.144    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5560000, episode_reward=76.00 +/- 19.47\n",
      "Episode length: 5729.60 +/- 792.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.73e+03 |\n",
      "|    mean_reward        | 76       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.762    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69499    |\n",
      "|    policy_loss        | 0.0181   |\n",
      "|    value_loss         | 0.0229   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.24e+03 |\n",
      "|    ep_rew_mean     | 77.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 701      |\n",
      "|    iterations      | 69500    |\n",
      "|    time_elapsed    | 7921     |\n",
      "|    total_timesteps | 5560000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.26e+03 |\n",
      "|    ep_rew_mean        | 78.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 702      |\n",
      "|    iterations         | 69600    |\n",
      "|    time_elapsed       | 7928     |\n",
      "|    total_timesteps    | 5568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.661    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69599    |\n",
      "|    policy_loss        | -0.0413  |\n",
      "|    value_loss         | 0.0278   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5570000, episode_reward=78.00 +/- 21.61\n",
      "Episode length: 5103.20 +/- 459.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.1e+03  |\n",
      "|    mean_reward        | 78       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5570000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.944   |\n",
      "|    explained_variance | 0.648    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69624    |\n",
      "|    policy_loss        | -0.0493  |\n",
      "|    value_loss         | 0.0821   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.2e+03  |\n",
      "|    ep_rew_mean        | 79.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 701      |\n",
      "|    iterations         | 69700    |\n",
      "|    time_elapsed       | 7946     |\n",
      "|    total_timesteps    | 5576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.893   |\n",
      "|    explained_variance | 0.283    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69699    |\n",
      "|    policy_loss        | -0.052   |\n",
      "|    value_loss         | 0.0916   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5580000, episode_reward=100.80 +/- 54.08\n",
      "Episode length: 5114.00 +/- 893.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.11e+03 |\n",
      "|    mean_reward        | 101      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.612    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69749    |\n",
      "|    policy_loss        | 0.0357   |\n",
      "|    value_loss         | 0.0787   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.89e+03 |\n",
      "|    ep_rew_mean        | 74.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 701      |\n",
      "|    iterations         | 69800    |\n",
      "|    time_elapsed       | 7964     |\n",
      "|    total_timesteps    | 5584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.901   |\n",
      "|    explained_variance | 0.605    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69799    |\n",
      "|    policy_loss        | -0.0184  |\n",
      "|    value_loss         | 0.0541   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5590000, episode_reward=94.20 +/- 23.93\n",
      "Episode length: 6076.00 +/- 742.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.08e+03 |\n",
      "|    mean_reward        | 94.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5590000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.981   |\n",
      "|    explained_variance | 0.194    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69874    |\n",
      "|    policy_loss        | -0.0621  |\n",
      "|    value_loss         | 0.0759   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.77e+03 |\n",
      "|    ep_rew_mean        | 72.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 700      |\n",
      "|    iterations         | 69900    |\n",
      "|    time_elapsed       | 7984     |\n",
      "|    total_timesteps    | 5592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.969   |\n",
      "|    explained_variance | 0.579    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69899    |\n",
      "|    policy_loss        | -0.00733 |\n",
      "|    value_loss         | 0.0459   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5600000, episode_reward=94.40 +/- 42.24\n",
      "Episode length: 5428.40 +/- 1294.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.43e+03 |\n",
      "|    mean_reward        | 94.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.977   |\n",
      "|    explained_variance | 0.448    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69999    |\n",
      "|    policy_loss        | -0.228   |\n",
      "|    value_loss         | 0.286    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.72e+03 |\n",
      "|    ep_rew_mean     | 67.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 699      |\n",
      "|    iterations      | 70000    |\n",
      "|    time_elapsed    | 8002     |\n",
      "|    total_timesteps | 5600000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.75e+03 |\n",
      "|    ep_rew_mean        | 68.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 700      |\n",
      "|    iterations         | 70100    |\n",
      "|    time_elapsed       | 8010     |\n",
      "|    total_timesteps    | 5608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.743    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70099    |\n",
      "|    policy_loss        | -0.0974  |\n",
      "|    value_loss         | 0.128    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5610000, episode_reward=150.40 +/- 106.76\n",
      "Episode length: 5377.20 +/- 742.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.38e+03 |\n",
      "|    mean_reward        | 150      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5610000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.961   |\n",
      "|    explained_variance | 0.662    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70124    |\n",
      "|    policy_loss        | 0.0536   |\n",
      "|    value_loss         | 0.0435   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.77e+03 |\n",
      "|    ep_rew_mean        | 68.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 699      |\n",
      "|    iterations         | 70200    |\n",
      "|    time_elapsed       | 8028     |\n",
      "|    total_timesteps    | 5616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.978   |\n",
      "|    explained_variance | 0.731    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70199    |\n",
      "|    policy_loss        | 0.00413  |\n",
      "|    value_loss         | 0.0311   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5620000, episode_reward=127.40 +/- 83.61\n",
      "Episode length: 5910.20 +/- 1080.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.91e+03 |\n",
      "|    mean_reward        | 127      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5620000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.909   |\n",
      "|    explained_variance | 0.71     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70249    |\n",
      "|    policy_loss        | -0.0371  |\n",
      "|    value_loss         | 0.14     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.76e+03 |\n",
      "|    ep_rew_mean        | 69.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 698      |\n",
      "|    iterations         | 70300    |\n",
      "|    time_elapsed       | 8048     |\n",
      "|    total_timesteps    | 5624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.918   |\n",
      "|    explained_variance | 0.621    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70299    |\n",
      "|    policy_loss        | -0.105   |\n",
      "|    value_loss         | 0.127    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5630000, episode_reward=83.20 +/- 69.96\n",
      "Episode length: 5081.80 +/- 1460.94\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 5.08e+03  |\n",
      "|    mean_reward        | 83.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 5630000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.977    |\n",
      "|    explained_variance | 0.996     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 70374     |\n",
      "|    policy_loss        | -0.000208 |\n",
      "|    value_loss         | 0.039     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.77e+03 |\n",
      "|    ep_rew_mean        | 70.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 698      |\n",
      "|    iterations         | 70400    |\n",
      "|    time_elapsed       | 8065     |\n",
      "|    total_timesteps    | 5632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.916   |\n",
      "|    explained_variance | 0.526    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70399    |\n",
      "|    policy_loss        | 0.00867  |\n",
      "|    value_loss         | 0.0244   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5640000, episode_reward=123.00 +/- 54.53\n",
      "Episode length: 5582.80 +/- 774.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.58e+03 |\n",
      "|    mean_reward        | 123      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.926   |\n",
      "|    explained_variance | 0.738    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70499    |\n",
      "|    policy_loss        | 0.0294   |\n",
      "|    value_loss         | 0.0716   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.74e+03 |\n",
      "|    ep_rew_mean     | 70.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 70500    |\n",
      "|    time_elapsed    | 8085     |\n",
      "|    total_timesteps | 5640000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.78e+03 |\n",
      "|    ep_rew_mean        | 72.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 697      |\n",
      "|    iterations         | 70600    |\n",
      "|    time_elapsed       | 8092     |\n",
      "|    total_timesteps    | 5648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.834   |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70599    |\n",
      "|    policy_loss        | 0.00845  |\n",
      "|    value_loss         | 0.014    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5650000, episode_reward=68.20 +/- 21.57\n",
      "Episode length: 5201.40 +/- 1396.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.2e+03  |\n",
      "|    mean_reward        | 68.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.712    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70624    |\n",
      "|    policy_loss        | -0.0291  |\n",
      "|    value_loss         | 0.0261   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.74e+03 |\n",
      "|    ep_rew_mean        | 70.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 697      |\n",
      "|    iterations         | 70700    |\n",
      "|    time_elapsed       | 8110     |\n",
      "|    total_timesteps    | 5656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.932   |\n",
      "|    explained_variance | 0.323    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70699    |\n",
      "|    policy_loss        | -0.0482  |\n",
      "|    value_loss         | 0.115    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5660000, episode_reward=143.60 +/- 87.08\n",
      "Episode length: 5658.00 +/- 1088.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.66e+03 |\n",
      "|    mean_reward        | 144      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.949   |\n",
      "|    explained_variance | 0.436    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70749    |\n",
      "|    policy_loss        | -0.00804 |\n",
      "|    value_loss         | 0.0649   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.75e+03 |\n",
      "|    ep_rew_mean        | 70.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 696      |\n",
      "|    iterations         | 70800    |\n",
      "|    time_elapsed       | 8129     |\n",
      "|    total_timesteps    | 5664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.88    |\n",
      "|    explained_variance | 0.798    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70799    |\n",
      "|    policy_loss        | 0.00566  |\n",
      "|    value_loss         | 0.0232   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5670000, episode_reward=154.80 +/- 30.45\n",
      "Episode length: 6563.00 +/- 627.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.56e+03 |\n",
      "|    mean_reward        | 155      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5670000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.964   |\n",
      "|    explained_variance | 0.693    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70874    |\n",
      "|    policy_loss        | -0.0479  |\n",
      "|    value_loss         | 0.0376   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.78e+03 |\n",
      "|    ep_rew_mean        | 72       |\n",
      "| time/                 |          |\n",
      "|    fps                | 695      |\n",
      "|    iterations         | 70900    |\n",
      "|    time_elapsed       | 8150     |\n",
      "|    total_timesteps    | 5672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.838   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70899    |\n",
      "|    policy_loss        | -0.0646  |\n",
      "|    value_loss         | 0.514    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5680000, episode_reward=79.00 +/- 35.37\n",
      "Episode length: 4998.60 +/- 1385.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5e+03    |\n",
      "|    mean_reward        | 79       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.944   |\n",
      "|    explained_variance | 0.767    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70999    |\n",
      "|    policy_loss        | -0.0328  |\n",
      "|    value_loss         | 0.0239   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.83e+03 |\n",
      "|    ep_rew_mean     | 77.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 695      |\n",
      "|    iterations      | 71000    |\n",
      "|    time_elapsed    | 8167     |\n",
      "|    total_timesteps | 5680000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.83e+03 |\n",
      "|    ep_rew_mean        | 77.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 695      |\n",
      "|    iterations         | 71100    |\n",
      "|    time_elapsed       | 8175     |\n",
      "|    total_timesteps    | 5688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.858   |\n",
      "|    explained_variance | 0.89     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71099    |\n",
      "|    policy_loss        | -0.0794  |\n",
      "|    value_loss         | 0.0398   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5690000, episode_reward=129.20 +/- 31.81\n",
      "Episode length: 6377.60 +/- 191.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.38e+03 |\n",
      "|    mean_reward        | 129      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5690000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.417    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71124    |\n",
      "|    policy_loss        | -0.213   |\n",
      "|    value_loss         | 0.405    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.86e+03 |\n",
      "|    ep_rew_mean        | 82.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 694      |\n",
      "|    iterations         | 71200    |\n",
      "|    time_elapsed       | 8195     |\n",
      "|    total_timesteps    | 5696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.897    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71199    |\n",
      "|    policy_loss        | 0.0567   |\n",
      "|    value_loss         | 0.0867   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5700000, episode_reward=113.60 +/- 123.48\n",
      "Episode length: 5413.80 +/- 1694.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.41e+03 |\n",
      "|    mean_reward        | 114      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.999   |\n",
      "|    explained_variance | 0.711    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71249    |\n",
      "|    policy_loss        | 0.0317   |\n",
      "|    value_loss         | 0.0317   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.93e+03 |\n",
      "|    ep_rew_mean        | 84.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 694      |\n",
      "|    iterations         | 71300    |\n",
      "|    time_elapsed       | 8214     |\n",
      "|    total_timesteps    | 5704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.967   |\n",
      "|    explained_variance | 0.79     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71299    |\n",
      "|    policy_loss        | 0.0205   |\n",
      "|    value_loss         | 0.0225   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5710000, episode_reward=109.20 +/- 37.76\n",
      "Episode length: 6043.00 +/- 806.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.04e+03 |\n",
      "|    mean_reward        | 109      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5710000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71374    |\n",
      "|    policy_loss        | 0.0424   |\n",
      "|    value_loss         | 0.0264   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.13e+03 |\n",
      "|    ep_rew_mean        | 88.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 693      |\n",
      "|    iterations         | 71400    |\n",
      "|    time_elapsed       | 8234     |\n",
      "|    total_timesteps    | 5712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.942   |\n",
      "|    explained_variance | 0.729    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71399    |\n",
      "|    policy_loss        | 0.0246   |\n",
      "|    value_loss         | 0.0223   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5720000, episode_reward=101.40 +/- 34.81\n",
      "Episode length: 5610.00 +/- 1076.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.61e+03 |\n",
      "|    mean_reward        | 101      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.97    |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71499    |\n",
      "|    policy_loss        | -0.0507  |\n",
      "|    value_loss         | 0.0108   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.34e+03 |\n",
      "|    ep_rew_mean     | 94.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 693      |\n",
      "|    iterations      | 71500    |\n",
      "|    time_elapsed    | 8252     |\n",
      "|    total_timesteps | 5720000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.45e+03 |\n",
      "|    ep_rew_mean        | 96       |\n",
      "| time/                 |          |\n",
      "|    fps                | 693      |\n",
      "|    iterations         | 71600    |\n",
      "|    time_elapsed       | 8260     |\n",
      "|    total_timesteps    | 5728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.537    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71599    |\n",
      "|    policy_loss        | -0.00127 |\n",
      "|    value_loss         | 0.106    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5730000, episode_reward=132.80 +/- 84.68\n",
      "Episode length: 5924.80 +/- 818.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.92e+03 |\n",
      "|    mean_reward        | 133      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5730000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.553    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71624    |\n",
      "|    policy_loss        | -0.207   |\n",
      "|    value_loss         | 0.179    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.4e+03  |\n",
      "|    ep_rew_mean        | 94.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 692      |\n",
      "|    iterations         | 71700    |\n",
      "|    time_elapsed       | 8279     |\n",
      "|    total_timesteps    | 5736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.546    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71699    |\n",
      "|    policy_loss        | -0.0696  |\n",
      "|    value_loss         | 0.0556   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5740000, episode_reward=128.00 +/- 108.10\n",
      "Episode length: 5334.20 +/- 1029.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.33e+03 |\n",
      "|    mean_reward        | 128      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.815    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71749    |\n",
      "|    policy_loss        | 0.00295  |\n",
      "|    value_loss         | 0.101    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.33e+03 |\n",
      "|    ep_rew_mean        | 92.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 692      |\n",
      "|    iterations         | 71800    |\n",
      "|    time_elapsed       | 8298     |\n",
      "|    total_timesteps    | 5744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.83     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71799    |\n",
      "|    policy_loss        | -0.011   |\n",
      "|    value_loss         | 0.0138   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5750000, episode_reward=154.00 +/- 50.37\n",
      "Episode length: 6487.20 +/- 970.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.49e+03 |\n",
      "|    mean_reward        | 154      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.413    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71874    |\n",
      "|    policy_loss        | -0.0331  |\n",
      "|    value_loss         | 0.0389   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.33e+03 |\n",
      "|    ep_rew_mean        | 91.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 691      |\n",
      "|    iterations         | 71900    |\n",
      "|    time_elapsed       | 8318     |\n",
      "|    total_timesteps    | 5752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71899    |\n",
      "|    policy_loss        | 0.014    |\n",
      "|    value_loss         | 0.0103   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5760000, episode_reward=64.40 +/- 21.67\n",
      "Episode length: 5021.40 +/- 384.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.02e+03 |\n",
      "|    mean_reward        | 64.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.968   |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71999    |\n",
      "|    policy_loss        | 0.0223   |\n",
      "|    value_loss         | 0.0227   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.34e+03 |\n",
      "|    ep_rew_mean     | 90.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 690      |\n",
      "|    iterations      | 72000    |\n",
      "|    time_elapsed    | 8336     |\n",
      "|    total_timesteps | 5760000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.42e+03 |\n",
      "|    ep_rew_mean        | 91.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 691      |\n",
      "|    iterations         | 72100    |\n",
      "|    time_elapsed       | 8344     |\n",
      "|    total_timesteps    | 5768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.97    |\n",
      "|    explained_variance | 0.895    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72099    |\n",
      "|    policy_loss        | 0.026    |\n",
      "|    value_loss         | 0.0322   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5770000, episode_reward=100.20 +/- 36.01\n",
      "Episode length: 5547.40 +/- 621.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.55e+03 |\n",
      "|    mean_reward        | 100      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5770000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.994   |\n",
      "|    explained_variance | 0.579    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72124    |\n",
      "|    policy_loss        | -0.0124  |\n",
      "|    value_loss         | 0.225    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.36e+03 |\n",
      "|    ep_rew_mean        | 91.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 690      |\n",
      "|    iterations         | 72200    |\n",
      "|    time_elapsed       | 8363     |\n",
      "|    total_timesteps    | 5776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.632    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72199    |\n",
      "|    policy_loss        | 0.00787  |\n",
      "|    value_loss         | 0.0123   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5780000, episode_reward=90.60 +/- 23.12\n",
      "Episode length: 5703.00 +/- 951.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.7e+03  |\n",
      "|    mean_reward        | 90.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.969   |\n",
      "|    explained_variance | 0.789    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72249    |\n",
      "|    policy_loss        | 0.0357   |\n",
      "|    value_loss         | 0.0321   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.34e+03 |\n",
      "|    ep_rew_mean        | 88.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 690      |\n",
      "|    iterations         | 72300    |\n",
      "|    time_elapsed       | 8382     |\n",
      "|    total_timesteps    | 5784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.954   |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72299    |\n",
      "|    policy_loss        | -0.0987  |\n",
      "|    value_loss         | 0.0413   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5790000, episode_reward=106.60 +/- 41.07\n",
      "Episode length: 5571.00 +/- 1114.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.57e+03 |\n",
      "|    mean_reward        | 107      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5790000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.53     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72374    |\n",
      "|    policy_loss        | -0.00407 |\n",
      "|    value_loss         | 0.0325   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.36e+03 |\n",
      "|    ep_rew_mean        | 89.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 689      |\n",
      "|    iterations         | 72400    |\n",
      "|    time_elapsed       | 8400     |\n",
      "|    total_timesteps    | 5792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.963   |\n",
      "|    explained_variance | 0.48     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72399    |\n",
      "|    policy_loss        | 0.0782   |\n",
      "|    value_loss         | 0.0682   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5800000, episode_reward=80.00 +/- 19.64\n",
      "Episode length: 5451.80 +/- 388.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.45e+03 |\n",
      "|    mean_reward        | 80       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.728    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72499    |\n",
      "|    policy_loss        | -0.0546  |\n",
      "|    value_loss         | 0.0418   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.35e+03 |\n",
      "|    ep_rew_mean     | 90.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 688      |\n",
      "|    iterations      | 72500    |\n",
      "|    time_elapsed    | 8419     |\n",
      "|    total_timesteps | 5800000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.34e+03 |\n",
      "|    ep_rew_mean        | 90.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 689      |\n",
      "|    iterations         | 72600    |\n",
      "|    time_elapsed       | 8426     |\n",
      "|    total_timesteps    | 5808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72599    |\n",
      "|    policy_loss        | 0.0415   |\n",
      "|    value_loss         | 0.0516   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5810000, episode_reward=90.40 +/- 25.77\n",
      "Episode length: 5380.60 +/- 232.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.38e+03 |\n",
      "|    mean_reward        | 90.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5810000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.921   |\n",
      "|    explained_variance | 0.606    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72624    |\n",
      "|    policy_loss        | 0.0779   |\n",
      "|    value_loss         | 0.0383   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.32e+03 |\n",
      "|    ep_rew_mean        | 89.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 688      |\n",
      "|    iterations         | 72700    |\n",
      "|    time_elapsed       | 8444     |\n",
      "|    total_timesteps    | 5816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.527    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72699    |\n",
      "|    policy_loss        | -0.0322  |\n",
      "|    value_loss         | 0.0863   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5820000, episode_reward=143.40 +/- 79.60\n",
      "Episode length: 6285.00 +/- 692.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.28e+03 |\n",
      "|    mean_reward        | 143      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.536    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72749    |\n",
      "|    policy_loss        | -0.0346  |\n",
      "|    value_loss         | 0.144    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.3e+03  |\n",
      "|    ep_rew_mean        | 86.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 687      |\n",
      "|    iterations         | 72800    |\n",
      "|    time_elapsed       | 8465     |\n",
      "|    total_timesteps    | 5824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.648    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72799    |\n",
      "|    policy_loss        | 0.112    |\n",
      "|    value_loss         | 0.093    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5830000, episode_reward=126.00 +/- 18.72\n",
      "Episode length: 6216.60 +/- 214.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.22e+03 |\n",
      "|    mean_reward        | 126      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5830000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.971   |\n",
      "|    explained_variance | 0.665    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72874    |\n",
      "|    policy_loss        | 0.0185   |\n",
      "|    value_loss         | 0.027    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.29e+03 |\n",
      "|    ep_rew_mean        | 85.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 687      |\n",
      "|    iterations         | 72900    |\n",
      "|    time_elapsed       | 8485     |\n",
      "|    total_timesteps    | 5832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.9     |\n",
      "|    explained_variance | 0.321    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72899    |\n",
      "|    policy_loss        | -0.0442  |\n",
      "|    value_loss         | 0.232    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5840000, episode_reward=169.00 +/- 111.36\n",
      "Episode length: 5951.60 +/- 1135.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.95e+03 |\n",
      "|    mean_reward        | 169      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.924   |\n",
      "|    explained_variance | 0.509    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72999    |\n",
      "|    policy_loss        | -0.0267  |\n",
      "|    value_loss         | 0.228    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.29e+03 |\n",
      "|    ep_rew_mean     | 86.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 73000    |\n",
      "|    time_elapsed    | 8505     |\n",
      "|    total_timesteps | 5840000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.31e+03 |\n",
      "|    ep_rew_mean        | 89       |\n",
      "| time/                 |          |\n",
      "|    fps                | 686      |\n",
      "|    iterations         | 73100    |\n",
      "|    time_elapsed       | 8512     |\n",
      "|    total_timesteps    | 5848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.882   |\n",
      "|    explained_variance | 0.602    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73099    |\n",
      "|    policy_loss        | -0.0592  |\n",
      "|    value_loss         | 0.0869   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5850000, episode_reward=83.80 +/- 11.51\n",
      "Episode length: 5789.40 +/- 343.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.79e+03 |\n",
      "|    mean_reward        | 83.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5850000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.952   |\n",
      "|    explained_variance | 0.526    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73124    |\n",
      "|    policy_loss        | -0.00676 |\n",
      "|    value_loss         | 0.0225   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.35e+03 |\n",
      "|    ep_rew_mean        | 89.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 686      |\n",
      "|    iterations         | 73200    |\n",
      "|    time_elapsed       | 8532     |\n",
      "|    total_timesteps    | 5856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.922   |\n",
      "|    explained_variance | 0.489    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73199    |\n",
      "|    policy_loss        | -0.0534  |\n",
      "|    value_loss         | 0.205    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5860000, episode_reward=83.20 +/- 43.02\n",
      "Episode length: 4877.20 +/- 843.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.88e+03 |\n",
      "|    mean_reward        | 83.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5860000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.939   |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73249    |\n",
      "|    policy_loss        | -0.179   |\n",
      "|    value_loss         | 1.03     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.37e+03 |\n",
      "|    ep_rew_mean        | 92.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 685      |\n",
      "|    iterations         | 73300    |\n",
      "|    time_elapsed       | 8549     |\n",
      "|    total_timesteps    | 5864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.812    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73299    |\n",
      "|    policy_loss        | 0.0434   |\n",
      "|    value_loss         | 0.0227   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5870000, episode_reward=121.40 +/- 82.43\n",
      "Episode length: 5818.20 +/- 1062.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.82e+03 |\n",
      "|    mean_reward        | 121      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5870000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.813    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73374    |\n",
      "|    policy_loss        | 0.0433   |\n",
      "|    value_loss         | 0.0291   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.46e+03 |\n",
      "|    ep_rew_mean        | 94.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 685      |\n",
      "|    iterations         | 73400    |\n",
      "|    time_elapsed       | 8568     |\n",
      "|    total_timesteps    | 5872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.702    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73399    |\n",
      "|    policy_loss        | 0.0237   |\n",
      "|    value_loss         | 0.0317   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5880000, episode_reward=85.20 +/- 31.29\n",
      "Episode length: 5194.60 +/- 628.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.19e+03 |\n",
      "|    mean_reward        | 85.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.995   |\n",
      "|    explained_variance | 0.397    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73499    |\n",
      "|    policy_loss        | -0.127   |\n",
      "|    value_loss         | 0.243    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.52e+03 |\n",
      "|    ep_rew_mean     | 96.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 684      |\n",
      "|    iterations      | 73500    |\n",
      "|    time_elapsed    | 8586     |\n",
      "|    total_timesteps | 5880000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.55e+03 |\n",
      "|    ep_rew_mean        | 100      |\n",
      "| time/                 |          |\n",
      "|    fps                | 685      |\n",
      "|    iterations         | 73600    |\n",
      "|    time_elapsed       | 8593     |\n",
      "|    total_timesteps    | 5888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.979   |\n",
      "|    explained_variance | 0.819    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73599    |\n",
      "|    policy_loss        | 0.026    |\n",
      "|    value_loss         | 0.0269   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5890000, episode_reward=84.00 +/- 41.01\n",
      "Episode length: 5384.60 +/- 914.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.38e+03 |\n",
      "|    mean_reward        | 84       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5890000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.924   |\n",
      "|    explained_variance | 0.629    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73624    |\n",
      "|    policy_loss        | 0.0312   |\n",
      "|    value_loss         | 0.0373   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.58e+03 |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 684      |\n",
      "|    iterations         | 73700    |\n",
      "|    time_elapsed       | 8612     |\n",
      "|    total_timesteps    | 5896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.985   |\n",
      "|    explained_variance | 0.501    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73699    |\n",
      "|    policy_loss        | -0.027   |\n",
      "|    value_loss         | 0.0582   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5900000, episode_reward=127.40 +/- 102.01\n",
      "Episode length: 5348.60 +/- 1412.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.35e+03 |\n",
      "|    mean_reward        | 127      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5900000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.948   |\n",
      "|    explained_variance | 0.824    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73749    |\n",
      "|    policy_loss        | 0.0538   |\n",
      "|    value_loss         | 0.0251   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.6e+03  |\n",
      "|    ep_rew_mean        | 105      |\n",
      "| time/                 |          |\n",
      "|    fps                | 684      |\n",
      "|    iterations         | 73800    |\n",
      "|    time_elapsed       | 8630     |\n",
      "|    total_timesteps    | 5904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.904   |\n",
      "|    explained_variance | 0.664    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73799    |\n",
      "|    policy_loss        | 0.0675   |\n",
      "|    value_loss         | 0.0701   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5910000, episode_reward=122.60 +/- 13.81\n",
      "Episode length: 6453.00 +/- 474.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.45e+03 |\n",
      "|    mean_reward        | 123      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5910000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.98    |\n",
      "|    explained_variance | 0.85     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73874    |\n",
      "|    policy_loss        | 0.0223   |\n",
      "|    value_loss         | 0.0934   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.65e+03 |\n",
      "|    ep_rew_mean        | 108      |\n",
      "| time/                 |          |\n",
      "|    fps                | 683      |\n",
      "|    iterations         | 73900    |\n",
      "|    time_elapsed       | 8651     |\n",
      "|    total_timesteps    | 5912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.921   |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73899    |\n",
      "|    policy_loss        | -0.0347  |\n",
      "|    value_loss         | 0.0787   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5920000, episode_reward=115.60 +/- 31.85\n",
      "Episode length: 6470.80 +/- 851.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.47e+03 |\n",
      "|    mean_reward        | 116      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.928   |\n",
      "|    explained_variance | 0.615    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73999    |\n",
      "|    policy_loss        | 0.015    |\n",
      "|    value_loss         | 0.0283   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.66e+03 |\n",
      "|    ep_rew_mean     | 110      |\n",
      "| time/              |          |\n",
      "|    fps             | 682      |\n",
      "|    iterations      | 74000    |\n",
      "|    time_elapsed    | 8671     |\n",
      "|    total_timesteps | 5920000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.68e+03 |\n",
      "|    ep_rew_mean        | 111      |\n",
      "| time/                 |          |\n",
      "|    fps                | 683      |\n",
      "|    iterations         | 74100    |\n",
      "|    time_elapsed       | 8678     |\n",
      "|    total_timesteps    | 5928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.769    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74099    |\n",
      "|    policy_loss        | -0.0584  |\n",
      "|    value_loss         | 0.0388   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5930000, episode_reward=90.40 +/- 23.52\n",
      "Episode length: 5893.20 +/- 706.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.89e+03 |\n",
      "|    mean_reward        | 90.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5930000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.666    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74124    |\n",
      "|    policy_loss        | -0.0279  |\n",
      "|    value_loss         | 0.0353   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.49e+03 |\n",
      "|    ep_rew_mean        | 105      |\n",
      "| time/                 |          |\n",
      "|    fps                | 682      |\n",
      "|    iterations         | 74200    |\n",
      "|    time_elapsed       | 8698     |\n",
      "|    total_timesteps    | 5936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.667    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74199    |\n",
      "|    policy_loss        | -0.111   |\n",
      "|    value_loss         | 0.0978   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5940000, episode_reward=146.00 +/- 82.18\n",
      "Episode length: 6121.80 +/- 690.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.12e+03 |\n",
      "|    mean_reward        | 146      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.954   |\n",
      "|    explained_variance | 0.5      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74249    |\n",
      "|    policy_loss        | -0.16    |\n",
      "|    value_loss         | 0.162    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.44e+03 |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 681      |\n",
      "|    iterations         | 74300    |\n",
      "|    time_elapsed       | 8718     |\n",
      "|    total_timesteps    | 5944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.911   |\n",
      "|    explained_variance | 0.809    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74299    |\n",
      "|    policy_loss        | -0.00613 |\n",
      "|    value_loss         | 0.0147   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5950000, episode_reward=116.60 +/- 39.95\n",
      "Episode length: 5651.00 +/- 775.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.65e+03 |\n",
      "|    mean_reward        | 117      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5950000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.834   |\n",
      "|    explained_variance | 0.742    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74374    |\n",
      "|    policy_loss        | 0.0318   |\n",
      "|    value_loss         | 0.0486   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.42e+03 |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 681      |\n",
      "|    iterations         | 74400    |\n",
      "|    time_elapsed       | 8737     |\n",
      "|    total_timesteps    | 5952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.993   |\n",
      "|    explained_variance | 0.19     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74399    |\n",
      "|    policy_loss        | -0.708   |\n",
      "|    value_loss         | 7.18     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5960000, episode_reward=57.60 +/- 23.12\n",
      "Episode length: 4393.40 +/- 1147.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.39e+03 |\n",
      "|    mean_reward        | 57.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.978   |\n",
      "|    explained_variance | 0.682    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74499    |\n",
      "|    policy_loss        | -0.0281  |\n",
      "|    value_loss         | 0.216    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.43e+03 |\n",
      "|    ep_rew_mean     | 102      |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 74500    |\n",
      "|    time_elapsed    | 8753     |\n",
      "|    total_timesteps | 5960000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.44e+03 |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 681      |\n",
      "|    iterations         | 74600    |\n",
      "|    time_elapsed       | 8761     |\n",
      "|    total_timesteps    | 5968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.974   |\n",
      "|    explained_variance | 0.522    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74599    |\n",
      "|    policy_loss        | -0.0852  |\n",
      "|    value_loss         | 0.135    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5970000, episode_reward=71.40 +/- 30.60\n",
      "Episode length: 5032.60 +/- 1583.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.03e+03 |\n",
      "|    mean_reward        | 71.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5970000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.653    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74624    |\n",
      "|    policy_loss        | -0.0334  |\n",
      "|    value_loss         | 0.142    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.46e+03 |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 680      |\n",
      "|    iterations         | 74700    |\n",
      "|    time_elapsed       | 8778     |\n",
      "|    total_timesteps    | 5976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.995   |\n",
      "|    explained_variance | 0.807    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74699    |\n",
      "|    policy_loss        | -0.0257  |\n",
      "|    value_loss         | 0.0162   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5980000, episode_reward=77.60 +/- 24.93\n",
      "Episode length: 5445.40 +/- 808.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.45e+03 |\n",
      "|    mean_reward        | 77.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.989   |\n",
      "|    explained_variance | 0.798    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74749    |\n",
      "|    policy_loss        | 0.0192   |\n",
      "|    value_loss         | 0.0278   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.45e+03 |\n",
      "|    ep_rew_mean        | 101      |\n",
      "| time/                 |          |\n",
      "|    fps                | 680      |\n",
      "|    iterations         | 74800    |\n",
      "|    time_elapsed       | 8797     |\n",
      "|    total_timesteps    | 5984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74799    |\n",
      "|    policy_loss        | 0.0382   |\n",
      "|    value_loss         | 0.117    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5990000, episode_reward=83.20 +/- 27.53\n",
      "Episode length: 4577.60 +/- 588.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.58e+03 |\n",
      "|    mean_reward        | 83.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5990000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.931   |\n",
      "|    explained_variance | 0.82     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74874    |\n",
      "|    policy_loss        | 0.0165   |\n",
      "|    value_loss         | 0.0319   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.46e+03 |\n",
      "|    ep_rew_mean        | 103      |\n",
      "| time/                 |          |\n",
      "|    fps                | 679      |\n",
      "|    iterations         | 74900    |\n",
      "|    time_elapsed       | 8814     |\n",
      "|    total_timesteps    | 5992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.27     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74899    |\n",
      "|    policy_loss        | -0.378   |\n",
      "|    value_loss         | 0.478    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6000000, episode_reward=112.80 +/- 65.64\n",
      "Episode length: 5016.80 +/- 1800.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.02e+03 |\n",
      "|    mean_reward        | 113      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74999    |\n",
      "|    policy_loss        | -0.0396  |\n",
      "|    value_loss         | 0.0308   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.39e+03 |\n",
      "|    ep_rew_mean     | 102      |\n",
      "| time/              |          |\n",
      "|    fps             | 679      |\n",
      "|    iterations      | 75000    |\n",
      "|    time_elapsed    | 8831     |\n",
      "|    total_timesteps | 6000000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.42e+03 |\n",
      "|    ep_rew_mean        | 106      |\n",
      "| time/                 |          |\n",
      "|    fps                | 679      |\n",
      "|    iterations         | 75100    |\n",
      "|    time_elapsed       | 8839     |\n",
      "|    total_timesteps    | 6008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75099    |\n",
      "|    policy_loss        | -0.0373  |\n",
      "|    value_loss         | 0.0478   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6010000, episode_reward=81.60 +/- 27.92\n",
      "Episode length: 5585.80 +/- 1112.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.59e+03 |\n",
      "|    mean_reward        | 81.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6010000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.624    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75124    |\n",
      "|    policy_loss        | -0.0705  |\n",
      "|    value_loss         | 0.0419   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.41e+03 |\n",
      "|    ep_rew_mean        | 106      |\n",
      "| time/                 |          |\n",
      "|    fps                | 679      |\n",
      "|    iterations         | 75200    |\n",
      "|    time_elapsed       | 8857     |\n",
      "|    total_timesteps    | 6016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.961   |\n",
      "|    explained_variance | 0.446    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75199    |\n",
      "|    policy_loss        | 0.012    |\n",
      "|    value_loss         | 0.0758   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6020000, episode_reward=119.60 +/- 99.57\n",
      "Episode length: 5550.40 +/- 1365.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.55e+03 |\n",
      "|    mean_reward        | 120      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.193    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75249    |\n",
      "|    policy_loss        | -0.025   |\n",
      "|    value_loss         | 0.09     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.41e+03 |\n",
      "|    ep_rew_mean        | 105      |\n",
      "| time/                 |          |\n",
      "|    fps                | 678      |\n",
      "|    iterations         | 75300    |\n",
      "|    time_elapsed       | 8876     |\n",
      "|    total_timesteps    | 6024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.793    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75299    |\n",
      "|    policy_loss        | -0.0817  |\n",
      "|    value_loss         | 0.087    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6030000, episode_reward=146.60 +/- 50.89\n",
      "Episode length: 6868.60 +/- 1036.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.87e+03 |\n",
      "|    mean_reward        | 147      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6030000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.984   |\n",
      "|    explained_variance | 0.787    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75374    |\n",
      "|    policy_loss        | 0.052    |\n",
      "|    value_loss         | 0.0208   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.38e+03 |\n",
      "|    ep_rew_mean        | 101      |\n",
      "| time/                 |          |\n",
      "|    fps                | 677      |\n",
      "|    iterations         | 75400    |\n",
      "|    time_elapsed       | 8898     |\n",
      "|    total_timesteps    | 6032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.874   |\n",
      "|    explained_variance | 0.751    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75399    |\n",
      "|    policy_loss        | -0.0169  |\n",
      "|    value_loss         | 0.0251   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6040000, episode_reward=104.40 +/- 27.11\n",
      "Episode length: 6384.40 +/- 564.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.38e+03 |\n",
      "|    mean_reward        | 104      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.96    |\n",
      "|    explained_variance | 0.542    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75499    |\n",
      "|    policy_loss        | 0.00256  |\n",
      "|    value_loss         | 0.101    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.36e+03 |\n",
      "|    ep_rew_mean     | 101      |\n",
      "| time/              |          |\n",
      "|    fps             | 677      |\n",
      "|    iterations      | 75500    |\n",
      "|    time_elapsed    | 8918     |\n",
      "|    total_timesteps | 6040000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.34e+03 |\n",
      "|    ep_rew_mean        | 99.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 677      |\n",
      "|    iterations         | 75600    |\n",
      "|    time_elapsed       | 8926     |\n",
      "|    total_timesteps    | 6048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.919   |\n",
      "|    explained_variance | 0.197    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75599    |\n",
      "|    policy_loss        | -0.0657  |\n",
      "|    value_loss         | 0.228    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6050000, episode_reward=103.20 +/- 8.45\n",
      "Episode length: 5428.80 +/- 557.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.43e+03 |\n",
      "|    mean_reward        | 103      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.947   |\n",
      "|    explained_variance | 0.809    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75624    |\n",
      "|    policy_loss        | 0.0231   |\n",
      "|    value_loss         | 0.0189   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.34e+03 |\n",
      "|    ep_rew_mean        | 99.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 677      |\n",
      "|    iterations         | 75700    |\n",
      "|    time_elapsed       | 8944     |\n",
      "|    total_timesteps    | 6056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.993   |\n",
      "|    explained_variance | 0.805    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75699    |\n",
      "|    policy_loss        | -0.0354  |\n",
      "|    value_loss         | 0.0281   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6060000, episode_reward=11.60 +/- 5.08\n",
      "Episode length: 2001.40 +/- 452.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2e+03    |\n",
      "|    mean_reward        | 11.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.956   |\n",
      "|    explained_variance | -3.34    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75749    |\n",
      "|    policy_loss        | 0.0476   |\n",
      "|    value_loss         | 0.281    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.3e+03  |\n",
      "|    ep_rew_mean        | 101      |\n",
      "| time/                 |          |\n",
      "|    fps                | 677      |\n",
      "|    iterations         | 75800    |\n",
      "|    time_elapsed       | 8956     |\n",
      "|    total_timesteps    | 6064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.988   |\n",
      "|    explained_variance | 0.903    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75799    |\n",
      "|    policy_loss        | -0.0139  |\n",
      "|    value_loss         | 0.014    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6070000, episode_reward=138.60 +/- 54.67\n",
      "Episode length: 5520.60 +/- 710.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.52e+03 |\n",
      "|    mean_reward        | 139      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6070000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.989   |\n",
      "|    explained_variance | 0.807    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75874    |\n",
      "|    policy_loss        | -0.0072  |\n",
      "|    value_loss         | 0.0168   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.49e+03 |\n",
      "|    ep_rew_mean        | 106      |\n",
      "| time/                 |          |\n",
      "|    fps                | 676      |\n",
      "|    iterations         | 75900    |\n",
      "|    time_elapsed       | 8975     |\n",
      "|    total_timesteps    | 6072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.69     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75899    |\n",
      "|    policy_loss        | -0.0793  |\n",
      "|    value_loss         | 0.0994   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6080000, episode_reward=106.20 +/- 79.72\n",
      "Episode length: 5820.40 +/- 1226.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.82e+03 |\n",
      "|    mean_reward        | 106      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.724    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75999    |\n",
      "|    policy_loss        | 0.0121   |\n",
      "|    value_loss         | 0.0346   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.5e+03  |\n",
      "|    ep_rew_mean     | 106      |\n",
      "| time/              |          |\n",
      "|    fps             | 675      |\n",
      "|    iterations      | 76000    |\n",
      "|    time_elapsed    | 8994     |\n",
      "|    total_timesteps | 6080000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.48e+03 |\n",
      "|    ep_rew_mean        | 106      |\n",
      "| time/                 |          |\n",
      "|    fps                | 676      |\n",
      "|    iterations         | 76100    |\n",
      "|    time_elapsed       | 9001     |\n",
      "|    total_timesteps    | 6088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.728    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76099    |\n",
      "|    policy_loss        | 0.0721   |\n",
      "|    value_loss         | 0.0276   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6090000, episode_reward=84.80 +/- 22.42\n",
      "Episode length: 5669.80 +/- 593.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.67e+03 |\n",
      "|    mean_reward        | 84.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6090000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76124    |\n",
      "|    policy_loss        | -0.0187  |\n",
      "|    value_loss         | 0.022    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.49e+03 |\n",
      "|    ep_rew_mean        | 104      |\n",
      "| time/                 |          |\n",
      "|    fps                | 675      |\n",
      "|    iterations         | 76200    |\n",
      "|    time_elapsed       | 9020     |\n",
      "|    total_timesteps    | 6096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.944   |\n",
      "|    explained_variance | 0.165    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76199    |\n",
      "|    policy_loss        | -0.099   |\n",
      "|    value_loss         | 0.188    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6100000, episode_reward=120.20 +/- 90.03\n",
      "Episode length: 5623.40 +/- 1412.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.62e+03 |\n",
      "|    mean_reward        | 120      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.926   |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76249    |\n",
      "|    policy_loss        | -0.0204  |\n",
      "|    value_loss         | 0.0232   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.42e+03 |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 675      |\n",
      "|    iterations         | 76300    |\n",
      "|    time_elapsed       | 9039     |\n",
      "|    total_timesteps    | 6104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.968   |\n",
      "|    explained_variance | 0.765    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76299    |\n",
      "|    policy_loss        | -0.00232 |\n",
      "|    value_loss         | 0.0254   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6110000, episode_reward=143.40 +/- 78.76\n",
      "Episode length: 5222.80 +/- 1315.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.22e+03 |\n",
      "|    mean_reward        | 143      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6110000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.693    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76374    |\n",
      "|    policy_loss        | 0.0525   |\n",
      "|    value_loss         | 0.0467   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.42e+03 |\n",
      "|    ep_rew_mean        | 101      |\n",
      "| time/                 |          |\n",
      "|    fps                | 674      |\n",
      "|    iterations         | 76400    |\n",
      "|    time_elapsed       | 9057     |\n",
      "|    total_timesteps    | 6112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.96    |\n",
      "|    explained_variance | 0.84     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76399    |\n",
      "|    policy_loss        | -0.0174  |\n",
      "|    value_loss         | 0.0138   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6120000, episode_reward=111.60 +/- 31.07\n",
      "Episode length: 6020.60 +/- 826.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.02e+03 |\n",
      "|    mean_reward        | 112      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.827    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76499    |\n",
      "|    policy_loss        | 0.0122   |\n",
      "|    value_loss         | 0.0216   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.42e+03 |\n",
      "|    ep_rew_mean     | 101      |\n",
      "| time/              |          |\n",
      "|    fps             | 674      |\n",
      "|    iterations      | 76500    |\n",
      "|    time_elapsed    | 9077     |\n",
      "|    total_timesteps | 6120000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.47e+03 |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 674      |\n",
      "|    iterations         | 76600    |\n",
      "|    time_elapsed       | 9084     |\n",
      "|    total_timesteps    | 6128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.951   |\n",
      "|    explained_variance | 0.77     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76599    |\n",
      "|    policy_loss        | -0.0102  |\n",
      "|    value_loss         | 0.194    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6130000, episode_reward=194.40 +/- 64.23\n",
      "Episode length: 6543.40 +/- 1023.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.54e+03 |\n",
      "|    mean_reward        | 194      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6130000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.985   |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76624    |\n",
      "|    policy_loss        | 0.0521   |\n",
      "|    value_loss         | 0.0449   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.53e+03 |\n",
      "|    ep_rew_mean        | 106      |\n",
      "| time/                 |          |\n",
      "|    fps                | 673      |\n",
      "|    iterations         | 76700    |\n",
      "|    time_elapsed       | 9105     |\n",
      "|    total_timesteps    | 6136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.524    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76699    |\n",
      "|    policy_loss        | 0.0529   |\n",
      "|    value_loss         | 0.0583   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6140000, episode_reward=116.40 +/- 56.91\n",
      "Episode length: 6098.20 +/- 666.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.1e+03  |\n",
      "|    mean_reward        | 116      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.596    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76749    |\n",
      "|    policy_loss        | 0.0272   |\n",
      "|    value_loss         | 0.0261   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.53e+03 |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 673      |\n",
      "|    iterations         | 76800    |\n",
      "|    time_elapsed       | 9125     |\n",
      "|    total_timesteps    | 6144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.945   |\n",
      "|    explained_variance | 0.843    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76799    |\n",
      "|    policy_loss        | -0.0678  |\n",
      "|    value_loss         | 0.117    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6150000, episode_reward=23.40 +/- 3.67\n",
      "Episode length: 3042.80 +/- 388.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.04e+03 |\n",
      "|    mean_reward        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | 0.295    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76874    |\n",
      "|    policy_loss        | 0.119    |\n",
      "|    value_loss         | 0.0807   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.54e+03 |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 673      |\n",
      "|    iterations         | 76900    |\n",
      "|    time_elapsed       | 9138     |\n",
      "|    total_timesteps    | 6152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.53     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76899    |\n",
      "|    policy_loss        | -0.0271  |\n",
      "|    value_loss         | 0.129    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6160000, episode_reward=78.20 +/- 56.68\n",
      "Episode length: 5086.00 +/- 1745.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.09e+03 |\n",
      "|    mean_reward        | 78.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.839    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76999    |\n",
      "|    policy_loss        | -0.0282  |\n",
      "|    value_loss         | 0.0146   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.45e+03 |\n",
      "|    ep_rew_mean     | 99.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 672      |\n",
      "|    iterations      | 77000    |\n",
      "|    time_elapsed    | 9156     |\n",
      "|    total_timesteps | 6160000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.38e+03 |\n",
      "|    ep_rew_mean        | 95.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 673      |\n",
      "|    iterations         | 77100    |\n",
      "|    time_elapsed       | 9164     |\n",
      "|    total_timesteps    | 6168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.897   |\n",
      "|    explained_variance | 0.697    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77099    |\n",
      "|    policy_loss        | -0.00212 |\n",
      "|    value_loss         | 0.0298   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6170000, episode_reward=105.40 +/- 63.89\n",
      "Episode length: 5354.20 +/- 1176.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.35e+03 |\n",
      "|    mean_reward        | 105      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6170000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.989   |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77124    |\n",
      "|    policy_loss        | -0.027   |\n",
      "|    value_loss         | 0.00775  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.37e+03 |\n",
      "|    ep_rew_mean        | 94.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 672      |\n",
      "|    iterations         | 77200    |\n",
      "|    time_elapsed       | 9182     |\n",
      "|    total_timesteps    | 6176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.4      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77199    |\n",
      "|    policy_loss        | -0.0628  |\n",
      "|    value_loss         | 0.338    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6180000, episode_reward=100.00 +/- 32.97\n",
      "Episode length: 5569.20 +/- 873.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.57e+03 |\n",
      "|    mean_reward        | 100      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.914   |\n",
      "|    explained_variance | 0.598    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77249    |\n",
      "|    policy_loss        | -0.123   |\n",
      "|    value_loss         | 0.0645   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.32e+03 |\n",
      "|    ep_rew_mean        | 94.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 672      |\n",
      "|    iterations         | 77300    |\n",
      "|    time_elapsed       | 9201     |\n",
      "|    total_timesteps    | 6184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77299    |\n",
      "|    policy_loss        | -0.00297 |\n",
      "|    value_loss         | 0.0156   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6190000, episode_reward=141.80 +/- 53.91\n",
      "Episode length: 5711.20 +/- 1023.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.71e+03 |\n",
      "|    mean_reward        | 142      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6190000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.617    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77374    |\n",
      "|    policy_loss        | -0.0713  |\n",
      "|    value_loss         | 0.0429   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.35e+03 |\n",
      "|    ep_rew_mean        | 93.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 671      |\n",
      "|    iterations         | 77400    |\n",
      "|    time_elapsed       | 9220     |\n",
      "|    total_timesteps    | 6192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.728    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77399    |\n",
      "|    policy_loss        | -0.0259  |\n",
      "|    value_loss         | 0.0352   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6200000, episode_reward=100.80 +/- 22.99\n",
      "Episode length: 5784.80 +/- 596.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.78e+03 |\n",
      "|    mean_reward        | 101      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.932   |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77499    |\n",
      "|    policy_loss        | -0.0112  |\n",
      "|    value_loss         | 0.0137   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.39e+03 |\n",
      "|    ep_rew_mean     | 95.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 671      |\n",
      "|    iterations      | 77500    |\n",
      "|    time_elapsed    | 9239     |\n",
      "|    total_timesteps | 6200000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.45e+03 |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 671      |\n",
      "|    iterations         | 77600    |\n",
      "|    time_elapsed       | 9246     |\n",
      "|    total_timesteps    | 6208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.909   |\n",
      "|    explained_variance | 0.567    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77599    |\n",
      "|    policy_loss        | -0.0593  |\n",
      "|    value_loss         | 0.193    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6210000, episode_reward=180.80 +/- 52.16\n",
      "Episode length: 6230.80 +/- 882.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.23e+03 |\n",
      "|    mean_reward        | 181      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6210000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.959   |\n",
      "|    explained_variance | 0.725    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77624    |\n",
      "|    policy_loss        | 0.0173   |\n",
      "|    value_loss         | 0.0503   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.5e+03  |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 670      |\n",
      "|    iterations         | 77700    |\n",
      "|    time_elapsed       | 9266     |\n",
      "|    total_timesteps    | 6216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.858   |\n",
      "|    explained_variance | 0.622    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77699    |\n",
      "|    policy_loss        | -0.0704  |\n",
      "|    value_loss         | 0.214    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6220000, episode_reward=185.60 +/- 74.21\n",
      "Episode length: 6238.60 +/- 602.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.24e+03 |\n",
      "|    mean_reward        | 186      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.901   |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77749    |\n",
      "|    policy_loss        | 0.0122   |\n",
      "|    value_loss         | 0.0157   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.61e+03 |\n",
      "|    ep_rew_mean        | 107      |\n",
      "| time/                 |          |\n",
      "|    fps                | 670      |\n",
      "|    iterations         | 77800    |\n",
      "|    time_elapsed       | 9286     |\n",
      "|    total_timesteps    | 6224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.894   |\n",
      "|    explained_variance | 0.216    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77799    |\n",
      "|    policy_loss        | 0.0239   |\n",
      "|    value_loss         | 0.0646   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6230000, episode_reward=99.00 +/- 49.55\n",
      "Episode length: 5042.80 +/- 468.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.04e+03 |\n",
      "|    mean_reward        | 99       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6230000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.783   |\n",
      "|    explained_variance | -0.137   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77874    |\n",
      "|    policy_loss        | -0.0689  |\n",
      "|    value_loss         | 0.708    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.64e+03 |\n",
      "|    ep_rew_mean        | 110      |\n",
      "| time/                 |          |\n",
      "|    fps                | 669      |\n",
      "|    iterations         | 77900    |\n",
      "|    time_elapsed       | 9304     |\n",
      "|    total_timesteps    | 6232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.941   |\n",
      "|    explained_variance | 0.617    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77899    |\n",
      "|    policy_loss        | -0.0295  |\n",
      "|    value_loss         | 0.0753   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6240000, episode_reward=103.40 +/- 42.28\n",
      "Episode length: 5217.80 +/- 856.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.22e+03 |\n",
      "|    mean_reward        | 103      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.991   |\n",
      "|    explained_variance | 0.729    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77999    |\n",
      "|    policy_loss        | 0.0027   |\n",
      "|    value_loss         | 0.0316   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.72e+03 |\n",
      "|    ep_rew_mean     | 115      |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 78000    |\n",
      "|    time_elapsed    | 9322     |\n",
      "|    total_timesteps | 6240000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.73e+03 |\n",
      "|    ep_rew_mean        | 118      |\n",
      "| time/                 |          |\n",
      "|    fps                | 669      |\n",
      "|    iterations         | 78100    |\n",
      "|    time_elapsed       | 9329     |\n",
      "|    total_timesteps    | 6248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.942   |\n",
      "|    explained_variance | 0.83     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78099    |\n",
      "|    policy_loss        | -0.00075 |\n",
      "|    value_loss         | 0.0296   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6250000, episode_reward=172.80 +/- 79.14\n",
      "Episode length: 5804.20 +/- 707.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.8e+03  |\n",
      "|    mean_reward        | 173      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.882   |\n",
      "|    explained_variance | 0.751    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78124    |\n",
      "|    policy_loss        | 0.0283   |\n",
      "|    value_loss         | 0.0444   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.73e+03  |\n",
      "|    ep_rew_mean        | 121       |\n",
      "| time/                 |           |\n",
      "|    fps                | 669       |\n",
      "|    iterations         | 78200     |\n",
      "|    time_elapsed       | 9349      |\n",
      "|    total_timesteps    | 6256000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.878    |\n",
      "|    explained_variance | 0.507     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 78199     |\n",
      "|    policy_loss        | -0.000336 |\n",
      "|    value_loss         | 0.24      |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=6260000, episode_reward=81.80 +/- 24.77\n",
      "Episode length: 4987.40 +/- 1080.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.99e+03 |\n",
      "|    mean_reward        | 81.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.919   |\n",
      "|    explained_variance | 0.688    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78249    |\n",
      "|    policy_loss        | -0.0233  |\n",
      "|    value_loss         | 0.166    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.67e+03 |\n",
      "|    ep_rew_mean        | 118      |\n",
      "| time/                 |          |\n",
      "|    fps                | 668      |\n",
      "|    iterations         | 78300    |\n",
      "|    time_elapsed       | 9366     |\n",
      "|    total_timesteps    | 6264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.996   |\n",
      "|    explained_variance | 0.793    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78299    |\n",
      "|    policy_loss        | -0.0391  |\n",
      "|    value_loss         | 0.0275   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6270000, episode_reward=162.80 +/- 97.31\n",
      "Episode length: 6060.20 +/- 1341.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.06e+03 |\n",
      "|    mean_reward        | 163      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6270000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.873   |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78374    |\n",
      "|    policy_loss        | 0.00939  |\n",
      "|    value_loss         | 0.0956   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.71e+03 |\n",
      "|    ep_rew_mean        | 121      |\n",
      "| time/                 |          |\n",
      "|    fps                | 668      |\n",
      "|    iterations         | 78400    |\n",
      "|    time_elapsed       | 9386     |\n",
      "|    total_timesteps    | 6272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.926   |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78399    |\n",
      "|    policy_loss        | 0.0628   |\n",
      "|    value_loss         | 0.034    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6280000, episode_reward=162.60 +/- 48.85\n",
      "Episode length: 6419.60 +/- 897.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.42e+03 |\n",
      "|    mean_reward        | 163      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.988   |\n",
      "|    explained_variance | 0.5      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78499    |\n",
      "|    policy_loss        | -0.0515  |\n",
      "|    value_loss         | 0.333    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.72e+03 |\n",
      "|    ep_rew_mean     | 120      |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 78500    |\n",
      "|    time_elapsed    | 9406     |\n",
      "|    total_timesteps | 6280000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.8e+03  |\n",
      "|    ep_rew_mean        | 126      |\n",
      "| time/                 |          |\n",
      "|    fps                | 667      |\n",
      "|    iterations         | 78600    |\n",
      "|    time_elapsed       | 9414     |\n",
      "|    total_timesteps    | 6288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.967   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78599    |\n",
      "|    policy_loss        | 0.00748  |\n",
      "|    value_loss         | 0.0258   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6290000, episode_reward=123.00 +/- 20.14\n",
      "Episode length: 6045.60 +/- 970.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.05e+03 |\n",
      "|    mean_reward        | 123      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6290000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.958   |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78624    |\n",
      "|    policy_loss        | -0.0335  |\n",
      "|    value_loss         | 0.0381   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.79e+03 |\n",
      "|    ep_rew_mean        | 129      |\n",
      "| time/                 |          |\n",
      "|    fps                | 667      |\n",
      "|    iterations         | 78700    |\n",
      "|    time_elapsed       | 9433     |\n",
      "|    total_timesteps    | 6296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.903   |\n",
      "|    explained_variance | 0.73     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78699    |\n",
      "|    policy_loss        | -0.278   |\n",
      "|    value_loss         | 0.284    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6300000, episode_reward=147.20 +/- 87.30\n",
      "Episode length: 5879.80 +/- 1089.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.88e+03 |\n",
      "|    mean_reward        | 147      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.936   |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78749    |\n",
      "|    policy_loss        | -0.0468  |\n",
      "|    value_loss         | 0.0226   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.78e+03 |\n",
      "|    ep_rew_mean        | 132      |\n",
      "| time/                 |          |\n",
      "|    fps                | 666      |\n",
      "|    iterations         | 78800    |\n",
      "|    time_elapsed       | 9453     |\n",
      "|    total_timesteps    | 6304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.977   |\n",
      "|    explained_variance | 0.749    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78799    |\n",
      "|    policy_loss        | -0.034   |\n",
      "|    value_loss         | 0.0206   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6310000, episode_reward=121.40 +/- 54.22\n",
      "Episode length: 5709.20 +/- 853.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.71e+03 |\n",
      "|    mean_reward        | 121      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6310000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.798    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78874    |\n",
      "|    policy_loss        | 0.075    |\n",
      "|    value_loss         | 0.274    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.94e+03 |\n",
      "|    ep_rew_mean        | 139      |\n",
      "| time/                 |          |\n",
      "|    fps                | 666      |\n",
      "|    iterations         | 78900    |\n",
      "|    time_elapsed       | 9472     |\n",
      "|    total_timesteps    | 6312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.956   |\n",
      "|    explained_variance | 0.798    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78899    |\n",
      "|    policy_loss        | 0.0181   |\n",
      "|    value_loss         | 0.0636   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6320000, episode_reward=153.00 +/- 81.85\n",
      "Episode length: 5915.80 +/- 538.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.92e+03 |\n",
      "|    mean_reward        | 153      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.992   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78999    |\n",
      "|    policy_loss        | 0.03     |\n",
      "|    value_loss         | 0.0543   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.08e+03 |\n",
      "|    ep_rew_mean     | 147      |\n",
      "| time/              |          |\n",
      "|    fps             | 665      |\n",
      "|    iterations      | 79000    |\n",
      "|    time_elapsed    | 9491     |\n",
      "|    total_timesteps | 6320000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.08e+03 |\n",
      "|    ep_rew_mean        | 150      |\n",
      "| time/                 |          |\n",
      "|    fps                | 666      |\n",
      "|    iterations         | 79100    |\n",
      "|    time_elapsed       | 9499     |\n",
      "|    total_timesteps    | 6328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.846   |\n",
      "|    explained_variance | 0.69     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79099    |\n",
      "|    policy_loss        | -0.0396  |\n",
      "|    value_loss         | 0.0248   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6330000, episode_reward=97.20 +/- 81.21\n",
      "Episode length: 4797.20 +/- 1645.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.8e+03  |\n",
      "|    mean_reward        | 97.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6330000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.911   |\n",
      "|    explained_variance | 0.716    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79124    |\n",
      "|    policy_loss        | 0.00188  |\n",
      "|    value_loss         | 0.0245   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.09e+03 |\n",
      "|    ep_rew_mean        | 150      |\n",
      "| time/                 |          |\n",
      "|    fps                | 665      |\n",
      "|    iterations         | 79200    |\n",
      "|    time_elapsed       | 9516     |\n",
      "|    total_timesteps    | 6336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.952   |\n",
      "|    explained_variance | 0.733    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79199    |\n",
      "|    policy_loss        | -0.0328  |\n",
      "|    value_loss         | 0.0445   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6340000, episode_reward=200.40 +/- 124.66\n",
      "Episode length: 6445.80 +/- 1280.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.45e+03 |\n",
      "|    mean_reward        | 200      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.915   |\n",
      "|    explained_variance | 0.688    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79249    |\n",
      "|    policy_loss        | 0.0241   |\n",
      "|    value_loss         | 0.0253   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.15e+03 |\n",
      "|    ep_rew_mean        | 153      |\n",
      "| time/                 |          |\n",
      "|    fps                | 665      |\n",
      "|    iterations         | 79300    |\n",
      "|    time_elapsed       | 9537     |\n",
      "|    total_timesteps    | 6344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.896   |\n",
      "|    explained_variance | 0.394    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79299    |\n",
      "|    policy_loss        | -0.0549  |\n",
      "|    value_loss         | 0.443    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6350000, episode_reward=203.40 +/- 122.80\n",
      "Episode length: 6471.00 +/- 1363.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.47e+03 |\n",
      "|    mean_reward        | 203      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79374    |\n",
      "|    policy_loss        | 0.0643   |\n",
      "|    value_loss         | 0.0839   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.15e+03 |\n",
      "|    ep_rew_mean        | 156      |\n",
      "| time/                 |          |\n",
      "|    fps                | 664      |\n",
      "|    iterations         | 79400    |\n",
      "|    time_elapsed       | 9557     |\n",
      "|    total_timesteps    | 6352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.863   |\n",
      "|    explained_variance | 0.844    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79399    |\n",
      "|    policy_loss        | 0.0291   |\n",
      "|    value_loss         | 0.0787   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6360000, episode_reward=99.80 +/- 86.53\n",
      "Episode length: 5117.40 +/- 1149.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.12e+03 |\n",
      "|    mean_reward        | 99.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.68     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79499    |\n",
      "|    policy_loss        | 0.0977   |\n",
      "|    value_loss         | 0.0754   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.19e+03 |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    fps             | 664      |\n",
      "|    iterations      | 79500    |\n",
      "|    time_elapsed    | 9576     |\n",
      "|    total_timesteps | 6360000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.15e+03 |\n",
      "|    ep_rew_mean        | 159      |\n",
      "| time/                 |          |\n",
      "|    fps                | 664      |\n",
      "|    iterations         | 79600    |\n",
      "|    time_elapsed       | 9583     |\n",
      "|    total_timesteps    | 6368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.762   |\n",
      "|    explained_variance | 0.739    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79599    |\n",
      "|    policy_loss        | -0.149   |\n",
      "|    value_loss         | 0.127    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6370000, episode_reward=95.20 +/- 51.77\n",
      "Episode length: 5127.20 +/- 1444.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.13e+03 |\n",
      "|    mean_reward        | 95.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6370000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.982   |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79624    |\n",
      "|    policy_loss        | -0.0454  |\n",
      "|    value_loss         | 0.0286   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.16e+03 |\n",
      "|    ep_rew_mean        | 160      |\n",
      "| time/                 |          |\n",
      "|    fps                | 664      |\n",
      "|    iterations         | 79700    |\n",
      "|    time_elapsed       | 9601     |\n",
      "|    total_timesteps    | 6376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.986   |\n",
      "|    explained_variance | 0.374    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79699    |\n",
      "|    policy_loss        | 0.0158   |\n",
      "|    value_loss         | 0.0334   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6380000, episode_reward=99.20 +/- 41.81\n",
      "Episode length: 5577.00 +/- 1081.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.58e+03 |\n",
      "|    mean_reward        | 99.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.949   |\n",
      "|    explained_variance | 0.613    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79749    |\n",
      "|    policy_loss        | -0.187   |\n",
      "|    value_loss         | 0.272    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.15e+03 |\n",
      "|    ep_rew_mean        | 157      |\n",
      "| time/                 |          |\n",
      "|    fps                | 663      |\n",
      "|    iterations         | 79800    |\n",
      "|    time_elapsed       | 9620     |\n",
      "|    total_timesteps    | 6384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.977   |\n",
      "|    explained_variance | 0.826    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79799    |\n",
      "|    policy_loss        | 0.0429   |\n",
      "|    value_loss         | 0.0911   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6390000, episode_reward=250.20 +/- 85.67\n",
      "Episode length: 6975.00 +/- 868.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.98e+03 |\n",
      "|    mean_reward        | 250      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6390000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.881   |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79874    |\n",
      "|    policy_loss        | 0.0413   |\n",
      "|    value_loss         | 0.014    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.12e+03 |\n",
      "|    ep_rew_mean        | 157      |\n",
      "| time/                 |          |\n",
      "|    fps                | 662      |\n",
      "|    iterations         | 79900    |\n",
      "|    time_elapsed       | 9642     |\n",
      "|    total_timesteps    | 6392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.916   |\n",
      "|    explained_variance | 0.408    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79899    |\n",
      "|    policy_loss        | -0.00314 |\n",
      "|    value_loss         | 0.0376   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6400000, episode_reward=136.60 +/- 72.89\n",
      "Episode length: 6111.40 +/- 928.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.11e+03 |\n",
      "|    mean_reward        | 137      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.807   |\n",
      "|    explained_variance | 0.89     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79999    |\n",
      "|    policy_loss        | -0.00148 |\n",
      "|    value_loss         | 0.0275   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.09e+03 |\n",
      "|    ep_rew_mean     | 156      |\n",
      "| time/              |          |\n",
      "|    fps             | 662      |\n",
      "|    iterations      | 80000    |\n",
      "|    time_elapsed    | 9662     |\n",
      "|    total_timesteps | 6400000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.08e+03 |\n",
      "|    ep_rew_mean        | 155      |\n",
      "| time/                 |          |\n",
      "|    fps                | 662      |\n",
      "|    iterations         | 80100    |\n",
      "|    time_elapsed       | 9669     |\n",
      "|    total_timesteps    | 6408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.837   |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80099    |\n",
      "|    policy_loss        | 0.024    |\n",
      "|    value_loss         | 0.0328   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6410000, episode_reward=94.60 +/- 29.77\n",
      "Episode length: 5089.00 +/- 565.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.09e+03 |\n",
      "|    mean_reward        | 94.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6410000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.856   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80124    |\n",
      "|    policy_loss        | -0.0273  |\n",
      "|    value_loss         | 0.128    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.17e+03 |\n",
      "|    ep_rew_mean        | 163      |\n",
      "| time/                 |          |\n",
      "|    fps                | 662      |\n",
      "|    iterations         | 80200    |\n",
      "|    time_elapsed       | 9687     |\n",
      "|    total_timesteps    | 6416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.882   |\n",
      "|    explained_variance | 0.898    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80199    |\n",
      "|    policy_loss        | 0.00254  |\n",
      "|    value_loss         | 0.0603   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6420000, episode_reward=123.40 +/- 50.84\n",
      "Episode length: 6172.60 +/- 963.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.17e+03 |\n",
      "|    mean_reward        | 123      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.956   |\n",
      "|    explained_variance | 0.756    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80249    |\n",
      "|    policy_loss        | 0.0205   |\n",
      "|    value_loss         | 0.0241   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.18e+03 |\n",
      "|    ep_rew_mean        | 166      |\n",
      "| time/                 |          |\n",
      "|    fps                | 661      |\n",
      "|    iterations         | 80300    |\n",
      "|    time_elapsed       | 9707     |\n",
      "|    total_timesteps    | 6424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.918   |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80299    |\n",
      "|    policy_loss        | 0.0404   |\n",
      "|    value_loss         | 0.0277   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6430000, episode_reward=164.00 +/- 76.72\n",
      "Episode length: 5873.60 +/- 800.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.87e+03 |\n",
      "|    mean_reward        | 164      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6430000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.995   |\n",
      "|    explained_variance | 0.754    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80374    |\n",
      "|    policy_loss        | -0.00899 |\n",
      "|    value_loss         | 0.0225   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.22e+03 |\n",
      "|    ep_rew_mean        | 167      |\n",
      "| time/                 |          |\n",
      "|    fps                | 661      |\n",
      "|    iterations         | 80400    |\n",
      "|    time_elapsed       | 9726     |\n",
      "|    total_timesteps    | 6432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.805   |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80399    |\n",
      "|    policy_loss        | -0.0502  |\n",
      "|    value_loss         | 0.139    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6440000, episode_reward=186.60 +/- 113.75\n",
      "Episode length: 6034.00 +/- 2156.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.03e+03 |\n",
      "|    mean_reward        | 187      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.846   |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80499    |\n",
      "|    policy_loss        | -0.0424  |\n",
      "|    value_loss         | 0.161    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.2e+03  |\n",
      "|    ep_rew_mean     | 165      |\n",
      "| time/              |          |\n",
      "|    fps             | 660      |\n",
      "|    iterations      | 80500    |\n",
      "|    time_elapsed    | 9745     |\n",
      "|    total_timesteps | 6440000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.24e+03 |\n",
      "|    ep_rew_mean        | 168      |\n",
      "| time/                 |          |\n",
      "|    fps                | 661      |\n",
      "|    iterations         | 80600    |\n",
      "|    time_elapsed       | 9752     |\n",
      "|    total_timesteps    | 6448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.886   |\n",
      "|    explained_variance | -0.551   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80599    |\n",
      "|    policy_loss        | 0.0301   |\n",
      "|    value_loss         | 0.133    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6450000, episode_reward=152.20 +/- 52.67\n",
      "Episode length: 6094.00 +/- 520.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.09e+03 |\n",
      "|    mean_reward        | 152      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.875   |\n",
      "|    explained_variance | 0.471    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80624    |\n",
      "|    policy_loss        | -0.213   |\n",
      "|    value_loss         | 0.297    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.29e+03 |\n",
      "|    ep_rew_mean        | 167      |\n",
      "| time/                 |          |\n",
      "|    fps                | 660      |\n",
      "|    iterations         | 80700    |\n",
      "|    time_elapsed       | 9771     |\n",
      "|    total_timesteps    | 6456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.702    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80699    |\n",
      "|    policy_loss        | -0.0153  |\n",
      "|    value_loss         | 0.0325   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6460000, episode_reward=150.20 +/- 78.72\n",
      "Episode length: 5027.00 +/- 658.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.03e+03 |\n",
      "|    mean_reward        | 150      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.894   |\n",
      "|    explained_variance | 0.462    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80749    |\n",
      "|    policy_loss        | 0.0582   |\n",
      "|    value_loss         | 0.0582   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.27e+03 |\n",
      "|    ep_rew_mean        | 163      |\n",
      "| time/                 |          |\n",
      "|    fps                | 660      |\n",
      "|    iterations         | 80800    |\n",
      "|    time_elapsed       | 9787     |\n",
      "|    total_timesteps    | 6464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.917   |\n",
      "|    explained_variance | 0.804    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80799    |\n",
      "|    policy_loss        | 0.0299   |\n",
      "|    value_loss         | 0.0215   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6470000, episode_reward=196.60 +/- 103.52\n",
      "Episode length: 6734.00 +/- 1106.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.73e+03 |\n",
      "|    mean_reward        | 197      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6470000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.881   |\n",
      "|    explained_variance | 0.75     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80874    |\n",
      "|    policy_loss        | 0.0114   |\n",
      "|    value_loss         | 0.0387   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.24e+03 |\n",
      "|    ep_rew_mean        | 161      |\n",
      "| time/                 |          |\n",
      "|    fps                | 659      |\n",
      "|    iterations         | 80900    |\n",
      "|    time_elapsed       | 9808     |\n",
      "|    total_timesteps    | 6472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.941   |\n",
      "|    explained_variance | 0.387    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80899    |\n",
      "|    policy_loss        | 0.0799   |\n",
      "|    value_loss         | 0.254    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6480000, episode_reward=195.60 +/- 98.19\n",
      "Episode length: 5897.00 +/- 1253.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.9e+03  |\n",
      "|    mean_reward        | 196      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.918   |\n",
      "|    explained_variance | 0.757    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80999    |\n",
      "|    policy_loss        | -0.0772  |\n",
      "|    value_loss         | 0.04     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.21e+03 |\n",
      "|    ep_rew_mean     | 161      |\n",
      "| time/              |          |\n",
      "|    fps             | 659      |\n",
      "|    iterations      | 81000    |\n",
      "|    time_elapsed    | 9828     |\n",
      "|    total_timesteps | 6480000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.27e+03 |\n",
      "|    ep_rew_mean        | 166      |\n",
      "| time/                 |          |\n",
      "|    fps                | 659      |\n",
      "|    iterations         | 81100    |\n",
      "|    time_elapsed       | 9836     |\n",
      "|    total_timesteps    | 6488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.872   |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81099    |\n",
      "|    policy_loss        | -0.00455 |\n",
      "|    value_loss         | 0.0165   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6490000, episode_reward=262.60 +/- 64.44\n",
      "Episode length: 6839.80 +/- 731.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.84e+03 |\n",
      "|    mean_reward        | 263      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6490000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.919   |\n",
      "|    explained_variance | 0.855    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81124    |\n",
      "|    policy_loss        | 0.0015   |\n",
      "|    value_loss         | 0.0273   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.29e+03 |\n",
      "|    ep_rew_mean        | 169      |\n",
      "| time/                 |          |\n",
      "|    fps                | 658      |\n",
      "|    iterations         | 81200    |\n",
      "|    time_elapsed       | 9859     |\n",
      "|    total_timesteps    | 6496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.959   |\n",
      "|    explained_variance | 0.863    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81199    |\n",
      "|    policy_loss        | 0.0623   |\n",
      "|    value_loss         | 0.0329   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6500000, episode_reward=205.80 +/- 91.54\n",
      "Episode length: 6825.80 +/- 813.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.83e+03 |\n",
      "|    mean_reward        | 206      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.968   |\n",
      "|    explained_variance | 0.398    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81249    |\n",
      "|    policy_loss        | -0.205   |\n",
      "|    value_loss         | 0.255    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.24e+03 |\n",
      "|    ep_rew_mean        | 164      |\n",
      "| time/                 |          |\n",
      "|    fps                | 658      |\n",
      "|    iterations         | 81300    |\n",
      "|    time_elapsed       | 9882     |\n",
      "|    total_timesteps    | 6504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.64     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81299    |\n",
      "|    policy_loss        | 0.0472   |\n",
      "|    value_loss         | 0.052    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6510000, episode_reward=148.40 +/- 45.77\n",
      "Episode length: 6187.20 +/- 1351.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.19e+03 |\n",
      "|    mean_reward        | 148      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6510000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.951   |\n",
      "|    explained_variance | -0.274   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81374    |\n",
      "|    policy_loss        | -0.205   |\n",
      "|    value_loss         | 1.38     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.22e+03 |\n",
      "|    ep_rew_mean        | 164      |\n",
      "| time/                 |          |\n",
      "|    fps                | 657      |\n",
      "|    iterations         | 81400    |\n",
      "|    time_elapsed       | 9902     |\n",
      "|    total_timesteps    | 6512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.906   |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81399    |\n",
      "|    policy_loss        | 0.0153   |\n",
      "|    value_loss         | 0.0372   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6520000, episode_reward=257.20 +/- 10.68\n",
      "Episode length: 7576.60 +/- 941.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.58e+03 |\n",
      "|    mean_reward        | 257      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81499    |\n",
      "|    policy_loss        | -0.00524 |\n",
      "|    value_loss         | 0.0234   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.31e+03 |\n",
      "|    ep_rew_mean     | 167      |\n",
      "| time/              |          |\n",
      "|    fps             | 656      |\n",
      "|    iterations      | 81500    |\n",
      "|    time_elapsed    | 9925     |\n",
      "|    total_timesteps | 6520000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.3e+03  |\n",
      "|    ep_rew_mean        | 166      |\n",
      "| time/                 |          |\n",
      "|    fps                | 657      |\n",
      "|    iterations         | 81600    |\n",
      "|    time_elapsed       | 9933     |\n",
      "|    total_timesteps    | 6528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.923   |\n",
      "|    explained_variance | 0.49     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81599    |\n",
      "|    policy_loss        | -0.285   |\n",
      "|    value_loss         | 0.628    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6530000, episode_reward=182.80 +/- 71.50\n",
      "Episode length: 6178.00 +/- 1337.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.18e+03 |\n",
      "|    mean_reward        | 183      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6530000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.933   |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81624    |\n",
      "|    policy_loss        | -0.012   |\n",
      "|    value_loss         | 0.0301   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.35e+03 |\n",
      "|    ep_rew_mean        | 171      |\n",
      "| time/                 |          |\n",
      "|    fps                | 656      |\n",
      "|    iterations         | 81700    |\n",
      "|    time_elapsed       | 9953     |\n",
      "|    total_timesteps    | 6536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.965   |\n",
      "|    explained_variance | 0.609    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81699    |\n",
      "|    policy_loss        | -0.0696  |\n",
      "|    value_loss         | 0.113    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6540000, episode_reward=122.80 +/- 64.44\n",
      "Episode length: 5755.20 +/- 735.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.76e+03 |\n",
      "|    mean_reward        | 123      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81749    |\n",
      "|    policy_loss        | -0.0917  |\n",
      "|    value_loss         | 0.151    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.32e+03 |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 656      |\n",
      "|    iterations         | 81800    |\n",
      "|    time_elapsed       | 9973     |\n",
      "|    total_timesteps    | 6544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.908   |\n",
      "|    explained_variance | 0.461    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81799    |\n",
      "|    policy_loss        | -0.178   |\n",
      "|    value_loss         | 0.399    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6550000, episode_reward=176.80 +/- 51.50\n",
      "Episode length: 6543.20 +/- 441.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.54e+03 |\n",
      "|    mean_reward        | 177      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.855   |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81874    |\n",
      "|    policy_loss        | -0.0184  |\n",
      "|    value_loss         | 0.0592   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.32e+03 |\n",
      "|    ep_rew_mean        | 175      |\n",
      "| time/                 |          |\n",
      "|    fps                | 655      |\n",
      "|    iterations         | 81900    |\n",
      "|    time_elapsed       | 9994     |\n",
      "|    total_timesteps    | 6552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.908   |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81899    |\n",
      "|    policy_loss        | -0.00205 |\n",
      "|    value_loss         | 0.0405   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6560000, episode_reward=307.80 +/- 60.86\n",
      "Episode length: 6883.60 +/- 558.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.88e+03 |\n",
      "|    mean_reward        | 308      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.97    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81999    |\n",
      "|    policy_loss        | -0.151   |\n",
      "|    value_loss         | 0.179    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.35e+03 |\n",
      "|    ep_rew_mean     | 178      |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 82000    |\n",
      "|    time_elapsed    | 10017    |\n",
      "|    total_timesteps | 6560000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.41e+03 |\n",
      "|    ep_rew_mean        | 182      |\n",
      "| time/                 |          |\n",
      "|    fps                | 655      |\n",
      "|    iterations         | 82100    |\n",
      "|    time_elapsed       | 10025    |\n",
      "|    total_timesteps    | 6568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.922   |\n",
      "|    explained_variance | 0.706    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82099    |\n",
      "|    policy_loss        | -0.0479  |\n",
      "|    value_loss         | 0.484    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6570000, episode_reward=182.20 +/- 74.53\n",
      "Episode length: 6462.00 +/- 795.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.46e+03 |\n",
      "|    mean_reward        | 182      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6570000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.956   |\n",
      "|    explained_variance | 0.686    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82124    |\n",
      "|    policy_loss        | -0.00459 |\n",
      "|    value_loss         | 0.0994   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.42e+03 |\n",
      "|    ep_rew_mean        | 181      |\n",
      "| time/                 |          |\n",
      "|    fps                | 654      |\n",
      "|    iterations         | 82200    |\n",
      "|    time_elapsed       | 10047    |\n",
      "|    total_timesteps    | 6576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.9     |\n",
      "|    explained_variance | 0.756    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82199    |\n",
      "|    policy_loss        | 0.00903  |\n",
      "|    value_loss         | 0.0304   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6580000, episode_reward=179.40 +/- 93.98\n",
      "Episode length: 5746.60 +/- 813.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.75e+03 |\n",
      "|    mean_reward        | 179      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.978   |\n",
      "|    explained_variance | 0.897    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82249    |\n",
      "|    policy_loss        | 0.0248   |\n",
      "|    value_loss         | 0.0271   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.42e+03 |\n",
      "|    ep_rew_mean        | 181      |\n",
      "| time/                 |          |\n",
      "|    fps                | 653      |\n",
      "|    iterations         | 82300    |\n",
      "|    time_elapsed       | 10067    |\n",
      "|    total_timesteps    | 6584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.954   |\n",
      "|    explained_variance | 0.896    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82299    |\n",
      "|    policy_loss        | 0.0587   |\n",
      "|    value_loss         | 0.0327   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6590000, episode_reward=231.40 +/- 52.29\n",
      "Episode length: 6258.00 +/- 847.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.26e+03 |\n",
      "|    mean_reward        | 231      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6590000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.852   |\n",
      "|    explained_variance | 0.705    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82374    |\n",
      "|    policy_loss        | 0.0312   |\n",
      "|    value_loss         | 0.0752   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.42e+03 |\n",
      "|    ep_rew_mean        | 183      |\n",
      "| time/                 |          |\n",
      "|    fps                | 653      |\n",
      "|    iterations         | 82400    |\n",
      "|    time_elapsed       | 10090    |\n",
      "|    total_timesteps    | 6592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.892   |\n",
      "|    explained_variance | 0.211    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82399    |\n",
      "|    policy_loss        | -0.159   |\n",
      "|    value_loss         | 0.456    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6600000, episode_reward=192.40 +/- 88.85\n",
      "Episode length: 6272.60 +/- 949.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.27e+03 |\n",
      "|    mean_reward        | 192      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.918   |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82499    |\n",
      "|    policy_loss        | 0.0231   |\n",
      "|    value_loss         | 0.0122   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.41e+03 |\n",
      "|    ep_rew_mean     | 183      |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 82500    |\n",
      "|    time_elapsed    | 10111    |\n",
      "|    total_timesteps | 6600000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.39e+03 |\n",
      "|    ep_rew_mean        | 183      |\n",
      "| time/                 |          |\n",
      "|    fps                | 653      |\n",
      "|    iterations         | 82600    |\n",
      "|    time_elapsed       | 10119    |\n",
      "|    total_timesteps    | 6608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.719    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82599    |\n",
      "|    policy_loss        | 0.0355   |\n",
      "|    value_loss         | 0.0266   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6610000, episode_reward=230.20 +/- 95.25\n",
      "Episode length: 6772.80 +/- 916.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.77e+03 |\n",
      "|    mean_reward        | 230      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6610000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.697    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82624    |\n",
      "|    policy_loss        | -0.0712  |\n",
      "|    value_loss         | 0.0307   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.36e+03 |\n",
      "|    ep_rew_mean        | 182      |\n",
      "| time/                 |          |\n",
      "|    fps                | 652      |\n",
      "|    iterations         | 82700    |\n",
      "|    time_elapsed       | 10141    |\n",
      "|    total_timesteps    | 6616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.36     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82699    |\n",
      "|    policy_loss        | 0.0139   |\n",
      "|    value_loss         | 0.0742   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6620000, episode_reward=6.60 +/- 1.36\n",
      "Episode length: 1046.80 +/- 83.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+03 |\n",
      "|    mean_reward        | 6.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6620000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.981   |\n",
      "|    explained_variance | 0.54     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82749    |\n",
      "|    policy_loss        | 0.041    |\n",
      "|    value_loss         | 0.153    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.2e+03  |\n",
      "|    ep_rew_mean        | 176      |\n",
      "| time/                 |          |\n",
      "|    fps                | 652      |\n",
      "|    iterations         | 82800    |\n",
      "|    time_elapsed       | 10151    |\n",
      "|    total_timesteps    | 6624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.942   |\n",
      "|    explained_variance | 0.717    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82799    |\n",
      "|    policy_loss        | -0.0322  |\n",
      "|    value_loss         | 0.0169   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6630000, episode_reward=177.20 +/- 100.94\n",
      "Episode length: 6242.80 +/- 841.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.24e+03 |\n",
      "|    mean_reward        | 177      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6630000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.83    |\n",
      "|    explained_variance | 0.782    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82874    |\n",
      "|    policy_loss        | 0.0497   |\n",
      "|    value_loss         | 0.0211   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.07e+03 |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 651      |\n",
      "|    iterations         | 82900    |\n",
      "|    time_elapsed       | 10172    |\n",
      "|    total_timesteps    | 6632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.874   |\n",
      "|    explained_variance | 0.821    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82899    |\n",
      "|    policy_loss        | -0.00242 |\n",
      "|    value_loss         | 0.0131   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6640000, episode_reward=169.00 +/- 74.64\n",
      "Episode length: 6073.80 +/- 1027.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.07e+03 |\n",
      "|    mean_reward        | 169      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.926   |\n",
      "|    explained_variance | 0.672    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82999    |\n",
      "|    policy_loss        | -0.119   |\n",
      "|    value_loss         | 0.0829   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.01e+03 |\n",
      "|    ep_rew_mean     | 166      |\n",
      "| time/              |          |\n",
      "|    fps             | 651      |\n",
      "|    iterations      | 83000    |\n",
      "|    time_elapsed    | 10192    |\n",
      "|    total_timesteps | 6640000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.94e+03 |\n",
      "|    ep_rew_mean        | 160      |\n",
      "| time/                 |          |\n",
      "|    fps                | 651      |\n",
      "|    iterations         | 83100    |\n",
      "|    time_elapsed       | 10200    |\n",
      "|    total_timesteps    | 6648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.906   |\n",
      "|    explained_variance | 0.806    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83099    |\n",
      "|    policy_loss        | 0.00917  |\n",
      "|    value_loss         | 0.0384   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6650000, episode_reward=190.80 +/- 113.31\n",
      "Episode length: 5762.60 +/- 900.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.76e+03 |\n",
      "|    mean_reward        | 191      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.959   |\n",
      "|    explained_variance | 0.798    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83124    |\n",
      "|    policy_loss        | -0.0106  |\n",
      "|    value_loss         | 0.0228   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.99e+03 |\n",
      "|    ep_rew_mean        | 162      |\n",
      "| time/                 |          |\n",
      "|    fps                | 651      |\n",
      "|    iterations         | 83200    |\n",
      "|    time_elapsed       | 10220    |\n",
      "|    total_timesteps    | 6656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.937   |\n",
      "|    explained_variance | 0.829    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83199    |\n",
      "|    policy_loss        | 0.000639 |\n",
      "|    value_loss         | 0.0228   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6660000, episode_reward=228.80 +/- 60.24\n",
      "Episode length: 6793.80 +/- 926.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.79e+03 |\n",
      "|    mean_reward        | 229      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.887   |\n",
      "|    explained_variance | 0.789    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83249    |\n",
      "|    policy_loss        | -0.0516  |\n",
      "|    value_loss         | 0.0362   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.92e+03 |\n",
      "|    ep_rew_mean        | 158      |\n",
      "| time/                 |          |\n",
      "|    fps                | 650      |\n",
      "|    iterations         | 83300    |\n",
      "|    time_elapsed       | 10242    |\n",
      "|    total_timesteps    | 6664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.862   |\n",
      "|    explained_variance | 0.792    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83299    |\n",
      "|    policy_loss        | -0.00311 |\n",
      "|    value_loss         | 0.0268   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6670000, episode_reward=205.80 +/- 108.62\n",
      "Episode length: 6187.20 +/- 1222.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.19e+03 |\n",
      "|    mean_reward        | 206      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6670000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.99    |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83374    |\n",
      "|    policy_loss        | 0.0686   |\n",
      "|    value_loss         | 0.111    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.92e+03 |\n",
      "|    ep_rew_mean        | 157      |\n",
      "| time/                 |          |\n",
      "|    fps                | 650      |\n",
      "|    iterations         | 83400    |\n",
      "|    time_elapsed       | 10263    |\n",
      "|    total_timesteps    | 6672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83399    |\n",
      "|    policy_loss        | -0.0568  |\n",
      "|    value_loss         | 0.095    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6680000, episode_reward=200.40 +/- 77.34\n",
      "Episode length: 6525.80 +/- 917.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.53e+03 |\n",
      "|    mean_reward        | 200      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.803   |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83499    |\n",
      "|    policy_loss        | -0.0154  |\n",
      "|    value_loss         | 0.0439   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.83e+03 |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 83500    |\n",
      "|    time_elapsed    | 10284    |\n",
      "|    total_timesteps | 6680000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.76e+03 |\n",
      "|    ep_rew_mean        | 149      |\n",
      "| time/                 |          |\n",
      "|    fps                | 649      |\n",
      "|    iterations         | 83600    |\n",
      "|    time_elapsed       | 10292    |\n",
      "|    total_timesteps    | 6688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.827   |\n",
      "|    explained_variance | 0.865    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83599    |\n",
      "|    policy_loss        | 0.019    |\n",
      "|    value_loss         | 0.0221   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6690000, episode_reward=152.80 +/- 60.58\n",
      "Episode length: 6577.20 +/- 557.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.58e+03 |\n",
      "|    mean_reward        | 153      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6690000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.92    |\n",
      "|    explained_variance | 0.364    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83624    |\n",
      "|    policy_loss        | 0.0176   |\n",
      "|    value_loss         | 0.0539   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.75e+03 |\n",
      "|    ep_rew_mean        | 145      |\n",
      "| time/                 |          |\n",
      "|    fps                | 649      |\n",
      "|    iterations         | 83700    |\n",
      "|    time_elapsed       | 10313    |\n",
      "|    total_timesteps    | 6696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83699    |\n",
      "|    policy_loss        | -0.0662  |\n",
      "|    value_loss         | 0.0419   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6700000, episode_reward=186.20 +/- 99.04\n",
      "Episode length: 6593.20 +/- 898.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.59e+03 |\n",
      "|    mean_reward        | 186      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.878   |\n",
      "|    explained_variance | 0.842    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83749    |\n",
      "|    policy_loss        | -0.0569  |\n",
      "|    value_loss         | 0.241    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.72e+03 |\n",
      "|    ep_rew_mean        | 140      |\n",
      "| time/                 |          |\n",
      "|    fps                | 648      |\n",
      "|    iterations         | 83800    |\n",
      "|    time_elapsed       | 10335    |\n",
      "|    total_timesteps    | 6704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.933   |\n",
      "|    explained_variance | 0.707    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83799    |\n",
      "|    policy_loss        | 0.0607   |\n",
      "|    value_loss         | 0.351    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6710000, episode_reward=121.20 +/- 79.19\n",
      "Episode length: 5791.80 +/- 976.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.79e+03 |\n",
      "|    mean_reward        | 121      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6710000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.936   |\n",
      "|    explained_variance | 0.833    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83874    |\n",
      "|    policy_loss        | -0.0115  |\n",
      "|    value_loss         | 0.0157   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.71e+03 |\n",
      "|    ep_rew_mean        | 139      |\n",
      "| time/                 |          |\n",
      "|    fps                | 648      |\n",
      "|    iterations         | 83900    |\n",
      "|    time_elapsed       | 10355    |\n",
      "|    total_timesteps    | 6712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.933   |\n",
      "|    explained_variance | 0.724    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83899    |\n",
      "|    policy_loss        | -0.0729  |\n",
      "|    value_loss         | 0.154    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6720000, episode_reward=221.40 +/- 83.27\n",
      "Episode length: 6259.00 +/- 1439.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.26e+03 |\n",
      "|    mean_reward        | 221      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.828   |\n",
      "|    explained_variance | 0.167    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83999    |\n",
      "|    policy_loss        | -0.0338  |\n",
      "|    value_loss         | 0.223    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.66e+03 |\n",
      "|    ep_rew_mean     | 137      |\n",
      "| time/              |          |\n",
      "|    fps             | 647      |\n",
      "|    iterations      | 84000    |\n",
      "|    time_elapsed    | 10376    |\n",
      "|    total_timesteps | 6720000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.65e+03 |\n",
      "|    ep_rew_mean        | 133      |\n",
      "| time/                 |          |\n",
      "|    fps                | 647      |\n",
      "|    iterations         | 84100    |\n",
      "|    time_elapsed       | 10384    |\n",
      "|    total_timesteps    | 6728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.943   |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84099    |\n",
      "|    policy_loss        | 0.00404  |\n",
      "|    value_loss         | 0.053    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6730000, episode_reward=171.80 +/- 67.20\n",
      "Episode length: 6827.80 +/- 795.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.83e+03 |\n",
      "|    mean_reward        | 172      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6730000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.874   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84124    |\n",
      "|    policy_loss        | -0.00415 |\n",
      "|    value_loss         | 0.0294   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.67e+03 |\n",
      "|    ep_rew_mean        | 137      |\n",
      "| time/                 |          |\n",
      "|    fps                | 647      |\n",
      "|    iterations         | 84200    |\n",
      "|    time_elapsed       | 10406    |\n",
      "|    total_timesteps    | 6736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84199    |\n",
      "|    policy_loss        | 0.025    |\n",
      "|    value_loss         | 0.0161   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6740000, episode_reward=269.80 +/- 72.49\n",
      "Episode length: 6688.80 +/- 770.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.69e+03 |\n",
      "|    mean_reward        | 270      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.949   |\n",
      "|    explained_variance | 0.722    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84249    |\n",
      "|    policy_loss        | -0.222   |\n",
      "|    value_loss         | 0.373    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.63e+03 |\n",
      "|    ep_rew_mean        | 138      |\n",
      "| time/                 |          |\n",
      "|    fps                | 646      |\n",
      "|    iterations         | 84300    |\n",
      "|    time_elapsed       | 10428    |\n",
      "|    total_timesteps    | 6744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.972   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84299    |\n",
      "|    policy_loss        | -0.0051  |\n",
      "|    value_loss         | 0.0586   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6750000, episode_reward=260.40 +/- 93.27\n",
      "Episode length: 7415.80 +/- 1045.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.42e+03 |\n",
      "|    mean_reward        | 260      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.921   |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84374    |\n",
      "|    policy_loss        | 0.0573   |\n",
      "|    value_loss         | 0.0295   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.61e+03 |\n",
      "|    ep_rew_mean        | 137      |\n",
      "| time/                 |          |\n",
      "|    fps                | 646      |\n",
      "|    iterations         | 84400    |\n",
      "|    time_elapsed       | 10451    |\n",
      "|    total_timesteps    | 6752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.879   |\n",
      "|    explained_variance | 0.802    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84399    |\n",
      "|    policy_loss        | -0.127   |\n",
      "|    value_loss         | 0.202    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6760000, episode_reward=234.20 +/- 90.78\n",
      "Episode length: 6612.20 +/- 773.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.61e+03 |\n",
      "|    mean_reward        | 234      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.795   |\n",
      "|    explained_variance | 0.662    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84499    |\n",
      "|    policy_loss        | -0.02    |\n",
      "|    value_loss         | 0.111    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.73e+03 |\n",
      "|    ep_rew_mean     | 146      |\n",
      "| time/              |          |\n",
      "|    fps             | 645      |\n",
      "|    iterations      | 84500    |\n",
      "|    time_elapsed    | 10473    |\n",
      "|    total_timesteps | 6760000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.76e+03 |\n",
      "|    ep_rew_mean        | 149      |\n",
      "| time/                 |          |\n",
      "|    fps                | 645      |\n",
      "|    iterations         | 84600    |\n",
      "|    time_elapsed       | 10481    |\n",
      "|    total_timesteps    | 6768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.861   |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84599    |\n",
      "|    policy_loss        | 0.101    |\n",
      "|    value_loss         | 0.0791   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6770000, episode_reward=145.60 +/- 37.45\n",
      "Episode length: 6086.40 +/- 424.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.09e+03 |\n",
      "|    mean_reward        | 146      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6770000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.885   |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84624    |\n",
      "|    policy_loss        | 0.0202   |\n",
      "|    value_loss         | 0.0648   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.88e+03 |\n",
      "|    ep_rew_mean        | 153      |\n",
      "| time/                 |          |\n",
      "|    fps                | 645      |\n",
      "|    iterations         | 84700    |\n",
      "|    time_elapsed       | 10501    |\n",
      "|    total_timesteps    | 6776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.937   |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84699    |\n",
      "|    policy_loss        | 0.0164   |\n",
      "|    value_loss         | 0.0204   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6780000, episode_reward=251.40 +/- 55.45\n",
      "Episode length: 6423.40 +/- 404.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.42e+03 |\n",
      "|    mean_reward        | 251      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.87    |\n",
      "|    explained_variance | 0.847    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84749    |\n",
      "|    policy_loss        | 0.0313   |\n",
      "|    value_loss         | 0.0361   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.97e+03 |\n",
      "|    ep_rew_mean        | 157      |\n",
      "| time/                 |          |\n",
      "|    fps                | 644      |\n",
      "|    iterations         | 84800    |\n",
      "|    time_elapsed       | 10522    |\n",
      "|    total_timesteps    | 6784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.981   |\n",
      "|    explained_variance | 0.835    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84799    |\n",
      "|    policy_loss        | 0.00778  |\n",
      "|    value_loss         | 0.0307   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6790000, episode_reward=179.80 +/- 56.55\n",
      "Episode length: 6286.60 +/- 620.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.29e+03 |\n",
      "|    mean_reward        | 180      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6790000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84874    |\n",
      "|    policy_loss        | 0.0102   |\n",
      "|    value_loss         | 0.0142   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.02e+03 |\n",
      "|    ep_rew_mean        | 161      |\n",
      "| time/                 |          |\n",
      "|    fps                | 644      |\n",
      "|    iterations         | 84900    |\n",
      "|    time_elapsed       | 10543    |\n",
      "|    total_timesteps    | 6792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.896   |\n",
      "|    explained_variance | 0.462    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84899    |\n",
      "|    policy_loss        | 0.0104   |\n",
      "|    value_loss         | 0.0558   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6800000, episode_reward=183.40 +/- 84.23\n",
      "Episode length: 6820.40 +/- 617.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.82e+03 |\n",
      "|    mean_reward        | 183      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84999    |\n",
      "|    policy_loss        | 0.0147   |\n",
      "|    value_loss         | 0.0286   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.05e+03 |\n",
      "|    ep_rew_mean     | 163      |\n",
      "| time/              |          |\n",
      "|    fps             | 643      |\n",
      "|    iterations      | 85000    |\n",
      "|    time_elapsed    | 10565    |\n",
      "|    total_timesteps | 6800000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.08e+03 |\n",
      "|    ep_rew_mean        | 166      |\n",
      "| time/                 |          |\n",
      "|    fps                | 643      |\n",
      "|    iterations         | 85100    |\n",
      "|    time_elapsed       | 10572    |\n",
      "|    total_timesteps    | 6808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.797   |\n",
      "|    explained_variance | 0.361    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85099    |\n",
      "|    policy_loss        | 0.0153   |\n",
      "|    value_loss         | 0.128    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6810000, episode_reward=203.40 +/- 96.42\n",
      "Episode length: 6785.00 +/- 1315.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.78e+03 |\n",
      "|    mean_reward        | 203      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6810000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.929   |\n",
      "|    explained_variance | 0.65     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85124    |\n",
      "|    policy_loss        | -0.14    |\n",
      "|    value_loss         | 0.303    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.07e+03 |\n",
      "|    ep_rew_mean        | 166      |\n",
      "| time/                 |          |\n",
      "|    fps                | 643      |\n",
      "|    iterations         | 85200    |\n",
      "|    time_elapsed       | 10595    |\n",
      "|    total_timesteps    | 6816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.961   |\n",
      "|    explained_variance | 0.68     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85199    |\n",
      "|    policy_loss        | -0.132   |\n",
      "|    value_loss         | 0.434    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6820000, episode_reward=149.80 +/- 61.12\n",
      "Episode length: 6166.80 +/- 906.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.17e+03 |\n",
      "|    mean_reward        | 150      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.943   |\n",
      "|    explained_variance | 0.817    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85249    |\n",
      "|    policy_loss        | 0.34     |\n",
      "|    value_loss         | 0.929    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.08e+03 |\n",
      "|    ep_rew_mean        | 167      |\n",
      "| time/                 |          |\n",
      "|    fps                | 642      |\n",
      "|    iterations         | 85300    |\n",
      "|    time_elapsed       | 10615    |\n",
      "|    total_timesteps    | 6824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.934   |\n",
      "|    explained_variance | 0.863    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85299    |\n",
      "|    policy_loss        | 0.0246   |\n",
      "|    value_loss         | 0.0295   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6830000, episode_reward=166.60 +/- 48.32\n",
      "Episode length: 6661.20 +/- 456.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.66e+03 |\n",
      "|    mean_reward        | 167      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6830000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85374    |\n",
      "|    policy_loss        | 0.0608   |\n",
      "|    value_loss         | 0.0287   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.13e+03 |\n",
      "|    ep_rew_mean        | 170      |\n",
      "| time/                 |          |\n",
      "|    fps                | 642      |\n",
      "|    iterations         | 85400    |\n",
      "|    time_elapsed       | 10637    |\n",
      "|    total_timesteps    | 6832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.783   |\n",
      "|    explained_variance | 0.896    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85399    |\n",
      "|    policy_loss        | 0.00467  |\n",
      "|    value_loss         | 0.0199   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6840000, episode_reward=172.80 +/- 80.91\n",
      "Episode length: 5245.60 +/- 1818.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.25e+03 |\n",
      "|    mean_reward        | 173      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.945   |\n",
      "|    explained_variance | 0.735    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85499    |\n",
      "|    policy_loss        | 0.128    |\n",
      "|    value_loss         | 1        |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.16e+03 |\n",
      "|    ep_rew_mean     | 173      |\n",
      "| time/              |          |\n",
      "|    fps             | 641      |\n",
      "|    iterations      | 85500    |\n",
      "|    time_elapsed    | 10656    |\n",
      "|    total_timesteps | 6840000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.25e+03 |\n",
      "|    ep_rew_mean        | 180      |\n",
      "| time/                 |          |\n",
      "|    fps                | 642      |\n",
      "|    iterations         | 85600    |\n",
      "|    time_elapsed       | 10664    |\n",
      "|    total_timesteps    | 6848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.855   |\n",
      "|    explained_variance | 0.817    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85599    |\n",
      "|    policy_loss        | -0.0522  |\n",
      "|    value_loss         | 0.0821   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6850000, episode_reward=261.40 +/- 91.18\n",
      "Episode length: 6975.80 +/- 442.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.98e+03 |\n",
      "|    mean_reward        | 261      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6850000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.911   |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85624    |\n",
      "|    policy_loss        | -0.0474  |\n",
      "|    value_loss         | 0.0627   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.28e+03 |\n",
      "|    ep_rew_mean        | 182      |\n",
      "| time/                 |          |\n",
      "|    fps                | 641      |\n",
      "|    iterations         | 85700    |\n",
      "|    time_elapsed       | 10687    |\n",
      "|    total_timesteps    | 6856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.859   |\n",
      "|    explained_variance | 0.789    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85699    |\n",
      "|    policy_loss        | 0.0749   |\n",
      "|    value_loss         | 0.0488   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6860000, episode_reward=153.20 +/- 75.16\n",
      "Episode length: 6166.20 +/- 1180.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.17e+03 |\n",
      "|    mean_reward        | 153      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6860000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.773   |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85749    |\n",
      "|    policy_loss        | -0.0247  |\n",
      "|    value_loss         | 0.0324   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.26e+03 |\n",
      "|    ep_rew_mean        | 183      |\n",
      "| time/                 |          |\n",
      "|    fps                | 641      |\n",
      "|    iterations         | 85800    |\n",
      "|    time_elapsed       | 10707    |\n",
      "|    total_timesteps    | 6864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.774   |\n",
      "|    explained_variance | 0.633    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85799    |\n",
      "|    policy_loss        | -0.122   |\n",
      "|    value_loss         | 0.45     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6870000, episode_reward=133.20 +/- 90.99\n",
      "Episode length: 5605.40 +/- 1520.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.61e+03 |\n",
      "|    mean_reward        | 133      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6870000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.794   |\n",
      "|    explained_variance | 0.766    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85874    |\n",
      "|    policy_loss        | -0.056   |\n",
      "|    value_loss         | 0.147    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.25e+03 |\n",
      "|    ep_rew_mean        | 183      |\n",
      "| time/                 |          |\n",
      "|    fps                | 640      |\n",
      "|    iterations         | 85900    |\n",
      "|    time_elapsed       | 10726    |\n",
      "|    total_timesteps    | 6872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.868   |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85899    |\n",
      "|    policy_loss        | -0.0187  |\n",
      "|    value_loss         | 0.017    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6880000, episode_reward=157.00 +/- 66.56\n",
      "Episode length: 6290.20 +/- 1259.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.29e+03 |\n",
      "|    mean_reward        | 157      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.895   |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85999    |\n",
      "|    policy_loss        | 0.00511  |\n",
      "|    value_loss         | 0.08     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.29e+03 |\n",
      "|    ep_rew_mean     | 183      |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 86000    |\n",
      "|    time_elapsed    | 10747    |\n",
      "|    total_timesteps | 6880000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.33e+03 |\n",
      "|    ep_rew_mean        | 188      |\n",
      "| time/                 |          |\n",
      "|    fps                | 640      |\n",
      "|    iterations         | 86100    |\n",
      "|    time_elapsed       | 10755    |\n",
      "|    total_timesteps    | 6888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.896   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86099    |\n",
      "|    policy_loss        | -0.0155  |\n",
      "|    value_loss         | 0.199    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6890000, episode_reward=134.00 +/- 115.65\n",
      "Episode length: 5383.60 +/- 838.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.38e+03 |\n",
      "|    mean_reward        | 134      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6890000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.904   |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86124    |\n",
      "|    policy_loss        | -0.0916  |\n",
      "|    value_loss         | 0.048    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.3e+03  |\n",
      "|    ep_rew_mean        | 189      |\n",
      "| time/                 |          |\n",
      "|    fps                | 640      |\n",
      "|    iterations         | 86200    |\n",
      "|    time_elapsed       | 10774    |\n",
      "|    total_timesteps    | 6896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.866   |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86199    |\n",
      "|    policy_loss        | -0.0205  |\n",
      "|    value_loss         | 0.0372   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6900000, episode_reward=261.00 +/- 78.96\n",
      "Episode length: 6802.80 +/- 634.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.8e+03  |\n",
      "|    mean_reward        | 261      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6900000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.946   |\n",
      "|    explained_variance | 0.871    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86249    |\n",
      "|    policy_loss        | 0.0342   |\n",
      "|    value_loss         | 0.046    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.37e+03 |\n",
      "|    ep_rew_mean        | 190      |\n",
      "| time/                 |          |\n",
      "|    fps                | 639      |\n",
      "|    iterations         | 86300    |\n",
      "|    time_elapsed       | 10796    |\n",
      "|    total_timesteps    | 6904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.919   |\n",
      "|    explained_variance | 0.677    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86299    |\n",
      "|    policy_loss        | -0.148   |\n",
      "|    value_loss         | 0.287    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6910000, episode_reward=156.60 +/- 83.70\n",
      "Episode length: 5894.60 +/- 1088.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.89e+03 |\n",
      "|    mean_reward        | 157      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6910000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.934   |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86374    |\n",
      "|    policy_loss        | -0.0687  |\n",
      "|    value_loss         | 0.0345   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.34e+03 |\n",
      "|    ep_rew_mean        | 187      |\n",
      "| time/                 |          |\n",
      "|    fps                | 639      |\n",
      "|    iterations         | 86400    |\n",
      "|    time_elapsed       | 10816    |\n",
      "|    total_timesteps    | 6912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.904   |\n",
      "|    explained_variance | 0.82     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86399    |\n",
      "|    policy_loss        | -0.0795  |\n",
      "|    value_loss         | 0.135    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6920000, episode_reward=218.00 +/- 101.63\n",
      "Episode length: 6022.00 +/- 919.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.02e+03 |\n",
      "|    mean_reward        | 218      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.928   |\n",
      "|    explained_variance | -0.0383  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86499    |\n",
      "|    policy_loss        | -0.323   |\n",
      "|    value_loss         | 1.58     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.34e+03 |\n",
      "|    ep_rew_mean     | 191      |\n",
      "| time/              |          |\n",
      "|    fps             | 638      |\n",
      "|    iterations      | 86500    |\n",
      "|    time_elapsed    | 10837    |\n",
      "|    total_timesteps | 6920000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.38e+03 |\n",
      "|    ep_rew_mean        | 193      |\n",
      "| time/                 |          |\n",
      "|    fps                | 638      |\n",
      "|    iterations         | 86600    |\n",
      "|    time_elapsed       | 10844    |\n",
      "|    total_timesteps    | 6928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86599    |\n",
      "|    policy_loss        | 0.0561   |\n",
      "|    value_loss         | 0.0175   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6930000, episode_reward=295.20 +/- 85.29\n",
      "Episode length: 6596.40 +/- 573.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.6e+03  |\n",
      "|    mean_reward        | 295      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6930000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.941   |\n",
      "|    explained_variance | 0.877    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86624    |\n",
      "|    policy_loss        | -0.0441  |\n",
      "|    value_loss         | 0.0445   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.39e+03 |\n",
      "|    ep_rew_mean        | 195      |\n",
      "| time/                 |          |\n",
      "|    fps                | 638      |\n",
      "|    iterations         | 86700    |\n",
      "|    time_elapsed       | 10866    |\n",
      "|    total_timesteps    | 6936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.959   |\n",
      "|    explained_variance | 0.698    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86699    |\n",
      "|    policy_loss        | 0.0146   |\n",
      "|    value_loss         | 0.0273   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6940000, episode_reward=194.20 +/- 63.73\n",
      "Episode length: 6343.60 +/- 859.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.34e+03 |\n",
      "|    mean_reward        | 194      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.89    |\n",
      "|    explained_variance | 0.27     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86749    |\n",
      "|    policy_loss        | -0.00811 |\n",
      "|    value_loss         | 0.662    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.42e+03 |\n",
      "|    ep_rew_mean        | 197      |\n",
      "| time/                 |          |\n",
      "|    fps                | 637      |\n",
      "|    iterations         | 86800    |\n",
      "|    time_elapsed       | 10886    |\n",
      "|    total_timesteps    | 6944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.862   |\n",
      "|    explained_variance | 0.786    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86799    |\n",
      "|    policy_loss        | -0.00492 |\n",
      "|    value_loss         | 0.0751   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6950000, episode_reward=180.40 +/- 97.01\n",
      "Episode length: 6657.60 +/- 1533.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.66e+03 |\n",
      "|    mean_reward        | 180      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6950000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.869   |\n",
      "|    explained_variance | -1.13    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86874    |\n",
      "|    policy_loss        | -0.0436  |\n",
      "|    value_loss         | 1.96     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.45e+03 |\n",
      "|    ep_rew_mean        | 199      |\n",
      "| time/                 |          |\n",
      "|    fps                | 637      |\n",
      "|    iterations         | 86900    |\n",
      "|    time_elapsed       | 10908    |\n",
      "|    total_timesteps    | 6952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.865   |\n",
      "|    explained_variance | 0.51     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86899    |\n",
      "|    policy_loss        | -0.251   |\n",
      "|    value_loss         | 1.06     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6960000, episode_reward=252.80 +/- 102.21\n",
      "Episode length: 7218.60 +/- 1146.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.22e+03 |\n",
      "|    mean_reward        | 253      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.889   |\n",
      "|    explained_variance | 0.748    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86999    |\n",
      "|    policy_loss        | -0.129   |\n",
      "|    value_loss         | 0.355    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.47e+03 |\n",
      "|    ep_rew_mean     | 203      |\n",
      "| time/              |          |\n",
      "|    fps             | 636      |\n",
      "|    iterations      | 87000    |\n",
      "|    time_elapsed    | 10931    |\n",
      "|    total_timesteps | 6960000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.44e+03 |\n",
      "|    ep_rew_mean        | 203      |\n",
      "| time/                 |          |\n",
      "|    fps                | 637      |\n",
      "|    iterations         | 87100    |\n",
      "|    time_elapsed       | 10938    |\n",
      "|    total_timesteps    | 6968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.901   |\n",
      "|    explained_variance | 0.847    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87099    |\n",
      "|    policy_loss        | -0.044   |\n",
      "|    value_loss         | 0.0274   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6970000, episode_reward=228.80 +/- 115.71\n",
      "Episode length: 6824.00 +/- 822.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.82e+03 |\n",
      "|    mean_reward        | 229      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6970000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.894   |\n",
      "|    explained_variance | 0.884    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87124    |\n",
      "|    policy_loss        | 0.0435   |\n",
      "|    value_loss         | 0.0346   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.43e+03 |\n",
      "|    ep_rew_mean        | 201      |\n",
      "| time/                 |          |\n",
      "|    fps                | 636      |\n",
      "|    iterations         | 87200    |\n",
      "|    time_elapsed       | 10960    |\n",
      "|    total_timesteps    | 6976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.995   |\n",
      "|    explained_variance | 0.691    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87199    |\n",
      "|    policy_loss        | 0.0235   |\n",
      "|    value_loss         | 0.0989   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6980000, episode_reward=220.80 +/- 83.72\n",
      "Episode length: 6550.80 +/- 1003.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.55e+03 |\n",
      "|    mean_reward        | 221      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.961   |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87249    |\n",
      "|    policy_loss        | 0.0765   |\n",
      "|    value_loss         | 0.0642   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.38e+03 |\n",
      "|    ep_rew_mean        | 199      |\n",
      "| time/                 |          |\n",
      "|    fps                | 635      |\n",
      "|    iterations         | 87300    |\n",
      "|    time_elapsed       | 10982    |\n",
      "|    total_timesteps    | 6984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.832   |\n",
      "|    explained_variance | 0.718    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87299    |\n",
      "|    policy_loss        | 0.152    |\n",
      "|    value_loss         | 0.714    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6990000, episode_reward=205.80 +/- 111.60\n",
      "Episode length: 7423.00 +/- 1178.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.42e+03 |\n",
      "|    mean_reward        | 206      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6990000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.879   |\n",
      "|    explained_variance | 0.0628   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87374    |\n",
      "|    policy_loss        | -0.184   |\n",
      "|    value_loss         | 1.73     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.42e+03 |\n",
      "|    ep_rew_mean        | 202      |\n",
      "| time/                 |          |\n",
      "|    fps                | 635      |\n",
      "|    iterations         | 87400    |\n",
      "|    time_elapsed       | 11005    |\n",
      "|    total_timesteps    | 6992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.899   |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87399    |\n",
      "|    policy_loss        | -0.0278  |\n",
      "|    value_loss         | 0.0283   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7000000, episode_reward=248.80 +/- 115.05\n",
      "Episode length: 7212.40 +/- 1451.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.21e+03 |\n",
      "|    mean_reward        | 249      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.95    |\n",
      "|    explained_variance | 0.886    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87499    |\n",
      "|    policy_loss        | -0.00484 |\n",
      "|    value_loss         | 0.0357   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.44e+03 |\n",
      "|    ep_rew_mean     | 204      |\n",
      "| time/              |          |\n",
      "|    fps             | 634      |\n",
      "|    iterations      | 87500    |\n",
      "|    time_elapsed    | 11028    |\n",
      "|    total_timesteps | 7000000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.47e+03 |\n",
      "|    ep_rew_mean        | 204      |\n",
      "| time/                 |          |\n",
      "|    fps                | 635      |\n",
      "|    iterations         | 87600    |\n",
      "|    time_elapsed       | 11036    |\n",
      "|    total_timesteps    | 7008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.829   |\n",
      "|    explained_variance | 0.496    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87599    |\n",
      "|    policy_loss        | -0.082   |\n",
      "|    value_loss         | 1.44     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7010000, episode_reward=307.20 +/- 71.21\n",
      "Episode length: 7833.20 +/- 1226.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.83e+03 |\n",
      "|    mean_reward        | 307      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7010000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.97    |\n",
      "|    explained_variance | 0.796    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87624    |\n",
      "|    policy_loss        | 0.0383   |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.49e+03 |\n",
      "|    ep_rew_mean        | 206      |\n",
      "| time/                 |          |\n",
      "|    fps                | 634      |\n",
      "|    iterations         | 87700    |\n",
      "|    time_elapsed       | 11058    |\n",
      "|    total_timesteps    | 7016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.808   |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87699    |\n",
      "|    policy_loss        | -0.00333 |\n",
      "|    value_loss         | 0.0261   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7020000, episode_reward=334.40 +/- 25.78\n",
      "Episode length: 6874.60 +/- 474.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.87e+03 |\n",
      "|    mean_reward        | 334      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.929   |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87749    |\n",
      "|    policy_loss        | -0.0218  |\n",
      "|    value_loss         | 0.071    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.49e+03 |\n",
      "|    ep_rew_mean        | 205      |\n",
      "| time/                 |          |\n",
      "|    fps                | 634      |\n",
      "|    iterations         | 87800    |\n",
      "|    time_elapsed       | 11078    |\n",
      "|    total_timesteps    | 7024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.9     |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87799    |\n",
      "|    policy_loss        | 0.0842   |\n",
      "|    value_loss         | 0.0646   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7030000, episode_reward=212.80 +/- 76.15\n",
      "Episode length: 6702.20 +/- 1139.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.7e+03  |\n",
      "|    mean_reward        | 213      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7030000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.832   |\n",
      "|    explained_variance | 0.885    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87874    |\n",
      "|    policy_loss        | 0.00694  |\n",
      "|    value_loss         | 0.0652   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.57e+03 |\n",
      "|    ep_rew_mean        | 211      |\n",
      "| time/                 |          |\n",
      "|    fps                | 633      |\n",
      "|    iterations         | 87900    |\n",
      "|    time_elapsed       | 11098    |\n",
      "|    total_timesteps    | 7032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.889   |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87899    |\n",
      "|    policy_loss        | -0.0602  |\n",
      "|    value_loss         | 0.17     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7040000, episode_reward=298.80 +/- 47.25\n",
      "Episode length: 7060.40 +/- 748.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.06e+03 |\n",
      "|    mean_reward        | 299      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.92    |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87999    |\n",
      "|    policy_loss        | 0.0312   |\n",
      "|    value_loss         | 0.104    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.62e+03 |\n",
      "|    ep_rew_mean     | 215      |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 88000    |\n",
      "|    time_elapsed    | 11118    |\n",
      "|    total_timesteps | 7040000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.59e+03 |\n",
      "|    ep_rew_mean        | 216      |\n",
      "| time/                 |          |\n",
      "|    fps                | 633      |\n",
      "|    iterations         | 88100    |\n",
      "|    time_elapsed       | 11126    |\n",
      "|    total_timesteps    | 7048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.895   |\n",
      "|    explained_variance | 0.486    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88099    |\n",
      "|    policy_loss        | 0.0401   |\n",
      "|    value_loss         | 1.5      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7050000, episode_reward=176.40 +/- 79.88\n",
      "Episode length: 6444.60 +/- 360.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.44e+03 |\n",
      "|    mean_reward        | 176      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.836   |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88124    |\n",
      "|    policy_loss        | -0.022   |\n",
      "|    value_loss         | 0.0746   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.55e+03 |\n",
      "|    ep_rew_mean        | 212      |\n",
      "| time/                 |          |\n",
      "|    fps                | 633      |\n",
      "|    iterations         | 88200    |\n",
      "|    time_elapsed       | 11145    |\n",
      "|    total_timesteps    | 7056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.989   |\n",
      "|    explained_variance | 0.4      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88199    |\n",
      "|    policy_loss        | 0.00418  |\n",
      "|    value_loss         | 0.255    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7060000, episode_reward=321.60 +/- 65.95\n",
      "Episode length: 7677.20 +/- 621.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.68e+03 |\n",
      "|    mean_reward        | 322      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.782    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88249    |\n",
      "|    policy_loss        | 0.0902   |\n",
      "|    value_loss         | 0.0914   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.51e+03 |\n",
      "|    ep_rew_mean        | 211      |\n",
      "| time/                 |          |\n",
      "|    fps                | 632      |\n",
      "|    iterations         | 88300    |\n",
      "|    time_elapsed       | 11167    |\n",
      "|    total_timesteps    | 7064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.976   |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88299    |\n",
      "|    policy_loss        | 0.00154  |\n",
      "|    value_loss         | 0.0383   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7070000, episode_reward=311.40 +/- 35.88\n",
      "Episode length: 7200.40 +/- 1122.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.2e+03  |\n",
      "|    mean_reward        | 311      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7070000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.844   |\n",
      "|    explained_variance | 0.812    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88374    |\n",
      "|    policy_loss        | 0.0663   |\n",
      "|    value_loss         | 0.0834   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.58e+03 |\n",
      "|    ep_rew_mean        | 217      |\n",
      "| time/                 |          |\n",
      "|    fps                | 632      |\n",
      "|    iterations         | 88400    |\n",
      "|    time_elapsed       | 11188    |\n",
      "|    total_timesteps    | 7072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.921   |\n",
      "|    explained_variance | 0.889    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88399    |\n",
      "|    policy_loss        | 0.0657   |\n",
      "|    value_loss         | 0.0547   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7080000, episode_reward=206.20 +/- 96.73\n",
      "Episode length: 5740.20 +/- 1572.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.74e+03 |\n",
      "|    mean_reward        | 206      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.981   |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88499    |\n",
      "|    policy_loss        | 0.0217   |\n",
      "|    value_loss         | 0.0758   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.62e+03 |\n",
      "|    ep_rew_mean     | 219      |\n",
      "| time/              |          |\n",
      "|    fps             | 631      |\n",
      "|    iterations      | 88500    |\n",
      "|    time_elapsed    | 11206    |\n",
      "|    total_timesteps | 7080000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.67e+03 |\n",
      "|    ep_rew_mean        | 225      |\n",
      "| time/                 |          |\n",
      "|    fps                | 632      |\n",
      "|    iterations         | 88600    |\n",
      "|    time_elapsed       | 11213    |\n",
      "|    total_timesteps    | 7088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.922   |\n",
      "|    explained_variance | 0.869    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88599    |\n",
      "|    policy_loss        | -0.0431  |\n",
      "|    value_loss         | 0.0521   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7090000, episode_reward=183.60 +/- 98.27\n",
      "Episode length: 6263.40 +/- 1456.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.26e+03 |\n",
      "|    mean_reward        | 184      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7090000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.809   |\n",
      "|    explained_variance | 0.235    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88624    |\n",
      "|    policy_loss        | -0.147   |\n",
      "|    value_loss         | 0.823    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.69e+03 |\n",
      "|    ep_rew_mean        | 227      |\n",
      "| time/                 |          |\n",
      "|    fps                | 631      |\n",
      "|    iterations         | 88700    |\n",
      "|    time_elapsed       | 11232    |\n",
      "|    total_timesteps    | 7096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.858   |\n",
      "|    explained_variance | 0.479    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88699    |\n",
      "|    policy_loss        | -0.194   |\n",
      "|    value_loss         | 0.768    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7100000, episode_reward=291.60 +/- 86.02\n",
      "Episode length: 7596.80 +/- 917.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.6e+03  |\n",
      "|    mean_reward        | 292      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.949   |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88749    |\n",
      "|    policy_loss        | 0.091    |\n",
      "|    value_loss         | 0.122    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.71e+03 |\n",
      "|    ep_rew_mean        | 227      |\n",
      "| time/                 |          |\n",
      "|    fps                | 631      |\n",
      "|    iterations         | 88800    |\n",
      "|    time_elapsed       | 11254    |\n",
      "|    total_timesteps    | 7104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.787   |\n",
      "|    explained_variance | 0.833    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88799    |\n",
      "|    policy_loss        | -0.166   |\n",
      "|    value_loss         | 0.923    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7110000, episode_reward=262.80 +/- 98.89\n",
      "Episode length: 7590.40 +/- 847.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.59e+03 |\n",
      "|    mean_reward        | 263      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7110000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.911   |\n",
      "|    explained_variance | 0.0451   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88874    |\n",
      "|    policy_loss        | -0.0394  |\n",
      "|    value_loss         | 0.374    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.77e+03 |\n",
      "|    ep_rew_mean        | 231      |\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 88900    |\n",
      "|    time_elapsed       | 11275    |\n",
      "|    total_timesteps    | 7112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.962   |\n",
      "|    explained_variance | 0.296    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88899    |\n",
      "|    policy_loss        | -0.157   |\n",
      "|    value_loss         | 0.389    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7120000, episode_reward=230.20 +/- 102.12\n",
      "Episode length: 6623.00 +/- 612.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.62e+03 |\n",
      "|    mean_reward        | 230      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.85    |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88999    |\n",
      "|    policy_loss        | 0.0285   |\n",
      "|    value_loss         | 0.0302   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.78e+03 |\n",
      "|    ep_rew_mean     | 231      |\n",
      "| time/              |          |\n",
      "|    fps             | 630      |\n",
      "|    iterations      | 89000    |\n",
      "|    time_elapsed    | 11295    |\n",
      "|    total_timesteps | 7120000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.84e+03 |\n",
      "|    ep_rew_mean        | 234      |\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 89100    |\n",
      "|    time_elapsed       | 11302    |\n",
      "|    total_timesteps    | 7128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.911   |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89099    |\n",
      "|    policy_loss        | 0.151    |\n",
      "|    value_loss         | 0.2      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7130000, episode_reward=252.40 +/- 101.48\n",
      "Episode length: 6417.60 +/- 718.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.42e+03 |\n",
      "|    mean_reward        | 252      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7130000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.912   |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89124    |\n",
      "|    policy_loss        | 0.0702   |\n",
      "|    value_loss         | 0.127    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.88e+03 |\n",
      "|    ep_rew_mean        | 238      |\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 89200    |\n",
      "|    time_elapsed       | 11321    |\n",
      "|    total_timesteps    | 7136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.952   |\n",
      "|    explained_variance | 0.744    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89199    |\n",
      "|    policy_loss        | 0.0494   |\n",
      "|    value_loss         | 0.119    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7140000, episode_reward=176.40 +/- 98.33\n",
      "Episode length: 6369.80 +/- 1384.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.37e+03 |\n",
      "|    mean_reward        | 176      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.931   |\n",
      "|    explained_variance | 0.49     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89249    |\n",
      "|    policy_loss        | -0.0719  |\n",
      "|    value_loss         | 0.439    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.96e+03 |\n",
      "|    ep_rew_mean        | 243      |\n",
      "| time/                 |          |\n",
      "|    fps                | 629      |\n",
      "|    iterations         | 89300    |\n",
      "|    time_elapsed       | 11340    |\n",
      "|    total_timesteps    | 7144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89299    |\n",
      "|    policy_loss        | -0.0359  |\n",
      "|    value_loss         | 0.0424   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7150000, episode_reward=290.80 +/- 122.75\n",
      "Episode length: 5846.60 +/- 651.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.85e+03 |\n",
      "|    mean_reward        | 291      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.914   |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89374    |\n",
      "|    policy_loss        | -0.0468  |\n",
      "|    value_loss         | 0.0482   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7e+03    |\n",
      "|    ep_rew_mean        | 247      |\n",
      "| time/                 |          |\n",
      "|    fps                | 629      |\n",
      "|    iterations         | 89400    |\n",
      "|    time_elapsed       | 11359    |\n",
      "|    total_timesteps    | 7152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.858   |\n",
      "|    explained_variance | 0.808    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89399    |\n",
      "|    policy_loss        | -0.0187  |\n",
      "|    value_loss         | 0.237    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7160000, episode_reward=218.60 +/- 78.69\n",
      "Episode length: 6640.60 +/- 511.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.64e+03 |\n",
      "|    mean_reward        | 219      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.823   |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89499    |\n",
      "|    policy_loss        | -0.00965 |\n",
      "|    value_loss         | 0.0903   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.05e+03 |\n",
      "|    ep_rew_mean     | 253      |\n",
      "| time/              |          |\n",
      "|    fps             | 629      |\n",
      "|    iterations      | 89500    |\n",
      "|    time_elapsed    | 11379    |\n",
      "|    total_timesteps | 7160000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.03e+03 |\n",
      "|    ep_rew_mean        | 252      |\n",
      "| time/                 |          |\n",
      "|    fps                | 629      |\n",
      "|    iterations         | 89600    |\n",
      "|    time_elapsed       | 11386    |\n",
      "|    total_timesteps    | 7168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.859   |\n",
      "|    explained_variance | 0.631    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89599    |\n",
      "|    policy_loss        | -0.0555  |\n",
      "|    value_loss         | 0.145    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7170000, episode_reward=252.80 +/- 68.24\n",
      "Episode length: 7126.80 +/- 869.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.13e+03 |\n",
      "|    mean_reward        | 253      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7170000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.795   |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89624    |\n",
      "|    policy_loss        | 0.0402   |\n",
      "|    value_loss         | 0.0386   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.06e+03 |\n",
      "|    ep_rew_mean        | 256      |\n",
      "| time/                 |          |\n",
      "|    fps                | 629      |\n",
      "|    iterations         | 89700    |\n",
      "|    time_elapsed       | 11406    |\n",
      "|    total_timesteps    | 7176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.756   |\n",
      "|    explained_variance | 0.206    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89699    |\n",
      "|    policy_loss        | -0.167   |\n",
      "|    value_loss         | 1.07     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7180000, episode_reward=329.00 +/- 36.38\n",
      "Episode length: 7029.20 +/- 1288.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.03e+03 |\n",
      "|    mean_reward        | 329      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.739   |\n",
      "|    explained_variance | 0.789    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89749    |\n",
      "|    policy_loss        | -0.0748  |\n",
      "|    value_loss         | 0.376    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.04e+03 |\n",
      "|    ep_rew_mean        | 254      |\n",
      "| time/                 |          |\n",
      "|    fps                | 628      |\n",
      "|    iterations         | 89800    |\n",
      "|    time_elapsed       | 11427    |\n",
      "|    total_timesteps    | 7184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.819   |\n",
      "|    explained_variance | 0.87     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89799    |\n",
      "|    policy_loss        | 0.0484   |\n",
      "|    value_loss         | 0.0369   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7190000, episode_reward=185.40 +/- 86.02\n",
      "Episode length: 6354.60 +/- 907.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.35e+03 |\n",
      "|    mean_reward        | 185      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7190000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.51     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89874    |\n",
      "|    policy_loss        | 0.00354  |\n",
      "|    value_loss         | 0.0808   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.06e+03 |\n",
      "|    ep_rew_mean        | 252      |\n",
      "| time/                 |          |\n",
      "|    fps                | 628      |\n",
      "|    iterations         | 89900    |\n",
      "|    time_elapsed       | 11446    |\n",
      "|    total_timesteps    | 7192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.875   |\n",
      "|    explained_variance | 0.695    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89899    |\n",
      "|    policy_loss        | 0.0417   |\n",
      "|    value_loss         | 0.0613   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7200000, episode_reward=253.00 +/- 61.54\n",
      "Episode length: 7281.00 +/- 765.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.28e+03 |\n",
      "|    mean_reward        | 253      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.803   |\n",
      "|    explained_variance | 0.829    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89999    |\n",
      "|    policy_loss        | 0.0313   |\n",
      "|    value_loss         | 0.0649   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.03e+03 |\n",
      "|    ep_rew_mean     | 250      |\n",
      "| time/              |          |\n",
      "|    fps             | 627      |\n",
      "|    iterations      | 90000    |\n",
      "|    time_elapsed    | 11468    |\n",
      "|    total_timesteps | 7200000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.98e+03 |\n",
      "|    ep_rew_mean        | 242      |\n",
      "| time/                 |          |\n",
      "|    fps                | 628      |\n",
      "|    iterations         | 90100    |\n",
      "|    time_elapsed       | 11475    |\n",
      "|    total_timesteps    | 7208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.839   |\n",
      "|    explained_variance | 0.827    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90099    |\n",
      "|    policy_loss        | 0.0353   |\n",
      "|    value_loss         | 0.0425   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7210000, episode_reward=163.40 +/- 84.18\n",
      "Episode length: 6595.00 +/- 1218.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.6e+03  |\n",
      "|    mean_reward        | 163      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7210000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.844   |\n",
      "|    explained_variance | 0.804    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90124    |\n",
      "|    policy_loss        | 0.0292   |\n",
      "|    value_loss         | 0.0375   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7e+03    |\n",
      "|    ep_rew_mean        | 240      |\n",
      "| time/                 |          |\n",
      "|    fps                | 627      |\n",
      "|    iterations         | 90200    |\n",
      "|    time_elapsed       | 11495    |\n",
      "|    total_timesteps    | 7216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.858   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90199    |\n",
      "|    policy_loss        | 0.0604   |\n",
      "|    value_loss         | 0.195    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7220000, episode_reward=221.40 +/- 91.67\n",
      "Episode length: 6704.40 +/- 1310.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.7e+03  |\n",
      "|    mean_reward        | 221      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.909   |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90249    |\n",
      "|    policy_loss        | -0.0323  |\n",
      "|    value_loss         | 0.0318   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.01e+03 |\n",
      "|    ep_rew_mean        | 243      |\n",
      "| time/                 |          |\n",
      "|    fps                | 627      |\n",
      "|    iterations         | 90300    |\n",
      "|    time_elapsed       | 11515    |\n",
      "|    total_timesteps    | 7224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.857   |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90299    |\n",
      "|    policy_loss        | 0.0132   |\n",
      "|    value_loss         | 0.0228   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7230000, episode_reward=306.60 +/- 66.97\n",
      "Episode length: 7728.40 +/- 832.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.73e+03 |\n",
      "|    mean_reward        | 307      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7230000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.887   |\n",
      "|    explained_variance | 0.764    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90374    |\n",
      "|    policy_loss        | -0.00691 |\n",
      "|    value_loss         | 0.0211   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.05e+03 |\n",
      "|    ep_rew_mean        | 241      |\n",
      "| time/                 |          |\n",
      "|    fps                | 626      |\n",
      "|    iterations         | 90400    |\n",
      "|    time_elapsed       | 11537    |\n",
      "|    total_timesteps    | 7232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.989   |\n",
      "|    explained_variance | 0.71     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90399    |\n",
      "|    policy_loss        | 0.0166   |\n",
      "|    value_loss         | 0.0445   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7240000, episode_reward=275.00 +/- 122.40\n",
      "Episode length: 6216.40 +/- 2413.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.22e+03 |\n",
      "|    mean_reward        | 275      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.848   |\n",
      "|    explained_variance | 0.801    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90499    |\n",
      "|    policy_loss        | 0.000966 |\n",
      "|    value_loss         | 0.0945   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.02e+03 |\n",
      "|    ep_rew_mean     | 237      |\n",
      "| time/              |          |\n",
      "|    fps             | 626      |\n",
      "|    iterations      | 90500    |\n",
      "|    time_elapsed    | 11557    |\n",
      "|    total_timesteps | 7240000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7e+03    |\n",
      "|    ep_rew_mean        | 240      |\n",
      "| time/                 |          |\n",
      "|    fps                | 626      |\n",
      "|    iterations         | 90600    |\n",
      "|    time_elapsed       | 11564    |\n",
      "|    total_timesteps    | 7248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.939   |\n",
      "|    explained_variance | 0.242    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90599    |\n",
      "|    policy_loss        | -0.182   |\n",
      "|    value_loss         | 0.866    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7250000, episode_reward=248.80 +/- 52.12\n",
      "Episode length: 6594.20 +/- 1220.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.59e+03 |\n",
      "|    mean_reward        | 249      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.951   |\n",
      "|    explained_variance | 0.746    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90624    |\n",
      "|    policy_loss        | 0.00349  |\n",
      "|    value_loss         | 0.426    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.93e+03 |\n",
      "|    ep_rew_mean        | 233      |\n",
      "| time/                 |          |\n",
      "|    fps                | 626      |\n",
      "|    iterations         | 90700    |\n",
      "|    time_elapsed       | 11584    |\n",
      "|    total_timesteps    | 7256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.94    |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90699    |\n",
      "|    policy_loss        | -0.00159 |\n",
      "|    value_loss         | 0.065    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7260000, episode_reward=209.60 +/- 97.79\n",
      "Episode length: 6701.60 +/- 800.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.7e+03  |\n",
      "|    mean_reward        | 210      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.888   |\n",
      "|    explained_variance | -1.47    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90749    |\n",
      "|    policy_loss        | -0.282   |\n",
      "|    value_loss         | 2.19     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.9e+03  |\n",
      "|    ep_rew_mean        | 232      |\n",
      "| time/                 |          |\n",
      "|    fps                | 625      |\n",
      "|    iterations         | 90800    |\n",
      "|    time_elapsed       | 11604    |\n",
      "|    total_timesteps    | 7264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.812   |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90799    |\n",
      "|    policy_loss        | 0.00187  |\n",
      "|    value_loss         | 0.0409   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7270000, episode_reward=223.00 +/- 109.80\n",
      "Episode length: 6747.80 +/- 551.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.75e+03 |\n",
      "|    mean_reward        | 223      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7270000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.896   |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90874    |\n",
      "|    policy_loss        | 0.0112   |\n",
      "|    value_loss         | 0.0317   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.84e+03 |\n",
      "|    ep_rew_mean        | 233      |\n",
      "| time/                 |          |\n",
      "|    fps                | 625      |\n",
      "|    iterations         | 90900    |\n",
      "|    time_elapsed       | 11624    |\n",
      "|    total_timesteps    | 7272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.931   |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90899    |\n",
      "|    policy_loss        | 0.037    |\n",
      "|    value_loss         | 0.0392   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7280000, episode_reward=284.60 +/- 42.25\n",
      "Episode length: 7955.60 +/- 822.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.96e+03 |\n",
      "|    mean_reward        | 285      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.834   |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90999    |\n",
      "|    policy_loss        | 0.00976  |\n",
      "|    value_loss         | 0.0768   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.84e+03 |\n",
      "|    ep_rew_mean     | 232      |\n",
      "| time/              |          |\n",
      "|    fps             | 625      |\n",
      "|    iterations      | 91000    |\n",
      "|    time_elapsed    | 11647    |\n",
      "|    total_timesteps | 7280000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.74e+03 |\n",
      "|    ep_rew_mean        | 226      |\n",
      "| time/                 |          |\n",
      "|    fps                | 625      |\n",
      "|    iterations         | 91100    |\n",
      "|    time_elapsed       | 11654    |\n",
      "|    total_timesteps    | 7288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.874   |\n",
      "|    explained_variance | 0.576    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91099    |\n",
      "|    policy_loss        | -0.0875  |\n",
      "|    value_loss         | 0.175    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7290000, episode_reward=299.60 +/- 49.21\n",
      "Episode length: 7119.80 +/- 794.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.12e+03 |\n",
      "|    mean_reward        | 300      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7290000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.982   |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91124    |\n",
      "|    policy_loss        | 0.0902   |\n",
      "|    value_loss         | 0.0753   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.76e+03 |\n",
      "|    ep_rew_mean        | 227      |\n",
      "| time/                 |          |\n",
      "|    fps                | 624      |\n",
      "|    iterations         | 91200    |\n",
      "|    time_elapsed       | 11674    |\n",
      "|    total_timesteps    | 7296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91199    |\n",
      "|    policy_loss        | 0.125    |\n",
      "|    value_loss         | 0.221    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7300000, episode_reward=347.80 +/- 44.53\n",
      "Episode length: 7773.60 +/- 1258.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.77e+03 |\n",
      "|    mean_reward        | 348      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.805   |\n",
      "|    explained_variance | 0.545    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91249    |\n",
      "|    policy_loss        | -0.395   |\n",
      "|    value_loss         | 0.798    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.73e+03 |\n",
      "|    ep_rew_mean        | 227      |\n",
      "| time/                 |          |\n",
      "|    fps                | 624      |\n",
      "|    iterations         | 91300    |\n",
      "|    time_elapsed       | 11697    |\n",
      "|    total_timesteps    | 7304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.925   |\n",
      "|    explained_variance | 0.749    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91299    |\n",
      "|    policy_loss        | -0.0655  |\n",
      "|    value_loss         | 0.344    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7310000, episode_reward=247.80 +/- 60.19\n",
      "Episode length: 7224.80 +/- 596.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.22e+03 |\n",
      "|    mean_reward        | 248      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7310000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91374    |\n",
      "|    policy_loss        | -0.0745  |\n",
      "|    value_loss         | 0.102    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.81e+03 |\n",
      "|    ep_rew_mean        | 234      |\n",
      "| time/                 |          |\n",
      "|    fps                | 623      |\n",
      "|    iterations         | 91400    |\n",
      "|    time_elapsed       | 11718    |\n",
      "|    total_timesteps    | 7312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.987   |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91399    |\n",
      "|    policy_loss        | -0.0891  |\n",
      "|    value_loss         | 0.0769   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7320000, episode_reward=126.20 +/- 39.76\n",
      "Episode length: 5966.60 +/- 1270.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.97e+03 |\n",
      "|    mean_reward        | 126      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.934   |\n",
      "|    explained_variance | 0.502    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91499    |\n",
      "|    policy_loss        | -0.174   |\n",
      "|    value_loss         | 0.294    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.82e+03 |\n",
      "|    ep_rew_mean     | 233      |\n",
      "| time/              |          |\n",
      "|    fps             | 623      |\n",
      "|    iterations      | 91500    |\n",
      "|    time_elapsed    | 11736    |\n",
      "|    total_timesteps | 7320000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.83e+03 |\n",
      "|    ep_rew_mean        | 234      |\n",
      "| time/                 |          |\n",
      "|    fps                | 623      |\n",
      "|    iterations         | 91600    |\n",
      "|    time_elapsed       | 11743    |\n",
      "|    total_timesteps    | 7328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.949   |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91599    |\n",
      "|    policy_loss        | -0.065   |\n",
      "|    value_loss         | 0.0448   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7330000, episode_reward=180.60 +/- 65.20\n",
      "Episode length: 6409.20 +/- 481.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.41e+03 |\n",
      "|    mean_reward        | 181      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7330000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.906   |\n",
      "|    explained_variance | 0.869    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91624    |\n",
      "|    policy_loss        | 0.0349   |\n",
      "|    value_loss         | 0.124    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.83e+03 |\n",
      "|    ep_rew_mean        | 232      |\n",
      "| time/                 |          |\n",
      "|    fps                | 623      |\n",
      "|    iterations         | 91700    |\n",
      "|    time_elapsed       | 11763    |\n",
      "|    total_timesteps    | 7336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.905   |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91699    |\n",
      "|    policy_loss        | 0.00283  |\n",
      "|    value_loss         | 0.024    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7340000, episode_reward=199.40 +/- 133.55\n",
      "Episode length: 6444.40 +/- 1850.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.44e+03 |\n",
      "|    mean_reward        | 199      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91749    |\n",
      "|    policy_loss        | 0.0125   |\n",
      "|    value_loss         | 0.0539   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.78e+03 |\n",
      "|    ep_rew_mean        | 233      |\n",
      "| time/                 |          |\n",
      "|    fps                | 623      |\n",
      "|    iterations         | 91800    |\n",
      "|    time_elapsed       | 11782    |\n",
      "|    total_timesteps    | 7344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.894   |\n",
      "|    explained_variance | -0.0178  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91799    |\n",
      "|    policy_loss        | -0.138   |\n",
      "|    value_loss         | 1.27     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7350000, episode_reward=150.60 +/- 93.98\n",
      "Episode length: 5647.20 +/- 1203.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.65e+03 |\n",
      "|    mean_reward        | 151      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.924   |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91874    |\n",
      "|    policy_loss        | -0.0699  |\n",
      "|    value_loss         | 0.0381   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.77e+03 |\n",
      "|    ep_rew_mean        | 235      |\n",
      "| time/                 |          |\n",
      "|    fps                | 623      |\n",
      "|    iterations         | 91900    |\n",
      "|    time_elapsed       | 11800    |\n",
      "|    total_timesteps    | 7352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.739    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91899    |\n",
      "|    policy_loss        | -0.0582  |\n",
      "|    value_loss         | 0.174    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7360000, episode_reward=193.20 +/- 110.30\n",
      "Episode length: 6726.40 +/- 1528.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.73e+03 |\n",
      "|    mean_reward        | 193      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.887   |\n",
      "|    explained_variance | 0.739    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91999    |\n",
      "|    policy_loss        | 0.103    |\n",
      "|    value_loss         | 0.299    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.73e+03 |\n",
      "|    ep_rew_mean     | 234      |\n",
      "| time/              |          |\n",
      "|    fps             | 622      |\n",
      "|    iterations      | 92000    |\n",
      "|    time_elapsed    | 11820    |\n",
      "|    total_timesteps | 7360000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.75e+03 |\n",
      "|    ep_rew_mean        | 237      |\n",
      "| time/                 |          |\n",
      "|    fps                | 622      |\n",
      "|    iterations         | 92100    |\n",
      "|    time_elapsed       | 11827    |\n",
      "|    total_timesteps    | 7368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.762   |\n",
      "|    explained_variance | 0.651    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92099    |\n",
      "|    policy_loss        | -0.326   |\n",
      "|    value_loss         | 0.525    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7370000, episode_reward=181.60 +/- 68.37\n",
      "Episode length: 6490.40 +/- 481.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.49e+03 |\n",
      "|    mean_reward        | 182      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7370000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.866   |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92124    |\n",
      "|    policy_loss        | -0.0166  |\n",
      "|    value_loss         | 0.0256   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.77e+03 |\n",
      "|    ep_rew_mean        | 242      |\n",
      "| time/                 |          |\n",
      "|    fps                | 622      |\n",
      "|    iterations         | 92200    |\n",
      "|    time_elapsed       | 11846    |\n",
      "|    total_timesteps    | 7376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.864   |\n",
      "|    explained_variance | 0.827    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92199    |\n",
      "|    policy_loss        | -0.0393  |\n",
      "|    value_loss         | 0.112    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7380000, episode_reward=274.40 +/- 79.68\n",
      "Episode length: 7563.60 +/- 518.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.56e+03 |\n",
      "|    mean_reward        | 274      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.822   |\n",
      "|    explained_variance | 0.808    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92249    |\n",
      "|    policy_loss        | -0.146   |\n",
      "|    value_loss         | 0.164    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.79e+03 |\n",
      "|    ep_rew_mean        | 246      |\n",
      "| time/                 |          |\n",
      "|    fps                | 622      |\n",
      "|    iterations         | 92300    |\n",
      "|    time_elapsed       | 11868    |\n",
      "|    total_timesteps    | 7384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.88    |\n",
      "|    explained_variance | 0.649    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92299    |\n",
      "|    policy_loss        | 0.205    |\n",
      "|    value_loss         | 0.836    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7390000, episode_reward=207.00 +/- 121.72\n",
      "Episode length: 5758.60 +/- 1254.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.76e+03 |\n",
      "|    mean_reward        | 207      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7390000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.873   |\n",
      "|    explained_variance | 0.698    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92374    |\n",
      "|    policy_loss        | -0.121   |\n",
      "|    value_loss         | 0.12     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.81e+03 |\n",
      "|    ep_rew_mean        | 248      |\n",
      "| time/                 |          |\n",
      "|    fps                | 621      |\n",
      "|    iterations         | 92400    |\n",
      "|    time_elapsed       | 11887    |\n",
      "|    total_timesteps    | 7392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.935   |\n",
      "|    explained_variance | 0.635    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92399    |\n",
      "|    policy_loss        | -0.0834  |\n",
      "|    value_loss         | 0.366    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7400000, episode_reward=343.40 +/- 38.38\n",
      "Episode length: 7470.20 +/- 700.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.47e+03 |\n",
      "|    mean_reward        | 343      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.871   |\n",
      "|    explained_variance | 0.541    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92499    |\n",
      "|    policy_loss        | 0.0659   |\n",
      "|    value_loss         | 0.18     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.82e+03 |\n",
      "|    ep_rew_mean     | 249      |\n",
      "| time/              |          |\n",
      "|    fps             | 621      |\n",
      "|    iterations      | 92500    |\n",
      "|    time_elapsed    | 11908    |\n",
      "|    total_timesteps | 7400000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.81e+03 |\n",
      "|    ep_rew_mean        | 250      |\n",
      "| time/                 |          |\n",
      "|    fps                | 621      |\n",
      "|    iterations         | 92600    |\n",
      "|    time_elapsed       | 11915    |\n",
      "|    total_timesteps    | 7408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.933   |\n",
      "|    explained_variance | 0.617    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92599    |\n",
      "|    policy_loss        | -0.202   |\n",
      "|    value_loss         | 0.417    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7410000, episode_reward=237.20 +/- 79.30\n",
      "Episode length: 7047.00 +/- 384.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.05e+03 |\n",
      "|    mean_reward        | 237      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7410000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.877   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92624    |\n",
      "|    policy_loss        | 0.00181  |\n",
      "|    value_loss         | 0.015    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.86e+03 |\n",
      "|    ep_rew_mean        | 252      |\n",
      "| time/                 |          |\n",
      "|    fps                | 621      |\n",
      "|    iterations         | 92700    |\n",
      "|    time_elapsed       | 11936    |\n",
      "|    total_timesteps    | 7416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.884   |\n",
      "|    explained_variance | 0.855    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92699    |\n",
      "|    policy_loss        | -0.0153  |\n",
      "|    value_loss         | 0.0614   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7420000, episode_reward=197.40 +/- 135.27\n",
      "Episode length: 5619.80 +/- 1389.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.62e+03 |\n",
      "|    mean_reward        | 197      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.843   |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92749    |\n",
      "|    policy_loss        | -0.0632  |\n",
      "|    value_loss         | 0.0787   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.86e+03 |\n",
      "|    ep_rew_mean        | 250      |\n",
      "| time/                 |          |\n",
      "|    fps                | 621      |\n",
      "|    iterations         | 92800    |\n",
      "|    time_elapsed       | 11953    |\n",
      "|    total_timesteps    | 7424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.912   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92799    |\n",
      "|    policy_loss        | 0.0313   |\n",
      "|    value_loss         | 0.03     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7430000, episode_reward=319.40 +/- 35.18\n",
      "Episode length: 7887.00 +/- 892.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.89e+03 |\n",
      "|    mean_reward        | 319      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7430000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.866   |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92874    |\n",
      "|    policy_loss        | -0.0758  |\n",
      "|    value_loss         | 0.124    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.87e+03 |\n",
      "|    ep_rew_mean        | 252      |\n",
      "| time/                 |          |\n",
      "|    fps                | 620      |\n",
      "|    iterations         | 92900    |\n",
      "|    time_elapsed       | 11976    |\n",
      "|    total_timesteps    | 7432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.947   |\n",
      "|    explained_variance | 0.896    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92899    |\n",
      "|    policy_loss        | 0.0532   |\n",
      "|    value_loss         | 0.0454   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7440000, episode_reward=289.60 +/- 91.24\n",
      "Episode length: 7324.60 +/- 551.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.32e+03 |\n",
      "|    mean_reward        | 290      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.894   |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92999    |\n",
      "|    policy_loss        | -0.0144  |\n",
      "|    value_loss         | 0.032    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.91e+03 |\n",
      "|    ep_rew_mean     | 254      |\n",
      "| time/              |          |\n",
      "|    fps             | 620      |\n",
      "|    iterations      | 93000    |\n",
      "|    time_elapsed    | 11997    |\n",
      "|    total_timesteps | 7440000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.98e+03 |\n",
      "|    ep_rew_mean        | 257      |\n",
      "| time/                 |          |\n",
      "|    fps                | 620      |\n",
      "|    iterations         | 93100    |\n",
      "|    time_elapsed       | 12004    |\n",
      "|    total_timesteps    | 7448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.93    |\n",
      "|    explained_variance | 0.804    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93099    |\n",
      "|    policy_loss        | -0.179   |\n",
      "|    value_loss         | 1.08     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7450000, episode_reward=334.80 +/- 34.15\n",
      "Episode length: 8087.60 +/- 909.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.09e+03 |\n",
      "|    mean_reward        | 335      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.9     |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93124    |\n",
      "|    policy_loss        | -0.102   |\n",
      "|    value_loss         | 0.462    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.05e+03 |\n",
      "|    ep_rew_mean        | 265      |\n",
      "| time/                 |          |\n",
      "|    fps                | 619      |\n",
      "|    iterations         | 93200    |\n",
      "|    time_elapsed       | 12026    |\n",
      "|    total_timesteps    | 7456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.838   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93199    |\n",
      "|    policy_loss        | 0.104    |\n",
      "|    value_loss         | 0.0751   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7460000, episode_reward=286.60 +/- 99.59\n",
      "Episode length: 8440.80 +/- 1579.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.44e+03 |\n",
      "|    mean_reward        | 287      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.879   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93249    |\n",
      "|    policy_loss        | 0.0471   |\n",
      "|    value_loss         | 0.0173   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.13e+03 |\n",
      "|    ep_rew_mean        | 272      |\n",
      "| time/                 |          |\n",
      "|    fps                | 619      |\n",
      "|    iterations         | 93300    |\n",
      "|    time_elapsed       | 12050    |\n",
      "|    total_timesteps    | 7464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.79    |\n",
      "|    explained_variance | 0.895    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93299    |\n",
      "|    policy_loss        | -0.0534  |\n",
      "|    value_loss         | 0.0521   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7470000, episode_reward=352.80 +/- 47.01\n",
      "Episode length: 8434.60 +/- 1699.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.43e+03 |\n",
      "|    mean_reward        | 353      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7470000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.779   |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93374    |\n",
      "|    policy_loss        | -0.0693  |\n",
      "|    value_loss         | 0.205    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.11e+03 |\n",
      "|    ep_rew_mean        | 270      |\n",
      "| time/                 |          |\n",
      "|    fps                | 618      |\n",
      "|    iterations         | 93400    |\n",
      "|    time_elapsed       | 12073    |\n",
      "|    total_timesteps    | 7472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.785   |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93399    |\n",
      "|    policy_loss        | 0.0866   |\n",
      "|    value_loss         | 0.121    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7480000, episode_reward=342.60 +/- 34.28\n",
      "Episode length: 7939.40 +/- 781.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.94e+03 |\n",
      "|    mean_reward        | 343      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.789   |\n",
      "|    explained_variance | 0.86     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93499    |\n",
      "|    policy_loss        | 0.0134   |\n",
      "|    value_loss         | 0.198    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.05e+03 |\n",
      "|    ep_rew_mean     | 267      |\n",
      "| time/              |          |\n",
      "|    fps             | 618      |\n",
      "|    iterations      | 93500    |\n",
      "|    time_elapsed    | 12095    |\n",
      "|    total_timesteps | 7480000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.07e+03 |\n",
      "|    ep_rew_mean        | 266      |\n",
      "| time/                 |          |\n",
      "|    fps                | 618      |\n",
      "|    iterations         | 93600    |\n",
      "|    time_elapsed       | 12102    |\n",
      "|    total_timesteps    | 7488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.834   |\n",
      "|    explained_variance | 0.62     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93599    |\n",
      "|    policy_loss        | 0.00235  |\n",
      "|    value_loss         | 0.368    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7490000, episode_reward=317.60 +/- 53.78\n",
      "Episode length: 7505.40 +/- 538.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.51e+03 |\n",
      "|    mean_reward        | 318      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7490000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.919   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93624    |\n",
      "|    policy_loss        | 0.0597   |\n",
      "|    value_loss         | 0.0236   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.04e+03 |\n",
      "|    ep_rew_mean        | 267      |\n",
      "| time/                 |          |\n",
      "|    fps                | 618      |\n",
      "|    iterations         | 93700    |\n",
      "|    time_elapsed       | 12123    |\n",
      "|    total_timesteps    | 7496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.838   |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93699    |\n",
      "|    policy_loss        | 0.0382   |\n",
      "|    value_loss         | 0.0615   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7500000, episode_reward=239.80 +/- 58.63\n",
      "Episode length: 6534.00 +/- 795.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.53e+03 |\n",
      "|    mean_reward        | 240      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.738   |\n",
      "|    explained_variance | 0.754    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93749    |\n",
      "|    policy_loss        | 0.0716   |\n",
      "|    value_loss         | 0.577    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.05e+03 |\n",
      "|    ep_rew_mean        | 266      |\n",
      "| time/                 |          |\n",
      "|    fps                | 617      |\n",
      "|    iterations         | 93800    |\n",
      "|    time_elapsed       | 12143    |\n",
      "|    total_timesteps    | 7504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.778   |\n",
      "|    explained_variance | 0.841    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93799    |\n",
      "|    policy_loss        | -0.0135  |\n",
      "|    value_loss         | 0.0926   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7510000, episode_reward=203.60 +/- 46.47\n",
      "Episode length: 6712.60 +/- 609.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.71e+03 |\n",
      "|    mean_reward        | 204      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7510000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.881   |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93874    |\n",
      "|    policy_loss        | -0.0161  |\n",
      "|    value_loss         | 0.0512   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.03e+03 |\n",
      "|    ep_rew_mean        | 265      |\n",
      "| time/                 |          |\n",
      "|    fps                | 617      |\n",
      "|    iterations         | 93900    |\n",
      "|    time_elapsed       | 12163    |\n",
      "|    total_timesteps    | 7512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.901   |\n",
      "|    explained_variance | 0.815    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93899    |\n",
      "|    policy_loss        | -0.0298  |\n",
      "|    value_loss         | 0.166    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7520000, episode_reward=360.80 +/- 51.52\n",
      "Episode length: 8563.80 +/- 809.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.56e+03 |\n",
      "|    mean_reward        | 361      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.705   |\n",
      "|    explained_variance | 0.869    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93999    |\n",
      "|    policy_loss        | -0.0484  |\n",
      "|    value_loss         | 0.0791   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.09e+03 |\n",
      "|    ep_rew_mean     | 267      |\n",
      "| time/              |          |\n",
      "|    fps             | 617      |\n",
      "|    iterations      | 94000    |\n",
      "|    time_elapsed    | 12187    |\n",
      "|    total_timesteps | 7520000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.07e+03 |\n",
      "|    ep_rew_mean        | 267      |\n",
      "| time/                 |          |\n",
      "|    fps                | 617      |\n",
      "|    iterations         | 94100    |\n",
      "|    time_elapsed       | 12194    |\n",
      "|    total_timesteps    | 7528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.716   |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94099    |\n",
      "|    policy_loss        | 0.135    |\n",
      "|    value_loss         | 0.0934   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7530000, episode_reward=219.00 +/- 110.46\n",
      "Episode length: 6075.40 +/- 755.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.08e+03 |\n",
      "|    mean_reward        | 219      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7530000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.778   |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94124    |\n",
      "|    policy_loss        | -0.0657  |\n",
      "|    value_loss         | 0.0359   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.11e+03 |\n",
      "|    ep_rew_mean        | 269      |\n",
      "| time/                 |          |\n",
      "|    fps                | 617      |\n",
      "|    iterations         | 94200    |\n",
      "|    time_elapsed       | 12212    |\n",
      "|    total_timesteps    | 7536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.835   |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94199    |\n",
      "|    policy_loss        | -0.0444  |\n",
      "|    value_loss         | 0.0743   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7540000, episode_reward=241.00 +/- 109.27\n",
      "Episode length: 7163.60 +/- 631.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.16e+03 |\n",
      "|    mean_reward        | 241      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.852   |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94249    |\n",
      "|    policy_loss        | 0.0284   |\n",
      "|    value_loss         | 0.0457   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.13e+03 |\n",
      "|    ep_rew_mean        | 270      |\n",
      "| time/                 |          |\n",
      "|    fps                | 616      |\n",
      "|    iterations         | 94300    |\n",
      "|    time_elapsed       | 12233    |\n",
      "|    total_timesteps    | 7544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.85    |\n",
      "|    explained_variance | 0.76     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94299    |\n",
      "|    policy_loss        | 0.0181   |\n",
      "|    value_loss         | 0.0872   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7550000, episode_reward=283.00 +/- 81.73\n",
      "Episode length: 6459.80 +/- 796.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.46e+03 |\n",
      "|    mean_reward        | 283      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.924   |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94374    |\n",
      "|    policy_loss        | 0.0305   |\n",
      "|    value_loss         | 0.0252   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.17e+03 |\n",
      "|    ep_rew_mean        | 273      |\n",
      "| time/                 |          |\n",
      "|    fps                | 616      |\n",
      "|    iterations         | 94400    |\n",
      "|    time_elapsed       | 12253    |\n",
      "|    total_timesteps    | 7552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.84    |\n",
      "|    explained_variance | 0.758    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94399    |\n",
      "|    policy_loss        | -0.071   |\n",
      "|    value_loss         | 0.21     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7560000, episode_reward=340.40 +/- 29.93\n",
      "Episode length: 7609.60 +/- 709.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.61e+03 |\n",
      "|    mean_reward        | 340      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.815   |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94499    |\n",
      "|    policy_loss        | 0.0403   |\n",
      "|    value_loss         | 0.0587   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.16e+03 |\n",
      "|    ep_rew_mean     | 268      |\n",
      "| time/              |          |\n",
      "|    fps             | 615      |\n",
      "|    iterations      | 94500    |\n",
      "|    time_elapsed    | 12274    |\n",
      "|    total_timesteps | 7560000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.22e+03 |\n",
      "|    ep_rew_mean        | 269      |\n",
      "| time/                 |          |\n",
      "|    fps                | 616      |\n",
      "|    iterations         | 94600    |\n",
      "|    time_elapsed       | 12281    |\n",
      "|    total_timesteps    | 7568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94599    |\n",
      "|    policy_loss        | 0.0331   |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7570000, episode_reward=283.00 +/- 131.17\n",
      "Episode length: 7358.40 +/- 2165.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.36e+03 |\n",
      "|    mean_reward        | 283      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7570000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.845   |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94624    |\n",
      "|    policy_loss        | 0.0452   |\n",
      "|    value_loss         | 0.0404   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.27e+03 |\n",
      "|    ep_rew_mean        | 271      |\n",
      "| time/                 |          |\n",
      "|    fps                | 615      |\n",
      "|    iterations         | 94700    |\n",
      "|    time_elapsed       | 12302    |\n",
      "|    total_timesteps    | 7576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.875   |\n",
      "|    explained_variance | 0.78     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94699    |\n",
      "|    policy_loss        | -0.0393  |\n",
      "|    value_loss         | 0.116    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7580000, episode_reward=293.80 +/- 100.80\n",
      "Episode length: 8183.80 +/- 625.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.18e+03 |\n",
      "|    mean_reward        | 294      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.92    |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94749    |\n",
      "|    policy_loss        | 0.123    |\n",
      "|    value_loss         | 0.0934   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.3e+03  |\n",
      "|    ep_rew_mean        | 270      |\n",
      "| time/                 |          |\n",
      "|    fps                | 615      |\n",
      "|    iterations         | 94800    |\n",
      "|    time_elapsed       | 12324    |\n",
      "|    total_timesteps    | 7584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.778   |\n",
      "|    explained_variance | 0.866    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94799    |\n",
      "|    policy_loss        | -0.118   |\n",
      "|    value_loss         | 0.105    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7590000, episode_reward=244.40 +/- 114.96\n",
      "Episode length: 7817.60 +/- 1166.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.82e+03 |\n",
      "|    mean_reward        | 244      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7590000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.724   |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94874    |\n",
      "|    policy_loss        | -0.00378 |\n",
      "|    value_loss         | 0.106    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.29e+03 |\n",
      "|    ep_rew_mean        | 265      |\n",
      "| time/                 |          |\n",
      "|    fps                | 614      |\n",
      "|    iterations         | 94900    |\n",
      "|    time_elapsed       | 12346    |\n",
      "|    total_timesteps    | 7592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.864   |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94899    |\n",
      "|    policy_loss        | 0.00152  |\n",
      "|    value_loss         | 0.081    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7600000, episode_reward=358.80 +/- 66.44\n",
      "Episode length: 7555.20 +/- 747.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.56e+03 |\n",
      "|    mean_reward        | 359      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.826   |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94999    |\n",
      "|    policy_loss        | 0.013    |\n",
      "|    value_loss         | 0.067    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.31e+03 |\n",
      "|    ep_rew_mean     | 268      |\n",
      "| time/              |          |\n",
      "|    fps             | 614      |\n",
      "|    iterations      | 95000    |\n",
      "|    time_elapsed    | 12368    |\n",
      "|    total_timesteps | 7600000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.33e+03 |\n",
      "|    ep_rew_mean        | 269      |\n",
      "| time/                 |          |\n",
      "|    fps                | 614      |\n",
      "|    iterations         | 95100    |\n",
      "|    time_elapsed       | 12374    |\n",
      "|    total_timesteps    | 7608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.851   |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95099    |\n",
      "|    policy_loss        | -0.0346  |\n",
      "|    value_loss         | 0.0486   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7610000, episode_reward=195.40 +/- 86.02\n",
      "Episode length: 6150.60 +/- 704.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.15e+03 |\n",
      "|    mean_reward        | 195      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7610000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.908   |\n",
      "|    explained_variance | 0.832    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95124    |\n",
      "|    policy_loss        | -0.132   |\n",
      "|    value_loss         | 0.131    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.33e+03 |\n",
      "|    ep_rew_mean        | 267      |\n",
      "| time/                 |          |\n",
      "|    fps                | 614      |\n",
      "|    iterations         | 95200    |\n",
      "|    time_elapsed       | 12393    |\n",
      "|    total_timesteps    | 7616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.907   |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95199    |\n",
      "|    policy_loss        | 0.0429   |\n",
      "|    value_loss         | 0.0374   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7620000, episode_reward=279.20 +/- 87.53\n",
      "Episode length: 7157.80 +/- 1400.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.16e+03 |\n",
      "|    mean_reward        | 279      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7620000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.891    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95249    |\n",
      "|    policy_loss        | 0.0148   |\n",
      "|    value_loss         | 0.0361   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.34e+03 |\n",
      "|    ep_rew_mean        | 270      |\n",
      "| time/                 |          |\n",
      "|    fps                | 614      |\n",
      "|    iterations         | 95300    |\n",
      "|    time_elapsed       | 12414    |\n",
      "|    total_timesteps    | 7624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.885   |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95299    |\n",
      "|    policy_loss        | -0.0192  |\n",
      "|    value_loss         | 0.0788   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7630000, episode_reward=122.20 +/- 105.75\n",
      "Episode length: 5751.00 +/- 701.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.75e+03 |\n",
      "|    mean_reward        | 122      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7630000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.859   |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95374    |\n",
      "|    policy_loss        | 0.0504   |\n",
      "|    value_loss         | 0.105    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.35e+03 |\n",
      "|    ep_rew_mean        | 265      |\n",
      "| time/                 |          |\n",
      "|    fps                | 613      |\n",
      "|    iterations         | 95400    |\n",
      "|    time_elapsed       | 12432    |\n",
      "|    total_timesteps    | 7632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.836   |\n",
      "|    explained_variance | 0.601    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95399    |\n",
      "|    policy_loss        | -0.211   |\n",
      "|    value_loss         | 0.264    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7640000, episode_reward=136.60 +/- 121.71\n",
      "Episode length: 5499.20 +/- 1412.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.5e+03  |\n",
      "|    mean_reward        | 137      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.911   |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95499    |\n",
      "|    policy_loss        | -0.0602  |\n",
      "|    value_loss         | 0.0871   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.3e+03  |\n",
      "|    ep_rew_mean     | 260      |\n",
      "| time/              |          |\n",
      "|    fps             | 613      |\n",
      "|    iterations      | 95500    |\n",
      "|    time_elapsed    | 12449    |\n",
      "|    total_timesteps | 7640000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.27e+03 |\n",
      "|    ep_rew_mean        | 256      |\n",
      "| time/                 |          |\n",
      "|    fps                | 613      |\n",
      "|    iterations         | 95600    |\n",
      "|    time_elapsed       | 12456    |\n",
      "|    total_timesteps    | 7648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.806   |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95599    |\n",
      "|    policy_loss        | -0.0514  |\n",
      "|    value_loss         | 0.0312   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7650000, episode_reward=291.60 +/- 81.45\n",
      "Episode length: 7147.20 +/- 866.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.15e+03 |\n",
      "|    mean_reward        | 292      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.847   |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95624    |\n",
      "|    policy_loss        | -0.07    |\n",
      "|    value_loss         | 0.0665   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.94e+03 |\n",
      "|    ep_rew_mean        | 233      |\n",
      "| time/                 |          |\n",
      "|    fps                | 613      |\n",
      "|    iterations         | 95700    |\n",
      "|    time_elapsed       | 12477    |\n",
      "|    total_timesteps    | 7656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.894   |\n",
      "|    explained_variance | -0.122   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95699    |\n",
      "|    policy_loss        | 0.0288   |\n",
      "|    value_loss         | 0.27     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7660000, episode_reward=284.40 +/- 102.39\n",
      "Episode length: 8243.80 +/- 2309.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.24e+03 |\n",
      "|    mean_reward        | 284      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.906   |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95749    |\n",
      "|    policy_loss        | -0.0164  |\n",
      "|    value_loss         | 0.0201   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.9e+03  |\n",
      "|    ep_rew_mean        | 230      |\n",
      "| time/                 |          |\n",
      "|    fps                | 613      |\n",
      "|    iterations         | 95800    |\n",
      "|    time_elapsed       | 12500    |\n",
      "|    total_timesteps    | 7664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.94    |\n",
      "|    explained_variance | 0.905    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95799    |\n",
      "|    policy_loss        | 0.00191  |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7670000, episode_reward=309.60 +/- 67.13\n",
      "Episode length: 7974.60 +/- 1163.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.97e+03 |\n",
      "|    mean_reward        | 310      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7670000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.832   |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95874    |\n",
      "|    policy_loss        | 0.0168   |\n",
      "|    value_loss         | 0.0355   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.84e+03 |\n",
      "|    ep_rew_mean        | 224      |\n",
      "| time/                 |          |\n",
      "|    fps                | 612      |\n",
      "|    iterations         | 95900    |\n",
      "|    time_elapsed       | 12522    |\n",
      "|    total_timesteps    | 7672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.872   |\n",
      "|    explained_variance | 0.401    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95899    |\n",
      "|    policy_loss        | -0.142   |\n",
      "|    value_loss         | 0.815    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7680000, episode_reward=329.40 +/- 64.66\n",
      "Episode length: 8246.60 +/- 794.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.25e+03 |\n",
      "|    mean_reward        | 329      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.774   |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95999    |\n",
      "|    policy_loss        | -0.136   |\n",
      "|    value_loss         | 0.343    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.78e+03 |\n",
      "|    ep_rew_mean     | 218      |\n",
      "| time/              |          |\n",
      "|    fps             | 612      |\n",
      "|    iterations      | 96000    |\n",
      "|    time_elapsed    | 12544    |\n",
      "|    total_timesteps | 7680000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.78e+03 |\n",
      "|    ep_rew_mean        | 218      |\n",
      "| time/                 |          |\n",
      "|    fps                | 612      |\n",
      "|    iterations         | 96100    |\n",
      "|    time_elapsed       | 12551    |\n",
      "|    total_timesteps    | 7688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.788   |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96099    |\n",
      "|    policy_loss        | 0.0665   |\n",
      "|    value_loss         | 0.174    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7690000, episode_reward=306.60 +/- 84.66\n",
      "Episode length: 7560.20 +/- 783.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.56e+03 |\n",
      "|    mean_reward        | 307      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7690000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.916   |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96124    |\n",
      "|    policy_loss        | 0.00878  |\n",
      "|    value_loss         | 0.0164   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.82e+03 |\n",
      "|    ep_rew_mean        | 214      |\n",
      "| time/                 |          |\n",
      "|    fps                | 612      |\n",
      "|    iterations         | 96200    |\n",
      "|    time_elapsed       | 12573    |\n",
      "|    total_timesteps    | 7696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96199    |\n",
      "|    policy_loss        | 0.00239  |\n",
      "|    value_loss         | 0.0133   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7700000, episode_reward=246.20 +/- 99.45\n",
      "Episode length: 6758.20 +/- 2110.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.76e+03 |\n",
      "|    mean_reward        | 246      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.859   |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96249    |\n",
      "|    policy_loss        | 0.0392   |\n",
      "|    value_loss         | 0.123    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.83e+03 |\n",
      "|    ep_rew_mean        | 214      |\n",
      "| time/                 |          |\n",
      "|    fps                | 611      |\n",
      "|    iterations         | 96300    |\n",
      "|    time_elapsed       | 12593    |\n",
      "|    total_timesteps    | 7704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.747   |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96299    |\n",
      "|    policy_loss        | -0.0343  |\n",
      "|    value_loss         | 0.0388   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7710000, episode_reward=290.20 +/- 95.54\n",
      "Episode length: 7470.80 +/- 1063.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.47e+03 |\n",
      "|    mean_reward        | 290      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7710000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.887   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96374    |\n",
      "|    policy_loss        | -0.00921 |\n",
      "|    value_loss         | 0.0185   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.8e+03  |\n",
      "|    ep_rew_mean        | 213      |\n",
      "| time/                 |          |\n",
      "|    fps                | 611      |\n",
      "|    iterations         | 96400    |\n",
      "|    time_elapsed       | 12614    |\n",
      "|    total_timesteps    | 7712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.8     |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96399    |\n",
      "|    policy_loss        | 0.0529   |\n",
      "|    value_loss         | 0.121    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7720000, episode_reward=248.60 +/- 75.83\n",
      "Episode length: 6695.20 +/- 717.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.7e+03  |\n",
      "|    mean_reward        | 249      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.895   |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96499    |\n",
      "|    policy_loss        | -0.00278 |\n",
      "|    value_loss         | 0.0339   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.78e+03 |\n",
      "|    ep_rew_mean     | 215      |\n",
      "| time/              |          |\n",
      "|    fps             | 611      |\n",
      "|    iterations      | 96500    |\n",
      "|    time_elapsed    | 12634    |\n",
      "|    total_timesteps | 7720000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.78e+03 |\n",
      "|    ep_rew_mean        | 217      |\n",
      "| time/                 |          |\n",
      "|    fps                | 611      |\n",
      "|    iterations         | 96600    |\n",
      "|    time_elapsed       | 12642    |\n",
      "|    total_timesteps    | 7728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.767   |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96599    |\n",
      "|    policy_loss        | 0.0688   |\n",
      "|    value_loss         | 0.485    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7730000, episode_reward=368.80 +/- 32.47\n",
      "Episode length: 7862.20 +/- 1610.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.86e+03 |\n",
      "|    mean_reward        | 369      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7730000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.77    |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96624    |\n",
      "|    policy_loss        | -0.0537  |\n",
      "|    value_loss         | 0.116    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.73e+03 |\n",
      "|    ep_rew_mean        | 216      |\n",
      "| time/                 |          |\n",
      "|    fps                | 610      |\n",
      "|    iterations         | 96700    |\n",
      "|    time_elapsed       | 12664    |\n",
      "|    total_timesteps    | 7736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.817   |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96699    |\n",
      "|    policy_loss        | -0.0824  |\n",
      "|    value_loss         | 0.0621   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7740000, episode_reward=262.60 +/- 103.95\n",
      "Episode length: 6168.80 +/- 909.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.17e+03 |\n",
      "|    mean_reward        | 263      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.767   |\n",
      "|    explained_variance | 0.77     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96749    |\n",
      "|    policy_loss        | -0.00364 |\n",
      "|    value_loss         | 0.265    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.69e+03 |\n",
      "|    ep_rew_mean        | 218      |\n",
      "| time/                 |          |\n",
      "|    fps                | 610      |\n",
      "|    iterations         | 96800    |\n",
      "|    time_elapsed       | 12682    |\n",
      "|    total_timesteps    | 7744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.918   |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96799    |\n",
      "|    policy_loss        | -0.0121  |\n",
      "|    value_loss         | 0.0504   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7750000, episode_reward=304.20 +/- 66.00\n",
      "Episode length: 7222.80 +/- 246.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.22e+03 |\n",
      "|    mean_reward        | 304      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.856   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96874    |\n",
      "|    policy_loss        | 0.0103   |\n",
      "|    value_loss         | 0.0539   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.7e+03  |\n",
      "|    ep_rew_mean        | 220      |\n",
      "| time/                 |          |\n",
      "|    fps                | 610      |\n",
      "|    iterations         | 96900    |\n",
      "|    time_elapsed       | 12703    |\n",
      "|    total_timesteps    | 7752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.852   |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96899    |\n",
      "|    policy_loss        | 0.0308   |\n",
      "|    value_loss         | 0.0469   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7760000, episode_reward=314.40 +/- 50.40\n",
      "Episode length: 7729.60 +/- 850.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.73e+03 |\n",
      "|    mean_reward        | 314      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.764   |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96999    |\n",
      "|    policy_loss        | 0.0976   |\n",
      "|    value_loss         | 0.28     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.69e+03 |\n",
      "|    ep_rew_mean     | 221      |\n",
      "| time/              |          |\n",
      "|    fps             | 609      |\n",
      "|    iterations      | 97000    |\n",
      "|    time_elapsed    | 12724    |\n",
      "|    total_timesteps | 7760000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.75e+03 |\n",
      "|    ep_rew_mean        | 226      |\n",
      "| time/                 |          |\n",
      "|    fps                | 610      |\n",
      "|    iterations         | 97100    |\n",
      "|    time_elapsed       | 12731    |\n",
      "|    total_timesteps    | 7768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.887   |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97099    |\n",
      "|    policy_loss        | -0.0221  |\n",
      "|    value_loss         | 0.0422   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7770000, episode_reward=221.20 +/- 76.60\n",
      "Episode length: 6583.40 +/- 784.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.58e+03 |\n",
      "|    mean_reward        | 221      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7770000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.772   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97124    |\n",
      "|    policy_loss        | -0.0224  |\n",
      "|    value_loss         | 0.0263   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.77e+03 |\n",
      "|    ep_rew_mean        | 227      |\n",
      "| time/                 |          |\n",
      "|    fps                | 609      |\n",
      "|    iterations         | 97200    |\n",
      "|    time_elapsed       | 12751    |\n",
      "|    total_timesteps    | 7776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.897   |\n",
      "|    explained_variance | 0.555    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97199    |\n",
      "|    policy_loss        | -0.15    |\n",
      "|    value_loss         | 0.516    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7780000, episode_reward=255.40 +/- 93.99\n",
      "Episode length: 8525.20 +/- 2016.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.53e+03 |\n",
      "|    mean_reward        | 255      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.986   |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97249    |\n",
      "|    policy_loss        | 0.0209   |\n",
      "|    value_loss         | 0.0624   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.75e+03 |\n",
      "|    ep_rew_mean        | 227      |\n",
      "| time/                 |          |\n",
      "|    fps                | 609      |\n",
      "|    iterations         | 97300    |\n",
      "|    time_elapsed       | 12774    |\n",
      "|    total_timesteps    | 7784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.929   |\n",
      "|    explained_variance | 0.577    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97299    |\n",
      "|    policy_loss        | 0.101    |\n",
      "|    value_loss         | 0.748    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7790000, episode_reward=133.40 +/- 88.67\n",
      "Episode length: 5327.20 +/- 1323.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.33e+03 |\n",
      "|    mean_reward        | 133      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7790000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.967   |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97374    |\n",
      "|    policy_loss        | 0.00951  |\n",
      "|    value_loss         | 0.0345   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.72e+03 |\n",
      "|    ep_rew_mean        | 223      |\n",
      "| time/                 |          |\n",
      "|    fps                | 609      |\n",
      "|    iterations         | 97400    |\n",
      "|    time_elapsed       | 12791    |\n",
      "|    total_timesteps    | 7792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.837   |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97399    |\n",
      "|    policy_loss        | 0.0824   |\n",
      "|    value_loss         | 0.0437   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7800000, episode_reward=353.20 +/- 37.56\n",
      "Episode length: 8768.40 +/- 1662.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.77e+03 |\n",
      "|    mean_reward        | 353      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.96    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97499    |\n",
      "|    policy_loss        | -0.0352  |\n",
      "|    value_loss         | 0.0423   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.73e+03 |\n",
      "|    ep_rew_mean     | 225      |\n",
      "| time/              |          |\n",
      "|    fps             | 608      |\n",
      "|    iterations      | 97500    |\n",
      "|    time_elapsed    | 12814    |\n",
      "|    total_timesteps | 7800000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.78e+03 |\n",
      "|    ep_rew_mean        | 228      |\n",
      "| time/                 |          |\n",
      "|    fps                | 608      |\n",
      "|    iterations         | 97600    |\n",
      "|    time_elapsed       | 12822    |\n",
      "|    total_timesteps    | 7808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.891   |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97599    |\n",
      "|    policy_loss        | -0.0505  |\n",
      "|    value_loss         | 0.154    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7810000, episode_reward=337.40 +/- 30.64\n",
      "Episode length: 7602.00 +/- 1124.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.6e+03  |\n",
      "|    mean_reward        | 337      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7810000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.84    |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97624    |\n",
      "|    policy_loss        | 0.0206   |\n",
      "|    value_loss         | 0.0414   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.8e+03  |\n",
      "|    ep_rew_mean        | 229      |\n",
      "| time/                 |          |\n",
      "|    fps                | 608      |\n",
      "|    iterations         | 97700    |\n",
      "|    time_elapsed       | 12843    |\n",
      "|    total_timesteps    | 7816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.923   |\n",
      "|    explained_variance | 0.906    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97699    |\n",
      "|    policy_loss        | 0.0143   |\n",
      "|    value_loss         | 0.124    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7820000, episode_reward=245.20 +/- 96.72\n",
      "Episode length: 7082.00 +/- 810.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.08e+03 |\n",
      "|    mean_reward        | 245      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.905   |\n",
      "|    explained_variance | 0.348    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97749    |\n",
      "|    policy_loss        | -0.178   |\n",
      "|    value_loss         | 0.874    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.82e+03 |\n",
      "|    ep_rew_mean        | 231      |\n",
      "| time/                 |          |\n",
      "|    fps                | 608      |\n",
      "|    iterations         | 97800    |\n",
      "|    time_elapsed       | 12864    |\n",
      "|    total_timesteps    | 7824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.865   |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97799    |\n",
      "|    policy_loss        | 0.0482   |\n",
      "|    value_loss         | 0.064    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7830000, episode_reward=291.00 +/- 85.28\n",
      "Episode length: 7447.60 +/- 582.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.45e+03 |\n",
      "|    mean_reward        | 291      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7830000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.909   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97874    |\n",
      "|    policy_loss        | -0.0384  |\n",
      "|    value_loss         | 0.102    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.99e+03 |\n",
      "|    ep_rew_mean        | 241      |\n",
      "| time/                 |          |\n",
      "|    fps                | 607      |\n",
      "|    iterations         | 97900    |\n",
      "|    time_elapsed       | 12886    |\n",
      "|    total_timesteps    | 7832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.886   |\n",
      "|    explained_variance | 0.693    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97899    |\n",
      "|    policy_loss        | -0.228   |\n",
      "|    value_loss         | 1.94     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7840000, episode_reward=178.60 +/- 107.10\n",
      "Episode length: 5823.00 +/- 1160.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.82e+03 |\n",
      "|    mean_reward        | 179      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.744   |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97999    |\n",
      "|    policy_loss        | -0.0104  |\n",
      "|    value_loss         | 0.0392   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.14e+03 |\n",
      "|    ep_rew_mean     | 252      |\n",
      "| time/              |          |\n",
      "|    fps             | 607      |\n",
      "|    iterations      | 98000    |\n",
      "|    time_elapsed    | 12904    |\n",
      "|    total_timesteps | 7840000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.23e+03 |\n",
      "|    ep_rew_mean        | 257      |\n",
      "| time/                 |          |\n",
      "|    fps                | 607      |\n",
      "|    iterations         | 98100    |\n",
      "|    time_elapsed       | 12911    |\n",
      "|    total_timesteps    | 7848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.864   |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98099    |\n",
      "|    policy_loss        | -0.0749  |\n",
      "|    value_loss         | 0.0533   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7850000, episode_reward=298.80 +/- 97.31\n",
      "Episode length: 6999.20 +/- 1246.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7e+03    |\n",
      "|    mean_reward        | 299      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7850000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.86    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98124    |\n",
      "|    policy_loss        | 0.019    |\n",
      "|    value_loss         | 0.0527   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 7.36e+03  |\n",
      "|    ep_rew_mean        | 271       |\n",
      "| time/                 |           |\n",
      "|    fps                | 607       |\n",
      "|    iterations         | 98200     |\n",
      "|    time_elapsed       | 12932     |\n",
      "|    total_timesteps    | 7856000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.939    |\n",
      "|    explained_variance | 0.971     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 98199     |\n",
      "|    policy_loss        | -6.44e-05 |\n",
      "|    value_loss         | 0.0238    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=7860000, episode_reward=286.60 +/- 105.66\n",
      "Episode length: 7283.60 +/- 1254.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.28e+03 |\n",
      "|    mean_reward        | 287      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7860000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.816   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98249    |\n",
      "|    policy_loss        | -0.0373  |\n",
      "|    value_loss         | 0.0284   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.36e+03 |\n",
      "|    ep_rew_mean        | 271      |\n",
      "| time/                 |          |\n",
      "|    fps                | 607      |\n",
      "|    iterations         | 98300    |\n",
      "|    time_elapsed       | 12953    |\n",
      "|    total_timesteps    | 7864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.787   |\n",
      "|    explained_variance | 0.863    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98299    |\n",
      "|    policy_loss        | 0.0631   |\n",
      "|    value_loss         | 0.136    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7870000, episode_reward=223.40 +/- 128.09\n",
      "Episode length: 7146.60 +/- 1774.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.15e+03 |\n",
      "|    mean_reward        | 223      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7870000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.915   |\n",
      "|    explained_variance | 0.786    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98374    |\n",
      "|    policy_loss        | 0.0544   |\n",
      "|    value_loss         | 0.215    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.37e+03 |\n",
      "|    ep_rew_mean        | 275      |\n",
      "| time/                 |          |\n",
      "|    fps                | 606      |\n",
      "|    iterations         | 98400    |\n",
      "|    time_elapsed       | 12974    |\n",
      "|    total_timesteps    | 7872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.772   |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98399    |\n",
      "|    policy_loss        | -0.0197  |\n",
      "|    value_loss         | 0.0355   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7880000, episode_reward=263.40 +/- 103.23\n",
      "Episode length: 7838.40 +/- 1577.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.84e+03 |\n",
      "|    mean_reward        | 263      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.831   |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98499    |\n",
      "|    policy_loss        | -0.0283  |\n",
      "|    value_loss         | 0.0717   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.33e+03 |\n",
      "|    ep_rew_mean     | 272      |\n",
      "| time/              |          |\n",
      "|    fps             | 606      |\n",
      "|    iterations      | 98500    |\n",
      "|    time_elapsed    | 12996    |\n",
      "|    total_timesteps | 7880000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.37e+03 |\n",
      "|    ep_rew_mean        | 279      |\n",
      "| time/                 |          |\n",
      "|    fps                | 606      |\n",
      "|    iterations         | 98600    |\n",
      "|    time_elapsed       | 13003    |\n",
      "|    total_timesteps    | 7888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.92    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98599    |\n",
      "|    policy_loss        | -0.0176  |\n",
      "|    value_loss         | 0.0851   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7890000, episode_reward=163.00 +/- 91.16\n",
      "Episode length: 6221.80 +/- 951.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.22e+03 |\n",
      "|    mean_reward        | 163      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7890000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.874   |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98624    |\n",
      "|    policy_loss        | -0.0137  |\n",
      "|    value_loss         | 0.0835   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.35e+03 |\n",
      "|    ep_rew_mean        | 278      |\n",
      "| time/                 |          |\n",
      "|    fps                | 606      |\n",
      "|    iterations         | 98700    |\n",
      "|    time_elapsed       | 13022    |\n",
      "|    total_timesteps    | 7896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.867   |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98699    |\n",
      "|    policy_loss        | 0.0332   |\n",
      "|    value_loss         | 0.0174   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7900000, episode_reward=372.20 +/- 27.98\n",
      "Episode length: 8372.20 +/- 821.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.37e+03 |\n",
      "|    mean_reward        | 372      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7900000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.85    |\n",
      "|    explained_variance | 0.778    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98749    |\n",
      "|    policy_loss        | 0.141    |\n",
      "|    value_loss         | 0.632    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.34e+03 |\n",
      "|    ep_rew_mean        | 276      |\n",
      "| time/                 |          |\n",
      "|    fps                | 605      |\n",
      "|    iterations         | 98800    |\n",
      "|    time_elapsed       | 13045    |\n",
      "|    total_timesteps    | 7904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.834   |\n",
      "|    explained_variance | 0.889    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98799    |\n",
      "|    policy_loss        | 0.0295   |\n",
      "|    value_loss         | 0.15     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7910000, episode_reward=332.20 +/- 52.09\n",
      "Episode length: 7468.20 +/- 1006.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.47e+03 |\n",
      "|    mean_reward        | 332      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7910000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.868   |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98874    |\n",
      "|    policy_loss        | -0.121   |\n",
      "|    value_loss         | 0.34     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.37e+03 |\n",
      "|    ep_rew_mean        | 277      |\n",
      "| time/                 |          |\n",
      "|    fps                | 605      |\n",
      "|    iterations         | 98900    |\n",
      "|    time_elapsed       | 13066    |\n",
      "|    total_timesteps    | 7912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.923   |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98899    |\n",
      "|    policy_loss        | -0.00785 |\n",
      "|    value_loss         | 0.088    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7920000, episode_reward=315.80 +/- 92.92\n",
      "Episode length: 7974.00 +/- 961.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.97e+03 |\n",
      "|    mean_reward        | 316      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.872   |\n",
      "|    explained_variance | 0.513    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98999    |\n",
      "|    policy_loss        | -0.0885  |\n",
      "|    value_loss         | 0.918    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.41e+03 |\n",
      "|    ep_rew_mean     | 278      |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 99000    |\n",
      "|    time_elapsed    | 13088    |\n",
      "|    total_timesteps | 7920000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.41e+03 |\n",
      "|    ep_rew_mean        | 277      |\n",
      "| time/                 |          |\n",
      "|    fps                | 605      |\n",
      "|    iterations         | 99100    |\n",
      "|    time_elapsed       | 13095    |\n",
      "|    total_timesteps    | 7928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.905   |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99099    |\n",
      "|    policy_loss        | -0.0123  |\n",
      "|    value_loss         | 0.0731   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7930000, episode_reward=223.80 +/- 65.36\n",
      "Episode length: 6659.20 +/- 1014.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.66e+03 |\n",
      "|    mean_reward        | 224      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7930000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.862   |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99124    |\n",
      "|    policy_loss        | 0.0088   |\n",
      "|    value_loss         | 0.0241   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.41e+03 |\n",
      "|    ep_rew_mean        | 277      |\n",
      "| time/                 |          |\n",
      "|    fps                | 605      |\n",
      "|    iterations         | 99200    |\n",
      "|    time_elapsed       | 13115    |\n",
      "|    total_timesteps    | 7936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.998   |\n",
      "|    explained_variance | 0.889    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99199    |\n",
      "|    policy_loss        | 0.024    |\n",
      "|    value_loss         | 0.0364   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7940000, episode_reward=259.00 +/- 58.80\n",
      "Episode length: 7241.40 +/- 728.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.24e+03 |\n",
      "|    mean_reward        | 259      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.829   |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99249    |\n",
      "|    policy_loss        | 0.105    |\n",
      "|    value_loss         | 0.146    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.39e+03 |\n",
      "|    ep_rew_mean        | 277      |\n",
      "| time/                 |          |\n",
      "|    fps                | 604      |\n",
      "|    iterations         | 99300    |\n",
      "|    time_elapsed       | 13136    |\n",
      "|    total_timesteps    | 7944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.84    |\n",
      "|    explained_variance | 0.537    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99299    |\n",
      "|    policy_loss        | -0.168   |\n",
      "|    value_loss         | 1.03     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7950000, episode_reward=282.40 +/- 132.76\n",
      "Episode length: 6299.40 +/- 1805.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.3e+03  |\n",
      "|    mean_reward        | 282      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7950000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.858   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99374    |\n",
      "|    policy_loss        | -0.00384 |\n",
      "|    value_loss         | 0.0648   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.38e+03 |\n",
      "|    ep_rew_mean        | 276      |\n",
      "| time/                 |          |\n",
      "|    fps                | 604      |\n",
      "|    iterations         | 99400    |\n",
      "|    time_elapsed       | 13155    |\n",
      "|    total_timesteps    | 7952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.794   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99399    |\n",
      "|    policy_loss        | 0.0296   |\n",
      "|    value_loss         | 0.0821   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7960000, episode_reward=263.00 +/- 46.31\n",
      "Episode length: 6350.60 +/- 780.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.35e+03 |\n",
      "|    mean_reward        | 263      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.87    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99499    |\n",
      "|    policy_loss        | -0.0949  |\n",
      "|    value_loss         | 0.0494   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.38e+03 |\n",
      "|    ep_rew_mean     | 276      |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 99500    |\n",
      "|    time_elapsed    | 13174    |\n",
      "|    total_timesteps | 7960000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.36e+03 |\n",
      "|    ep_rew_mean        | 275      |\n",
      "| time/                 |          |\n",
      "|    fps                | 604      |\n",
      "|    iterations         | 99600    |\n",
      "|    time_elapsed       | 13181    |\n",
      "|    total_timesteps    | 7968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.826   |\n",
      "|    explained_variance | 0.898    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99599    |\n",
      "|    policy_loss        | -0.0197  |\n",
      "|    value_loss         | 0.0302   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7970000, episode_reward=268.00 +/- 77.74\n",
      "Episode length: 7205.60 +/- 999.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.21e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7970000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.821   |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99624    |\n",
      "|    policy_loss        | 0.0804   |\n",
      "|    value_loss         | 0.0595   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.33e+03 |\n",
      "|    ep_rew_mean        | 274      |\n",
      "| time/                 |          |\n",
      "|    fps                | 604      |\n",
      "|    iterations         | 99700    |\n",
      "|    time_elapsed       | 13201    |\n",
      "|    total_timesteps    | 7976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.946   |\n",
      "|    explained_variance | 0.898    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99699    |\n",
      "|    policy_loss        | 0.0562   |\n",
      "|    value_loss         | 0.111    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7980000, episode_reward=198.80 +/- 82.50\n",
      "Episode length: 6549.80 +/- 572.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.55e+03 |\n",
      "|    mean_reward        | 199      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.875   |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99749    |\n",
      "|    policy_loss        | 0.000214 |\n",
      "|    value_loss         | 0.0471   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.28e+03 |\n",
      "|    ep_rew_mean        | 272      |\n",
      "| time/                 |          |\n",
      "|    fps                | 603      |\n",
      "|    iterations         | 99800    |\n",
      "|    time_elapsed       | 13221    |\n",
      "|    total_timesteps    | 7984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.766   |\n",
      "|    explained_variance | 0.793    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99799    |\n",
      "|    policy_loss        | -0.132   |\n",
      "|    value_loss         | 0.276    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7990000, episode_reward=255.80 +/- 83.99\n",
      "Episode length: 6924.20 +/- 811.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.92e+03 |\n",
      "|    mean_reward        | 256      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7990000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.752   |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99874    |\n",
      "|    policy_loss        | 0.128    |\n",
      "|    value_loss         | 0.389    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.28e+03 |\n",
      "|    ep_rew_mean        | 273      |\n",
      "| time/                 |          |\n",
      "|    fps                | 603      |\n",
      "|    iterations         | 99900    |\n",
      "|    time_elapsed       | 13241    |\n",
      "|    total_timesteps    | 7992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.734   |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99899    |\n",
      "|    policy_loss        | 0.0147   |\n",
      "|    value_loss         | 0.153    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8000000, episode_reward=350.20 +/- 12.72\n",
      "Episode length: 7959.20 +/- 988.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.96e+03 |\n",
      "|    mean_reward        | 350      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.883   |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99999    |\n",
      "|    policy_loss        | -0.14    |\n",
      "|    value_loss         | 0.198    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.24e+03 |\n",
      "|    ep_rew_mean     | 275      |\n",
      "| time/              |          |\n",
      "|    fps             | 603      |\n",
      "|    iterations      | 100000   |\n",
      "|    time_elapsed    | 13264    |\n",
      "|    total_timesteps | 8000000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.21e+03 |\n",
      "|    ep_rew_mean        | 277      |\n",
      "| time/                 |          |\n",
      "|    fps                | 603      |\n",
      "|    iterations         | 100100   |\n",
      "|    time_elapsed       | 13271    |\n",
      "|    total_timesteps    | 8008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.875   |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100099   |\n",
      "|    policy_loss        | -0.0105  |\n",
      "|    value_loss         | 0.0303   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8010000, episode_reward=359.80 +/- 18.88\n",
      "Episode length: 8508.80 +/- 2652.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.51e+03 |\n",
      "|    mean_reward        | 360      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8010000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.892   |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100124   |\n",
      "|    policy_loss        | -0.00484 |\n",
      "|    value_loss         | 0.0427   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.2e+03  |\n",
      "|    ep_rew_mean        | 275      |\n",
      "| time/                 |          |\n",
      "|    fps                | 602      |\n",
      "|    iterations         | 100200   |\n",
      "|    time_elapsed       | 13294    |\n",
      "|    total_timesteps    | 8016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.789   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100199   |\n",
      "|    policy_loss        | -0.0918  |\n",
      "|    value_loss         | 0.184    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8020000, episode_reward=306.20 +/- 108.52\n",
      "Episode length: 8044.60 +/- 1112.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.04e+03 |\n",
      "|    mean_reward        | 306      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.857   |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100249   |\n",
      "|    policy_loss        | 0.0725   |\n",
      "|    value_loss         | 0.062    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.22e+03 |\n",
      "|    ep_rew_mean        | 276      |\n",
      "| time/                 |          |\n",
      "|    fps                | 602      |\n",
      "|    iterations         | 100300   |\n",
      "|    time_elapsed       | 13316    |\n",
      "|    total_timesteps    | 8024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.706   |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100299   |\n",
      "|    policy_loss        | -0.019   |\n",
      "|    value_loss         | 0.0348   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8030000, episode_reward=246.20 +/- 101.15\n",
      "Episode length: 7027.40 +/- 839.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.03e+03 |\n",
      "|    mean_reward        | 246      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8030000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.855   |\n",
      "|    explained_variance | 0.84     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100374   |\n",
      "|    policy_loss        | 0.0152   |\n",
      "|    value_loss         | 0.0767   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.3e+03  |\n",
      "|    ep_rew_mean        | 282      |\n",
      "| time/                 |          |\n",
      "|    fps                | 602      |\n",
      "|    iterations         | 100400   |\n",
      "|    time_elapsed       | 13336    |\n",
      "|    total_timesteps    | 8032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.937   |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100399   |\n",
      "|    policy_loss        | 0.0406   |\n",
      "|    value_loss         | 0.0433   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8040000, episode_reward=350.60 +/- 26.54\n",
      "Episode length: 8265.00 +/- 785.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.26e+03 |\n",
      "|    mean_reward        | 351      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.811   |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100499   |\n",
      "|    policy_loss        | 0.0842   |\n",
      "|    value_loss         | 0.182    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.32e+03 |\n",
      "|    ep_rew_mean     | 280      |\n",
      "| time/              |          |\n",
      "|    fps             | 601      |\n",
      "|    iterations      | 100500   |\n",
      "|    time_elapsed    | 13359    |\n",
      "|    total_timesteps | 8040000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.3e+03  |\n",
      "|    ep_rew_mean        | 279      |\n",
      "| time/                 |          |\n",
      "|    fps                | 602      |\n",
      "|    iterations         | 100600   |\n",
      "|    time_elapsed       | 13366    |\n",
      "|    total_timesteps    | 8048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.836   |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100599   |\n",
      "|    policy_loss        | 0.0511   |\n",
      "|    value_loss         | 0.102    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8050000, episode_reward=219.00 +/- 128.56\n",
      "Episode length: 6885.80 +/- 1609.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.89e+03 |\n",
      "|    mean_reward        | 219      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.714   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100624   |\n",
      "|    policy_loss        | 0.0254   |\n",
      "|    value_loss         | 0.0824   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.32e+03 |\n",
      "|    ep_rew_mean        | 282      |\n",
      "| time/                 |          |\n",
      "|    fps                | 601      |\n",
      "|    iterations         | 100700   |\n",
      "|    time_elapsed       | 13386    |\n",
      "|    total_timesteps    | 8056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.928   |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100699   |\n",
      "|    policy_loss        | 0.0443   |\n",
      "|    value_loss         | 0.0436   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8060000, episode_reward=358.20 +/- 50.78\n",
      "Episode length: 8885.60 +/- 1204.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.89e+03 |\n",
      "|    mean_reward        | 358      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100749   |\n",
      "|    policy_loss        | 0.0243   |\n",
      "|    value_loss         | 0.0348   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.33e+03 |\n",
      "|    ep_rew_mean        | 283      |\n",
      "| time/                 |          |\n",
      "|    fps                | 601      |\n",
      "|    iterations         | 100800   |\n",
      "|    time_elapsed       | 13410    |\n",
      "|    total_timesteps    | 8064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.848   |\n",
      "|    explained_variance | 0.9      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100799   |\n",
      "|    policy_loss        | 0.0392   |\n",
      "|    value_loss         | 0.0624   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8070000, episode_reward=237.80 +/- 130.46\n",
      "Episode length: 6703.80 +/- 1782.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.7e+03  |\n",
      "|    mean_reward        | 238      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8070000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.868   |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100874   |\n",
      "|    policy_loss        | 0.0532   |\n",
      "|    value_loss         | 0.0579   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.33e+03 |\n",
      "|    ep_rew_mean        | 283      |\n",
      "| time/                 |          |\n",
      "|    fps                | 601      |\n",
      "|    iterations         | 100900   |\n",
      "|    time_elapsed       | 13430    |\n",
      "|    total_timesteps    | 8072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.864   |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100899   |\n",
      "|    policy_loss        | -0.0382  |\n",
      "|    value_loss         | 0.04     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8080000, episode_reward=314.40 +/- 76.36\n",
      "Episode length: 7241.20 +/- 370.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.24e+03 |\n",
      "|    mean_reward        | 314      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.815   |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100999   |\n",
      "|    policy_loss        | -0.102   |\n",
      "|    value_loss         | 0.138    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.41e+03 |\n",
      "|    ep_rew_mean     | 278      |\n",
      "| time/              |          |\n",
      "|    fps             | 600      |\n",
      "|    iterations      | 101000   |\n",
      "|    time_elapsed    | 13451    |\n",
      "|    total_timesteps | 8080000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.44e+03 |\n",
      "|    ep_rew_mean        | 283      |\n",
      "| time/                 |          |\n",
      "|    fps                | 600      |\n",
      "|    iterations         | 101100   |\n",
      "|    time_elapsed       | 13457    |\n",
      "|    total_timesteps    | 8088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.755   |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101099   |\n",
      "|    policy_loss        | -0.0288  |\n",
      "|    value_loss         | 0.0348   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8090000, episode_reward=262.60 +/- 50.18\n",
      "Episode length: 7189.00 +/- 962.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.19e+03 |\n",
      "|    mean_reward        | 263      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8090000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.745   |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101124   |\n",
      "|    policy_loss        | -0.0709  |\n",
      "|    value_loss         | 0.361    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.48e+03 |\n",
      "|    ep_rew_mean        | 286      |\n",
      "| time/                 |          |\n",
      "|    fps                | 600      |\n",
      "|    iterations         | 101200   |\n",
      "|    time_elapsed       | 13478    |\n",
      "|    total_timesteps    | 8096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.764   |\n",
      "|    explained_variance | 0.681    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101199   |\n",
      "|    policy_loss        | -0.128   |\n",
      "|    value_loss         | 0.97     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8100000, episode_reward=292.60 +/- 78.44\n",
      "Episode length: 7743.00 +/- 1169.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.74e+03 |\n",
      "|    mean_reward        | 293      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.876   |\n",
      "|    explained_variance | 0.829    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101249   |\n",
      "|    policy_loss        | -0.0487  |\n",
      "|    value_loss         | 0.288    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.47e+03 |\n",
      "|    ep_rew_mean        | 284      |\n",
      "| time/                 |          |\n",
      "|    fps                | 600      |\n",
      "|    iterations         | 101300   |\n",
      "|    time_elapsed       | 13500    |\n",
      "|    total_timesteps    | 8104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.727   |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101299   |\n",
      "|    policy_loss        | -0.091   |\n",
      "|    value_loss         | 0.277    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8110000, episode_reward=318.60 +/- 45.61\n",
      "Episode length: 8524.00 +/- 457.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.52e+03 |\n",
      "|    mean_reward        | 319      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8110000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.785   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101374   |\n",
      "|    policy_loss        | 0.0121   |\n",
      "|    value_loss         | 0.0566   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.47e+03 |\n",
      "|    ep_rew_mean        | 285      |\n",
      "| time/                 |          |\n",
      "|    fps                | 599      |\n",
      "|    iterations         | 101400   |\n",
      "|    time_elapsed       | 13523    |\n",
      "|    total_timesteps    | 8112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.86    |\n",
      "|    explained_variance | 0.738    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101399   |\n",
      "|    policy_loss        | -0.131   |\n",
      "|    value_loss         | 0.649    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8120000, episode_reward=304.20 +/- 39.12\n",
      "Episode length: 7167.40 +/- 961.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.17e+03 |\n",
      "|    mean_reward        | 304      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.869   |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101499   |\n",
      "|    policy_loss        | -0.0358  |\n",
      "|    value_loss         | 0.106    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.46e+03 |\n",
      "|    ep_rew_mean     | 288      |\n",
      "| time/              |          |\n",
      "|    fps             | 599      |\n",
      "|    iterations      | 101500   |\n",
      "|    time_elapsed    | 13543    |\n",
      "|    total_timesteps | 8120000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.47e+03 |\n",
      "|    ep_rew_mean        | 292      |\n",
      "| time/                 |          |\n",
      "|    fps                | 599      |\n",
      "|    iterations         | 101600   |\n",
      "|    time_elapsed       | 13550    |\n",
      "|    total_timesteps    | 8128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.824   |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101599   |\n",
      "|    policy_loss        | -0.00646 |\n",
      "|    value_loss         | 0.0673   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8130000, episode_reward=293.40 +/- 84.81\n",
      "Episode length: 7433.80 +/- 1115.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.43e+03 |\n",
      "|    mean_reward        | 293      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8130000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.897   |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101624   |\n",
      "|    policy_loss        | 0.103    |\n",
      "|    value_loss         | 0.0343   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.47e+03 |\n",
      "|    ep_rew_mean        | 292      |\n",
      "| time/                 |          |\n",
      "|    fps                | 599      |\n",
      "|    iterations         | 101700   |\n",
      "|    time_elapsed       | 13572    |\n",
      "|    total_timesteps    | 8136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.977   |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101699   |\n",
      "|    policy_loss        | 0.0164   |\n",
      "|    value_loss         | 0.0394   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8140000, episode_reward=298.00 +/- 106.38\n",
      "Episode length: 8608.40 +/- 1658.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.61e+03 |\n",
      "|    mean_reward        | 298      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.93    |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101749   |\n",
      "|    policy_loss        | 0.0666   |\n",
      "|    value_loss         | 0.136    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.43e+03 |\n",
      "|    ep_rew_mean        | 291      |\n",
      "| time/                 |          |\n",
      "|    fps                | 599      |\n",
      "|    iterations         | 101800   |\n",
      "|    time_elapsed       | 13595    |\n",
      "|    total_timesteps    | 8144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.777   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101799   |\n",
      "|    policy_loss        | 0.033    |\n",
      "|    value_loss         | 0.0261   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8150000, episode_reward=255.20 +/- 118.12\n",
      "Episode length: 7183.60 +/- 1531.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.18e+03 |\n",
      "|    mean_reward        | 255      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.923   |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101874   |\n",
      "|    policy_loss        | 0.00766  |\n",
      "|    value_loss         | 0.0514   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.48e+03 |\n",
      "|    ep_rew_mean        | 295      |\n",
      "| time/                 |          |\n",
      "|    fps                | 598      |\n",
      "|    iterations         | 101900   |\n",
      "|    time_elapsed       | 13616    |\n",
      "|    total_timesteps    | 8152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.92    |\n",
      "|    explained_variance | 0.889    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101899   |\n",
      "|    policy_loss        | -0.127   |\n",
      "|    value_loss         | 0.0932   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8160000, episode_reward=273.20 +/- 143.35\n",
      "Episode length: 7479.00 +/- 1421.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.48e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.815   |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101999   |\n",
      "|    policy_loss        | 0.0556   |\n",
      "|    value_loss         | 0.1      |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.53e+03 |\n",
      "|    ep_rew_mean     | 298      |\n",
      "| time/              |          |\n",
      "|    fps             | 598      |\n",
      "|    iterations      | 102000   |\n",
      "|    time_elapsed    | 13638    |\n",
      "|    total_timesteps | 8160000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.51e+03 |\n",
      "|    ep_rew_mean        | 296      |\n",
      "| time/                 |          |\n",
      "|    fps                | 598      |\n",
      "|    iterations         | 102100   |\n",
      "|    time_elapsed       | 13645    |\n",
      "|    total_timesteps    | 8168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.921   |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102099   |\n",
      "|    policy_loss        | 0.0694   |\n",
      "|    value_loss         | 0.0671   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8170000, episode_reward=345.20 +/- 121.78\n",
      "Episode length: 8588.00 +/- 1742.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.59e+03 |\n",
      "|    mean_reward        | 345      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8170000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.828   |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102124   |\n",
      "|    policy_loss        | 0.249    |\n",
      "|    value_loss         | 0.343    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.58e+03 |\n",
      "|    ep_rew_mean        | 302      |\n",
      "| time/                 |          |\n",
      "|    fps                | 598      |\n",
      "|    iterations         | 102200   |\n",
      "|    time_elapsed       | 13668    |\n",
      "|    total_timesteps    | 8176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.849   |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102199   |\n",
      "|    policy_loss        | 0.0138   |\n",
      "|    value_loss         | 0.0658   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8180000, episode_reward=392.40 +/- 21.56\n",
      "Episode length: 8686.60 +/- 913.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.69e+03 |\n",
      "|    mean_reward        | 392      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.866   |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102249   |\n",
      "|    policy_loss        | 0.0318   |\n",
      "|    value_loss         | 0.195    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.59e+03 |\n",
      "|    ep_rew_mean        | 303      |\n",
      "| time/                 |          |\n",
      "|    fps                | 597      |\n",
      "|    iterations         | 102300   |\n",
      "|    time_elapsed       | 13691    |\n",
      "|    total_timesteps    | 8184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.88    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102299   |\n",
      "|    policy_loss        | -0.0314  |\n",
      "|    value_loss         | 0.0437   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8190000, episode_reward=351.60 +/- 38.04\n",
      "Episode length: 8621.00 +/- 726.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.62e+03 |\n",
      "|    mean_reward        | 352      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8190000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.764   |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102374   |\n",
      "|    policy_loss        | 0.102    |\n",
      "|    value_loss         | 0.122    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.67e+03 |\n",
      "|    ep_rew_mean        | 303      |\n",
      "| time/                 |          |\n",
      "|    fps                | 597      |\n",
      "|    iterations         | 102400   |\n",
      "|    time_elapsed       | 13714    |\n",
      "|    total_timesteps    | 8192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.828   |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102399   |\n",
      "|    policy_loss        | -0.0364  |\n",
      "|    value_loss         | 0.0352   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8200000, episode_reward=241.00 +/- 129.43\n",
      "Episode length: 6128.20 +/- 1176.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.13e+03 |\n",
      "|    mean_reward        | 241      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.801   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102499   |\n",
      "|    policy_loss        | -0.00844 |\n",
      "|    value_loss         | 0.12     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.73e+03 |\n",
      "|    ep_rew_mean     | 302      |\n",
      "| time/              |          |\n",
      "|    fps             | 597      |\n",
      "|    iterations      | 102500   |\n",
      "|    time_elapsed    | 13733    |\n",
      "|    total_timesteps | 8200000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.75e+03 |\n",
      "|    ep_rew_mean        | 304      |\n",
      "| time/                 |          |\n",
      "|    fps                | 597      |\n",
      "|    iterations         | 102600   |\n",
      "|    time_elapsed       | 13740    |\n",
      "|    total_timesteps    | 8208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.862   |\n",
      "|    explained_variance | 0.816    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102599   |\n",
      "|    policy_loss        | 0.0254   |\n",
      "|    value_loss         | 0.275    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8210000, episode_reward=276.00 +/- 103.78\n",
      "Episode length: 7470.40 +/- 813.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.47e+03 |\n",
      "|    mean_reward        | 276      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8210000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.927   |\n",
      "|    explained_variance | 0.681    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102624   |\n",
      "|    policy_loss        | -0.145   |\n",
      "|    value_loss         | 0.655    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.76e+03 |\n",
      "|    ep_rew_mean        | 305      |\n",
      "| time/                 |          |\n",
      "|    fps                | 597      |\n",
      "|    iterations         | 102700   |\n",
      "|    time_elapsed       | 13761    |\n",
      "|    total_timesteps    | 8216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.944   |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102699   |\n",
      "|    policy_loss        | 0.0306   |\n",
      "|    value_loss         | 0.024    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8220000, episode_reward=261.60 +/- 104.62\n",
      "Episode length: 7178.80 +/- 1926.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.18e+03 |\n",
      "|    mean_reward        | 262      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.905   |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102749   |\n",
      "|    policy_loss        | -0.0197  |\n",
      "|    value_loss         | 0.0502   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.72e+03 |\n",
      "|    ep_rew_mean        | 305      |\n",
      "| time/                 |          |\n",
      "|    fps                | 596      |\n",
      "|    iterations         | 102800   |\n",
      "|    time_elapsed       | 13782    |\n",
      "|    total_timesteps    | 8224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.867   |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102799   |\n",
      "|    policy_loss        | -0.0436  |\n",
      "|    value_loss         | 0.166    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8230000, episode_reward=361.00 +/- 16.40\n",
      "Episode length: 7383.40 +/- 1256.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.38e+03 |\n",
      "|    mean_reward        | 361      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8230000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.885   |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102874   |\n",
      "|    policy_loss        | -0.219   |\n",
      "|    value_loss         | 0.378    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.74e+03 |\n",
      "|    ep_rew_mean        | 305      |\n",
      "| time/                 |          |\n",
      "|    fps                | 596      |\n",
      "|    iterations         | 102900   |\n",
      "|    time_elapsed       | 13803    |\n",
      "|    total_timesteps    | 8232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.981   |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102899   |\n",
      "|    policy_loss        | -0.0134  |\n",
      "|    value_loss         | 0.184    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8240000, episode_reward=290.40 +/- 79.11\n",
      "Episode length: 7186.80 +/- 1004.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.19e+03 |\n",
      "|    mean_reward        | 290      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.85    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102999   |\n",
      "|    policy_loss        | -0.0566  |\n",
      "|    value_loss         | 0.144    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.75e+03 |\n",
      "|    ep_rew_mean     | 307      |\n",
      "| time/              |          |\n",
      "|    fps             | 596      |\n",
      "|    iterations      | 103000   |\n",
      "|    time_elapsed    | 13823    |\n",
      "|    total_timesteps | 8240000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.79e+03 |\n",
      "|    ep_rew_mean        | 310      |\n",
      "| time/                 |          |\n",
      "|    fps                | 596      |\n",
      "|    iterations         | 103100   |\n",
      "|    time_elapsed       | 13830    |\n",
      "|    total_timesteps    | 8248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.973   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103099   |\n",
      "|    policy_loss        | 0.0483   |\n",
      "|    value_loss         | 0.0567   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8250000, episode_reward=353.40 +/- 43.88\n",
      "Episode length: 8414.00 +/- 935.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.41e+03 |\n",
      "|    mean_reward        | 353      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.798   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103124   |\n",
      "|    policy_loss        | 0.0186   |\n",
      "|    value_loss         | 0.0308   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.76e+03 |\n",
      "|    ep_rew_mean        | 309      |\n",
      "| time/                 |          |\n",
      "|    fps                | 595      |\n",
      "|    iterations         | 103200   |\n",
      "|    time_elapsed       | 13853    |\n",
      "|    total_timesteps    | 8256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.78    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103199   |\n",
      "|    policy_loss        | 0.0276   |\n",
      "|    value_loss         | 0.128    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8260000, episode_reward=361.00 +/- 30.21\n",
      "Episode length: 7994.60 +/- 640.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.99e+03 |\n",
      "|    mean_reward        | 361      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.79    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103249   |\n",
      "|    policy_loss        | 0.0778   |\n",
      "|    value_loss         | 0.0861   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.78e+03 |\n",
      "|    ep_rew_mean        | 312      |\n",
      "| time/                 |          |\n",
      "|    fps                | 595      |\n",
      "|    iterations         | 103300   |\n",
      "|    time_elapsed       | 13875    |\n",
      "|    total_timesteps    | 8264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.894   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103299   |\n",
      "|    policy_loss        | -0.0562  |\n",
      "|    value_loss         | 0.029    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8270000, episode_reward=358.60 +/- 49.33\n",
      "Episode length: 8630.80 +/- 978.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.63e+03 |\n",
      "|    mean_reward        | 359      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8270000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.817   |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103374   |\n",
      "|    policy_loss        | 0.0312   |\n",
      "|    value_loss         | 0.0676   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.71e+03 |\n",
      "|    ep_rew_mean        | 317      |\n",
      "| time/                 |          |\n",
      "|    fps                | 595      |\n",
      "|    iterations         | 103400   |\n",
      "|    time_elapsed       | 13899    |\n",
      "|    total_timesteps    | 8272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.884   |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103399   |\n",
      "|    policy_loss        | 0.0152   |\n",
      "|    value_loss         | 0.0639   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8280000, episode_reward=272.20 +/- 62.36\n",
      "Episode length: 7474.40 +/- 1035.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.47e+03 |\n",
      "|    mean_reward        | 272      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.894   |\n",
      "|    explained_variance | 0.529    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103499   |\n",
      "|    policy_loss        | -0.505   |\n",
      "|    value_loss         | 0.933    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.74e+03 |\n",
      "|    ep_rew_mean     | 319      |\n",
      "| time/              |          |\n",
      "|    fps             | 594      |\n",
      "|    iterations      | 103500   |\n",
      "|    time_elapsed    | 13920    |\n",
      "|    total_timesteps | 8280000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.71e+03 |\n",
      "|    ep_rew_mean        | 319      |\n",
      "| time/                 |          |\n",
      "|    fps                | 595      |\n",
      "|    iterations         | 103600   |\n",
      "|    time_elapsed       | 13927    |\n",
      "|    total_timesteps    | 8288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.891   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103599   |\n",
      "|    policy_loss        | 0.0437   |\n",
      "|    value_loss         | 0.0451   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8290000, episode_reward=325.80 +/- 42.61\n",
      "Episode length: 8243.40 +/- 1953.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.24e+03 |\n",
      "|    mean_reward        | 326      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8290000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.961   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103624   |\n",
      "|    policy_loss        | 0.0789   |\n",
      "|    value_loss         | 0.0891   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.77e+03 |\n",
      "|    ep_rew_mean        | 323      |\n",
      "| time/                 |          |\n",
      "|    fps                | 594      |\n",
      "|    iterations         | 103700   |\n",
      "|    time_elapsed       | 13950    |\n",
      "|    total_timesteps    | 8296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.887   |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103699   |\n",
      "|    policy_loss        | 0.0408   |\n",
      "|    value_loss         | 0.0994   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8300000, episode_reward=364.20 +/- 43.92\n",
      "Episode length: 8624.20 +/- 945.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.62e+03 |\n",
      "|    mean_reward        | 364      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.71    |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103749   |\n",
      "|    policy_loss        | -0.036   |\n",
      "|    value_loss         | 0.0528   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.77e+03 |\n",
      "|    ep_rew_mean        | 321      |\n",
      "| time/                 |          |\n",
      "|    fps                | 594      |\n",
      "|    iterations         | 103800   |\n",
      "|    time_elapsed       | 13974    |\n",
      "|    total_timesteps    | 8304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.954   |\n",
      "|    explained_variance | 0.543    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103799   |\n",
      "|    policy_loss        | -0.38    |\n",
      "|    value_loss         | 1.08     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8310000, episode_reward=359.60 +/- 35.42\n",
      "Episode length: 8497.80 +/- 668.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.5e+03  |\n",
      "|    mean_reward        | 360      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8310000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.811   |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103874   |\n",
      "|    policy_loss        | -0.024   |\n",
      "|    value_loss         | 0.0568   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.77e+03 |\n",
      "|    ep_rew_mean        | 319      |\n",
      "| time/                 |          |\n",
      "|    fps                | 593      |\n",
      "|    iterations         | 103900   |\n",
      "|    time_elapsed       | 13997    |\n",
      "|    total_timesteps    | 8312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.84    |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103899   |\n",
      "|    policy_loss        | -0.0229  |\n",
      "|    value_loss         | 0.0387   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8320000, episode_reward=333.40 +/- 99.00\n",
      "Episode length: 8935.20 +/- 1166.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.94e+03 |\n",
      "|    mean_reward        | 333      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.795   |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103999   |\n",
      "|    policy_loss        | -0.0108  |\n",
      "|    value_loss         | 0.105    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.81e+03 |\n",
      "|    ep_rew_mean     | 322      |\n",
      "| time/              |          |\n",
      "|    fps             | 593      |\n",
      "|    iterations      | 104000   |\n",
      "|    time_elapsed    | 14021    |\n",
      "|    total_timesteps | 8320000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.83e+03 |\n",
      "|    ep_rew_mean        | 321      |\n",
      "| time/                 |          |\n",
      "|    fps                | 593      |\n",
      "|    iterations         | 104100   |\n",
      "|    time_elapsed       | 14028    |\n",
      "|    total_timesteps    | 8328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.874   |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104099   |\n",
      "|    policy_loss        | 0.0064   |\n",
      "|    value_loss         | 0.0991   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8330000, episode_reward=342.00 +/- 51.90\n",
      "Episode length: 8889.20 +/- 1399.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.89e+03 |\n",
      "|    mean_reward        | 342      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8330000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.94    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104124   |\n",
      "|    policy_loss        | -0.0633  |\n",
      "|    value_loss         | 0.0605   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.89e+03 |\n",
      "|    ep_rew_mean        | 323      |\n",
      "| time/                 |          |\n",
      "|    fps                | 593      |\n",
      "|    iterations         | 104200   |\n",
      "|    time_elapsed       | 14052    |\n",
      "|    total_timesteps    | 8336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.847   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104199   |\n",
      "|    policy_loss        | -0.0327  |\n",
      "|    value_loss         | 0.0271   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8340000, episode_reward=347.80 +/- 53.24\n",
      "Episode length: 8186.60 +/- 1267.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.19e+03 |\n",
      "|    mean_reward        | 348      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.836   |\n",
      "|    explained_variance | 0.648    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104249   |\n",
      "|    policy_loss        | 0.0182   |\n",
      "|    value_loss         | 0.493    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.9e+03  |\n",
      "|    ep_rew_mean        | 325      |\n",
      "| time/                 |          |\n",
      "|    fps                | 592      |\n",
      "|    iterations         | 104300   |\n",
      "|    time_elapsed       | 14074    |\n",
      "|    total_timesteps    | 8344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.921   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104299   |\n",
      "|    policy_loss        | -0.0959  |\n",
      "|    value_loss         | 0.139    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8350000, episode_reward=386.40 +/- 35.40\n",
      "Episode length: 8223.20 +/- 599.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.22e+03 |\n",
      "|    mean_reward        | 386      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.894   |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104374   |\n",
      "|    policy_loss        | 0.113    |\n",
      "|    value_loss         | 0.147    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.94e+03 |\n",
      "|    ep_rew_mean        | 327      |\n",
      "| time/                 |          |\n",
      "|    fps                | 592      |\n",
      "|    iterations         | 104400   |\n",
      "|    time_elapsed       | 14097    |\n",
      "|    total_timesteps    | 8352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104399   |\n",
      "|    policy_loss        | -0.0382  |\n",
      "|    value_loss         | 0.0805   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8360000, episode_reward=302.20 +/- 65.86\n",
      "Episode length: 6582.40 +/- 1608.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.58e+03 |\n",
      "|    mean_reward        | 302      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.828   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104499   |\n",
      "|    policy_loss        | 0.061    |\n",
      "|    value_loss         | 0.0427   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.98e+03 |\n",
      "|    ep_rew_mean     | 334      |\n",
      "| time/              |          |\n",
      "|    fps             | 592      |\n",
      "|    iterations      | 104500   |\n",
      "|    time_elapsed    | 14117    |\n",
      "|    total_timesteps | 8360000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.98e+03 |\n",
      "|    ep_rew_mean        | 331      |\n",
      "| time/                 |          |\n",
      "|    fps                | 592      |\n",
      "|    iterations         | 104600   |\n",
      "|    time_elapsed       | 14124    |\n",
      "|    total_timesteps    | 8368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.898   |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104599   |\n",
      "|    policy_loss        | -0.108   |\n",
      "|    value_loss         | 0.0985   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8370000, episode_reward=393.80 +/- 20.14\n",
      "Episode length: 9608.80 +/- 1275.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.61e+03 |\n",
      "|    mean_reward        | 394      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8370000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.913   |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104624   |\n",
      "|    policy_loss        | 0.0989   |\n",
      "|    value_loss         | 0.107    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.97e+03 |\n",
      "|    ep_rew_mean        | 330      |\n",
      "| time/                 |          |\n",
      "|    fps                | 591      |\n",
      "|    iterations         | 104700   |\n",
      "|    time_elapsed       | 14149    |\n",
      "|    total_timesteps    | 8376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.83    |\n",
      "|    explained_variance | 0.817    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104699   |\n",
      "|    policy_loss        | -0.0914  |\n",
      "|    value_loss         | 0.422    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8380000, episode_reward=343.40 +/- 101.58\n",
      "Episode length: 8032.80 +/- 1260.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.03e+03 |\n",
      "|    mean_reward        | 343      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.846   |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104749   |\n",
      "|    policy_loss        | 0.0397   |\n",
      "|    value_loss         | 0.0676   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.91e+03 |\n",
      "|    ep_rew_mean        | 327      |\n",
      "| time/                 |          |\n",
      "|    fps                | 591      |\n",
      "|    iterations         | 104800   |\n",
      "|    time_elapsed       | 14171    |\n",
      "|    total_timesteps    | 8384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.823   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104799   |\n",
      "|    policy_loss        | -0.0502  |\n",
      "|    value_loss         | 0.086    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8390000, episode_reward=326.60 +/- 32.85\n",
      "Episode length: 7698.40 +/- 379.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.7e+03  |\n",
      "|    mean_reward        | 327      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8390000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.802   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104874   |\n",
      "|    policy_loss        | 0.0206   |\n",
      "|    value_loss         | 0.0601   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.9e+03  |\n",
      "|    ep_rew_mean        | 328      |\n",
      "| time/                 |          |\n",
      "|    fps                | 591      |\n",
      "|    iterations         | 104900   |\n",
      "|    time_elapsed       | 14193    |\n",
      "|    total_timesteps    | 8392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.822   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104899   |\n",
      "|    policy_loss        | 0.0277   |\n",
      "|    value_loss         | 0.0753   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8400000, episode_reward=369.60 +/- 33.47\n",
      "Episode length: 8919.60 +/- 674.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.92e+03 |\n",
      "|    mean_reward        | 370      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.876   |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104999   |\n",
      "|    policy_loss        | 0.059    |\n",
      "|    value_loss         | 0.0763   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.87e+03 |\n",
      "|    ep_rew_mean     | 331      |\n",
      "| time/              |          |\n",
      "|    fps             | 590      |\n",
      "|    iterations      | 105000   |\n",
      "|    time_elapsed    | 14217    |\n",
      "|    total_timesteps | 8400000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.91e+03 |\n",
      "|    ep_rew_mean        | 333      |\n",
      "| time/                 |          |\n",
      "|    fps                | 591      |\n",
      "|    iterations         | 105100   |\n",
      "|    time_elapsed       | 14224    |\n",
      "|    total_timesteps    | 8408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.882   |\n",
      "|    explained_variance | 0.692    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105099   |\n",
      "|    policy_loss        | -0.256   |\n",
      "|    value_loss         | 0.455    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8410000, episode_reward=339.20 +/- 92.71\n",
      "Episode length: 7471.80 +/- 1223.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.47e+03 |\n",
      "|    mean_reward        | 339      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8410000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.86    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105124   |\n",
      "|    policy_loss        | 0.0216   |\n",
      "|    value_loss         | 0.12     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.93e+03 |\n",
      "|    ep_rew_mean        | 334      |\n",
      "| time/                 |          |\n",
      "|    fps                | 590      |\n",
      "|    iterations         | 105200   |\n",
      "|    time_elapsed       | 14245    |\n",
      "|    total_timesteps    | 8416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.857   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105199   |\n",
      "|    policy_loss        | 0.0711   |\n",
      "|    value_loss         | 0.0712   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8420000, episode_reward=338.20 +/- 86.78\n",
      "Episode length: 8793.80 +/- 982.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.79e+03 |\n",
      "|    mean_reward        | 338      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.889   |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105249   |\n",
      "|    policy_loss        | -0.0798  |\n",
      "|    value_loss         | 0.338    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.95e+03 |\n",
      "|    ep_rew_mean        | 335      |\n",
      "| time/                 |          |\n",
      "|    fps                | 590      |\n",
      "|    iterations         | 105300   |\n",
      "|    time_elapsed       | 14269    |\n",
      "|    total_timesteps    | 8424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.815   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105299   |\n",
      "|    policy_loss        | -0.0149  |\n",
      "|    value_loss         | 0.0702   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8430000, episode_reward=365.20 +/- 55.08\n",
      "Episode length: 8652.00 +/- 1131.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.65e+03 |\n",
      "|    mean_reward        | 365      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8430000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.869   |\n",
      "|    explained_variance | 0.85     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105374   |\n",
      "|    policy_loss        | -0.185   |\n",
      "|    value_loss         | 0.472    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.97e+03 |\n",
      "|    ep_rew_mean        | 337      |\n",
      "| time/                 |          |\n",
      "|    fps                | 589      |\n",
      "|    iterations         | 105400   |\n",
      "|    time_elapsed       | 14292    |\n",
      "|    total_timesteps    | 8432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.986   |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105399   |\n",
      "|    policy_loss        | 0.0888   |\n",
      "|    value_loss         | 0.0995   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8440000, episode_reward=368.00 +/- 22.97\n",
      "Episode length: 8464.20 +/- 681.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.46e+03 |\n",
      "|    mean_reward        | 368      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.969   |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105499   |\n",
      "|    policy_loss        | -0.00151 |\n",
      "|    value_loss         | 0.126    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.97e+03 |\n",
      "|    ep_rew_mean     | 337      |\n",
      "| time/              |          |\n",
      "|    fps             | 589      |\n",
      "|    iterations      | 105500   |\n",
      "|    time_elapsed    | 14316    |\n",
      "|    total_timesteps | 8440000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8e+03    |\n",
      "|    ep_rew_mean        | 336      |\n",
      "| time/                 |          |\n",
      "|    fps                | 589      |\n",
      "|    iterations         | 105600   |\n",
      "|    time_elapsed       | 14323    |\n",
      "|    total_timesteps    | 8448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.874   |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105599   |\n",
      "|    policy_loss        | 0.044    |\n",
      "|    value_loss         | 0.0896   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8450000, episode_reward=321.60 +/- 121.00\n",
      "Episode length: 8212.00 +/- 2286.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.21e+03 |\n",
      "|    mean_reward        | 322      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.958   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105624   |\n",
      "|    policy_loss        | -0.0393  |\n",
      "|    value_loss         | 0.0285   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.01e+03 |\n",
      "|    ep_rew_mean        | 334      |\n",
      "| time/                 |          |\n",
      "|    fps                | 589      |\n",
      "|    iterations         | 105700   |\n",
      "|    time_elapsed       | 14346    |\n",
      "|    total_timesteps    | 8456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.847   |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105699   |\n",
      "|    policy_loss        | -0.217   |\n",
      "|    value_loss         | 0.34     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8460000, episode_reward=299.60 +/- 34.78\n",
      "Episode length: 7080.40 +/- 1591.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.08e+03 |\n",
      "|    mean_reward        | 300      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.925   |\n",
      "|    explained_variance | 0.833    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105749   |\n",
      "|    policy_loss        | 0.0164   |\n",
      "|    value_loss         | 0.296    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.05e+03 |\n",
      "|    ep_rew_mean        | 336      |\n",
      "| time/                 |          |\n",
      "|    fps                | 589      |\n",
      "|    iterations         | 105800   |\n",
      "|    time_elapsed       | 14366    |\n",
      "|    total_timesteps    | 8464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.935   |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105799   |\n",
      "|    policy_loss        | 0.0516   |\n",
      "|    value_loss         | 0.0748   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8470000, episode_reward=293.80 +/- 121.78\n",
      "Episode length: 7697.40 +/- 1680.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.7e+03  |\n",
      "|    mean_reward        | 294      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8470000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.996   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105874   |\n",
      "|    policy_loss        | -0.0044  |\n",
      "|    value_loss         | 0.0118   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.04e+03 |\n",
      "|    ep_rew_mean        | 335      |\n",
      "| time/                 |          |\n",
      "|    fps                | 588      |\n",
      "|    iterations         | 105900   |\n",
      "|    time_elapsed       | 14388    |\n",
      "|    total_timesteps    | 8472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.96    |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105899   |\n",
      "|    policy_loss        | 0.0734   |\n",
      "|    value_loss         | 0.233    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8480000, episode_reward=319.00 +/- 72.87\n",
      "Episode length: 7750.40 +/- 1063.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.75e+03 |\n",
      "|    mean_reward        | 319      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.803   |\n",
      "|    explained_variance | 0.768    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105999   |\n",
      "|    policy_loss        | -0.121   |\n",
      "|    value_loss         | 0.305    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.02e+03 |\n",
      "|    ep_rew_mean     | 333      |\n",
      "| time/              |          |\n",
      "|    fps             | 588      |\n",
      "|    iterations      | 106000   |\n",
      "|    time_elapsed    | 14410    |\n",
      "|    total_timesteps | 8480000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.98e+03 |\n",
      "|    ep_rew_mean        | 329      |\n",
      "| time/                 |          |\n",
      "|    fps                | 588      |\n",
      "|    iterations         | 106100   |\n",
      "|    time_elapsed       | 14417    |\n",
      "|    total_timesteps    | 8488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.798   |\n",
      "|    explained_variance | 0.808    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106099   |\n",
      "|    policy_loss        | 0.0216   |\n",
      "|    value_loss         | 1.18     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8490000, episode_reward=381.00 +/- 34.76\n",
      "Episode length: 9166.20 +/- 1470.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.17e+03 |\n",
      "|    mean_reward        | 381      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8490000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.874   |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106124   |\n",
      "|    policy_loss        | 0.116    |\n",
      "|    value_loss         | 0.33     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.02e+03 |\n",
      "|    ep_rew_mean        | 332      |\n",
      "| time/                 |          |\n",
      "|    fps                | 588      |\n",
      "|    iterations         | 106200   |\n",
      "|    time_elapsed       | 14442    |\n",
      "|    total_timesteps    | 8496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.917   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106199   |\n",
      "|    policy_loss        | -0.00503 |\n",
      "|    value_loss         | 0.0368   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8500000, episode_reward=363.80 +/- 69.15\n",
      "Episode length: 8639.60 +/- 615.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.64e+03 |\n",
      "|    mean_reward        | 364      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.847   |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106249   |\n",
      "|    policy_loss        | -0.0653  |\n",
      "|    value_loss         | 0.0744   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.04e+03 |\n",
      "|    ep_rew_mean        | 335      |\n",
      "| time/                 |          |\n",
      "|    fps                | 587      |\n",
      "|    iterations         | 106300   |\n",
      "|    time_elapsed       | 14465    |\n",
      "|    total_timesteps    | 8504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.845   |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106299   |\n",
      "|    policy_loss        | 0.0285   |\n",
      "|    value_loss         | 0.0825   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8510000, episode_reward=320.40 +/- 132.32\n",
      "Episode length: 8161.60 +/- 1419.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.16e+03 |\n",
      "|    mean_reward        | 320      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8510000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.944   |\n",
      "|    explained_variance | 0.541    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106374   |\n",
      "|    policy_loss        | -0.384   |\n",
      "|    value_loss         | 0.799    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.02e+03 |\n",
      "|    ep_rew_mean        | 332      |\n",
      "| time/                 |          |\n",
      "|    fps                | 587      |\n",
      "|    iterations         | 106400   |\n",
      "|    time_elapsed       | 14488    |\n",
      "|    total_timesteps    | 8512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106399   |\n",
      "|    policy_loss        | -0.0095  |\n",
      "|    value_loss         | 0.0299   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8520000, episode_reward=293.20 +/- 69.80\n",
      "Episode length: 6765.80 +/- 918.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.77e+03 |\n",
      "|    mean_reward        | 293      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.867   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106499   |\n",
      "|    policy_loss        | -0.0144  |\n",
      "|    value_loss         | 0.0315   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.99e+03 |\n",
      "|    ep_rew_mean     | 332      |\n",
      "| time/              |          |\n",
      "|    fps             | 587      |\n",
      "|    iterations      | 106500   |\n",
      "|    time_elapsed    | 14508    |\n",
      "|    total_timesteps | 8520000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.97e+03 |\n",
      "|    ep_rew_mean        | 331      |\n",
      "| time/                 |          |\n",
      "|    fps                | 587      |\n",
      "|    iterations         | 106600   |\n",
      "|    time_elapsed       | 14515    |\n",
      "|    total_timesteps    | 8528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.951   |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106599   |\n",
      "|    policy_loss        | 0.0705   |\n",
      "|    value_loss         | 0.0673   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8530000, episode_reward=373.20 +/- 41.60\n",
      "Episode length: 8482.40 +/- 898.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.48e+03 |\n",
      "|    mean_reward        | 373      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8530000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.975   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106624   |\n",
      "|    policy_loss        | 0.00731  |\n",
      "|    value_loss         | 0.0283   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.97e+03 |\n",
      "|    ep_rew_mean        | 330      |\n",
      "| time/                 |          |\n",
      "|    fps                | 587      |\n",
      "|    iterations         | 106700   |\n",
      "|    time_elapsed       | 14538    |\n",
      "|    total_timesteps    | 8536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.896   |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106699   |\n",
      "|    policy_loss        | -0.0957  |\n",
      "|    value_loss         | 0.284    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8540000, episode_reward=383.00 +/- 30.80\n",
      "Episode length: 9772.20 +/- 486.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.77e+03 |\n",
      "|    mean_reward        | 383      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.914   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106749   |\n",
      "|    policy_loss        | -0.0141  |\n",
      "|    value_loss         | 0.0424   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8e+03    |\n",
      "|    ep_rew_mean        | 330      |\n",
      "| time/                 |          |\n",
      "|    fps                | 586      |\n",
      "|    iterations         | 106800   |\n",
      "|    time_elapsed       | 14564    |\n",
      "|    total_timesteps    | 8544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106799   |\n",
      "|    policy_loss        | -0.00158 |\n",
      "|    value_loss         | 0.0644   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8550000, episode_reward=349.20 +/- 52.78\n",
      "Episode length: 8677.60 +/- 1015.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.68e+03 |\n",
      "|    mean_reward        | 349      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.962   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106874   |\n",
      "|    policy_loss        | 0.0148   |\n",
      "|    value_loss         | 0.0306   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.99e+03 |\n",
      "|    ep_rew_mean        | 329      |\n",
      "| time/                 |          |\n",
      "|    fps                | 586      |\n",
      "|    iterations         | 106900   |\n",
      "|    time_elapsed       | 14587    |\n",
      "|    total_timesteps    | 8552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.886   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106899   |\n",
      "|    policy_loss        | -0.0193  |\n",
      "|    value_loss         | 0.052    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8560000, episode_reward=281.40 +/- 111.46\n",
      "Episode length: 7432.20 +/- 1461.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.43e+03 |\n",
      "|    mean_reward        | 281      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.809   |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106999   |\n",
      "|    policy_loss        | 0.276    |\n",
      "|    value_loss         | 0.779    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.03e+03 |\n",
      "|    ep_rew_mean     | 327      |\n",
      "| time/              |          |\n",
      "|    fps             | 585      |\n",
      "|    iterations      | 107000   |\n",
      "|    time_elapsed    | 14609    |\n",
      "|    total_timesteps | 8560000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.02e+03 |\n",
      "|    ep_rew_mean        | 327      |\n",
      "| time/                 |          |\n",
      "|    fps                | 586      |\n",
      "|    iterations         | 107100   |\n",
      "|    time_elapsed       | 14616    |\n",
      "|    total_timesteps    | 8568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.9     |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107099   |\n",
      "|    policy_loss        | -0.0474  |\n",
      "|    value_loss         | 0.0531   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8570000, episode_reward=335.20 +/- 42.40\n",
      "Episode length: 8140.40 +/- 1278.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.14e+03 |\n",
      "|    mean_reward        | 335      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8570000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.917   |\n",
      "|    explained_variance | 0.903    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107124   |\n",
      "|    policy_loss        | 0.0573   |\n",
      "|    value_loss         | 0.139    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.98e+03 |\n",
      "|    ep_rew_mean        | 327      |\n",
      "| time/                 |          |\n",
      "|    fps                | 585      |\n",
      "|    iterations         | 107200   |\n",
      "|    time_elapsed       | 14638    |\n",
      "|    total_timesteps    | 8576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.841   |\n",
      "|    explained_variance | 0.773    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107199   |\n",
      "|    policy_loss        | -0.27    |\n",
      "|    value_loss         | 0.633    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8580000, episode_reward=350.60 +/- 109.11\n",
      "Episode length: 8586.80 +/- 1582.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.59e+03 |\n",
      "|    mean_reward        | 351      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.868   |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107249   |\n",
      "|    policy_loss        | 0.0998   |\n",
      "|    value_loss         | 0.0947   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.02e+03 |\n",
      "|    ep_rew_mean        | 327      |\n",
      "| time/                 |          |\n",
      "|    fps                | 585      |\n",
      "|    iterations         | 107300   |\n",
      "|    time_elapsed       | 14661    |\n",
      "|    total_timesteps    | 8584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.97    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107299   |\n",
      "|    policy_loss        | 0.0393   |\n",
      "|    value_loss         | 0.028    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8590000, episode_reward=367.60 +/- 29.13\n",
      "Episode length: 9558.00 +/- 1808.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.56e+03 |\n",
      "|    mean_reward        | 368      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8590000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.806   |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107374   |\n",
      "|    policy_loss        | -0.0408  |\n",
      "|    value_loss         | 0.103    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.03e+03 |\n",
      "|    ep_rew_mean        | 326      |\n",
      "| time/                 |          |\n",
      "|    fps                | 585      |\n",
      "|    iterations         | 107400   |\n",
      "|    time_elapsed       | 14686    |\n",
      "|    total_timesteps    | 8592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.849   |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107399   |\n",
      "|    policy_loss        | -0.0604  |\n",
      "|    value_loss         | 0.0793   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8600000, episode_reward=346.60 +/- 22.83\n",
      "Episode length: 8563.20 +/- 2545.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.56e+03 |\n",
      "|    mean_reward        | 347      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.941   |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107499   |\n",
      "|    policy_loss        | -0.115   |\n",
      "|    value_loss         | 0.127    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.05e+03 |\n",
      "|    ep_rew_mean     | 326      |\n",
      "| time/              |          |\n",
      "|    fps             | 584      |\n",
      "|    iterations      | 107500   |\n",
      "|    time_elapsed    | 14710    |\n",
      "|    total_timesteps | 8600000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.13e+03 |\n",
      "|    ep_rew_mean        | 329      |\n",
      "| time/                 |          |\n",
      "|    fps                | 584      |\n",
      "|    iterations         | 107600   |\n",
      "|    time_elapsed       | 14717    |\n",
      "|    total_timesteps    | 8608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.986   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107599   |\n",
      "|    policy_loss        | -0.0106  |\n",
      "|    value_loss         | 0.0579   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8610000, episode_reward=40.80 +/- 31.13\n",
      "Episode length: 3735.20 +/- 1562.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.74e+03 |\n",
      "|    mean_reward        | 40.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8610000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.937   |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107624   |\n",
      "|    policy_loss        | 0.0736   |\n",
      "|    value_loss         | 0.0453   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.11e+03 |\n",
      "|    ep_rew_mean        | 328      |\n",
      "| time/                 |          |\n",
      "|    fps                | 584      |\n",
      "|    iterations         | 107700   |\n",
      "|    time_elapsed       | 14731    |\n",
      "|    total_timesteps    | 8616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.925   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107699   |\n",
      "|    policy_loss        | 0.0467   |\n",
      "|    value_loss         | 0.0373   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8620000, episode_reward=309.60 +/- 102.57\n",
      "Episode length: 7968.20 +/- 1132.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.97e+03 |\n",
      "|    mean_reward        | 310      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8620000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107749   |\n",
      "|    policy_loss        | -0.0321  |\n",
      "|    value_loss         | 0.0567   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.08e+03 |\n",
      "|    ep_rew_mean        | 322      |\n",
      "| time/                 |          |\n",
      "|    fps                | 584      |\n",
      "|    iterations         | 107800   |\n",
      "|    time_elapsed       | 14752    |\n",
      "|    total_timesteps    | 8624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.787   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107799   |\n",
      "|    policy_loss        | -0.0322  |\n",
      "|    value_loss         | 0.0414   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8630000, episode_reward=317.60 +/- 77.85\n",
      "Episode length: 7985.20 +/- 822.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.99e+03 |\n",
      "|    mean_reward        | 318      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8630000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.945   |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107874   |\n",
      "|    policy_loss        | -0.0321  |\n",
      "|    value_loss         | 0.251    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.11e+03 |\n",
      "|    ep_rew_mean        | 321      |\n",
      "| time/                 |          |\n",
      "|    fps                | 584      |\n",
      "|    iterations         | 107900   |\n",
      "|    time_elapsed       | 14775    |\n",
      "|    total_timesteps    | 8632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.801   |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107899   |\n",
      "|    policy_loss        | -0.126   |\n",
      "|    value_loss         | 0.177    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8640000, episode_reward=383.00 +/- 33.69\n",
      "Episode length: 9081.80 +/- 1333.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.08e+03 |\n",
      "|    mean_reward        | 383      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107999   |\n",
      "|    policy_loss        | 0.0153   |\n",
      "|    value_loss         | 0.229    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.12e+03 |\n",
      "|    ep_rew_mean     | 319      |\n",
      "| time/              |          |\n",
      "|    fps             | 583      |\n",
      "|    iterations      | 108000   |\n",
      "|    time_elapsed    | 14800    |\n",
      "|    total_timesteps | 8640000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.19e+03 |\n",
      "|    ep_rew_mean        | 319      |\n",
      "| time/                 |          |\n",
      "|    fps                | 584      |\n",
      "|    iterations         | 108100   |\n",
      "|    time_elapsed       | 14807    |\n",
      "|    total_timesteps    | 8648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.879   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108099   |\n",
      "|    policy_loss        | -0.0098  |\n",
      "|    value_loss         | 0.0294   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8650000, episode_reward=365.20 +/- 30.92\n",
      "Episode length: 8703.00 +/- 237.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.7e+03  |\n",
      "|    mean_reward        | 365      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.82    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108124   |\n",
      "|    policy_loss        | 0.05     |\n",
      "|    value_loss         | 0.0833   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.21e+03 |\n",
      "|    ep_rew_mean        | 319      |\n",
      "| time/                 |          |\n",
      "|    fps                | 583      |\n",
      "|    iterations         | 108200   |\n",
      "|    time_elapsed       | 14831    |\n",
      "|    total_timesteps    | 8656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.918   |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108199   |\n",
      "|    policy_loss        | 0.0566   |\n",
      "|    value_loss         | 0.0556   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8660000, episode_reward=368.00 +/- 43.99\n",
      "Episode length: 10502.20 +/- 3435.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+04 |\n",
      "|    mean_reward        | 368      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.858   |\n",
      "|    explained_variance | 0.683    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108249   |\n",
      "|    policy_loss        | -0.405   |\n",
      "|    value_loss         | 0.836    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 323      |\n",
      "| time/                 |          |\n",
      "|    fps                | 583      |\n",
      "|    iterations         | 108300   |\n",
      "|    time_elapsed       | 14858    |\n",
      "|    total_timesteps    | 8664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.883   |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108299   |\n",
      "|    policy_loss        | 0.0291   |\n",
      "|    value_loss         | 0.109    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8670000, episode_reward=1.80 +/- 2.71\n",
      "Episode length: 757.40 +/- 305.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 757      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8670000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.758   |\n",
      "|    explained_variance | 0.702    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108374   |\n",
      "|    policy_loss        | 0.0999   |\n",
      "|    value_loss         | 0.131    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.54e+03 |\n",
      "|    ep_rew_mean        | 240      |\n",
      "| time/                 |          |\n",
      "|    fps                | 583      |\n",
      "|    iterations         | 108400   |\n",
      "|    time_elapsed       | 14867    |\n",
      "|    total_timesteps    | 8672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.745   |\n",
      "|    explained_variance | 0.624    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108399   |\n",
      "|    policy_loss        | 0.0119   |\n",
      "|    value_loss         | 0.0308   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8680000, episode_reward=367.80 +/- 38.15\n",
      "Episode length: 8544.80 +/- 684.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.54e+03 |\n",
      "|    mean_reward        | 368      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.991   |\n",
      "|    explained_variance | 0.747    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108499   |\n",
      "|    policy_loss        | -0.0224  |\n",
      "|    value_loss         | 0.027    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.73e+03 |\n",
      "|    ep_rew_mean     | 118      |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 108500   |\n",
      "|    time_elapsed    | 14891    |\n",
      "|    total_timesteps | 8680000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.52e+03 |\n",
      "|    ep_rew_mean        | 107      |\n",
      "| time/                 |          |\n",
      "|    fps                | 583      |\n",
      "|    iterations         | 108600   |\n",
      "|    time_elapsed       | 14898    |\n",
      "|    total_timesteps    | 8688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.891   |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108599   |\n",
      "|    policy_loss        | -0.0213  |\n",
      "|    value_loss         | 0.0122   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8690000, episode_reward=233.40 +/- 118.17\n",
      "Episode length: 6752.40 +/- 985.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.75e+03 |\n",
      "|    mean_reward        | 233      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8690000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.998   |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108624   |\n",
      "|    policy_loss        | 0.000103 |\n",
      "|    value_loss         | 0.0366   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.38e+03 |\n",
      "|    ep_rew_mean        | 97.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 582      |\n",
      "|    iterations         | 108700   |\n",
      "|    time_elapsed       | 14918    |\n",
      "|    total_timesteps    | 8696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.9     |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108699   |\n",
      "|    policy_loss        | -0.0271  |\n",
      "|    value_loss         | 0.0445   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8700000, episode_reward=354.60 +/- 54.10\n",
      "Episode length: 8201.00 +/- 708.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.2e+03  |\n",
      "|    mean_reward        | 355      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.899   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108749   |\n",
      "|    policy_loss        | -0.0275  |\n",
      "|    value_loss         | 0.0172   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.27e+03 |\n",
      "|    ep_rew_mean        | 95       |\n",
      "| time/                 |          |\n",
      "|    fps                | 582      |\n",
      "|    iterations         | 108800   |\n",
      "|    time_elapsed       | 14940    |\n",
      "|    total_timesteps    | 8704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.832   |\n",
      "|    explained_variance | 0.475    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108799   |\n",
      "|    policy_loss        | -0.482   |\n",
      "|    value_loss         | 3.36     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8710000, episode_reward=373.60 +/- 36.73\n",
      "Episode length: 8408.00 +/- 1174.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.41e+03 |\n",
      "|    mean_reward        | 374      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8710000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.8     |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108874   |\n",
      "|    policy_loss        | -0.074   |\n",
      "|    value_loss         | 0.151    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.22e+03 |\n",
      "|    ep_rew_mean        | 93.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 582      |\n",
      "|    iterations         | 108900   |\n",
      "|    time_elapsed       | 14964    |\n",
      "|    total_timesteps    | 8712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.957   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108899   |\n",
      "|    policy_loss        | 0.058    |\n",
      "|    value_loss         | 0.0488   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8720000, episode_reward=344.40 +/- 25.06\n",
      "Episode length: 7288.40 +/- 680.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.29e+03 |\n",
      "|    mean_reward        | 344      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.887   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108999   |\n",
      "|    policy_loss        | 0.0446   |\n",
      "|    value_loss         | 0.0546   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.21e+03 |\n",
      "|    ep_rew_mean     | 93.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 109000   |\n",
      "|    time_elapsed    | 14985    |\n",
      "|    total_timesteps | 8720000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.16e+03 |\n",
      "|    ep_rew_mean        | 95.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 582      |\n",
      "|    iterations         | 109100   |\n",
      "|    time_elapsed       | 14992    |\n",
      "|    total_timesteps    | 8728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.909   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109099   |\n",
      "|    policy_loss        | -0.0051  |\n",
      "|    value_loss         | 0.0602   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8730000, episode_reward=371.60 +/- 52.16\n",
      "Episode length: 12780.20 +/- 4418.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.28e+04 |\n",
      "|    mean_reward        | 372      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8730000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.842   |\n",
      "|    explained_variance | 0.85     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109124   |\n",
      "|    policy_loss        | -0.163   |\n",
      "|    value_loss         | 0.359    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.15e+03 |\n",
      "|    ep_rew_mean        | 95.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 581      |\n",
      "|    iterations         | 109200   |\n",
      "|    time_elapsed       | 15023    |\n",
      "|    total_timesteps    | 8736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.921   |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109199   |\n",
      "|    policy_loss        | 0.049    |\n",
      "|    value_loss         | 0.417    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8740000, episode_reward=293.40 +/- 72.34\n",
      "Episode length: 6863.80 +/- 954.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.86e+03 |\n",
      "|    mean_reward        | 293      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.985   |\n",
      "|    explained_variance | -0.0119  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109249   |\n",
      "|    policy_loss        | -0.225   |\n",
      "|    value_loss         | 0.897    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.22e+03 |\n",
      "|    ep_rew_mean        | 98.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 581      |\n",
      "|    iterations         | 109300   |\n",
      "|    time_elapsed       | 15043    |\n",
      "|    total_timesteps    | 8744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.852   |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109299   |\n",
      "|    policy_loss        | 0.0552   |\n",
      "|    value_loss         | 0.0455   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8750000, episode_reward=377.20 +/- 23.21\n",
      "Episode length: 8295.40 +/- 858.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.3e+03  |\n",
      "|    mean_reward        | 377      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.982   |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109374   |\n",
      "|    policy_loss        | -0.00932 |\n",
      "|    value_loss         | 0.107    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.29e+03 |\n",
      "|    ep_rew_mean        | 103      |\n",
      "| time/                 |          |\n",
      "|    fps                | 580      |\n",
      "|    iterations         | 109400   |\n",
      "|    time_elapsed       | 15066    |\n",
      "|    total_timesteps    | 8752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.81    |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109399   |\n",
      "|    policy_loss        | 0.0684   |\n",
      "|    value_loss         | 0.0838   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8760000, episode_reward=338.60 +/- 55.65\n",
      "Episode length: 8057.20 +/- 1732.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.06e+03 |\n",
      "|    mean_reward        | 339      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.94    |\n",
      "|    explained_variance | 0.843    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109499   |\n",
      "|    policy_loss        | -0.111   |\n",
      "|    value_loss         | 0.183    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.33e+03 |\n",
      "|    ep_rew_mean     | 106      |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 109500   |\n",
      "|    time_elapsed    | 15089    |\n",
      "|    total_timesteps | 8760000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.51e+03 |\n",
      "|    ep_rew_mean        | 115      |\n",
      "| time/                 |          |\n",
      "|    fps                | 580      |\n",
      "|    iterations         | 109600   |\n",
      "|    time_elapsed       | 15096    |\n",
      "|    total_timesteps    | 8768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.874   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109599   |\n",
      "|    policy_loss        | 0.0483   |\n",
      "|    value_loss         | 0.0386   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8770000, episode_reward=308.20 +/- 104.11\n",
      "Episode length: 8005.20 +/- 991.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.01e+03 |\n",
      "|    mean_reward        | 308      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8770000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.928   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109624   |\n",
      "|    policy_loss        | 0.0721   |\n",
      "|    value_loss         | 0.197    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.85e+03 |\n",
      "|    ep_rew_mean        | 133      |\n",
      "| time/                 |          |\n",
      "|    fps                | 580      |\n",
      "|    iterations         | 109700   |\n",
      "|    time_elapsed       | 15118    |\n",
      "|    total_timesteps    | 8776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.882   |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109699   |\n",
      "|    policy_loss        | -0.0163  |\n",
      "|    value_loss         | 0.0737   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8780000, episode_reward=400.80 +/- 24.74\n",
      "Episode length: 9245.60 +/- 865.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.25e+03 |\n",
      "|    mean_reward        | 401      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.924   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109749   |\n",
      "|    policy_loss        | 0.0244   |\n",
      "|    value_loss         | 0.0762   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.06e+03 |\n",
      "|    ep_rew_mean        | 140      |\n",
      "| time/                 |          |\n",
      "|    fps                | 580      |\n",
      "|    iterations         | 109800   |\n",
      "|    time_elapsed       | 15143    |\n",
      "|    total_timesteps    | 8784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.866   |\n",
      "|    explained_variance | 0.889    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109799   |\n",
      "|    policy_loss        | -0.107   |\n",
      "|    value_loss         | 0.298    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8790000, episode_reward=355.80 +/- 77.13\n",
      "Episode length: 9548.40 +/- 2415.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.55e+03 |\n",
      "|    mean_reward        | 356      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8790000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.825   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109874   |\n",
      "|    policy_loss        | -0.0204  |\n",
      "|    value_loss         | 0.0376   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.28e+03 |\n",
      "|    ep_rew_mean        | 150      |\n",
      "| time/                 |          |\n",
      "|    fps                | 579      |\n",
      "|    iterations         | 109900   |\n",
      "|    time_elapsed       | 15168    |\n",
      "|    total_timesteps    | 8792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.844   |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109899   |\n",
      "|    policy_loss        | 0.044    |\n",
      "|    value_loss         | 0.224    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8800000, episode_reward=309.20 +/- 93.37\n",
      "Episode length: 7420.80 +/- 1363.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.42e+03 |\n",
      "|    mean_reward        | 309      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.893   |\n",
      "|    explained_variance | 0.773    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109999   |\n",
      "|    policy_loss        | -0.144   |\n",
      "|    value_loss         | 0.334    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.59e+03 |\n",
      "|    ep_rew_mean     | 164      |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 110000   |\n",
      "|    time_elapsed    | 15189    |\n",
      "|    total_timesteps | 8800000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.05e+03 |\n",
      "|    ep_rew_mean        | 184      |\n",
      "| time/                 |          |\n",
      "|    fps                | 579      |\n",
      "|    iterations         | 110100   |\n",
      "|    time_elapsed       | 15196    |\n",
      "|    total_timesteps    | 8808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110099   |\n",
      "|    policy_loss        | 0.186    |\n",
      "|    value_loss         | 0.496    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8810000, episode_reward=213.60 +/- 126.59\n",
      "Episode length: 7029.40 +/- 1320.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.03e+03 |\n",
      "|    mean_reward        | 214      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8810000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.935   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110124   |\n",
      "|    policy_loss        | 0.000416 |\n",
      "|    value_loss         | 0.0587   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.39e+03 |\n",
      "|    ep_rew_mean        | 197      |\n",
      "| time/                 |          |\n",
      "|    fps                | 579      |\n",
      "|    iterations         | 110200   |\n",
      "|    time_elapsed       | 15217    |\n",
      "|    total_timesteps    | 8816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.888   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110199   |\n",
      "|    policy_loss        | 0.0255   |\n",
      "|    value_loss         | 0.0591   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8820000, episode_reward=364.80 +/- 44.87\n",
      "Episode length: 7412.40 +/- 586.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.41e+03 |\n",
      "|    mean_reward        | 365      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.853   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110249   |\n",
      "|    policy_loss        | -0.0446  |\n",
      "|    value_loss         | 0.0479   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.53e+03 |\n",
      "|    ep_rew_mean        | 202      |\n",
      "| time/                 |          |\n",
      "|    fps                | 579      |\n",
      "|    iterations         | 110300   |\n",
      "|    time_elapsed       | 15238    |\n",
      "|    total_timesteps    | 8824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.844   |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110299   |\n",
      "|    policy_loss        | 0.0114   |\n",
      "|    value_loss         | 0.407    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8830000, episode_reward=344.60 +/- 64.32\n",
      "Episode length: 9825.20 +/- 4863.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.83e+03 |\n",
      "|    mean_reward        | 345      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8830000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.864   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110374   |\n",
      "|    policy_loss        | -0.00786 |\n",
      "|    value_loss         | 0.0457   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.79e+03 |\n",
      "|    ep_rew_mean        | 215      |\n",
      "| time/                 |          |\n",
      "|    fps                | 578      |\n",
      "|    iterations         | 110400   |\n",
      "|    time_elapsed       | 15264    |\n",
      "|    total_timesteps    | 8832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.87    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110399   |\n",
      "|    policy_loss        | 0.0187   |\n",
      "|    value_loss         | 0.102    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8840000, episode_reward=336.00 +/- 105.25\n",
      "Episode length: 10322.20 +/- 3424.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+04 |\n",
      "|    mean_reward        | 336      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.837   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110499   |\n",
      "|    policy_loss        | 0.03     |\n",
      "|    value_loss         | 0.0231   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.02e+03 |\n",
      "|    ep_rew_mean     | 225      |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 110500   |\n",
      "|    time_elapsed    | 15290    |\n",
      "|    total_timesteps | 8840000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.34e+03 |\n",
      "|    ep_rew_mean        | 241      |\n",
      "| time/                 |          |\n",
      "|    fps                | 578      |\n",
      "|    iterations         | 110600   |\n",
      "|    time_elapsed       | 15297    |\n",
      "|    total_timesteps    | 8848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.886   |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110599   |\n",
      "|    policy_loss        | 0.128    |\n",
      "|    value_loss         | 0.0781   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8850000, episode_reward=272.60 +/- 140.16\n",
      "Episode length: 7361.80 +/- 1938.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.36e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8850000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.857   |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110624   |\n",
      "|    policy_loss        | 0.00547  |\n",
      "|    value_loss         | 0.122    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.64e+03 |\n",
      "|    ep_rew_mean        | 255      |\n",
      "| time/                 |          |\n",
      "|    fps                | 578      |\n",
      "|    iterations         | 110700   |\n",
      "|    time_elapsed       | 15319    |\n",
      "|    total_timesteps    | 8856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.863   |\n",
      "|    explained_variance | 0.787    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110699   |\n",
      "|    policy_loss        | -0.214   |\n",
      "|    value_loss         | 0.457    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8860000, episode_reward=307.40 +/- 115.29\n",
      "Episode length: 7582.40 +/- 1974.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.58e+03 |\n",
      "|    mean_reward        | 307      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8860000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.779   |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110749   |\n",
      "|    policy_loss        | 0.0299   |\n",
      "|    value_loss         | 0.176    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.8e+03  |\n",
      "|    ep_rew_mean        | 261      |\n",
      "| time/                 |          |\n",
      "|    fps                | 577      |\n",
      "|    iterations         | 110800   |\n",
      "|    time_elapsed       | 15340    |\n",
      "|    total_timesteps    | 8864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.76    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110799   |\n",
      "|    policy_loss        | -0.141   |\n",
      "|    value_loss         | 0.379    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8870000, episode_reward=171.80 +/- 83.12\n",
      "Episode length: 6679.80 +/- 893.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.68e+03 |\n",
      "|    mean_reward        | 172      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8870000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.912   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110874   |\n",
      "|    policy_loss        | -0.0547  |\n",
      "|    value_loss         | 0.0767   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.2e+03  |\n",
      "|    ep_rew_mean        | 278      |\n",
      "| time/                 |          |\n",
      "|    fps                | 577      |\n",
      "|    iterations         | 110900   |\n",
      "|    time_elapsed       | 15360    |\n",
      "|    total_timesteps    | 8872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.892   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110899   |\n",
      "|    policy_loss        | 0.0313   |\n",
      "|    value_loss         | 0.0214   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8880000, episode_reward=335.00 +/- 67.92\n",
      "Episode length: 7040.20 +/- 1767.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.04e+03 |\n",
      "|    mean_reward        | 335      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.908   |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110999   |\n",
      "|    policy_loss        | 0.039    |\n",
      "|    value_loss         | 0.0927   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.68e+03 |\n",
      "|    ep_rew_mean     | 295      |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 111000   |\n",
      "|    time_elapsed    | 15380    |\n",
      "|    total_timesteps | 8880000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.81e+03 |\n",
      "|    ep_rew_mean        | 300      |\n",
      "| time/                 |          |\n",
      "|    fps                | 577      |\n",
      "|    iterations         | 111100   |\n",
      "|    time_elapsed       | 15387    |\n",
      "|    total_timesteps    | 8888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.942   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111099   |\n",
      "|    policy_loss        | 0.0278   |\n",
      "|    value_loss         | 0.0372   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8890000, episode_reward=289.00 +/- 93.16\n",
      "Episode length: 7675.20 +/- 1231.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.68e+03 |\n",
      "|    mean_reward        | 289      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8890000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.812   |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111124   |\n",
      "|    policy_loss        | 0.0314   |\n",
      "|    value_loss         | 0.294    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.1e+03  |\n",
      "|    ep_rew_mean        | 317      |\n",
      "| time/                 |          |\n",
      "|    fps                | 577      |\n",
      "|    iterations         | 111200   |\n",
      "|    time_elapsed       | 15409    |\n",
      "|    total_timesteps    | 8896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.998   |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111199   |\n",
      "|    policy_loss        | 0.035    |\n",
      "|    value_loss         | 0.0701   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8900000, episode_reward=311.60 +/- 102.58\n",
      "Episode length: 8125.40 +/- 1872.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.13e+03 |\n",
      "|    mean_reward        | 312      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8900000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.885   |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111249   |\n",
      "|    policy_loss        | 0.0238   |\n",
      "|    value_loss         | 0.0562   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.16e+03 |\n",
      "|    ep_rew_mean        | 327      |\n",
      "| time/                 |          |\n",
      "|    fps                | 576      |\n",
      "|    iterations         | 111300   |\n",
      "|    time_elapsed       | 15432    |\n",
      "|    total_timesteps    | 8904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.882   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111299   |\n",
      "|    policy_loss        | -0.0567  |\n",
      "|    value_loss         | 0.132    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8910000, episode_reward=293.20 +/- 117.87\n",
      "Episode length: 8488.40 +/- 2847.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.49e+03 |\n",
      "|    mean_reward        | 293      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8910000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.816   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111374   |\n",
      "|    policy_loss        | 0.0236   |\n",
      "|    value_loss         | 0.0813   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.2e+03  |\n",
      "|    ep_rew_mean        | 330      |\n",
      "| time/                 |          |\n",
      "|    fps                | 576      |\n",
      "|    iterations         | 111400   |\n",
      "|    time_elapsed       | 15455    |\n",
      "|    total_timesteps    | 8912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.83    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111399   |\n",
      "|    policy_loss        | 0.0338   |\n",
      "|    value_loss         | 0.242    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8920000, episode_reward=327.60 +/- 60.56\n",
      "Episode length: 8709.20 +/- 2678.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.71e+03 |\n",
      "|    mean_reward        | 328      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.849   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111499   |\n",
      "|    policy_loss        | 0.103    |\n",
      "|    value_loss         | 0.152    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.2e+03  |\n",
      "|    ep_rew_mean     | 329      |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 111500   |\n",
      "|    time_elapsed    | 15478    |\n",
      "|    total_timesteps | 8920000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.18e+03 |\n",
      "|    ep_rew_mean        | 327      |\n",
      "| time/                 |          |\n",
      "|    fps                | 576      |\n",
      "|    iterations         | 111600   |\n",
      "|    time_elapsed       | 15485    |\n",
      "|    total_timesteps    | 8928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.901   |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111599   |\n",
      "|    policy_loss        | -0.0117  |\n",
      "|    value_loss         | 0.0736   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8930000, episode_reward=348.80 +/- 69.59\n",
      "Episode length: 7632.40 +/- 1084.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.63e+03 |\n",
      "|    mean_reward        | 349      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8930000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.941   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111624   |\n",
      "|    policy_loss        | 0.0185   |\n",
      "|    value_loss         | 0.0199   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.2e+03  |\n",
      "|    ep_rew_mean        | 329      |\n",
      "| time/                 |          |\n",
      "|    fps                | 576      |\n",
      "|    iterations         | 111700   |\n",
      "|    time_elapsed       | 15507    |\n",
      "|    total_timesteps    | 8936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.963   |\n",
      "|    explained_variance | 0.675    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111699   |\n",
      "|    policy_loss        | -0.233   |\n",
      "|    value_loss         | 1.01     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8940000, episode_reward=373.60 +/- 30.76\n",
      "Episode length: 9628.80 +/- 2515.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.63e+03 |\n",
      "|    mean_reward        | 374      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111749   |\n",
      "|    policy_loss        | -0.0174  |\n",
      "|    value_loss         | 0.0298   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.19e+03 |\n",
      "|    ep_rew_mean        | 326      |\n",
      "| time/                 |          |\n",
      "|    fps                | 575      |\n",
      "|    iterations         | 111800   |\n",
      "|    time_elapsed       | 15532    |\n",
      "|    total_timesteps    | 8944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.874   |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111799   |\n",
      "|    policy_loss        | -0.133   |\n",
      "|    value_loss         | 0.927    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8950000, episode_reward=340.20 +/- 69.14\n",
      "Episode length: 7261.40 +/- 463.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.26e+03 |\n",
      "|    mean_reward        | 340      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8950000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.963   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111874   |\n",
      "|    policy_loss        | 0.00496  |\n",
      "|    value_loss         | 0.0454   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.16e+03 |\n",
      "|    ep_rew_mean        | 324      |\n",
      "| time/                 |          |\n",
      "|    fps                | 575      |\n",
      "|    iterations         | 111900   |\n",
      "|    time_elapsed       | 15553    |\n",
      "|    total_timesteps    | 8952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.936   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111899   |\n",
      "|    policy_loss        | 0.0134   |\n",
      "|    value_loss         | 0.0339   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8960000, episode_reward=317.60 +/- 117.65\n",
      "Episode length: 8157.40 +/- 1993.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.16e+03 |\n",
      "|    mean_reward        | 318      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.951   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111999   |\n",
      "|    policy_loss        | 0.0254   |\n",
      "|    value_loss         | 0.104    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.2e+03  |\n",
      "|    ep_rew_mean     | 328      |\n",
      "| time/              |          |\n",
      "|    fps             | 575      |\n",
      "|    iterations      | 112000   |\n",
      "|    time_elapsed    | 15576    |\n",
      "|    total_timesteps | 8960000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.15e+03 |\n",
      "|    ep_rew_mean        | 326      |\n",
      "| time/                 |          |\n",
      "|    fps                | 575      |\n",
      "|    iterations         | 112100   |\n",
      "|    time_elapsed       | 15583    |\n",
      "|    total_timesteps    | 8968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.838   |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112099   |\n",
      "|    policy_loss        | 0.00292  |\n",
      "|    value_loss         | 0.197    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8970000, episode_reward=322.80 +/- 85.46\n",
      "Episode length: 12234.00 +/- 9365.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.22e+04 |\n",
      "|    mean_reward        | 323      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8970000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.838   |\n",
      "|    explained_variance | 0.734    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112124   |\n",
      "|    policy_loss        | -0.226   |\n",
      "|    value_loss         | 1.34     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.07e+03 |\n",
      "|    ep_rew_mean        | 323      |\n",
      "| time/                 |          |\n",
      "|    fps                | 574      |\n",
      "|    iterations         | 112200   |\n",
      "|    time_elapsed       | 15613    |\n",
      "|    total_timesteps    | 8976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.821   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112199   |\n",
      "|    policy_loss        | 0.024    |\n",
      "|    value_loss         | 0.016    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8980000, episode_reward=322.20 +/- 78.59\n",
      "Episode length: 8261.00 +/- 2306.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.26e+03 |\n",
      "|    mean_reward        | 322      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.968   |\n",
      "|    explained_variance | 0.693    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112249   |\n",
      "|    policy_loss        | -0.114   |\n",
      "|    value_loss         | 0.389    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.03e+03 |\n",
      "|    ep_rew_mean        | 320      |\n",
      "| time/                 |          |\n",
      "|    fps                | 574      |\n",
      "|    iterations         | 112300   |\n",
      "|    time_elapsed       | 15635    |\n",
      "|    total_timesteps    | 8984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112299   |\n",
      "|    policy_loss        | -0.0139  |\n",
      "|    value_loss         | 0.0378   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8990000, episode_reward=302.00 +/- 130.08\n",
      "Episode length: 8062.60 +/- 1206.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.06e+03 |\n",
      "|    mean_reward        | 302      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8990000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.849   |\n",
      "|    explained_variance | 0.855    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112374   |\n",
      "|    policy_loss        | 0.0982   |\n",
      "|    value_loss         | 0.494    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.04e+03 |\n",
      "|    ep_rew_mean        | 320      |\n",
      "| time/                 |          |\n",
      "|    fps                | 574      |\n",
      "|    iterations         | 112400   |\n",
      "|    time_elapsed       | 15658    |\n",
      "|    total_timesteps    | 8992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.868   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112399   |\n",
      "|    policy_loss        | -0.0737  |\n",
      "|    value_loss         | 0.0767   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9000000, episode_reward=403.20 +/- 40.29\n",
      "Episode length: 9356.60 +/- 1338.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.36e+03 |\n",
      "|    mean_reward        | 403      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.815   |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112499   |\n",
      "|    policy_loss        | 0.0553   |\n",
      "|    value_loss         | 0.0924   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.02e+03 |\n",
      "|    ep_rew_mean     | 318      |\n",
      "| time/              |          |\n",
      "|    fps             | 573      |\n",
      "|    iterations      | 112500   |\n",
      "|    time_elapsed    | 15682    |\n",
      "|    total_timesteps | 9000000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.91e+03 |\n",
      "|    ep_rew_mean        | 319      |\n",
      "| time/                 |          |\n",
      "|    fps                | 574      |\n",
      "|    iterations         | 112600   |\n",
      "|    time_elapsed       | 15690    |\n",
      "|    total_timesteps    | 9008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.804   |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112599   |\n",
      "|    policy_loss        | -0.118   |\n",
      "|    value_loss         | 0.271    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9010000, episode_reward=364.20 +/- 40.40\n",
      "Episode length: 8801.00 +/- 1009.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.8e+03  |\n",
      "|    mean_reward        | 364      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9010000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.759   |\n",
      "|    explained_variance | 0.76     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112624   |\n",
      "|    policy_loss        | -0.109   |\n",
      "|    value_loss         | 0.48     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.9e+03  |\n",
      "|    ep_rew_mean        | 319      |\n",
      "| time/                 |          |\n",
      "|    fps                | 573      |\n",
      "|    iterations         | 112700   |\n",
      "|    time_elapsed       | 15713    |\n",
      "|    total_timesteps    | 9016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.96    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112699   |\n",
      "|    policy_loss        | -0.00612 |\n",
      "|    value_loss         | 0.0527   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9020000, episode_reward=378.00 +/- 33.56\n",
      "Episode length: 8609.40 +/- 713.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.61e+03 |\n",
      "|    mean_reward        | 378      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.807   |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112749   |\n",
      "|    policy_loss        | 0.0308   |\n",
      "|    value_loss         | 0.0344   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.02e+03 |\n",
      "|    ep_rew_mean        | 327      |\n",
      "| time/                 |          |\n",
      "|    fps                | 573      |\n",
      "|    iterations         | 112800   |\n",
      "|    time_elapsed       | 15737    |\n",
      "|    total_timesteps    | 9024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.791   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112799   |\n",
      "|    policy_loss        | -0.0123  |\n",
      "|    value_loss         | 0.0193   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9030000, episode_reward=339.20 +/- 47.32\n",
      "Episode length: 8455.80 +/- 1337.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.46e+03 |\n",
      "|    mean_reward        | 339      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9030000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.859   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112874   |\n",
      "|    policy_loss        | -0.0405  |\n",
      "|    value_loss         | 0.175    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.02e+03 |\n",
      "|    ep_rew_mean        | 327      |\n",
      "| time/                 |          |\n",
      "|    fps                | 573      |\n",
      "|    iterations         | 112900   |\n",
      "|    time_elapsed       | 15760    |\n",
      "|    total_timesteps    | 9032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.894   |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112899   |\n",
      "|    policy_loss        | -0.0216  |\n",
      "|    value_loss         | 0.104    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9040000, episode_reward=260.80 +/- 97.85\n",
      "Episode length: 7483.20 +/- 1144.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.48e+03 |\n",
      "|    mean_reward        | 261      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.783   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112999   |\n",
      "|    policy_loss        | 0.0595   |\n",
      "|    value_loss         | 0.0762   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.02e+03 |\n",
      "|    ep_rew_mean     | 328      |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 113000   |\n",
      "|    time_elapsed    | 15781    |\n",
      "|    total_timesteps | 9040000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.03e+03 |\n",
      "|    ep_rew_mean        | 326      |\n",
      "| time/                 |          |\n",
      "|    fps                | 573      |\n",
      "|    iterations         | 113100   |\n",
      "|    time_elapsed       | 15788    |\n",
      "|    total_timesteps    | 9048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.819   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113099   |\n",
      "|    policy_loss        | 0.0314   |\n",
      "|    value_loss         | 0.0513   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9050000, episode_reward=296.60 +/- 115.05\n",
      "Episode length: 7723.20 +/- 2297.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.72e+03 |\n",
      "|    mean_reward        | 297      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.902   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113124   |\n",
      "|    policy_loss        | -0.0241  |\n",
      "|    value_loss         | 0.0301   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.01e+03 |\n",
      "|    ep_rew_mean        | 326      |\n",
      "| time/                 |          |\n",
      "|    fps                | 572      |\n",
      "|    iterations         | 113200   |\n",
      "|    time_elapsed       | 15810    |\n",
      "|    total_timesteps    | 9056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.894   |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113199   |\n",
      "|    policy_loss        | 0.0391   |\n",
      "|    value_loss         | 0.0517   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9060000, episode_reward=310.60 +/- 64.42\n",
      "Episode length: 8111.20 +/- 493.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.11e+03 |\n",
      "|    mean_reward        | 311      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.881   |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113249   |\n",
      "|    policy_loss        | 0.0451   |\n",
      "|    value_loss         | 0.0935   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.03e+03 |\n",
      "|    ep_rew_mean        | 328      |\n",
      "| time/                 |          |\n",
      "|    fps                | 572      |\n",
      "|    iterations         | 113300   |\n",
      "|    time_elapsed       | 15832    |\n",
      "|    total_timesteps    | 9064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.978   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113299   |\n",
      "|    policy_loss        | -0.0138  |\n",
      "|    value_loss         | 0.0424   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9070000, episode_reward=339.60 +/- 83.46\n",
      "Episode length: 8898.40 +/- 2025.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.9e+03  |\n",
      "|    mean_reward        | 340      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9070000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.881   |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113374   |\n",
      "|    policy_loss        | -0.0132  |\n",
      "|    value_loss         | 0.0685   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8e+03    |\n",
      "|    ep_rew_mean        | 334      |\n",
      "| time/                 |          |\n",
      "|    fps                | 572      |\n",
      "|    iterations         | 113400   |\n",
      "|    time_elapsed       | 15857    |\n",
      "|    total_timesteps    | 9072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.952   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113399   |\n",
      "|    policy_loss        | 0.05     |\n",
      "|    value_loss         | 0.0656   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9080000, episode_reward=325.80 +/- 20.90\n",
      "Episode length: 8466.20 +/- 777.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.47e+03 |\n",
      "|    mean_reward        | 326      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.832   |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113499   |\n",
      "|    policy_loss        | 0.118    |\n",
      "|    value_loss         | 0.383    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8e+03    |\n",
      "|    ep_rew_mean     | 335      |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 113500   |\n",
      "|    time_elapsed    | 15880    |\n",
      "|    total_timesteps | 9080000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.96e+03 |\n",
      "|    ep_rew_mean        | 336      |\n",
      "| time/                 |          |\n",
      "|    fps                | 572      |\n",
      "|    iterations         | 113600   |\n",
      "|    time_elapsed       | 15887    |\n",
      "|    total_timesteps    | 9088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.786   |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113599   |\n",
      "|    policy_loss        | 0.0369   |\n",
      "|    value_loss         | 0.0553   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9090000, episode_reward=387.20 +/- 20.82\n",
      "Episode length: 9842.00 +/- 2436.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.84e+03 |\n",
      "|    mean_reward        | 387      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9090000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.824   |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113624   |\n",
      "|    policy_loss        | -0.143   |\n",
      "|    value_loss         | 0.443    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.95e+03 |\n",
      "|    ep_rew_mean        | 335      |\n",
      "| time/                 |          |\n",
      "|    fps                | 571      |\n",
      "|    iterations         | 113700   |\n",
      "|    time_elapsed       | 15913    |\n",
      "|    total_timesteps    | 9096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.784   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113699   |\n",
      "|    policy_loss        | -0.0579  |\n",
      "|    value_loss         | 0.0756   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9100000, episode_reward=383.20 +/- 30.37\n",
      "Episode length: 9121.20 +/- 470.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.12e+03 |\n",
      "|    mean_reward        | 383      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.755   |\n",
      "|    explained_variance | 0.845    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113749   |\n",
      "|    policy_loss        | 0.0229   |\n",
      "|    value_loss         | 0.185    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.99e+03 |\n",
      "|    ep_rew_mean        | 334      |\n",
      "| time/                 |          |\n",
      "|    fps                | 571      |\n",
      "|    iterations         | 113800   |\n",
      "|    time_elapsed       | 15937    |\n",
      "|    total_timesteps    | 9104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.782   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113799   |\n",
      "|    policy_loss        | -0.0613  |\n",
      "|    value_loss         | 0.0298   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9110000, episode_reward=318.40 +/- 100.54\n",
      "Episode length: 8002.60 +/- 1177.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8e+03    |\n",
      "|    mean_reward        | 318      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9110000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.869   |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113874   |\n",
      "|    policy_loss        | 0.03     |\n",
      "|    value_loss         | 0.213    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.02e+03 |\n",
      "|    ep_rew_mean        | 335      |\n",
      "| time/                 |          |\n",
      "|    fps                | 570      |\n",
      "|    iterations         | 113900   |\n",
      "|    time_elapsed       | 15960    |\n",
      "|    total_timesteps    | 9112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.878   |\n",
      "|    explained_variance | 0.757    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113899   |\n",
      "|    policy_loss        | -0.337   |\n",
      "|    value_loss         | 1.22     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9120000, episode_reward=322.60 +/- 33.89\n",
      "Episode length: 7749.80 +/- 906.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.75e+03 |\n",
      "|    mean_reward        | 323      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.896   |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113999   |\n",
      "|    policy_loss        | -0.0154  |\n",
      "|    value_loss         | 0.32     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01e+03 |\n",
      "|    ep_rew_mean     | 333      |\n",
      "| time/              |          |\n",
      "|    fps             | 570      |\n",
      "|    iterations      | 114000   |\n",
      "|    time_elapsed    | 15982    |\n",
      "|    total_timesteps | 9120000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.98e+03 |\n",
      "|    ep_rew_mean        | 333      |\n",
      "| time/                 |          |\n",
      "|    fps                | 570      |\n",
      "|    iterations         | 114100   |\n",
      "|    time_elapsed       | 15989    |\n",
      "|    total_timesteps    | 9128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.856   |\n",
      "|    explained_variance | 0.634    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114099   |\n",
      "|    policy_loss        | -0.148   |\n",
      "|    value_loss         | 0.592    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9130000, episode_reward=341.40 +/- 20.97\n",
      "Episode length: 8044.20 +/- 775.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.04e+03 |\n",
      "|    mean_reward        | 341      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9130000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.801   |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114124   |\n",
      "|    policy_loss        | -0.0382  |\n",
      "|    value_loss         | 0.168    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.98e+03 |\n",
      "|    ep_rew_mean        | 332      |\n",
      "| time/                 |          |\n",
      "|    fps                | 570      |\n",
      "|    iterations         | 114200   |\n",
      "|    time_elapsed       | 16012    |\n",
      "|    total_timesteps    | 9136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.86    |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114199   |\n",
      "|    policy_loss        | 0.0292   |\n",
      "|    value_loss         | 0.112    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9140000, episode_reward=386.40 +/- 31.41\n",
      "Episode length: 9501.60 +/- 2548.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.5e+03  |\n",
      "|    mean_reward        | 386      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.894   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114249   |\n",
      "|    policy_loss        | -0.109   |\n",
      "|    value_loss         | 0.144    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.93e+03 |\n",
      "|    ep_rew_mean        | 331      |\n",
      "| time/                 |          |\n",
      "|    fps                | 570      |\n",
      "|    iterations         | 114300   |\n",
      "|    time_elapsed       | 16038    |\n",
      "|    total_timesteps    | 9144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.899   |\n",
      "|    explained_variance | 0.906    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114299   |\n",
      "|    policy_loss        | -0.108   |\n",
      "|    value_loss         | 0.261    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9150000, episode_reward=347.40 +/- 91.71\n",
      "Episode length: 8814.00 +/- 1475.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.81e+03 |\n",
      "|    mean_reward        | 347      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.83    |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114374   |\n",
      "|    policy_loss        | 0.0368   |\n",
      "|    value_loss         | 0.0742   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.95e+03 |\n",
      "|    ep_rew_mean        | 335      |\n",
      "| time/                 |          |\n",
      "|    fps                | 569      |\n",
      "|    iterations         | 114400   |\n",
      "|    time_elapsed       | 16061    |\n",
      "|    total_timesteps    | 9152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.869   |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114399   |\n",
      "|    policy_loss        | -0.0774  |\n",
      "|    value_loss         | 0.0589   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9160000, episode_reward=357.40 +/- 26.47\n",
      "Episode length: 8075.00 +/- 665.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.08e+03 |\n",
      "|    mean_reward        | 357      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.761   |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114499   |\n",
      "|    policy_loss        | 0.094    |\n",
      "|    value_loss         | 0.086    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.02e+03 |\n",
      "|    ep_rew_mean     | 337      |\n",
      "| time/              |          |\n",
      "|    fps             | 569      |\n",
      "|    iterations      | 114500   |\n",
      "|    time_elapsed    | 16084    |\n",
      "|    total_timesteps | 9160000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.04e+03 |\n",
      "|    ep_rew_mean        | 337      |\n",
      "| time/                 |          |\n",
      "|    fps                | 569      |\n",
      "|    iterations         | 114600   |\n",
      "|    time_elapsed       | 16091    |\n",
      "|    total_timesteps    | 9168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.799   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114599   |\n",
      "|    policy_loss        | 0.116    |\n",
      "|    value_loss         | 0.21     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9170000, episode_reward=354.00 +/- 49.36\n",
      "Episode length: 8724.20 +/- 1496.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.72e+03 |\n",
      "|    mean_reward        | 354      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9170000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.892   |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114624   |\n",
      "|    policy_loss        | -0.0636  |\n",
      "|    value_loss         | 0.372    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.08e+03 |\n",
      "|    ep_rew_mean        | 336      |\n",
      "| time/                 |          |\n",
      "|    fps                | 569      |\n",
      "|    iterations         | 114700   |\n",
      "|    time_elapsed       | 16115    |\n",
      "|    total_timesteps    | 9176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.875   |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114699   |\n",
      "|    policy_loss        | 0.162    |\n",
      "|    value_loss         | 0.134    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9180000, episode_reward=270.80 +/- 72.36\n",
      "Episode length: 8093.00 +/- 2425.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.09e+03 |\n",
      "|    mean_reward        | 271      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.913   |\n",
      "|    explained_variance | 0.895    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114749   |\n",
      "|    policy_loss        | -0.0294  |\n",
      "|    value_loss         | 0.0827   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.09e+03 |\n",
      "|    ep_rew_mean        | 339      |\n",
      "| time/                 |          |\n",
      "|    fps                | 569      |\n",
      "|    iterations         | 114800   |\n",
      "|    time_elapsed       | 16137    |\n",
      "|    total_timesteps    | 9184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.968   |\n",
      "|    explained_variance | 0.699    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114799   |\n",
      "|    policy_loss        | -0.124   |\n",
      "|    value_loss         | 0.678    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9190000, episode_reward=376.40 +/- 32.43\n",
      "Episode length: 8925.20 +/- 1934.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.93e+03 |\n",
      "|    mean_reward        | 376      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9190000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.911   |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114874   |\n",
      "|    policy_loss        | 0.0192   |\n",
      "|    value_loss         | 0.0216   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.08e+03 |\n",
      "|    ep_rew_mean        | 340      |\n",
      "| time/                 |          |\n",
      "|    fps                | 568      |\n",
      "|    iterations         | 114900   |\n",
      "|    time_elapsed       | 16162    |\n",
      "|    total_timesteps    | 9192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.953   |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114899   |\n",
      "|    policy_loss        | 0.0313   |\n",
      "|    value_loss         | 0.0343   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9200000, episode_reward=360.20 +/- 81.96\n",
      "Episode length: 8010.00 +/- 2125.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.01e+03 |\n",
      "|    mean_reward        | 360      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.926   |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114999   |\n",
      "|    policy_loss        | 0.0134   |\n",
      "|    value_loss         | 0.0718   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.1e+03  |\n",
      "|    ep_rew_mean     | 341      |\n",
      "| time/              |          |\n",
      "|    fps             | 568      |\n",
      "|    iterations      | 115000   |\n",
      "|    time_elapsed    | 16184    |\n",
      "|    total_timesteps | 9200000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.07e+03 |\n",
      "|    ep_rew_mean        | 337      |\n",
      "| time/                 |          |\n",
      "|    fps                | 568      |\n",
      "|    iterations         | 115100   |\n",
      "|    time_elapsed       | 16191    |\n",
      "|    total_timesteps    | 9208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.892   |\n",
      "|    explained_variance | 0.864    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115099   |\n",
      "|    policy_loss        | -0.0454  |\n",
      "|    value_loss         | 0.4      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9210000, episode_reward=398.80 +/- 15.21\n",
      "Episode length: 9816.20 +/- 1677.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.82e+03 |\n",
      "|    mean_reward        | 399      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9210000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.947   |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115124   |\n",
      "|    policy_loss        | 0.0315   |\n",
      "|    value_loss         | 0.0937   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 8.09e+03  |\n",
      "|    ep_rew_mean        | 336       |\n",
      "| time/                 |           |\n",
      "|    fps                | 568       |\n",
      "|    iterations         | 115200    |\n",
      "|    time_elapsed       | 16217     |\n",
      "|    total_timesteps    | 9216000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.923    |\n",
      "|    explained_variance | 0.992     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 115199    |\n",
      "|    policy_loss        | -0.000511 |\n",
      "|    value_loss         | 0.02      |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=9220000, episode_reward=386.20 +/- 24.49\n",
      "Episode length: 8929.00 +/- 1354.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.93e+03 |\n",
      "|    mean_reward        | 386      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.817   |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115249   |\n",
      "|    policy_loss        | 0.0417   |\n",
      "|    value_loss         | 0.191    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.12e+03 |\n",
      "|    ep_rew_mean        | 337      |\n",
      "| time/                 |          |\n",
      "|    fps                | 567      |\n",
      "|    iterations         | 115300   |\n",
      "|    time_elapsed       | 16241    |\n",
      "|    total_timesteps    | 9224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.834   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115299   |\n",
      "|    policy_loss        | 0.0185   |\n",
      "|    value_loss         | 0.0242   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9230000, episode_reward=353.60 +/- 73.47\n",
      "Episode length: 12692.40 +/- 8812.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.27e+04 |\n",
      "|    mean_reward        | 354      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9230000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.864   |\n",
      "|    explained_variance | 0.89     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115374   |\n",
      "|    policy_loss        | 0.0636   |\n",
      "|    value_loss         | 0.492    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.19e+03 |\n",
      "|    ep_rew_mean        | 339      |\n",
      "| time/                 |          |\n",
      "|    fps                | 567      |\n",
      "|    iterations         | 115400   |\n",
      "|    time_elapsed       | 16272    |\n",
      "|    total_timesteps    | 9232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.836   |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115399   |\n",
      "|    policy_loss        | -0.00153 |\n",
      "|    value_loss         | 0.0518   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9240000, episode_reward=349.60 +/- 75.54\n",
      "Episode length: 7515.20 +/- 1524.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.52e+03 |\n",
      "|    mean_reward        | 350      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.894   |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115499   |\n",
      "|    policy_loss        | -0.04    |\n",
      "|    value_loss         | 0.129    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.2e+03  |\n",
      "|    ep_rew_mean     | 338      |\n",
      "| time/              |          |\n",
      "|    fps             | 567      |\n",
      "|    iterations      | 115500   |\n",
      "|    time_elapsed    | 16293    |\n",
      "|    total_timesteps | 9240000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.21e+03 |\n",
      "|    ep_rew_mean        | 338      |\n",
      "| time/                 |          |\n",
      "|    fps                | 567      |\n",
      "|    iterations         | 115600   |\n",
      "|    time_elapsed       | 16300    |\n",
      "|    total_timesteps    | 9248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.63     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115599   |\n",
      "|    policy_loss        | -0.144   |\n",
      "|    value_loss         | 0.697    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9250000, episode_reward=393.20 +/- 25.98\n",
      "Episode length: 10180.00 +/- 3143.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+04 |\n",
      "|    mean_reward        | 393      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.975   |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115624   |\n",
      "|    policy_loss        | 0.0657   |\n",
      "|    value_loss         | 0.101    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.21e+03 |\n",
      "|    ep_rew_mean        | 340      |\n",
      "| time/                 |          |\n",
      "|    fps                | 566      |\n",
      "|    iterations         | 115700   |\n",
      "|    time_elapsed       | 16326    |\n",
      "|    total_timesteps    | 9256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.819   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115699   |\n",
      "|    policy_loss        | 0.0193   |\n",
      "|    value_loss         | 0.022    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9260000, episode_reward=309.20 +/- 62.74\n",
      "Episode length: 10880.20 +/- 7188.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.09e+04 |\n",
      "|    mean_reward        | 309      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.913   |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115749   |\n",
      "|    policy_loss        | -0.0093  |\n",
      "|    value_loss         | 0.0412   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.24e+03 |\n",
      "|    ep_rew_mean        | 342      |\n",
      "| time/                 |          |\n",
      "|    fps                | 566      |\n",
      "|    iterations         | 115800   |\n",
      "|    time_elapsed       | 16354    |\n",
      "|    total_timesteps    | 9264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.89    |\n",
      "|    explained_variance | 0.823    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115799   |\n",
      "|    policy_loss        | 0.0469   |\n",
      "|    value_loss         | 0.381    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9270000, episode_reward=328.40 +/- 111.76\n",
      "Episode length: 7784.60 +/- 1341.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.78e+03 |\n",
      "|    mean_reward        | 328      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9270000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.79    |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115874   |\n",
      "|    policy_loss        | -0.0518  |\n",
      "|    value_loss         | 0.265    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.24e+03 |\n",
      "|    ep_rew_mean        | 342      |\n",
      "| time/                 |          |\n",
      "|    fps                | 566      |\n",
      "|    iterations         | 115900   |\n",
      "|    time_elapsed       | 16376    |\n",
      "|    total_timesteps    | 9272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.86    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115899   |\n",
      "|    policy_loss        | 0.0157   |\n",
      "|    value_loss         | 0.0948   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9280000, episode_reward=281.20 +/- 120.95\n",
      "Episode length: 7288.20 +/- 1497.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.29e+03 |\n",
      "|    mean_reward        | 281      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.854   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115999   |\n",
      "|    policy_loss        | -0.0374  |\n",
      "|    value_loss         | 0.0412   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.32e+03 |\n",
      "|    ep_rew_mean     | 345      |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 116000   |\n",
      "|    time_elapsed    | 16397    |\n",
      "|    total_timesteps | 9280000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.34e+03 |\n",
      "|    ep_rew_mean        | 344      |\n",
      "| time/                 |          |\n",
      "|    fps                | 566      |\n",
      "|    iterations         | 116100   |\n",
      "|    time_elapsed       | 16404    |\n",
      "|    total_timesteps    | 9288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.831   |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116099   |\n",
      "|    policy_loss        | 0.00543  |\n",
      "|    value_loss         | 0.0641   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9290000, episode_reward=366.20 +/- 85.97\n",
      "Episode length: 8724.40 +/- 1848.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.72e+03 |\n",
      "|    mean_reward        | 366      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9290000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.853   |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116124   |\n",
      "|    policy_loss        | 0.0448   |\n",
      "|    value_loss         | 0.0859   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.43e+03 |\n",
      "|    ep_rew_mean        | 349      |\n",
      "| time/                 |          |\n",
      "|    fps                | 565      |\n",
      "|    iterations         | 116200   |\n",
      "|    time_elapsed       | 16427    |\n",
      "|    total_timesteps    | 9296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.939   |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116199   |\n",
      "|    policy_loss        | -0.0208  |\n",
      "|    value_loss         | 0.0752   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9300000, episode_reward=227.00 +/- 131.94\n",
      "Episode length: 6929.40 +/- 1400.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.93e+03 |\n",
      "|    mean_reward        | 227      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.922   |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116249   |\n",
      "|    policy_loss        | 0.142    |\n",
      "|    value_loss         | 0.223    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.45e+03 |\n",
      "|    ep_rew_mean        | 350      |\n",
      "| time/                 |          |\n",
      "|    fps                | 565      |\n",
      "|    iterations         | 116300   |\n",
      "|    time_elapsed       | 16448    |\n",
      "|    total_timesteps    | 9304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.673   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116299   |\n",
      "|    policy_loss        | -0.0698  |\n",
      "|    value_loss         | 0.234    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9310000, episode_reward=385.80 +/- 44.63\n",
      "Episode length: 10095.80 +/- 2632.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+04 |\n",
      "|    mean_reward        | 386      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9310000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.767   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116374   |\n",
      "|    policy_loss        | -0.127   |\n",
      "|    value_loss         | 0.3      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.46e+03 |\n",
      "|    ep_rew_mean        | 351      |\n",
      "| time/                 |          |\n",
      "|    fps                | 565      |\n",
      "|    iterations         | 116400   |\n",
      "|    time_elapsed       | 16475    |\n",
      "|    total_timesteps    | 9312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.889   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116399   |\n",
      "|    policy_loss        | 0.0351   |\n",
      "|    value_loss         | 0.0861   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9320000, episode_reward=269.20 +/- 105.91\n",
      "Episode length: 7038.80 +/- 1502.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.04e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.89    |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116499   |\n",
      "|    policy_loss        | -0.0159  |\n",
      "|    value_loss         | 0.0934   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.44e+03 |\n",
      "|    ep_rew_mean     | 351      |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 116500   |\n",
      "|    time_elapsed    | 16495    |\n",
      "|    total_timesteps | 9320000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 348      |\n",
      "| time/                 |          |\n",
      "|    fps                | 565      |\n",
      "|    iterations         | 116600   |\n",
      "|    time_elapsed       | 16502    |\n",
      "|    total_timesteps    | 9328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.929   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116599   |\n",
      "|    policy_loss        | -0.0176  |\n",
      "|    value_loss         | 0.0183   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9330000, episode_reward=324.20 +/- 122.66\n",
      "Episode length: 8360.00 +/- 1336.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.36e+03 |\n",
      "|    mean_reward        | 324      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9330000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.928   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116624   |\n",
      "|    policy_loss        | 0.0411   |\n",
      "|    value_loss         | 0.0537   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.43e+03 |\n",
      "|    ep_rew_mean        | 350      |\n",
      "| time/                 |          |\n",
      "|    fps                | 564      |\n",
      "|    iterations         | 116700   |\n",
      "|    time_elapsed       | 16525    |\n",
      "|    total_timesteps    | 9336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.928   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116699   |\n",
      "|    policy_loss        | -0.0238  |\n",
      "|    value_loss         | 0.019    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9340000, episode_reward=313.60 +/- 44.79\n",
      "Episode length: 7744.20 +/- 888.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.74e+03 |\n",
      "|    mean_reward        | 314      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.921   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116749   |\n",
      "|    policy_loss        | 0.0398   |\n",
      "|    value_loss         | 0.0509   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.47e+03 |\n",
      "|    ep_rew_mean        | 350      |\n",
      "| time/                 |          |\n",
      "|    fps                | 564      |\n",
      "|    iterations         | 116800   |\n",
      "|    time_elapsed       | 16547    |\n",
      "|    total_timesteps    | 9344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.928   |\n",
      "|    explained_variance | 0.785    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116799   |\n",
      "|    policy_loss        | -0.0291  |\n",
      "|    value_loss         | 0.137    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9350000, episode_reward=388.40 +/- 12.61\n",
      "Episode length: 8387.20 +/- 811.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.39e+03 |\n",
      "|    mean_reward        | 388      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.859   |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116874   |\n",
      "|    policy_loss        | -0.0561  |\n",
      "|    value_loss         | 0.222    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.5e+03  |\n",
      "|    ep_rew_mean        | 350      |\n",
      "| time/                 |          |\n",
      "|    fps                | 564      |\n",
      "|    iterations         | 116900   |\n",
      "|    time_elapsed       | 16570    |\n",
      "|    total_timesteps    | 9352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.925   |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116899   |\n",
      "|    policy_loss        | -0.0781  |\n",
      "|    value_loss         | 0.097    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9360000, episode_reward=326.80 +/- 45.63\n",
      "Episode length: 10705.80 +/- 4827.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.07e+04 |\n",
      "|    mean_reward        | 327      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.92    |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116999   |\n",
      "|    policy_loss        | 0.0488   |\n",
      "|    value_loss         | 0.258    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.43e+03 |\n",
      "|    ep_rew_mean     | 345      |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 117000   |\n",
      "|    time_elapsed    | 16597    |\n",
      "|    total_timesteps | 9360000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.41e+03 |\n",
      "|    ep_rew_mean        | 345      |\n",
      "| time/                 |          |\n",
      "|    fps                | 564      |\n",
      "|    iterations         | 117100   |\n",
      "|    time_elapsed       | 16604    |\n",
      "|    total_timesteps    | 9368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.864   |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117099   |\n",
      "|    policy_loss        | -0.00837 |\n",
      "|    value_loss         | 0.0345   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9370000, episode_reward=393.20 +/- 38.36\n",
      "Episode length: 9692.60 +/- 1391.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.69e+03 |\n",
      "|    mean_reward        | 393      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9370000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.934   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117124   |\n",
      "|    policy_loss        | 0.0283   |\n",
      "|    value_loss         | 0.0535   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.3e+03  |\n",
      "|    ep_rew_mean        | 337      |\n",
      "| time/                 |          |\n",
      "|    fps                | 563      |\n",
      "|    iterations         | 117200   |\n",
      "|    time_elapsed       | 16630    |\n",
      "|    total_timesteps    | 9376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.895   |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117199   |\n",
      "|    policy_loss        | -0.054   |\n",
      "|    value_loss         | 0.117    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9380000, episode_reward=408.80 +/- 15.78\n",
      "Episode length: 13420.40 +/- 4586.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.34e+04 |\n",
      "|    mean_reward        | 409      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117249   |\n",
      "|    policy_loss        | 0.0403   |\n",
      "|    value_loss         | 0.0355   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.38e+03 |\n",
      "|    ep_rew_mean        | 345      |\n",
      "| time/                 |          |\n",
      "|    fps                | 563      |\n",
      "|    iterations         | 117300   |\n",
      "|    time_elapsed       | 16662    |\n",
      "|    total_timesteps    | 9384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.844   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117299   |\n",
      "|    policy_loss        | 0.0011   |\n",
      "|    value_loss         | 0.0372   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9390000, episode_reward=385.00 +/- 24.52\n",
      "Episode length: 9799.60 +/- 2134.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.8e+03  |\n",
      "|    mean_reward        | 385      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9390000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.94    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117374   |\n",
      "|    policy_loss        | -0.0192  |\n",
      "|    value_loss         | 0.0458   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.36e+03 |\n",
      "|    ep_rew_mean        | 341      |\n",
      "| time/                 |          |\n",
      "|    fps                | 562      |\n",
      "|    iterations         | 117400   |\n",
      "|    time_elapsed       | 16688    |\n",
      "|    total_timesteps    | 9392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.88    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117399   |\n",
      "|    policy_loss        | 0.0243   |\n",
      "|    value_loss         | 0.038    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9400000, episode_reward=259.80 +/- 107.24\n",
      "Episode length: 7427.80 +/- 1429.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.43e+03 |\n",
      "|    mean_reward        | 260      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.801   |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117499   |\n",
      "|    policy_loss        | -0.144   |\n",
      "|    value_loss         | 0.732    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.39e+03 |\n",
      "|    ep_rew_mean     | 342      |\n",
      "| time/              |          |\n",
      "|    fps             | 562      |\n",
      "|    iterations      | 117500   |\n",
      "|    time_elapsed    | 16709    |\n",
      "|    total_timesteps | 9400000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.48e+03 |\n",
      "|    ep_rew_mean        | 346      |\n",
      "| time/                 |          |\n",
      "|    fps                | 562      |\n",
      "|    iterations         | 117600   |\n",
      "|    time_elapsed       | 16716    |\n",
      "|    total_timesteps    | 9408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117599   |\n",
      "|    policy_loss        | -0.0104  |\n",
      "|    value_loss         | 0.0671   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9410000, episode_reward=290.40 +/- 119.81\n",
      "Episode length: 8365.80 +/- 2313.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.37e+03 |\n",
      "|    mean_reward        | 290      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9410000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117624   |\n",
      "|    policy_loss        | -0.021   |\n",
      "|    value_loss         | 0.0217   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.5e+03  |\n",
      "|    ep_rew_mean        | 348      |\n",
      "| time/                 |          |\n",
      "|    fps                | 562      |\n",
      "|    iterations         | 117700   |\n",
      "|    time_elapsed       | 16739    |\n",
      "|    total_timesteps    | 9416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.893   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117699   |\n",
      "|    policy_loss        | 0.0338   |\n",
      "|    value_loss         | 0.0558   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9420000, episode_reward=317.00 +/- 135.57\n",
      "Episode length: 10506.80 +/- 5177.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+04 |\n",
      "|    mean_reward        | 317      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.808   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117749   |\n",
      "|    policy_loss        | -0.0604  |\n",
      "|    value_loss         | 0.0258   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.54e+03 |\n",
      "|    ep_rew_mean        | 350      |\n",
      "| time/                 |          |\n",
      "|    fps                | 562      |\n",
      "|    iterations         | 117800   |\n",
      "|    time_elapsed       | 16766    |\n",
      "|    total_timesteps    | 9424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.818   |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117799   |\n",
      "|    policy_loss        | -0.00998 |\n",
      "|    value_loss         | 0.0654   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9430000, episode_reward=343.00 +/- 117.41\n",
      "Episode length: 8844.00 +/- 1371.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.84e+03 |\n",
      "|    mean_reward        | 343      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9430000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.724   |\n",
      "|    explained_variance | 0.868    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117874   |\n",
      "|    policy_loss        | 0.0212   |\n",
      "|    value_loss         | 0.24     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.54e+03 |\n",
      "|    ep_rew_mean        | 349      |\n",
      "| time/                 |          |\n",
      "|    fps                | 561      |\n",
      "|    iterations         | 117900   |\n",
      "|    time_elapsed       | 16790    |\n",
      "|    total_timesteps    | 9432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.864   |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117899   |\n",
      "|    policy_loss        | 0.0902   |\n",
      "|    value_loss         | 0.137    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9440000, episode_reward=377.20 +/- 36.44\n",
      "Episode length: 10441.80 +/- 1436.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+04 |\n",
      "|    mean_reward        | 377      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.866   |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117999   |\n",
      "|    policy_loss        | 0.0392   |\n",
      "|    value_loss         | 0.0649   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.46e+03 |\n",
      "|    ep_rew_mean     | 345      |\n",
      "| time/              |          |\n",
      "|    fps             | 561      |\n",
      "|    iterations      | 118000   |\n",
      "|    time_elapsed    | 16816    |\n",
      "|    total_timesteps | 9440000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.45e+03 |\n",
      "|    ep_rew_mean        | 345      |\n",
      "| time/                 |          |\n",
      "|    fps                | 561      |\n",
      "|    iterations         | 118100   |\n",
      "|    time_elapsed       | 16823    |\n",
      "|    total_timesteps    | 9448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.879   |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118099   |\n",
      "|    policy_loss        | 0.0494   |\n",
      "|    value_loss         | 0.0903   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9450000, episode_reward=351.40 +/- 50.41\n",
      "Episode length: 12332.80 +/- 6422.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.23e+04 |\n",
      "|    mean_reward        | 351      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.83    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118124   |\n",
      "|    policy_loss        | 0.0117   |\n",
      "|    value_loss         | 0.0324   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.42e+03 |\n",
      "|    ep_rew_mean        | 342      |\n",
      "| time/                 |          |\n",
      "|    fps                | 561      |\n",
      "|    iterations         | 118200   |\n",
      "|    time_elapsed       | 16854    |\n",
      "|    total_timesteps    | 9456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.841   |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118199   |\n",
      "|    policy_loss        | 0.00436  |\n",
      "|    value_loss         | 0.198    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9460000, episode_reward=379.20 +/- 23.51\n",
      "Episode length: 8793.60 +/- 1480.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.79e+03 |\n",
      "|    mean_reward        | 379      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.929   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118249   |\n",
      "|    policy_loss        | 0.0145   |\n",
      "|    value_loss         | 0.0309   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 342      |\n",
      "| time/                 |          |\n",
      "|    fps                | 560      |\n",
      "|    iterations         | 118300   |\n",
      "|    time_elapsed       | 16877    |\n",
      "|    total_timesteps    | 9464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.847   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118299   |\n",
      "|    policy_loss        | -0.0279  |\n",
      "|    value_loss         | 0.0622   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9470000, episode_reward=379.80 +/- 47.91\n",
      "Episode length: 11027.60 +/- 1470.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.1e+04  |\n",
      "|    mean_reward        | 380      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9470000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.874   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118374   |\n",
      "|    policy_loss        | 0.0539   |\n",
      "|    value_loss         | 0.115    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.42e+03 |\n",
      "|    ep_rew_mean        | 342      |\n",
      "| time/                 |          |\n",
      "|    fps                | 560      |\n",
      "|    iterations         | 118400   |\n",
      "|    time_elapsed       | 16905    |\n",
      "|    total_timesteps    | 9472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.896   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118399   |\n",
      "|    policy_loss        | -0.0302  |\n",
      "|    value_loss         | 0.055    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9480000, episode_reward=388.40 +/- 56.68\n",
      "Episode length: 10395.40 +/- 2658.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+04 |\n",
      "|    mean_reward        | 388      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.921   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118499   |\n",
      "|    policy_loss        | -0.0208  |\n",
      "|    value_loss         | 0.0308   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.45e+03 |\n",
      "|    ep_rew_mean     | 344      |\n",
      "| time/              |          |\n",
      "|    fps             | 559      |\n",
      "|    iterations      | 118500   |\n",
      "|    time_elapsed    | 16932    |\n",
      "|    total_timesteps | 9480000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.41e+03 |\n",
      "|    ep_rew_mean        | 345      |\n",
      "| time/                 |          |\n",
      "|    fps                | 560      |\n",
      "|    iterations         | 118600   |\n",
      "|    time_elapsed       | 16939    |\n",
      "|    total_timesteps    | 9488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.874   |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118599   |\n",
      "|    policy_loss        | 0.0761   |\n",
      "|    value_loss         | 0.182    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9490000, episode_reward=374.80 +/- 36.25\n",
      "Episode length: 8314.20 +/- 361.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.31e+03 |\n",
      "|    mean_reward        | 375      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9490000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.673    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118624   |\n",
      "|    policy_loss        | 0.0206   |\n",
      "|    value_loss         | 0.728    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 344      |\n",
      "| time/                 |          |\n",
      "|    fps                | 559      |\n",
      "|    iterations         | 118700   |\n",
      "|    time_elapsed       | 16962    |\n",
      "|    total_timesteps    | 9496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.876   |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118699   |\n",
      "|    policy_loss        | -0.0528  |\n",
      "|    value_loss         | 0.051    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9500000, episode_reward=377.60 +/- 46.14\n",
      "Episode length: 9514.40 +/- 847.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.51e+03 |\n",
      "|    mean_reward        | 378      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.901   |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118749   |\n",
      "|    policy_loss        | 0.055    |\n",
      "|    value_loss         | 0.131    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.33e+03 |\n",
      "|    ep_rew_mean        | 341      |\n",
      "| time/                 |          |\n",
      "|    fps                | 559      |\n",
      "|    iterations         | 118800   |\n",
      "|    time_elapsed       | 16987    |\n",
      "|    total_timesteps    | 9504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.923   |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118799   |\n",
      "|    policy_loss        | 0.0317   |\n",
      "|    value_loss         | 0.0727   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9510000, episode_reward=382.20 +/- 48.87\n",
      "Episode length: 8755.60 +/- 1340.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.76e+03 |\n",
      "|    mean_reward        | 382      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9510000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.925   |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118874   |\n",
      "|    policy_loss        | -0.0245  |\n",
      "|    value_loss         | 0.142    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.33e+03 |\n",
      "|    ep_rew_mean        | 341      |\n",
      "| time/                 |          |\n",
      "|    fps                | 559      |\n",
      "|    iterations         | 118900   |\n",
      "|    time_elapsed       | 17010    |\n",
      "|    total_timesteps    | 9512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.761   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118899   |\n",
      "|    policy_loss        | -0.0291  |\n",
      "|    value_loss         | 0.0232   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9520000, episode_reward=413.80 +/- 19.04\n",
      "Episode length: 11157.80 +/- 1197.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 414      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.949   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118999   |\n",
      "|    policy_loss        | -0.048   |\n",
      "|    value_loss         | 0.0214   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.34e+03 |\n",
      "|    ep_rew_mean     | 340      |\n",
      "| time/              |          |\n",
      "|    fps             | 558      |\n",
      "|    iterations      | 119000   |\n",
      "|    time_elapsed    | 17039    |\n",
      "|    total_timesteps | 9520000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.33e+03 |\n",
      "|    ep_rew_mean        | 338      |\n",
      "| time/                 |          |\n",
      "|    fps                | 558      |\n",
      "|    iterations         | 119100   |\n",
      "|    time_elapsed       | 17046    |\n",
      "|    total_timesteps    | 9528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.832   |\n",
      "|    explained_variance | 0.629    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119099   |\n",
      "|    policy_loss        | -0.127   |\n",
      "|    value_loss         | 0.754    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9530000, episode_reward=393.00 +/- 43.71\n",
      "Episode length: 9669.40 +/- 1464.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.67e+03 |\n",
      "|    mean_reward        | 393      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9530000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.848   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119124   |\n",
      "|    policy_loss        | 0.0487   |\n",
      "|    value_loss         | 0.132    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 343      |\n",
      "| time/                 |          |\n",
      "|    fps                | 558      |\n",
      "|    iterations         | 119200   |\n",
      "|    time_elapsed       | 17071    |\n",
      "|    total_timesteps    | 9536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.882   |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119199   |\n",
      "|    policy_loss        | -0.406   |\n",
      "|    value_loss         | 0.915    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9540000, episode_reward=371.40 +/- 50.19\n",
      "Episode length: 8513.00 +/- 1454.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.51e+03 |\n",
      "|    mean_reward        | 371      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.904   |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119249   |\n",
      "|    policy_loss        | 0.0346   |\n",
      "|    value_loss         | 0.205    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 344      |\n",
      "| time/                 |          |\n",
      "|    fps                | 558      |\n",
      "|    iterations         | 119300   |\n",
      "|    time_elapsed       | 17095    |\n",
      "|    total_timesteps    | 9544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.881   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119299   |\n",
      "|    policy_loss        | -0.0326  |\n",
      "|    value_loss         | 0.0329   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9550000, episode_reward=330.60 +/- 111.82\n",
      "Episode length: 7949.00 +/- 1639.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.95e+03 |\n",
      "|    mean_reward        | 331      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.845   |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119374   |\n",
      "|    policy_loss        | 0.0386   |\n",
      "|    value_loss         | 0.0405   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.43e+03 |\n",
      "|    ep_rew_mean        | 345      |\n",
      "| time/                 |          |\n",
      "|    fps                | 558      |\n",
      "|    iterations         | 119400   |\n",
      "|    time_elapsed       | 17118    |\n",
      "|    total_timesteps    | 9552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.908   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119399   |\n",
      "|    policy_loss        | 0.0314   |\n",
      "|    value_loss         | 0.0702   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9560000, episode_reward=338.80 +/- 132.42\n",
      "Episode length: 11339.80 +/- 4459.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+04 |\n",
      "|    mean_reward        | 339      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.981   |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119499   |\n",
      "|    policy_loss        | -0.0438  |\n",
      "|    value_loss         | 0.0817   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.38e+03 |\n",
      "|    ep_rew_mean     | 342      |\n",
      "| time/              |          |\n",
      "|    fps             | 557      |\n",
      "|    iterations      | 119500   |\n",
      "|    time_elapsed    | 17146    |\n",
      "|    total_timesteps | 9560000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.55e+03 |\n",
      "|    ep_rew_mean        | 345      |\n",
      "| time/                 |          |\n",
      "|    fps                | 557      |\n",
      "|    iterations         | 119600   |\n",
      "|    time_elapsed       | 17153    |\n",
      "|    total_timesteps    | 9568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.936   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119599   |\n",
      "|    policy_loss        | -0.0553  |\n",
      "|    value_loss         | 0.0535   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9570000, episode_reward=380.40 +/- 32.67\n",
      "Episode length: 10109.80 +/- 2428.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+04 |\n",
      "|    mean_reward        | 380      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9570000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.947   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119624   |\n",
      "|    policy_loss        | -0.0516  |\n",
      "|    value_loss         | 0.0409   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.62e+03 |\n",
      "|    ep_rew_mean        | 347      |\n",
      "| time/                 |          |\n",
      "|    fps                | 557      |\n",
      "|    iterations         | 119700   |\n",
      "|    time_elapsed       | 17180    |\n",
      "|    total_timesteps    | 9576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.914   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119699   |\n",
      "|    policy_loss        | 0.0214   |\n",
      "|    value_loss         | 0.0191   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9580000, episode_reward=389.20 +/- 12.25\n",
      "Episode length: 14573.60 +/- 9607.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.46e+04 |\n",
      "|    mean_reward        | 389      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.823   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119749   |\n",
      "|    policy_loss        | -0.005   |\n",
      "|    value_loss         | 0.0468   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.62e+03 |\n",
      "|    ep_rew_mean        | 347      |\n",
      "| time/                 |          |\n",
      "|    fps                | 556      |\n",
      "|    iterations         | 119800   |\n",
      "|    time_elapsed       | 17214    |\n",
      "|    total_timesteps    | 9584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.81    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119799   |\n",
      "|    policy_loss        | 0.0147   |\n",
      "|    value_loss         | 0.0334   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9590000, episode_reward=204.60 +/- 66.77\n",
      "Episode length: 6881.40 +/- 408.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.88e+03 |\n",
      "|    mean_reward        | 205      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9590000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.979   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119874   |\n",
      "|    policy_loss        | 0.0708   |\n",
      "|    value_loss         | 0.0541   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.74e+03 |\n",
      "|    ep_rew_mean        | 352      |\n",
      "| time/                 |          |\n",
      "|    fps                | 556      |\n",
      "|    iterations         | 119900   |\n",
      "|    time_elapsed       | 17234    |\n",
      "|    total_timesteps    | 9592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.915   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119899   |\n",
      "|    policy_loss        | -0.0306  |\n",
      "|    value_loss         | 0.0151   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9600000, episode_reward=341.00 +/- 46.75\n",
      "Episode length: 8814.20 +/- 1498.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.81e+03 |\n",
      "|    mean_reward        | 341      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.842   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119999   |\n",
      "|    policy_loss        | 0.0363   |\n",
      "|    value_loss         | 0.0218   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.76e+03 |\n",
      "|    ep_rew_mean     | 353      |\n",
      "| time/              |          |\n",
      "|    fps             | 556      |\n",
      "|    iterations      | 120000   |\n",
      "|    time_elapsed    | 17258    |\n",
      "|    total_timesteps | 9600000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.69e+03 |\n",
      "|    ep_rew_mean        | 352      |\n",
      "| time/                 |          |\n",
      "|    fps                | 556      |\n",
      "|    iterations         | 120100   |\n",
      "|    time_elapsed       | 17265    |\n",
      "|    total_timesteps    | 9608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.807   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120099   |\n",
      "|    policy_loss        | 0.017    |\n",
      "|    value_loss         | 0.0383   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9610000, episode_reward=363.60 +/- 59.90\n",
      "Episode length: 9334.20 +/- 1571.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.33e+03 |\n",
      "|    mean_reward        | 364      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9610000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.824   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120124   |\n",
      "|    policy_loss        | -0.0132  |\n",
      "|    value_loss         | 0.0253   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.7e+03  |\n",
      "|    ep_rew_mean        | 352      |\n",
      "| time/                 |          |\n",
      "|    fps                | 556      |\n",
      "|    iterations         | 120200   |\n",
      "|    time_elapsed       | 17290    |\n",
      "|    total_timesteps    | 9616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.854   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120199   |\n",
      "|    policy_loss        | 0.0381   |\n",
      "|    value_loss         | 0.047    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9620000, episode_reward=314.00 +/- 85.33\n",
      "Episode length: 8052.80 +/- 1692.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.05e+03 |\n",
      "|    mean_reward        | 314      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9620000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.876   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120249   |\n",
      "|    policy_loss        | 0.00988  |\n",
      "|    value_loss         | 0.086    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.61e+03 |\n",
      "|    ep_rew_mean        | 349      |\n",
      "| time/                 |          |\n",
      "|    fps                | 555      |\n",
      "|    iterations         | 120300   |\n",
      "|    time_elapsed       | 17313    |\n",
      "|    total_timesteps    | 9624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120299   |\n",
      "|    policy_loss        | -0.054   |\n",
      "|    value_loss         | 0.0364   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9630000, episode_reward=330.00 +/- 67.21\n",
      "Episode length: 8919.00 +/- 1290.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.92e+03 |\n",
      "|    mean_reward        | 330      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9630000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.872   |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120374   |\n",
      "|    policy_loss        | 0.0264   |\n",
      "|    value_loss         | 0.0565   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.56e+03 |\n",
      "|    ep_rew_mean        | 348      |\n",
      "| time/                 |          |\n",
      "|    fps                | 555      |\n",
      "|    iterations         | 120400   |\n",
      "|    time_elapsed       | 17337    |\n",
      "|    total_timesteps    | 9632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.85    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120399   |\n",
      "|    policy_loss        | -0.00501 |\n",
      "|    value_loss         | 0.018    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9640000, episode_reward=387.40 +/- 36.41\n",
      "Episode length: 9084.80 +/- 1918.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.08e+03 |\n",
      "|    mean_reward        | 387      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.801   |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120499   |\n",
      "|    policy_loss        | 0.000904 |\n",
      "|    value_loss         | 0.0805   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.56e+03 |\n",
      "|    ep_rew_mean     | 345      |\n",
      "| time/              |          |\n",
      "|    fps             | 555      |\n",
      "|    iterations      | 120500   |\n",
      "|    time_elapsed    | 17362    |\n",
      "|    total_timesteps | 9640000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 341      |\n",
      "| time/                 |          |\n",
      "|    fps                | 555      |\n",
      "|    iterations         | 120600   |\n",
      "|    time_elapsed       | 17369    |\n",
      "|    total_timesteps    | 9648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.819   |\n",
      "|    explained_variance | 0.55     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120599   |\n",
      "|    policy_loss        | -0.223   |\n",
      "|    value_loss         | 1.13     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9650000, episode_reward=305.40 +/- 120.00\n",
      "Episode length: 8248.60 +/- 2306.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.25e+03 |\n",
      "|    mean_reward        | 305      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.884   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120624   |\n",
      "|    policy_loss        | 0.000119 |\n",
      "|    value_loss         | 0.036    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.53e+03 |\n",
      "|    ep_rew_mean        | 342      |\n",
      "| time/                 |          |\n",
      "|    fps                | 555      |\n",
      "|    iterations         | 120700   |\n",
      "|    time_elapsed       | 17393    |\n",
      "|    total_timesteps    | 9656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.838   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120699   |\n",
      "|    policy_loss        | 0.0184   |\n",
      "|    value_loss         | 0.0273   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9660000, episode_reward=323.20 +/- 109.81\n",
      "Episode length: 9185.80 +/- 2016.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.19e+03 |\n",
      "|    mean_reward        | 323      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.744   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120749   |\n",
      "|    policy_loss        | -0.00678 |\n",
      "|    value_loss         | 0.0431   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.5e+03  |\n",
      "|    ep_rew_mean        | 340      |\n",
      "| time/                 |          |\n",
      "|    fps                | 554      |\n",
      "|    iterations         | 120800   |\n",
      "|    time_elapsed       | 17417    |\n",
      "|    total_timesteps    | 9664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.96    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120799   |\n",
      "|    policy_loss        | -0.031   |\n",
      "|    value_loss         | 0.0636   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9670000, episode_reward=304.60 +/- 97.00\n",
      "Episode length: 7680.80 +/- 1599.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.68e+03 |\n",
      "|    mean_reward        | 305      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9670000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120874   |\n",
      "|    policy_loss        | 0.122    |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.55e+03 |\n",
      "|    ep_rew_mean        | 342      |\n",
      "| time/                 |          |\n",
      "|    fps                | 554      |\n",
      "|    iterations         | 120900   |\n",
      "|    time_elapsed       | 17439    |\n",
      "|    total_timesteps    | 9672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.884   |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120899   |\n",
      "|    policy_loss        | -0.0653  |\n",
      "|    value_loss         | 0.115    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9680000, episode_reward=391.80 +/- 48.86\n",
      "Episode length: 9595.20 +/- 2757.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.6e+03  |\n",
      "|    mean_reward        | 392      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.87    |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120999   |\n",
      "|    policy_loss        | -0.257   |\n",
      "|    value_loss         | 0.371    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.58e+03 |\n",
      "|    ep_rew_mean     | 343      |\n",
      "| time/              |          |\n",
      "|    fps             | 554      |\n",
      "|    iterations      | 121000   |\n",
      "|    time_elapsed    | 17463    |\n",
      "|    total_timesteps | 9680000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.52e+03 |\n",
      "|    ep_rew_mean        | 341      |\n",
      "| time/                 |          |\n",
      "|    fps                | 554      |\n",
      "|    iterations         | 121100   |\n",
      "|    time_elapsed       | 17470    |\n",
      "|    total_timesteps    | 9688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.956   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121099   |\n",
      "|    policy_loss        | 0.0178   |\n",
      "|    value_loss         | 0.0783   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9690000, episode_reward=332.00 +/- 54.09\n",
      "Episode length: 7339.80 +/- 1203.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.34e+03 |\n",
      "|    mean_reward        | 332      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9690000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.954   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121124   |\n",
      "|    policy_loss        | -0.0473  |\n",
      "|    value_loss         | 0.0669   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.5e+03  |\n",
      "|    ep_rew_mean        | 339      |\n",
      "| time/                 |          |\n",
      "|    fps                | 554      |\n",
      "|    iterations         | 121200   |\n",
      "|    time_elapsed       | 17491    |\n",
      "|    total_timesteps    | 9696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.782   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121199   |\n",
      "|    policy_loss        | 0.045    |\n",
      "|    value_loss         | 0.0501   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9700000, episode_reward=282.80 +/- 113.65\n",
      "Episode length: 7367.80 +/- 1461.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.37e+03 |\n",
      "|    mean_reward        | 283      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121249   |\n",
      "|    policy_loss        | -0.0439  |\n",
      "|    value_loss         | 0.0265   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.56e+03 |\n",
      "|    ep_rew_mean        | 339      |\n",
      "| time/                 |          |\n",
      "|    fps                | 554      |\n",
      "|    iterations         | 121300   |\n",
      "|    time_elapsed       | 17512    |\n",
      "|    total_timesteps    | 9704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.962   |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121299   |\n",
      "|    policy_loss        | -0.168   |\n",
      "|    value_loss         | 0.282    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9710000, episode_reward=366.00 +/- 63.68\n",
      "Episode length: 9869.20 +/- 2954.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.87e+03 |\n",
      "|    mean_reward        | 366      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9710000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.902   |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121374   |\n",
      "|    policy_loss        | -0.0971  |\n",
      "|    value_loss         | 0.0352   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.54e+03 |\n",
      "|    ep_rew_mean        | 336      |\n",
      "| time/                 |          |\n",
      "|    fps                | 553      |\n",
      "|    iterations         | 121400   |\n",
      "|    time_elapsed       | 17538    |\n",
      "|    total_timesteps    | 9712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.989   |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121399   |\n",
      "|    policy_loss        | -0.0841  |\n",
      "|    value_loss         | 0.0997   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9720000, episode_reward=152.00 +/- 123.21\n",
      "Episode length: 6501.80 +/- 2057.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.5e+03  |\n",
      "|    mean_reward        | 152      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.803   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121499   |\n",
      "|    policy_loss        | 0.0418   |\n",
      "|    value_loss         | 0.0236   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.54e+03 |\n",
      "|    ep_rew_mean     | 334      |\n",
      "| time/              |          |\n",
      "|    fps             | 553      |\n",
      "|    iterations      | 121500   |\n",
      "|    time_elapsed    | 17557    |\n",
      "|    total_timesteps | 9720000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.61e+03 |\n",
      "|    ep_rew_mean        | 335      |\n",
      "| time/                 |          |\n",
      "|    fps                | 553      |\n",
      "|    iterations         | 121600   |\n",
      "|    time_elapsed       | 17564    |\n",
      "|    total_timesteps    | 9728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.818   |\n",
      "|    explained_variance | 0.905    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121599   |\n",
      "|    policy_loss        | 0.244    |\n",
      "|    value_loss         | 0.222    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9730000, episode_reward=393.00 +/- 15.76\n",
      "Episode length: 8072.20 +/- 515.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.07e+03 |\n",
      "|    mean_reward        | 393      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9730000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.903   |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121624   |\n",
      "|    policy_loss        | -0.0303  |\n",
      "|    value_loss         | 0.296    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.6e+03  |\n",
      "|    ep_rew_mean        | 334      |\n",
      "| time/                 |          |\n",
      "|    fps                | 553      |\n",
      "|    iterations         | 121700   |\n",
      "|    time_elapsed       | 17587    |\n",
      "|    total_timesteps    | 9736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.788   |\n",
      "|    explained_variance | 0.65     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121699   |\n",
      "|    policy_loss        | -0.185   |\n",
      "|    value_loss         | 1.21     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9740000, episode_reward=366.20 +/- 39.20\n",
      "Episode length: 8772.80 +/- 974.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.77e+03 |\n",
      "|    mean_reward        | 366      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.973   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121749   |\n",
      "|    policy_loss        | -0.0251  |\n",
      "|    value_loss         | 0.0625   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.62e+03 |\n",
      "|    ep_rew_mean        | 336      |\n",
      "| time/                 |          |\n",
      "|    fps                | 553      |\n",
      "|    iterations         | 121800   |\n",
      "|    time_elapsed       | 17611    |\n",
      "|    total_timesteps    | 9744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.928   |\n",
      "|    explained_variance | 0.816    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121799   |\n",
      "|    policy_loss        | -0.408   |\n",
      "|    value_loss         | 0.872    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9750000, episode_reward=388.20 +/- 26.80\n",
      "Episode length: 10450.80 +/- 731.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+04 |\n",
      "|    mean_reward        | 388      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.981   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121874   |\n",
      "|    policy_loss        | -0.00104 |\n",
      "|    value_loss         | 0.0519   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.64e+03 |\n",
      "|    ep_rew_mean        | 337      |\n",
      "| time/                 |          |\n",
      "|    fps                | 552      |\n",
      "|    iterations         | 121900   |\n",
      "|    time_elapsed       | 17638    |\n",
      "|    total_timesteps    | 9752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.965   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121899   |\n",
      "|    policy_loss        | 0.0947   |\n",
      "|    value_loss         | 0.0741   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9760000, episode_reward=332.60 +/- 104.69\n",
      "Episode length: 8815.40 +/- 1509.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.82e+03 |\n",
      "|    mean_reward        | 333      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.958   |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121999   |\n",
      "|    policy_loss        | 0.0624   |\n",
      "|    value_loss         | 0.0454   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.65e+03 |\n",
      "|    ep_rew_mean     | 337      |\n",
      "| time/              |          |\n",
      "|    fps             | 552      |\n",
      "|    iterations      | 122000   |\n",
      "|    time_elapsed    | 17662    |\n",
      "|    total_timesteps | 9760000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.6e+03  |\n",
      "|    ep_rew_mean        | 334      |\n",
      "| time/                 |          |\n",
      "|    fps                | 552      |\n",
      "|    iterations         | 122100   |\n",
      "|    time_elapsed       | 17669    |\n",
      "|    total_timesteps    | 9768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.835   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122099   |\n",
      "|    policy_loss        | 0.0188   |\n",
      "|    value_loss         | 0.0261   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9770000, episode_reward=393.60 +/- 23.98\n",
      "Episode length: 9605.60 +/- 1606.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.61e+03 |\n",
      "|    mean_reward        | 394      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9770000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.931   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122124   |\n",
      "|    policy_loss        | -0.0367  |\n",
      "|    value_loss         | 0.113    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.63e+03 |\n",
      "|    ep_rew_mean        | 335      |\n",
      "| time/                 |          |\n",
      "|    fps                | 552      |\n",
      "|    iterations         | 122200   |\n",
      "|    time_elapsed       | 17694    |\n",
      "|    total_timesteps    | 9776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.866   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122199   |\n",
      "|    policy_loss        | -0.0721  |\n",
      "|    value_loss         | 0.0688   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9780000, episode_reward=401.20 +/- 20.53\n",
      "Episode length: 8750.80 +/- 784.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.75e+03 |\n",
      "|    mean_reward        | 401      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.929   |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122249   |\n",
      "|    policy_loss        | -0.0307  |\n",
      "|    value_loss         | 0.0718   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.52e+03 |\n",
      "|    ep_rew_mean        | 334      |\n",
      "| time/                 |          |\n",
      "|    fps                | 552      |\n",
      "|    iterations         | 122300   |\n",
      "|    time_elapsed       | 17717    |\n",
      "|    total_timesteps    | 9784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.893   |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122299   |\n",
      "|    policy_loss        | 0.0944   |\n",
      "|    value_loss         | 0.0755   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9790000, episode_reward=374.60 +/- 69.36\n",
      "Episode length: 10518.00 +/- 3026.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+04 |\n",
      "|    mean_reward        | 375      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9790000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.955   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122374   |\n",
      "|    policy_loss        | 0.0259   |\n",
      "|    value_loss         | 0.0323   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.54e+03 |\n",
      "|    ep_rew_mean        | 337      |\n",
      "| time/                 |          |\n",
      "|    fps                | 551      |\n",
      "|    iterations         | 122400   |\n",
      "|    time_elapsed       | 17744    |\n",
      "|    total_timesteps    | 9792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.964   |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122399   |\n",
      "|    policy_loss        | -0.0103  |\n",
      "|    value_loss         | 0.111    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9800000, episode_reward=368.40 +/- 59.22\n",
      "Episode length: 8790.20 +/- 1272.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.79e+03 |\n",
      "|    mean_reward        | 368      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.965   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122499   |\n",
      "|    policy_loss        | 0.0501   |\n",
      "|    value_loss         | 0.0213   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.56e+03 |\n",
      "|    ep_rew_mean     | 337      |\n",
      "| time/              |          |\n",
      "|    fps             | 551      |\n",
      "|    iterations      | 122500   |\n",
      "|    time_elapsed    | 17768    |\n",
      "|    total_timesteps | 9800000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.55e+03 |\n",
      "|    ep_rew_mean        | 337      |\n",
      "| time/                 |          |\n",
      "|    fps                | 551      |\n",
      "|    iterations         | 122600   |\n",
      "|    time_elapsed       | 17775    |\n",
      "|    total_timesteps    | 9808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.941   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122599   |\n",
      "|    policy_loss        | -0.102   |\n",
      "|    value_loss         | 0.107    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9810000, episode_reward=371.60 +/- 65.70\n",
      "Episode length: 8823.40 +/- 1262.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.82e+03 |\n",
      "|    mean_reward        | 372      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9810000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.906   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122624   |\n",
      "|    policy_loss        | -0.0153  |\n",
      "|    value_loss         | 0.0408   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.73e+03 |\n",
      "|    ep_rew_mean        | 341      |\n",
      "| time/                 |          |\n",
      "|    fps                | 551      |\n",
      "|    iterations         | 122700   |\n",
      "|    time_elapsed       | 17799    |\n",
      "|    total_timesteps    | 9816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.885   |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122699   |\n",
      "|    policy_loss        | 0.00244  |\n",
      "|    value_loss         | 0.0882   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9820000, episode_reward=398.20 +/- 25.01\n",
      "Episode length: 11480.60 +/- 4214.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.15e+04 |\n",
      "|    mean_reward        | 398      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.92    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122749   |\n",
      "|    policy_loss        | 0.0382   |\n",
      "|    value_loss         | 0.0989   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.79e+03 |\n",
      "|    ep_rew_mean        | 344      |\n",
      "| time/                 |          |\n",
      "|    fps                | 551      |\n",
      "|    iterations         | 122800   |\n",
      "|    time_elapsed       | 17828    |\n",
      "|    total_timesteps    | 9824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122799   |\n",
      "|    policy_loss        | -0.044   |\n",
      "|    value_loss         | 0.0365   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9830000, episode_reward=333.20 +/- 70.92\n",
      "Episode length: 7801.60 +/- 1137.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.8e+03  |\n",
      "|    mean_reward        | 333      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9830000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.968   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122874   |\n",
      "|    policy_loss        | 0.0218   |\n",
      "|    value_loss         | 0.048    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.84e+03 |\n",
      "|    ep_rew_mean        | 346      |\n",
      "| time/                 |          |\n",
      "|    fps                | 550      |\n",
      "|    iterations         | 122900   |\n",
      "|    time_elapsed       | 17849    |\n",
      "|    total_timesteps    | 9832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.809   |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122899   |\n",
      "|    policy_loss        | 0.05     |\n",
      "|    value_loss         | 0.908    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9840000, episode_reward=358.40 +/- 28.65\n",
      "Episode length: 8163.20 +/- 598.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.16e+03 |\n",
      "|    mean_reward        | 358      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.933   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122999   |\n",
      "|    policy_loss        | -0.0745  |\n",
      "|    value_loss         | 0.1      |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.95e+03 |\n",
      "|    ep_rew_mean     | 349      |\n",
      "| time/              |          |\n",
      "|    fps             | 550      |\n",
      "|    iterations      | 123000   |\n",
      "|    time_elapsed    | 17872    |\n",
      "|    total_timesteps | 9840000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.94e+03 |\n",
      "|    ep_rew_mean        | 346      |\n",
      "| time/                 |          |\n",
      "|    fps                | 550      |\n",
      "|    iterations         | 123100   |\n",
      "|    time_elapsed       | 17879    |\n",
      "|    total_timesteps    | 9848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.841   |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123099   |\n",
      "|    policy_loss        | -0.111   |\n",
      "|    value_loss         | 0.142    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9850000, episode_reward=393.00 +/- 23.02\n",
      "Episode length: 9589.60 +/- 952.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.59e+03 |\n",
      "|    mean_reward        | 393      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9850000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.99    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123124   |\n",
      "|    policy_loss        | 0.0294   |\n",
      "|    value_loss         | 0.0376   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.1e+03  |\n",
      "|    ep_rew_mean        | 349      |\n",
      "| time/                 |          |\n",
      "|    fps                | 550      |\n",
      "|    iterations         | 123200   |\n",
      "|    time_elapsed       | 17904    |\n",
      "|    total_timesteps    | 9856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.929   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123199   |\n",
      "|    policy_loss        | -0.0282  |\n",
      "|    value_loss         | 0.0307   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9860000, episode_reward=399.20 +/- 32.34\n",
      "Episode length: 12988.20 +/- 5056.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.3e+04  |\n",
      "|    mean_reward        | 399      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9860000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.897   |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123249   |\n",
      "|    policy_loss        | 0.000466 |\n",
      "|    value_loss         | 0.105    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.06e+03 |\n",
      "|    ep_rew_mean        | 343      |\n",
      "| time/                 |          |\n",
      "|    fps                | 549      |\n",
      "|    iterations         | 123300   |\n",
      "|    time_elapsed       | 17936    |\n",
      "|    total_timesteps    | 9864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.943   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123299   |\n",
      "|    policy_loss        | -0.0581  |\n",
      "|    value_loss         | 0.0482   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9870000, episode_reward=365.80 +/- 92.32\n",
      "Episode length: 9896.00 +/- 2803.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.9e+03  |\n",
      "|    mean_reward        | 366      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9870000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.863   |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123374   |\n",
      "|    policy_loss        | 0.114    |\n",
      "|    value_loss         | 0.273    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.14e+03 |\n",
      "|    ep_rew_mean        | 348      |\n",
      "| time/                 |          |\n",
      "|    fps                | 549      |\n",
      "|    iterations         | 123400   |\n",
      "|    time_elapsed       | 17962    |\n",
      "|    total_timesteps    | 9872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.921   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123399   |\n",
      "|    policy_loss        | -0.0808  |\n",
      "|    value_loss         | 0.0796   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9880000, episode_reward=352.80 +/- 50.35\n",
      "Episode length: 7402.00 +/- 1166.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.4e+03  |\n",
      "|    mean_reward        | 353      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.883   |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123499   |\n",
      "|    policy_loss        | -0.0455  |\n",
      "|    value_loss         | 0.094    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.18e+03 |\n",
      "|    ep_rew_mean     | 352      |\n",
      "| time/              |          |\n",
      "|    fps             | 549      |\n",
      "|    iterations      | 123500   |\n",
      "|    time_elapsed    | 17983    |\n",
      "|    total_timesteps | 9880000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.17e+03 |\n",
      "|    ep_rew_mean        | 353      |\n",
      "| time/                 |          |\n",
      "|    fps                | 549      |\n",
      "|    iterations         | 123600   |\n",
      "|    time_elapsed       | 17990    |\n",
      "|    total_timesteps    | 9888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.84    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123599   |\n",
      "|    policy_loss        | 0.0476   |\n",
      "|    value_loss         | 0.0894   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9890000, episode_reward=338.40 +/- 55.51\n",
      "Episode length: 7496.40 +/- 734.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.5e+03  |\n",
      "|    mean_reward        | 338      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9890000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.951   |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123624   |\n",
      "|    policy_loss        | -0.0348  |\n",
      "|    value_loss         | 0.125    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.16e+03 |\n",
      "|    ep_rew_mean        | 352      |\n",
      "| time/                 |          |\n",
      "|    fps                | 549      |\n",
      "|    iterations         | 123700   |\n",
      "|    time_elapsed       | 18011    |\n",
      "|    total_timesteps    | 9896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123699   |\n",
      "|    policy_loss        | -0.0229  |\n",
      "|    value_loss         | 0.0725   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9900000, episode_reward=287.20 +/- 123.26\n",
      "Episode length: 9466.60 +/- 5807.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.47e+03 |\n",
      "|    mean_reward        | 287      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9900000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.856   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123749   |\n",
      "|    policy_loss        | 0.0482   |\n",
      "|    value_loss         | 0.206    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.19e+03 |\n",
      "|    ep_rew_mean        | 352      |\n",
      "| time/                 |          |\n",
      "|    fps                | 549      |\n",
      "|    iterations         | 123800   |\n",
      "|    time_elapsed       | 18036    |\n",
      "|    total_timesteps    | 9904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.896   |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123799   |\n",
      "|    policy_loss        | -0.0772  |\n",
      "|    value_loss         | 0.0763   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9910000, episode_reward=471.20 +/- 144.64\n",
      "Episode length: 12364.60 +/- 2755.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.24e+04 |\n",
      "|    mean_reward        | 471      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9910000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.947   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123874   |\n",
      "|    policy_loss        | 0.0812   |\n",
      "|    value_loss         | 0.0884   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.21e+03 |\n",
      "|    ep_rew_mean        | 353      |\n",
      "| time/                 |          |\n",
      "|    fps                | 548      |\n",
      "|    iterations         | 123900   |\n",
      "|    time_elapsed       | 18067    |\n",
      "|    total_timesteps    | 9912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.777   |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123899   |\n",
      "|    policy_loss        | -0.0196  |\n",
      "|    value_loss         | 0.369    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9920000, episode_reward=374.60 +/- 53.27\n",
      "Episode length: 9818.60 +/- 1209.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.82e+03 |\n",
      "|    mean_reward        | 375      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.855   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123999   |\n",
      "|    policy_loss        | 0.0353   |\n",
      "|    value_loss         | 0.0256   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.28e+03 |\n",
      "|    ep_rew_mean     | 356      |\n",
      "| time/              |          |\n",
      "|    fps             | 548      |\n",
      "|    iterations      | 124000   |\n",
      "|    time_elapsed    | 18092    |\n",
      "|    total_timesteps | 9920000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.36e+03 |\n",
      "|    ep_rew_mean        | 358      |\n",
      "| time/                 |          |\n",
      "|    fps                | 548      |\n",
      "|    iterations         | 124100   |\n",
      "|    time_elapsed       | 18099    |\n",
      "|    total_timesteps    | 9928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.882   |\n",
      "|    explained_variance | 0.891    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124099   |\n",
      "|    policy_loss        | -0.0231  |\n",
      "|    value_loss         | 0.368    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9930000, episode_reward=331.60 +/- 60.42\n",
      "Episode length: 8249.00 +/- 1918.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.25e+03 |\n",
      "|    mean_reward        | 332      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9930000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.958   |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124124   |\n",
      "|    policy_loss        | 0.036    |\n",
      "|    value_loss         | 0.0731   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.38e+03 |\n",
      "|    ep_rew_mean        | 357      |\n",
      "| time/                 |          |\n",
      "|    fps                | 548      |\n",
      "|    iterations         | 124200   |\n",
      "|    time_elapsed       | 18122    |\n",
      "|    total_timesteps    | 9936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.967   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124199   |\n",
      "|    policy_loss        | 0.0302   |\n",
      "|    value_loss         | 0.107    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9940000, episode_reward=396.60 +/- 33.35\n",
      "Episode length: 9539.80 +/- 661.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.54e+03 |\n",
      "|    mean_reward        | 397      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.896   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124249   |\n",
      "|    policy_loss        | -0.031   |\n",
      "|    value_loss         | 0.0191   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.42e+03 |\n",
      "|    ep_rew_mean        | 359      |\n",
      "| time/                 |          |\n",
      "|    fps                | 547      |\n",
      "|    iterations         | 124300   |\n",
      "|    time_elapsed       | 18147    |\n",
      "|    total_timesteps    | 9944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.948   |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124299   |\n",
      "|    policy_loss        | 0.0229   |\n",
      "|    value_loss         | 0.144    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9950000, episode_reward=347.80 +/- 53.58\n",
      "Episode length: 8698.00 +/- 1393.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.7e+03  |\n",
      "|    mean_reward        | 348      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9950000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.93    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124374   |\n",
      "|    policy_loss        | -0.094   |\n",
      "|    value_loss         | 0.0989   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.45e+03 |\n",
      "|    ep_rew_mean        | 361      |\n",
      "| time/                 |          |\n",
      "|    fps                | 547      |\n",
      "|    iterations         | 124400   |\n",
      "|    time_elapsed       | 18170    |\n",
      "|    total_timesteps    | 9952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.899   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124399   |\n",
      "|    policy_loss        | -0.0109  |\n",
      "|    value_loss         | 0.0599   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9960000, episode_reward=267.00 +/- 134.82\n",
      "Episode length: 8226.00 +/- 2687.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.23e+03 |\n",
      "|    mean_reward        | 267      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.935   |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124499   |\n",
      "|    policy_loss        | -0.0474  |\n",
      "|    value_loss         | 0.156    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.52e+03 |\n",
      "|    ep_rew_mean     | 363      |\n",
      "| time/              |          |\n",
      "|    fps             | 547      |\n",
      "|    iterations      | 124500   |\n",
      "|    time_elapsed    | 18193    |\n",
      "|    total_timesteps | 9960000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.49e+03 |\n",
      "|    ep_rew_mean        | 363      |\n",
      "| time/                 |          |\n",
      "|    fps                | 547      |\n",
      "|    iterations         | 124600   |\n",
      "|    time_elapsed       | 18199    |\n",
      "|    total_timesteps    | 9968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.948   |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124599   |\n",
      "|    policy_loss        | -0.00701 |\n",
      "|    value_loss         | 0.452    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9970000, episode_reward=286.40 +/- 123.69\n",
      "Episode length: 8001.60 +/- 1927.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8e+03    |\n",
      "|    mean_reward        | 286      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9970000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.924   |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124624   |\n",
      "|    policy_loss        | 0.129    |\n",
      "|    value_loss         | 0.125    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.55e+03 |\n",
      "|    ep_rew_mean        | 365      |\n",
      "| time/                 |          |\n",
      "|    fps                | 547      |\n",
      "|    iterations         | 124700   |\n",
      "|    time_elapsed       | 18222    |\n",
      "|    total_timesteps    | 9976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.826   |\n",
      "|    explained_variance | 0.753    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124699   |\n",
      "|    policy_loss        | -0.201   |\n",
      "|    value_loss         | 1.48     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9980000, episode_reward=333.80 +/- 56.99\n",
      "Episode length: 7955.20 +/- 1545.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.96e+03 |\n",
      "|    mean_reward        | 334      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.964   |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124749   |\n",
      "|    policy_loss        | -0.177   |\n",
      "|    value_loss         | 0.726    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.59e+03 |\n",
      "|    ep_rew_mean        | 364      |\n",
      "| time/                 |          |\n",
      "|    fps                | 547      |\n",
      "|    iterations         | 124800   |\n",
      "|    time_elapsed       | 18245    |\n",
      "|    total_timesteps    | 9984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.922   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124799   |\n",
      "|    policy_loss        | -0.0078  |\n",
      "|    value_loss         | 0.0531   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9990000, episode_reward=178.60 +/- 119.82\n",
      "Episode length: 6314.40 +/- 1500.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.31e+03 |\n",
      "|    mean_reward        | 179      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9990000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.955   |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124874   |\n",
      "|    policy_loss        | -0.13    |\n",
      "|    value_loss         | 0.157    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.57e+03 |\n",
      "|    ep_rew_mean        | 364      |\n",
      "| time/                 |          |\n",
      "|    fps                | 547      |\n",
      "|    iterations         | 124900   |\n",
      "|    time_elapsed       | 18264    |\n",
      "|    total_timesteps    | 9992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.858   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124899   |\n",
      "|    policy_loss        | 0.0135   |\n",
      "|    value_loss         | 0.0937   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10000000, episode_reward=369.00 +/- 36.08\n",
      "Episode length: 8637.40 +/- 1158.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.64e+03 |\n",
      "|    mean_reward        | 369      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10000000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.98    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124999   |\n",
      "|    policy_loss        | 0.00274  |\n",
      "|    value_loss         | 0.0419   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.55e+03 |\n",
      "|    ep_rew_mean     | 367      |\n",
      "| time/              |          |\n",
      "|    fps             | 546      |\n",
      "|    iterations      | 125000   |\n",
      "|    time_elapsed    | 18288    |\n",
      "|    total_timesteps | 10000000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.54e+03 |\n",
      "|    ep_rew_mean        | 368      |\n",
      "| time/                 |          |\n",
      "|    fps                | 547      |\n",
      "|    iterations         | 125100   |\n",
      "|    time_elapsed       | 18295    |\n",
      "|    total_timesteps    | 10008000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.827    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 125099   |\n",
      "|    policy_loss        | -0.0448  |\n",
      "|    value_loss         | 0.4      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10010000, episode_reward=355.20 +/- 118.77\n",
      "Episode length: 9474.00 +/- 2633.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.47e+03 |\n",
      "|    mean_reward        | 355      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10010000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.805   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 125124   |\n",
      "|    policy_loss        | 0.0999   |\n",
      "|    value_loss         | 0.0766   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.53e+03 |\n",
      "|    ep_rew_mean        | 370      |\n",
      "| time/                 |          |\n",
      "|    fps                | 546      |\n",
      "|    iterations         | 125200   |\n",
      "|    time_elapsed       | 18320    |\n",
      "|    total_timesteps    | 10016000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.74    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 125199   |\n",
      "|    policy_loss        | -0.104   |\n",
      "|    value_loss         | 0.069    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10020000, episode_reward=365.00 +/- 39.21\n",
      "Episode length: 10612.40 +/- 2399.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+04 |\n",
      "|    mean_reward        | 365      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10020000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.98    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 125249   |\n",
      "|    policy_loss        | -0.0179  |\n",
      "|    value_loss         | 0.336    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.46e+03 |\n",
      "|    ep_rew_mean        | 369      |\n",
      "| time/                 |          |\n",
      "|    fps                | 546      |\n",
      "|    iterations         | 125300   |\n",
      "|    time_elapsed       | 18347    |\n",
      "|    total_timesteps    | 10024000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.962   |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 125299   |\n",
      "|    policy_loss        | 0.00432  |\n",
      "|    value_loss         | 0.0911   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10030000, episode_reward=388.00 +/- 44.73\n",
      "Episode length: 11022.60 +/- 3294.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.1e+04  |\n",
      "|    mean_reward        | 388      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10030000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.855   |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 125374   |\n",
      "|    policy_loss        | 0.00269  |\n",
      "|    value_loss         | 0.165    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.4e+03  |\n",
      "|    ep_rew_mean        | 368      |\n",
      "| time/                 |          |\n",
      "|    fps                | 545      |\n",
      "|    iterations         | 125400   |\n",
      "|    time_elapsed       | 18376    |\n",
      "|    total_timesteps    | 10032000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.844   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 125399   |\n",
      "|    policy_loss        | 0.0203   |\n",
      "|    value_loss         | 0.0438   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10040000, episode_reward=246.40 +/- 132.64\n",
      "Episode length: 8174.60 +/- 3394.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.17e+03 |\n",
      "|    mean_reward        | 246      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10040000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.811   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 125499   |\n",
      "|    policy_loss        | 0.0297   |\n",
      "|    value_loss         | 0.0791   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.41e+03 |\n",
      "|    ep_rew_mean     | 368      |\n",
      "| time/              |          |\n",
      "|    fps             | 545      |\n",
      "|    iterations      | 125500   |\n",
      "|    time_elapsed    | 18398    |\n",
      "|    total_timesteps | 10040000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.35e+03 |\n",
      "|    ep_rew_mean        | 368      |\n",
      "| time/                 |          |\n",
      "|    fps                | 545      |\n",
      "|    iterations         | 125600   |\n",
      "|    time_elapsed       | 18405    |\n",
      "|    total_timesteps    | 10048000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.919   |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 125599   |\n",
      "|    policy_loss        | -0.0279  |\n",
      "|    value_loss         | 0.238    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10050000, episode_reward=383.20 +/- 38.86\n",
      "Episode length: 10111.40 +/- 2735.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+04 |\n",
      "|    mean_reward        | 383      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10050000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.925   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 125624   |\n",
      "|    policy_loss        | -0.0205  |\n",
      "|    value_loss         | 0.037    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.32e+03 |\n",
      "|    ep_rew_mean        | 368      |\n",
      "| time/                 |          |\n",
      "|    fps                | 545      |\n",
      "|    iterations         | 125700   |\n",
      "|    time_elapsed       | 18432    |\n",
      "|    total_timesteps    | 10056000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.988   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 125699   |\n",
      "|    policy_loss        | 0.0424   |\n",
      "|    value_loss         | 0.0672   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10060000, episode_reward=378.20 +/- 23.16\n",
      "Episode length: 9206.80 +/- 756.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.21e+03 |\n",
      "|    mean_reward        | 378      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10060000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.974   |\n",
      "|    explained_variance | 0.897    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 125749   |\n",
      "|    policy_loss        | -0.146   |\n",
      "|    value_loss         | 0.235    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.25e+03 |\n",
      "|    ep_rew_mean        | 366      |\n",
      "| time/                 |          |\n",
      "|    fps                | 545      |\n",
      "|    iterations         | 125800   |\n",
      "|    time_elapsed       | 18457    |\n",
      "|    total_timesteps    | 10064000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.848   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 125799   |\n",
      "|    policy_loss        | 0.0695   |\n",
      "|    value_loss         | 0.0719   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10070000, episode_reward=484.60 +/- 170.51\n",
      "Episode length: 12484.60 +/- 5747.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.25e+04 |\n",
      "|    mean_reward        | 485      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10070000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.821   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 125874   |\n",
      "|    policy_loss        | -0.102   |\n",
      "|    value_loss         | 0.171    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.36e+03 |\n",
      "|    ep_rew_mean        | 370      |\n",
      "| time/                 |          |\n",
      "|    fps                | 544      |\n",
      "|    iterations         | 125900   |\n",
      "|    time_elapsed       | 18488    |\n",
      "|    total_timesteps    | 10072000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.907   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 125899   |\n",
      "|    policy_loss        | -0.00614 |\n",
      "|    value_loss         | 0.0523   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10080000, episode_reward=411.60 +/- 15.81\n",
      "Episode length: 13740.20 +/- 974.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.37e+04 |\n",
      "|    mean_reward        | 412      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10080000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.853   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 125999   |\n",
      "|    policy_loss        | -0.00839 |\n",
      "|    value_loss         | 0.121    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.35e+03 |\n",
      "|    ep_rew_mean     | 371      |\n",
      "| time/              |          |\n",
      "|    fps             | 544      |\n",
      "|    iterations      | 126000   |\n",
      "|    time_elapsed    | 18522    |\n",
      "|    total_timesteps | 10080000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.19e+03 |\n",
      "|    ep_rew_mean        | 369      |\n",
      "| time/                 |          |\n",
      "|    fps                | 544      |\n",
      "|    iterations         | 126100   |\n",
      "|    time_elapsed       | 18529    |\n",
      "|    total_timesteps    | 10088000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.887   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 126099   |\n",
      "|    policy_loss        | -0.0487  |\n",
      "|    value_loss         | 0.0425   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10090000, episode_reward=400.80 +/- 12.69\n",
      "Episode length: 8919.20 +/- 913.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.92e+03 |\n",
      "|    mean_reward        | 401      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10090000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.921   |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 126124   |\n",
      "|    policy_loss        | -0.18    |\n",
      "|    value_loss         | 0.12     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.3e+03  |\n",
      "|    ep_rew_mean        | 375      |\n",
      "| time/                 |          |\n",
      "|    fps                | 544      |\n",
      "|    iterations         | 126200   |\n",
      "|    time_elapsed       | 18553    |\n",
      "|    total_timesteps    | 10096000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 126199   |\n",
      "|    policy_loss        | -0.112   |\n",
      "|    value_loss         | 0.34     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10100000, episode_reward=404.00 +/- 14.52\n",
      "Episode length: 10561.80 +/- 1742.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+04 |\n",
      "|    mean_reward        | 404      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10100000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.857   |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 126249   |\n",
      "|    policy_loss        | -0.052   |\n",
      "|    value_loss         | 0.055    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.22e+03 |\n",
      "|    ep_rew_mean        | 375      |\n",
      "| time/                 |          |\n",
      "|    fps                | 543      |\n",
      "|    iterations         | 126300   |\n",
      "|    time_elapsed       | 18580    |\n",
      "|    total_timesteps    | 10104000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.958   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 126299   |\n",
      "|    policy_loss        | -0.0563  |\n",
      "|    value_loss         | 0.0376   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10110000, episode_reward=404.60 +/- 21.14\n",
      "Episode length: 10434.00 +/- 1456.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+04 |\n",
      "|    mean_reward        | 405      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10110000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.918   |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 126374   |\n",
      "|    policy_loss        | 0.0715   |\n",
      "|    value_loss         | 0.49     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.27e+03 |\n",
      "|    ep_rew_mean        | 373      |\n",
      "| time/                 |          |\n",
      "|    fps                | 543      |\n",
      "|    iterations         | 126400   |\n",
      "|    time_elapsed       | 18607    |\n",
      "|    total_timesteps    | 10112000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.853   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 126399   |\n",
      "|    policy_loss        | 0.00203  |\n",
      "|    value_loss         | 0.0302   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10120000, episode_reward=380.20 +/- 46.26\n",
      "Episode length: 10787.20 +/- 2771.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+04 |\n",
      "|    mean_reward        | 380      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10120000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.816   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 126499   |\n",
      "|    policy_loss        | -0.0735  |\n",
      "|    value_loss         | 0.055    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.26e+03 |\n",
      "|    ep_rew_mean     | 371      |\n",
      "| time/              |          |\n",
      "|    fps             | 543      |\n",
      "|    iterations      | 126500   |\n",
      "|    time_elapsed    | 18635    |\n",
      "|    total_timesteps | 10120000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.27e+03 |\n",
      "|    ep_rew_mean        | 372      |\n",
      "| time/                 |          |\n",
      "|    fps                | 543      |\n",
      "|    iterations         | 126600   |\n",
      "|    time_elapsed       | 18642    |\n",
      "|    total_timesteps    | 10128000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.838   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 126599   |\n",
      "|    policy_loss        | 0.0165   |\n",
      "|    value_loss         | 0.023    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10130000, episode_reward=287.80 +/- 114.63\n",
      "Episode length: 7232.20 +/- 752.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.23e+03 |\n",
      "|    mean_reward        | 288      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10130000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.879   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 126624   |\n",
      "|    policy_loss        | 0.0105   |\n",
      "|    value_loss         | 0.0524   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.2e+03  |\n",
      "|    ep_rew_mean        | 366      |\n",
      "| time/                 |          |\n",
      "|    fps                | 543      |\n",
      "|    iterations         | 126700   |\n",
      "|    time_elapsed       | 18663    |\n",
      "|    total_timesteps    | 10136000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.918   |\n",
      "|    explained_variance | 0.7      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 126699   |\n",
      "|    policy_loss        | -0.00233 |\n",
      "|    value_loss         | 0.727    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10140000, episode_reward=387.00 +/- 33.69\n",
      "Episode length: 8829.80 +/- 1795.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.83e+03 |\n",
      "|    mean_reward        | 387      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10140000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.933   |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 126749   |\n",
      "|    policy_loss        | 0.0374   |\n",
      "|    value_loss         | 0.0663   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.22e+03 |\n",
      "|    ep_rew_mean        | 366      |\n",
      "| time/                 |          |\n",
      "|    fps                | 542      |\n",
      "|    iterations         | 126800   |\n",
      "|    time_elapsed       | 18686    |\n",
      "|    total_timesteps    | 10144000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.878   |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 126799   |\n",
      "|    policy_loss        | -0.00651 |\n",
      "|    value_loss         | 0.762    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10150000, episode_reward=365.00 +/- 52.93\n",
      "Episode length: 7921.80 +/- 1693.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.92e+03 |\n",
      "|    mean_reward        | 365      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10150000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.911   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 126874   |\n",
      "|    policy_loss        | 0.0796   |\n",
      "|    value_loss         | 0.0509   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.11e+03 |\n",
      "|    ep_rew_mean        | 364      |\n",
      "| time/                 |          |\n",
      "|    fps                | 542      |\n",
      "|    iterations         | 126900   |\n",
      "|    time_elapsed       | 18708    |\n",
      "|    total_timesteps    | 10152000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.933   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 126899   |\n",
      "|    policy_loss        | -0.079   |\n",
      "|    value_loss         | 0.0763   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10160000, episode_reward=317.40 +/- 64.63\n",
      "Episode length: 7849.40 +/- 1043.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.85e+03 |\n",
      "|    mean_reward        | 317      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10160000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.952   |\n",
      "|    explained_variance | 0.811    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 126999   |\n",
      "|    policy_loss        | 0.0983   |\n",
      "|    value_loss         | 0.429    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.02e+03 |\n",
      "|    ep_rew_mean     | 366      |\n",
      "| time/              |          |\n",
      "|    fps             | 542      |\n",
      "|    iterations      | 127000   |\n",
      "|    time_elapsed    | 18730    |\n",
      "|    total_timesteps | 10160000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9e+03    |\n",
      "|    ep_rew_mean        | 362      |\n",
      "| time/                 |          |\n",
      "|    fps                | 542      |\n",
      "|    iterations         | 127100   |\n",
      "|    time_elapsed       | 18737    |\n",
      "|    total_timesteps    | 10168000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.901   |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 127099   |\n",
      "|    policy_loss        | -0.158   |\n",
      "|    value_loss         | 0.165    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10170000, episode_reward=407.20 +/- 26.09\n",
      "Episode length: 9877.20 +/- 772.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.88e+03 |\n",
      "|    mean_reward        | 407      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10170000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.924   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 127124   |\n",
      "|    policy_loss        | -0.0259  |\n",
      "|    value_loss         | 0.0195   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.03e+03 |\n",
      "|    ep_rew_mean        | 362      |\n",
      "| time/                 |          |\n",
      "|    fps                | 542      |\n",
      "|    iterations         | 127200   |\n",
      "|    time_elapsed       | 18763    |\n",
      "|    total_timesteps    | 10176000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.831   |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 127199   |\n",
      "|    policy_loss        | -0.0306  |\n",
      "|    value_loss         | 0.134    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10180000, episode_reward=322.60 +/- 69.24\n",
      "Episode length: 8205.80 +/- 747.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.21e+03 |\n",
      "|    mean_reward        | 323      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10180000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.794   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 127249   |\n",
      "|    policy_loss        | 0.0271   |\n",
      "|    value_loss         | 0.0792   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.02e+03 |\n",
      "|    ep_rew_mean        | 362      |\n",
      "| time/                 |          |\n",
      "|    fps                | 542      |\n",
      "|    iterations         | 127300   |\n",
      "|    time_elapsed       | 18785    |\n",
      "|    total_timesteps    | 10184000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.966   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 127299   |\n",
      "|    policy_loss        | 0.0771   |\n",
      "|    value_loss         | 0.0372   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10190000, episode_reward=369.00 +/- 53.30\n",
      "Episode length: 8108.40 +/- 1391.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.11e+03 |\n",
      "|    mean_reward        | 369      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10190000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.924   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 127374   |\n",
      "|    policy_loss        | 0.0045   |\n",
      "|    value_loss         | 0.0695   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.07e+03 |\n",
      "|    ep_rew_mean        | 365      |\n",
      "| time/                 |          |\n",
      "|    fps                | 541      |\n",
      "|    iterations         | 127400   |\n",
      "|    time_elapsed       | 18808    |\n",
      "|    total_timesteps    | 10192000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.989   |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 127399   |\n",
      "|    policy_loss        | 0.00358  |\n",
      "|    value_loss         | 0.12     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10200000, episode_reward=358.80 +/- 62.68\n",
      "Episode length: 11593.60 +/- 5770.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+04 |\n",
      "|    mean_reward        | 359      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10200000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.995   |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 127499   |\n",
      "|    policy_loss        | 0.0494   |\n",
      "|    value_loss         | 0.124    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.91e+03 |\n",
      "|    ep_rew_mean     | 362      |\n",
      "| time/              |          |\n",
      "|    fps             | 541      |\n",
      "|    iterations      | 127500   |\n",
      "|    time_elapsed    | 18837    |\n",
      "|    total_timesteps | 10200000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.98e+03 |\n",
      "|    ep_rew_mean        | 364      |\n",
      "| time/                 |          |\n",
      "|    fps                | 541      |\n",
      "|    iterations         | 127600   |\n",
      "|    time_elapsed       | 18844    |\n",
      "|    total_timesteps    | 10208000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.994   |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 127599   |\n",
      "|    policy_loss        | 0.0814   |\n",
      "|    value_loss         | 0.133    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10210000, episode_reward=331.00 +/- 139.47\n",
      "Episode length: 8286.20 +/- 2449.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.29e+03 |\n",
      "|    mean_reward        | 331      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10210000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.898    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 127624   |\n",
      "|    policy_loss        | -0.0483  |\n",
      "|    value_loss         | 0.27     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.89e+03 |\n",
      "|    ep_rew_mean        | 357      |\n",
      "| time/                 |          |\n",
      "|    fps                | 541      |\n",
      "|    iterations         | 127700   |\n",
      "|    time_elapsed       | 18867    |\n",
      "|    total_timesteps    | 10216000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.947   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 127699   |\n",
      "|    policy_loss        | 0.00859  |\n",
      "|    value_loss         | 0.0662   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10220000, episode_reward=374.40 +/- 28.65\n",
      "Episode length: 9811.60 +/- 1932.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.81e+03 |\n",
      "|    mean_reward        | 374      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10220000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.919   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 127749   |\n",
      "|    policy_loss        | -0.0534  |\n",
      "|    value_loss         | 0.0412   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.9e+03  |\n",
      "|    ep_rew_mean        | 358      |\n",
      "| time/                 |          |\n",
      "|    fps                | 541      |\n",
      "|    iterations         | 127800   |\n",
      "|    time_elapsed       | 18893    |\n",
      "|    total_timesteps    | 10224000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.883   |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 127799   |\n",
      "|    policy_loss        | -0.0794  |\n",
      "|    value_loss         | 0.09     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10230000, episode_reward=354.80 +/- 23.44\n",
      "Episode length: 9107.80 +/- 1512.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.11e+03 |\n",
      "|    mean_reward        | 355      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10230000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.942   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 127874   |\n",
      "|    policy_loss        | 0.0319   |\n",
      "|    value_loss         | 0.0901   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.91e+03 |\n",
      "|    ep_rew_mean        | 356      |\n",
      "| time/                 |          |\n",
      "|    fps                | 540      |\n",
      "|    iterations         | 127900   |\n",
      "|    time_elapsed       | 18917    |\n",
      "|    total_timesteps    | 10232000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.903   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 127899   |\n",
      "|    policy_loss        | 0.0181   |\n",
      "|    value_loss         | 0.0339   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10240000, episode_reward=371.60 +/- 38.71\n",
      "Episode length: 8414.40 +/- 1315.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.41e+03 |\n",
      "|    mean_reward        | 372      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10240000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.929   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 127999   |\n",
      "|    policy_loss        | -0.0533  |\n",
      "|    value_loss         | 0.0238   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.97e+03 |\n",
      "|    ep_rew_mean     | 353      |\n",
      "| time/              |          |\n",
      "|    fps             | 540      |\n",
      "|    iterations      | 128000   |\n",
      "|    time_elapsed    | 18940    |\n",
      "|    total_timesteps | 10240000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.97e+03 |\n",
      "|    ep_rew_mean        | 353      |\n",
      "| time/                 |          |\n",
      "|    fps                | 540      |\n",
      "|    iterations         | 128100   |\n",
      "|    time_elapsed       | 18947    |\n",
      "|    total_timesteps    | 10248000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.916   |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 128099   |\n",
      "|    policy_loss        | 0.0506   |\n",
      "|    value_loss         | 0.0594   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10250000, episode_reward=369.60 +/- 34.65\n",
      "Episode length: 8530.40 +/- 1006.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.53e+03 |\n",
      "|    mean_reward        | 370      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10250000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.886   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 128124   |\n",
      "|    policy_loss        | -0.123   |\n",
      "|    value_loss         | 0.215    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.07e+03 |\n",
      "|    ep_rew_mean        | 356      |\n",
      "| time/                 |          |\n",
      "|    fps                | 540      |\n",
      "|    iterations         | 128200   |\n",
      "|    time_elapsed       | 18970    |\n",
      "|    total_timesteps    | 10256000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.943   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 128199   |\n",
      "|    policy_loss        | 0.13     |\n",
      "|    value_loss         | 0.28     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10260000, episode_reward=376.60 +/- 23.91\n",
      "Episode length: 9582.80 +/- 1725.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.58e+03 |\n",
      "|    mean_reward        | 377      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10260000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.943   |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 128249   |\n",
      "|    policy_loss        | -0.0421  |\n",
      "|    value_loss         | 0.227    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.09e+03 |\n",
      "|    ep_rew_mean        | 356      |\n",
      "| time/                 |          |\n",
      "|    fps                | 540      |\n",
      "|    iterations         | 128300   |\n",
      "|    time_elapsed       | 18995    |\n",
      "|    total_timesteps    | 10264000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.994   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 128299   |\n",
      "|    policy_loss        | -0.0423  |\n",
      "|    value_loss         | 0.0564   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10270000, episode_reward=369.00 +/- 22.89\n",
      "Episode length: 8731.40 +/- 1332.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.73e+03 |\n",
      "|    mean_reward        | 369      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10270000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.936   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 128374   |\n",
      "|    policy_loss        | 0.0125   |\n",
      "|    value_loss         | 0.0275   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.97e+03 |\n",
      "|    ep_rew_mean        | 351      |\n",
      "| time/                 |          |\n",
      "|    fps                | 540      |\n",
      "|    iterations         | 128400   |\n",
      "|    time_elapsed       | 19018    |\n",
      "|    total_timesteps    | 10272000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.893   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 128399   |\n",
      "|    policy_loss        | -0.0161  |\n",
      "|    value_loss         | 0.0334   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10280000, episode_reward=402.80 +/- 18.35\n",
      "Episode length: 10504.20 +/- 1281.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+04 |\n",
      "|    mean_reward        | 403      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10280000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.999   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 128499   |\n",
      "|    policy_loss        | 0.0601   |\n",
      "|    value_loss         | 0.0201   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.01e+03 |\n",
      "|    ep_rew_mean     | 352      |\n",
      "| time/              |          |\n",
      "|    fps             | 539      |\n",
      "|    iterations      | 128500   |\n",
      "|    time_elapsed    | 19046    |\n",
      "|    total_timesteps | 10280000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.98e+03 |\n",
      "|    ep_rew_mean        | 352      |\n",
      "| time/                 |          |\n",
      "|    fps                | 539      |\n",
      "|    iterations         | 128600   |\n",
      "|    time_elapsed       | 19053    |\n",
      "|    total_timesteps    | 10288000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.726   |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 128599   |\n",
      "|    policy_loss        | -0.0141  |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10290000, episode_reward=325.80 +/- 83.29\n",
      "Episode length: 9096.80 +/- 3467.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.1e+03  |\n",
      "|    mean_reward        | 326      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10290000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.79    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 128624   |\n",
      "|    policy_loss        | 0.0675   |\n",
      "|    value_loss         | 0.193    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.9e+03  |\n",
      "|    ep_rew_mean        | 350      |\n",
      "| time/                 |          |\n",
      "|    fps                | 539      |\n",
      "|    iterations         | 128700   |\n",
      "|    time_elapsed       | 19077    |\n",
      "|    total_timesteps    | 10296000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.946   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 128699   |\n",
      "|    policy_loss        | 0.101    |\n",
      "|    value_loss         | 0.0954   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10300000, episode_reward=411.00 +/- 29.96\n",
      "Episode length: 9492.40 +/- 1485.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.49e+03 |\n",
      "|    mean_reward        | 411      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10300000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.968   |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 128749   |\n",
      "|    policy_loss        | 0.0609   |\n",
      "|    value_loss         | 0.113    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.92e+03 |\n",
      "|    ep_rew_mean        | 349      |\n",
      "| time/                 |          |\n",
      "|    fps                | 539      |\n",
      "|    iterations         | 128800   |\n",
      "|    time_elapsed       | 19102    |\n",
      "|    total_timesteps    | 10304000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.875   |\n",
      "|    explained_variance | 0.891    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 128799   |\n",
      "|    policy_loss        | 0.203    |\n",
      "|    value_loss         | 0.783    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10310000, episode_reward=404.40 +/- 26.78\n",
      "Episode length: 9752.00 +/- 1170.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.75e+03 |\n",
      "|    mean_reward        | 404      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10310000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.921   |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 128874   |\n",
      "|    policy_loss        | -0.0132  |\n",
      "|    value_loss         | 0.0696   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.96e+03 |\n",
      "|    ep_rew_mean        | 352      |\n",
      "| time/                 |          |\n",
      "|    fps                | 539      |\n",
      "|    iterations         | 128900   |\n",
      "|    time_elapsed       | 19127    |\n",
      "|    total_timesteps    | 10312000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.994   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 128899   |\n",
      "|    policy_loss        | 0.00415  |\n",
      "|    value_loss         | 0.0251   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10320000, episode_reward=397.20 +/- 25.21\n",
      "Episode length: 11161.80 +/- 3568.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 397      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10320000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.861   |\n",
      "|    explained_variance | 0.576    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 128999   |\n",
      "|    policy_loss        | -0.352   |\n",
      "|    value_loss         | 1.1      |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.94e+03 |\n",
      "|    ep_rew_mean     | 351      |\n",
      "| time/              |          |\n",
      "|    fps             | 538      |\n",
      "|    iterations      | 129000   |\n",
      "|    time_elapsed    | 19155    |\n",
      "|    total_timesteps | 10320000 |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 8.96e+03  |\n",
      "|    ep_rew_mean        | 354       |\n",
      "| time/                 |           |\n",
      "|    fps                | 538       |\n",
      "|    iterations         | 129100    |\n",
      "|    time_elapsed       | 19163     |\n",
      "|    total_timesteps    | 10328000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.01     |\n",
      "|    explained_variance | 0.983     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 129099    |\n",
      "|    policy_loss        | -0.000941 |\n",
      "|    value_loss         | 0.0458    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=10330000, episode_reward=374.00 +/- 38.38\n",
      "Episode length: 9111.80 +/- 1280.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.11e+03 |\n",
      "|    mean_reward        | 374      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10330000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.848   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 129124   |\n",
      "|    policy_loss        | -0.0145  |\n",
      "|    value_loss         | 0.0834   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.93e+03 |\n",
      "|    ep_rew_mean        | 356      |\n",
      "| time/                 |          |\n",
      "|    fps                | 538      |\n",
      "|    iterations         | 129200   |\n",
      "|    time_elapsed       | 19187    |\n",
      "|    total_timesteps    | 10336000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.811   |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 129199   |\n",
      "|    policy_loss        | 0.0151   |\n",
      "|    value_loss         | 0.256    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10340000, episode_reward=365.00 +/- 65.02\n",
      "Episode length: 8912.60 +/- 1255.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.91e+03 |\n",
      "|    mean_reward        | 365      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10340000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.97    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 129249   |\n",
      "|    policy_loss        | -0.0498  |\n",
      "|    value_loss         | 0.126    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.92e+03 |\n",
      "|    ep_rew_mean        | 357      |\n",
      "| time/                 |          |\n",
      "|    fps                | 538      |\n",
      "|    iterations         | 129300   |\n",
      "|    time_elapsed       | 19211    |\n",
      "|    total_timesteps    | 10344000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.927   |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 129299   |\n",
      "|    policy_loss        | 0.0138   |\n",
      "|    value_loss         | 0.199    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10350000, episode_reward=293.60 +/- 132.77\n",
      "Episode length: 8048.20 +/- 1602.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.05e+03 |\n",
      "|    mean_reward        | 294      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10350000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.878   |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 129374   |\n",
      "|    policy_loss        | 0.0882   |\n",
      "|    value_loss         | 0.113    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.91e+03 |\n",
      "|    ep_rew_mean        | 355      |\n",
      "| time/                 |          |\n",
      "|    fps                | 538      |\n",
      "|    iterations         | 129400   |\n",
      "|    time_elapsed       | 19233    |\n",
      "|    total_timesteps    | 10352000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.8     |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 129399   |\n",
      "|    policy_loss        | -0.0309  |\n",
      "|    value_loss         | 0.0639   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10360000, episode_reward=301.00 +/- 116.68\n",
      "Episode length: 7954.80 +/- 2085.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.95e+03 |\n",
      "|    mean_reward        | 301      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10360000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.903   |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 129499   |\n",
      "|    policy_loss        | 0.314    |\n",
      "|    value_loss         | 0.656    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.99e+03 |\n",
      "|    ep_rew_mean     | 360      |\n",
      "| time/              |          |\n",
      "|    fps             | 538      |\n",
      "|    iterations      | 129500   |\n",
      "|    time_elapsed    | 19256    |\n",
      "|    total_timesteps | 10360000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.99e+03 |\n",
      "|    ep_rew_mean        | 361      |\n",
      "| time/                 |          |\n",
      "|    fps                | 538      |\n",
      "|    iterations         | 129600   |\n",
      "|    time_elapsed       | 19263    |\n",
      "|    total_timesteps    | 10368000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.782   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 129599   |\n",
      "|    policy_loss        | -0.0467  |\n",
      "|    value_loss         | 0.121    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10370000, episode_reward=395.40 +/- 24.08\n",
      "Episode length: 12254.40 +/- 6116.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.23e+04 |\n",
      "|    mean_reward        | 395      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10370000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.869   |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 129624   |\n",
      "|    policy_loss        | 0.105    |\n",
      "|    value_loss         | 0.0787   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.99e+03 |\n",
      "|    ep_rew_mean        | 361      |\n",
      "| time/                 |          |\n",
      "|    fps                | 537      |\n",
      "|    iterations         | 129700   |\n",
      "|    time_elapsed       | 19293    |\n",
      "|    total_timesteps    | 10376000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.823   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 129699   |\n",
      "|    policy_loss        | -0.0374  |\n",
      "|    value_loss         | 0.0508   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10380000, episode_reward=382.00 +/- 39.61\n",
      "Episode length: 8918.40 +/- 2184.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.92e+03 |\n",
      "|    mean_reward        | 382      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10380000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.899   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 129749   |\n",
      "|    policy_loss        | 0.0405   |\n",
      "|    value_loss         | 0.0756   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.01e+03 |\n",
      "|    ep_rew_mean        | 361      |\n",
      "| time/                 |          |\n",
      "|    fps                | 537      |\n",
      "|    iterations         | 129800   |\n",
      "|    time_elapsed       | 19316    |\n",
      "|    total_timesteps    | 10384000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 129799   |\n",
      "|    policy_loss        | 0.0051   |\n",
      "|    value_loss         | 0.0589   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10390000, episode_reward=411.40 +/- 18.35\n",
      "Episode length: 11295.60 +/- 3021.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+04 |\n",
      "|    mean_reward        | 411      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10390000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.972   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 129874   |\n",
      "|    policy_loss        | -0.11    |\n",
      "|    value_loss         | 0.0415   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.18e+03 |\n",
      "|    ep_rew_mean        | 366      |\n",
      "| time/                 |          |\n",
      "|    fps                | 537      |\n",
      "|    iterations         | 129900   |\n",
      "|    time_elapsed       | 19345    |\n",
      "|    total_timesteps    | 10392000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.996   |\n",
      "|    explained_variance | -0.165   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 129899   |\n",
      "|    policy_loss        | -0.412   |\n",
      "|    value_loss         | 2        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10400000, episode_reward=370.60 +/- 50.84\n",
      "Episode length: 9174.00 +/- 2869.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.17e+03 |\n",
      "|    mean_reward        | 371      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10400000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.849   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 129999   |\n",
      "|    policy_loss        | -0.0391  |\n",
      "|    value_loss         | 0.0293   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.15e+03 |\n",
      "|    ep_rew_mean     | 366      |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 130000   |\n",
      "|    time_elapsed    | 19369    |\n",
      "|    total_timesteps | 10400000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.15e+03 |\n",
      "|    ep_rew_mean        | 365      |\n",
      "| time/                 |          |\n",
      "|    fps                | 537      |\n",
      "|    iterations         | 130100   |\n",
      "|    time_elapsed       | 19376    |\n",
      "|    total_timesteps    | 10408000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.849   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 130099   |\n",
      "|    policy_loss        | 0.017    |\n",
      "|    value_loss         | 0.0314   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10410000, episode_reward=411.40 +/- 39.95\n",
      "Episode length: 11088.80 +/- 1953.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.11e+04 |\n",
      "|    mean_reward        | 411      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10410000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.956   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 130124   |\n",
      "|    policy_loss        | -0.0687  |\n",
      "|    value_loss         | 0.0636   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.15e+03 |\n",
      "|    ep_rew_mean        | 365      |\n",
      "| time/                 |          |\n",
      "|    fps                | 536      |\n",
      "|    iterations         | 130200   |\n",
      "|    time_elapsed       | 19404    |\n",
      "|    total_timesteps    | 10416000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.852   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 130199   |\n",
      "|    policy_loss        | 0.00683  |\n",
      "|    value_loss         | 0.0511   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10420000, episode_reward=349.60 +/- 159.09\n",
      "Episode length: 11383.40 +/- 4216.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+04 |\n",
      "|    mean_reward        | 350      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10420000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.912   |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 130249   |\n",
      "|    policy_loss        | -0.109   |\n",
      "|    value_loss         | 0.135    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.07e+03 |\n",
      "|    ep_rew_mean        | 361      |\n",
      "| time/                 |          |\n",
      "|    fps                | 536      |\n",
      "|    iterations         | 130300   |\n",
      "|    time_elapsed       | 19433    |\n",
      "|    total_timesteps    | 10424000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.933   |\n",
      "|    explained_variance | 0.621    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 130299   |\n",
      "|    policy_loss        | -0.306   |\n",
      "|    value_loss         | 1.01     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10430000, episode_reward=193.60 +/- 140.15\n",
      "Episode length: 17932.00 +/- 18572.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.79e+04 |\n",
      "|    mean_reward        | 194      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10430000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.896   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 130374   |\n",
      "|    policy_loss        | -0.0297  |\n",
      "|    value_loss         | 0.013    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.18e+03 |\n",
      "|    ep_rew_mean        | 362      |\n",
      "| time/                 |          |\n",
      "|    fps                | 535      |\n",
      "|    iterations         | 130400   |\n",
      "|    time_elapsed       | 19474    |\n",
      "|    total_timesteps    | 10432000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 130399   |\n",
      "|    policy_loss        | -0.0225  |\n",
      "|    value_loss         | 0.0695   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10440000, episode_reward=326.00 +/- 80.74\n",
      "Episode length: 9457.80 +/- 1922.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.46e+03 |\n",
      "|    mean_reward        | 326      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10440000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.861   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 130499   |\n",
      "|    policy_loss        | 0.0287   |\n",
      "|    value_loss         | 0.0439   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.15e+03 |\n",
      "|    ep_rew_mean     | 362      |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 130500   |\n",
      "|    time_elapsed    | 19499    |\n",
      "|    total_timesteps | 10440000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.25e+03 |\n",
      "|    ep_rew_mean        | 365      |\n",
      "| time/                 |          |\n",
      "|    fps                | 535      |\n",
      "|    iterations         | 130600   |\n",
      "|    time_elapsed       | 19506    |\n",
      "|    total_timesteps    | 10448000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.84    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 130599   |\n",
      "|    policy_loss        | -0.00156 |\n",
      "|    value_loss         | 0.0622   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10450000, episode_reward=401.00 +/- 17.40\n",
      "Episode length: 12012.60 +/- 4881.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.2e+04  |\n",
      "|    mean_reward        | 401      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10450000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.923   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 130624   |\n",
      "|    policy_loss        | -0.114   |\n",
      "|    value_loss         | 0.133    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.33e+03 |\n",
      "|    ep_rew_mean        | 370      |\n",
      "| time/                 |          |\n",
      "|    fps                | 535      |\n",
      "|    iterations         | 130700   |\n",
      "|    time_elapsed       | 19536    |\n",
      "|    total_timesteps    | 10456000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.996   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 130699   |\n",
      "|    policy_loss        | 0.0203   |\n",
      "|    value_loss         | 0.0475   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10460000, episode_reward=537.40 +/- 188.83\n",
      "Episode length: 14445.00 +/- 7620.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.44e+04 |\n",
      "|    mean_reward        | 537      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10460000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.863   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 130749   |\n",
      "|    policy_loss        | 0.0265   |\n",
      "|    value_loss         | 0.035    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.33e+03 |\n",
      "|    ep_rew_mean        | 372      |\n",
      "| time/                 |          |\n",
      "|    fps                | 534      |\n",
      "|    iterations         | 130800   |\n",
      "|    time_elapsed       | 19571    |\n",
      "|    total_timesteps    | 10464000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.912   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 130799   |\n",
      "|    policy_loss        | -0.0332  |\n",
      "|    value_loss         | 0.0358   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10470000, episode_reward=373.80 +/- 23.75\n",
      "Episode length: 8627.60 +/- 1159.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.63e+03 |\n",
      "|    mean_reward        | 374      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10470000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.913   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 130874   |\n",
      "|    policy_loss        | -0.0588  |\n",
      "|    value_loss         | 0.123    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.52e+03 |\n",
      "|    ep_rew_mean        | 375      |\n",
      "| time/                 |          |\n",
      "|    fps                | 534      |\n",
      "|    iterations         | 130900   |\n",
      "|    time_elapsed       | 19594    |\n",
      "|    total_timesteps    | 10472000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.899   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 130899   |\n",
      "|    policy_loss        | 0.0295   |\n",
      "|    value_loss         | 0.0575   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10480000, episode_reward=399.00 +/- 27.00\n",
      "Episode length: 10314.40 +/- 2971.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+04 |\n",
      "|    mean_reward        | 399      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10480000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.834   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 130999   |\n",
      "|    policy_loss        | 0.00726  |\n",
      "|    value_loss         | 0.054    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.54e+03 |\n",
      "|    ep_rew_mean     | 377      |\n",
      "| time/              |          |\n",
      "|    fps             | 534      |\n",
      "|    iterations      | 131000   |\n",
      "|    time_elapsed    | 19620    |\n",
      "|    total_timesteps | 10480000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.54e+03 |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 534      |\n",
      "|    iterations         | 131100   |\n",
      "|    time_elapsed       | 19627    |\n",
      "|    total_timesteps    | 10488000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.86    |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 131099   |\n",
      "|    policy_loss        | -0.0167  |\n",
      "|    value_loss         | 0.033    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10490000, episode_reward=415.60 +/- 30.63\n",
      "Episode length: 10182.40 +/- 1602.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+04 |\n",
      "|    mean_reward        | 416      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10490000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.867   |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 131124   |\n",
      "|    policy_loss        | 0.074    |\n",
      "|    value_loss         | 0.116    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.51e+03 |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 534      |\n",
      "|    iterations         | 131200   |\n",
      "|    time_elapsed       | 19653    |\n",
      "|    total_timesteps    | 10496000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.994   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 131199   |\n",
      "|    policy_loss        | 0.0176   |\n",
      "|    value_loss         | 0.0145   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10500000, episode_reward=399.60 +/- 35.01\n",
      "Episode length: 9810.60 +/- 844.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.81e+03 |\n",
      "|    mean_reward        | 400      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10500000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 131249   |\n",
      "|    policy_loss        | 0.0382   |\n",
      "|    value_loss         | 0.0737   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.55e+03 |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 533      |\n",
      "|    iterations         | 131300   |\n",
      "|    time_elapsed       | 19679    |\n",
      "|    total_timesteps    | 10504000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.953   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 131299   |\n",
      "|    policy_loss        | 0.0591   |\n",
      "|    value_loss         | 0.055    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10510000, episode_reward=404.00 +/- 32.50\n",
      "Episode length: 11863.60 +/- 3055.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.19e+04 |\n",
      "|    mean_reward        | 404      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10510000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.858   |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 131374   |\n",
      "|    policy_loss        | -0.0943  |\n",
      "|    value_loss         | 0.0801   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.62e+03 |\n",
      "|    ep_rew_mean        | 379      |\n",
      "| time/                 |          |\n",
      "|    fps                | 533      |\n",
      "|    iterations         | 131400   |\n",
      "|    time_elapsed       | 19708    |\n",
      "|    total_timesteps    | 10512000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.846   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 131399   |\n",
      "|    policy_loss        | -0.0221  |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10520000, episode_reward=398.20 +/- 29.09\n",
      "Episode length: 13019.00 +/- 4638.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.3e+04  |\n",
      "|    mean_reward        | 398      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10520000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.856   |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 131499   |\n",
      "|    policy_loss        | 0.024    |\n",
      "|    value_loss         | 0.0869   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.61e+03 |\n",
      "|    ep_rew_mean     | 379      |\n",
      "| time/              |          |\n",
      "|    fps             | 532      |\n",
      "|    iterations      | 131500   |\n",
      "|    time_elapsed    | 19740    |\n",
      "|    total_timesteps | 10520000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.59e+03 |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 533      |\n",
      "|    iterations         | 131600   |\n",
      "|    time_elapsed       | 19747    |\n",
      "|    total_timesteps    | 10528000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.835   |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 131599   |\n",
      "|    policy_loss        | 0.0375   |\n",
      "|    value_loss         | 0.0388   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10530000, episode_reward=336.80 +/- 117.86\n",
      "Episode length: 8914.40 +/- 2241.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.91e+03 |\n",
      "|    mean_reward        | 337      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10530000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.932   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 131624   |\n",
      "|    policy_loss        | 0.238    |\n",
      "|    value_loss         | 0.165    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.58e+03 |\n",
      "|    ep_rew_mean        | 380      |\n",
      "| time/                 |          |\n",
      "|    fps                | 532      |\n",
      "|    iterations         | 131700   |\n",
      "|    time_elapsed       | 19771    |\n",
      "|    total_timesteps    | 10536000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.87    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 131699   |\n",
      "|    policy_loss        | 0.00763  |\n",
      "|    value_loss         | 0.057    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10540000, episode_reward=293.20 +/- 136.12\n",
      "Episode length: 7372.40 +/- 2644.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.37e+03 |\n",
      "|    mean_reward        | 293      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10540000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.894   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 131749   |\n",
      "|    policy_loss        | 0.00857  |\n",
      "|    value_loss         | 0.0462   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.71e+03 |\n",
      "|    ep_rew_mean        | 382      |\n",
      "| time/                 |          |\n",
      "|    fps                | 532      |\n",
      "|    iterations         | 131800   |\n",
      "|    time_elapsed       | 19793    |\n",
      "|    total_timesteps    | 10544000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.95    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 131799   |\n",
      "|    policy_loss        | -0.0272  |\n",
      "|    value_loss         | 0.0307   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10550000, episode_reward=398.20 +/- 19.03\n",
      "Episode length: 10153.00 +/- 3052.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+04 |\n",
      "|    mean_reward        | 398      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10550000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.947   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 131874   |\n",
      "|    policy_loss        | -0.00569 |\n",
      "|    value_loss         | 0.0155   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.68e+03 |\n",
      "|    ep_rew_mean        | 381      |\n",
      "| time/                 |          |\n",
      "|    fps                | 532      |\n",
      "|    iterations         | 131900   |\n",
      "|    time_elapsed       | 19819    |\n",
      "|    total_timesteps    | 10552000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.795   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 131899   |\n",
      "|    policy_loss        | -0.00631 |\n",
      "|    value_loss         | 0.0577   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10560000, episode_reward=398.60 +/- 44.47\n",
      "Episode length: 10101.40 +/- 2933.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+04 |\n",
      "|    mean_reward        | 399      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10560000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.822   |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 131999   |\n",
      "|    policy_loss        | -0.0486  |\n",
      "|    value_loss         | 0.139    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.68e+03 |\n",
      "|    ep_rew_mean     | 382      |\n",
      "| time/              |          |\n",
      "|    fps             | 532      |\n",
      "|    iterations      | 132000   |\n",
      "|    time_elapsed    | 19845    |\n",
      "|    total_timesteps | 10560000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.66e+03 |\n",
      "|    ep_rew_mean        | 380      |\n",
      "| time/                 |          |\n",
      "|    fps                | 532      |\n",
      "|    iterations         | 132100   |\n",
      "|    time_elapsed       | 19852    |\n",
      "|    total_timesteps    | 10568000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.913   |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 132099   |\n",
      "|    policy_loss        | -0.129   |\n",
      "|    value_loss         | 0.161    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10570000, episode_reward=301.60 +/- 114.35\n",
      "Episode length: 9798.20 +/- 4091.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.8e+03  |\n",
      "|    mean_reward        | 302      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10570000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.945   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 132124   |\n",
      "|    policy_loss        | -0.0149  |\n",
      "|    value_loss         | 0.111    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.69e+03 |\n",
      "|    ep_rew_mean        | 380      |\n",
      "| time/                 |          |\n",
      "|    fps                | 532      |\n",
      "|    iterations         | 132200   |\n",
      "|    time_elapsed       | 19878    |\n",
      "|    total_timesteps    | 10576000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.807   |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 132199   |\n",
      "|    policy_loss        | -0.0331  |\n",
      "|    value_loss         | 0.22     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10580000, episode_reward=344.40 +/- 40.97\n",
      "Episode length: 14664.20 +/- 8069.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.47e+04 |\n",
      "|    mean_reward        | 344      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10580000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.861   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 132249   |\n",
      "|    policy_loss        | -0.0421  |\n",
      "|    value_loss         | 0.018    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.67e+03 |\n",
      "|    ep_rew_mean        | 380      |\n",
      "| time/                 |          |\n",
      "|    fps                | 531      |\n",
      "|    iterations         | 132300   |\n",
      "|    time_elapsed       | 19912    |\n",
      "|    total_timesteps    | 10584000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.875   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 132299   |\n",
      "|    policy_loss        | -0.0186  |\n",
      "|    value_loss         | 0.0374   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10590000, episode_reward=413.80 +/- 11.44\n",
      "Episode length: 9716.20 +/- 1964.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.72e+03 |\n",
      "|    mean_reward        | 414      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10590000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.88    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 132374   |\n",
      "|    policy_loss        | 0.0733   |\n",
      "|    value_loss         | 0.132    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.67e+03 |\n",
      "|    ep_rew_mean        | 380      |\n",
      "| time/                 |          |\n",
      "|    fps                | 531      |\n",
      "|    iterations         | 132400   |\n",
      "|    time_elapsed       | 19938    |\n",
      "|    total_timesteps    | 10592000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 132399   |\n",
      "|    policy_loss        | -0.148   |\n",
      "|    value_loss         | 0.162    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10600000, episode_reward=395.60 +/- 23.92\n",
      "Episode length: 9061.20 +/- 1354.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.06e+03 |\n",
      "|    mean_reward        | 396      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10600000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.953   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 132499   |\n",
      "|    policy_loss        | -0.0604  |\n",
      "|    value_loss         | 0.0279   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.64e+03 |\n",
      "|    ep_rew_mean     | 376      |\n",
      "| time/              |          |\n",
      "|    fps             | 531      |\n",
      "|    iterations      | 132500   |\n",
      "|    time_elapsed    | 19962    |\n",
      "|    total_timesteps | 10600000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.67e+03 |\n",
      "|    ep_rew_mean        | 376      |\n",
      "| time/                 |          |\n",
      "|    fps                | 531      |\n",
      "|    iterations         | 132600   |\n",
      "|    time_elapsed       | 19968    |\n",
      "|    total_timesteps    | 10608000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 132599   |\n",
      "|    policy_loss        | -0.0739  |\n",
      "|    value_loss         | 0.0299   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10610000, episode_reward=350.60 +/- 96.29\n",
      "Episode length: 9742.60 +/- 3201.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.74e+03 |\n",
      "|    mean_reward        | 351      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10610000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 132624   |\n",
      "|    policy_loss        | 0.0172   |\n",
      "|    value_loss         | 0.0539   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.82e+03 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 530      |\n",
      "|    iterations         | 132700   |\n",
      "|    time_elapsed       | 19994    |\n",
      "|    total_timesteps    | 10616000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 132699   |\n",
      "|    policy_loss        | -0.0123  |\n",
      "|    value_loss         | 0.0313   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10620000, episode_reward=288.40 +/- 96.32\n",
      "Episode length: 7947.20 +/- 2528.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.95e+03 |\n",
      "|    mean_reward        | 288      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10620000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.854   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 132749   |\n",
      "|    policy_loss        | 0.101    |\n",
      "|    value_loss         | 0.268    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.81e+03 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 530      |\n",
      "|    iterations         | 132800   |\n",
      "|    time_elapsed       | 20016    |\n",
      "|    total_timesteps    | 10624000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.898   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 132799   |\n",
      "|    policy_loss        | -0.118   |\n",
      "|    value_loss         | 0.0731   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10630000, episode_reward=380.60 +/- 29.52\n",
      "Episode length: 12202.40 +/- 6720.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.22e+04 |\n",
      "|    mean_reward        | 381      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10630000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.909   |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 132874   |\n",
      "|    policy_loss        | -0.0081  |\n",
      "|    value_loss         | 0.21     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.85e+03 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 530      |\n",
      "|    iterations         | 132900   |\n",
      "|    time_elapsed       | 20047    |\n",
      "|    total_timesteps    | 10632000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.881   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 132899   |\n",
      "|    policy_loss        | 0.0119   |\n",
      "|    value_loss         | 0.0438   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10640000, episode_reward=382.60 +/- 36.56\n",
      "Episode length: 23553.60 +/- 18050.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.36e+04 |\n",
      "|    mean_reward        | 383      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10640000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.819   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 132999   |\n",
      "|    policy_loss        | 0.0576   |\n",
      "|    value_loss         | 0.0473   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.82e+03 |\n",
      "|    ep_rew_mean     | 375      |\n",
      "| time/              |          |\n",
      "|    fps             | 529      |\n",
      "|    iterations      | 133000   |\n",
      "|    time_elapsed    | 20099    |\n",
      "|    total_timesteps | 10640000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.78e+03 |\n",
      "|    ep_rew_mean        | 375      |\n",
      "| time/                 |          |\n",
      "|    fps                | 529      |\n",
      "|    iterations         | 133100   |\n",
      "|    time_elapsed       | 20106    |\n",
      "|    total_timesteps    | 10648000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.818   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 133099   |\n",
      "|    policy_loss        | 0.0056   |\n",
      "|    value_loss         | 0.032    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10650000, episode_reward=312.80 +/- 147.57\n",
      "Episode length: 10973.00 +/- 5553.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.1e+04  |\n",
      "|    mean_reward        | 313      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10650000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.841   |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 133124   |\n",
      "|    policy_loss        | -0.0164  |\n",
      "|    value_loss         | 0.0475   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.78e+03 |\n",
      "|    ep_rew_mean        | 375      |\n",
      "| time/                 |          |\n",
      "|    fps                | 529      |\n",
      "|    iterations         | 133200   |\n",
      "|    time_elapsed       | 20134    |\n",
      "|    total_timesteps    | 10656000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.825   |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 133199   |\n",
      "|    policy_loss        | 0.101    |\n",
      "|    value_loss         | 0.155    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10660000, episode_reward=419.20 +/- 12.56\n",
      "Episode length: 11552.00 +/- 3025.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+04 |\n",
      "|    mean_reward        | 419      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10660000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.702   |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 133249   |\n",
      "|    policy_loss        | -0.12    |\n",
      "|    value_loss         | 0.377    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.73e+03 |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 528      |\n",
      "|    iterations         | 133300   |\n",
      "|    time_elapsed       | 20163    |\n",
      "|    total_timesteps    | 10664000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.717   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 133299   |\n",
      "|    policy_loss        | -0.0505  |\n",
      "|    value_loss         | 0.128    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10670000, episode_reward=323.00 +/- 120.21\n",
      "Episode length: 12325.60 +/- 10292.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.23e+04 |\n",
      "|    mean_reward        | 323      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10670000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.854   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 133374   |\n",
      "|    policy_loss        | 0.0362   |\n",
      "|    value_loss         | 0.071    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.73e+03 |\n",
      "|    ep_rew_mean        | 376      |\n",
      "| time/                 |          |\n",
      "|    fps                | 528      |\n",
      "|    iterations         | 133400   |\n",
      "|    time_elapsed       | 20195    |\n",
      "|    total_timesteps    | 10672000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.901   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 133399   |\n",
      "|    policy_loss        | -0.0579  |\n",
      "|    value_loss         | 0.0713   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10680000, episode_reward=402.80 +/- 15.61\n",
      "Episode length: 11588.20 +/- 2463.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+04 |\n",
      "|    mean_reward        | 403      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10680000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.802   |\n",
      "|    explained_variance | 0.866    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 133499   |\n",
      "|    policy_loss        | -0.283   |\n",
      "|    value_loss         | 0.61     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.65e+03 |\n",
      "|    ep_rew_mean     | 375      |\n",
      "| time/              |          |\n",
      "|    fps             | 528      |\n",
      "|    iterations      | 133500   |\n",
      "|    time_elapsed    | 20224    |\n",
      "|    total_timesteps | 10680000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.65e+03 |\n",
      "|    ep_rew_mean        | 375      |\n",
      "| time/                 |          |\n",
      "|    fps                | 528      |\n",
      "|    iterations         | 133600   |\n",
      "|    time_elapsed       | 20231    |\n",
      "|    total_timesteps    | 10688000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.714   |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 133599   |\n",
      "|    policy_loss        | -0.0246  |\n",
      "|    value_loss         | 0.35     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10690000, episode_reward=331.00 +/- 94.19\n",
      "Episode length: 8814.80 +/- 1601.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.81e+03 |\n",
      "|    mean_reward        | 331      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10690000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.747   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 133624   |\n",
      "|    policy_loss        | -0.018   |\n",
      "|    value_loss         | 0.0429   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.64e+03 |\n",
      "|    ep_rew_mean        | 375      |\n",
      "| time/                 |          |\n",
      "|    fps                | 528      |\n",
      "|    iterations         | 133700   |\n",
      "|    time_elapsed       | 20255    |\n",
      "|    total_timesteps    | 10696000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.677   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 133699   |\n",
      "|    policy_loss        | 0.046    |\n",
      "|    value_loss         | 0.0601   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10700000, episode_reward=365.80 +/- 83.10\n",
      "Episode length: 14162.20 +/- 11255.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.42e+04 |\n",
      "|    mean_reward        | 366      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10700000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.892   |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 133749   |\n",
      "|    policy_loss        | 0.0795   |\n",
      "|    value_loss         | 0.168    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.73e+03 |\n",
      "|    ep_rew_mean        | 375      |\n",
      "| time/                 |          |\n",
      "|    fps                | 527      |\n",
      "|    iterations         | 133800   |\n",
      "|    time_elapsed       | 20289    |\n",
      "|    total_timesteps    | 10704000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.874   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 133799   |\n",
      "|    policy_loss        | -0.0694  |\n",
      "|    value_loss         | 0.0681   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10710000, episode_reward=142.20 +/- 60.95\n",
      "Episode length: 5719.00 +/- 1524.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.72e+03 |\n",
      "|    mean_reward        | 142      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10710000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.907   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 133874   |\n",
      "|    policy_loss        | 0.00702  |\n",
      "|    value_loss         | 0.0194   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.59e+03 |\n",
      "|    ep_rew_mean        | 371      |\n",
      "| time/                 |          |\n",
      "|    fps                | 527      |\n",
      "|    iterations         | 133900   |\n",
      "|    time_elapsed       | 20307    |\n",
      "|    total_timesteps    | 10712000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.987   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 133899   |\n",
      "|    policy_loss        | -0.0236  |\n",
      "|    value_loss         | 0.0124   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10720000, episode_reward=406.00 +/- 31.52\n",
      "Episode length: 10330.20 +/- 902.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+04 |\n",
      "|    mean_reward        | 406      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10720000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.812   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 133999   |\n",
      "|    policy_loss        | -0.0139  |\n",
      "|    value_loss         | 0.11     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.56e+03 |\n",
      "|    ep_rew_mean     | 367      |\n",
      "| time/              |          |\n",
      "|    fps             | 527      |\n",
      "|    iterations      | 134000   |\n",
      "|    time_elapsed    | 20334    |\n",
      "|    total_timesteps | 10720000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.55e+03 |\n",
      "|    ep_rew_mean        | 367      |\n",
      "| time/                 |          |\n",
      "|    fps                | 527      |\n",
      "|    iterations         | 134100   |\n",
      "|    time_elapsed       | 20341    |\n",
      "|    total_timesteps    | 10728000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.748   |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 134099   |\n",
      "|    policy_loss        | -0.11    |\n",
      "|    value_loss         | 0.236    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10730000, episode_reward=401.20 +/- 25.55\n",
      "Episode length: 9034.20 +/- 1201.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.03e+03 |\n",
      "|    mean_reward        | 401      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10730000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.712   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 134124   |\n",
      "|    policy_loss        | -0.048   |\n",
      "|    value_loss         | 0.0227   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.6e+03  |\n",
      "|    ep_rew_mean        | 363      |\n",
      "| time/                 |          |\n",
      "|    fps                | 527      |\n",
      "|    iterations         | 134200   |\n",
      "|    time_elapsed       | 20365    |\n",
      "|    total_timesteps    | 10736000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.849   |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 134199   |\n",
      "|    policy_loss        | 0.0611   |\n",
      "|    value_loss         | 0.0518   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10740000, episode_reward=264.40 +/- 147.22\n",
      "Episode length: 8063.60 +/- 2457.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.06e+03 |\n",
      "|    mean_reward        | 264      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10740000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.926   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 134249   |\n",
      "|    policy_loss        | -0.0446  |\n",
      "|    value_loss         | 0.0226   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.61e+03 |\n",
      "|    ep_rew_mean        | 367      |\n",
      "| time/                 |          |\n",
      "|    fps                | 526      |\n",
      "|    iterations         | 134300   |\n",
      "|    time_elapsed       | 20387    |\n",
      "|    total_timesteps    | 10744000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.84    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 134299   |\n",
      "|    policy_loss        | 0.0406   |\n",
      "|    value_loss         | 0.0638   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10750000, episode_reward=303.40 +/- 136.02\n",
      "Episode length: 7883.80 +/- 1811.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.88e+03 |\n",
      "|    mean_reward        | 303      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10750000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 134374   |\n",
      "|    policy_loss        | 0.0266   |\n",
      "|    value_loss         | 0.0306   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.56e+03 |\n",
      "|    ep_rew_mean        | 366      |\n",
      "| time/                 |          |\n",
      "|    fps                | 526      |\n",
      "|    iterations         | 134400   |\n",
      "|    time_elapsed       | 20409    |\n",
      "|    total_timesteps    | 10752000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.94    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 134399   |\n",
      "|    policy_loss        | 0.0236   |\n",
      "|    value_loss         | 0.0206   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10760000, episode_reward=412.80 +/- 19.94\n",
      "Episode length: 10246.60 +/- 1756.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+04 |\n",
      "|    mean_reward        | 413      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10760000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.839   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 134499   |\n",
      "|    policy_loss        | -0.0434  |\n",
      "|    value_loss         | 0.0273   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.5e+03  |\n",
      "|    ep_rew_mean     | 363      |\n",
      "| time/              |          |\n",
      "|    fps             | 526      |\n",
      "|    iterations      | 134500   |\n",
      "|    time_elapsed    | 20435    |\n",
      "|    total_timesteps | 10760000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.48e+03 |\n",
      "|    ep_rew_mean        | 362      |\n",
      "| time/                 |          |\n",
      "|    fps                | 526      |\n",
      "|    iterations         | 134600   |\n",
      "|    time_elapsed       | 20442    |\n",
      "|    total_timesteps    | 10768000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.806   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 134599   |\n",
      "|    policy_loss        | 0.119    |\n",
      "|    value_loss         | 0.148    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10770000, episode_reward=359.00 +/- 39.45\n",
      "Episode length: 9615.20 +/- 3518.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.62e+03 |\n",
      "|    mean_reward        | 359      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10770000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.8     |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 134624   |\n",
      "|    policy_loss        | 0.00117  |\n",
      "|    value_loss         | 0.0608   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.53e+03 |\n",
      "|    ep_rew_mean        | 361      |\n",
      "| time/                 |          |\n",
      "|    fps                | 526      |\n",
      "|    iterations         | 134700   |\n",
      "|    time_elapsed       | 20467    |\n",
      "|    total_timesteps    | 10776000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.869   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 134699   |\n",
      "|    policy_loss        | 0.0233   |\n",
      "|    value_loss         | 0.0763   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10780000, episode_reward=411.00 +/- 5.06\n",
      "Episode length: 12610.40 +/- 5008.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.26e+04 |\n",
      "|    mean_reward        | 411      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10780000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.876   |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 134749   |\n",
      "|    policy_loss        | 0.0532   |\n",
      "|    value_loss         | 0.0642   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.47e+03 |\n",
      "|    ep_rew_mean        | 360      |\n",
      "| time/                 |          |\n",
      "|    fps                | 526      |\n",
      "|    iterations         | 134800   |\n",
      "|    time_elapsed       | 20498    |\n",
      "|    total_timesteps    | 10784000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.99    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 134799   |\n",
      "|    policy_loss        | -0.0321  |\n",
      "|    value_loss         | 0.0214   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10790000, episode_reward=365.80 +/- 106.26\n",
      "Episode length: 9378.80 +/- 1989.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.38e+03 |\n",
      "|    mean_reward        | 366      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10790000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 134874   |\n",
      "|    policy_loss        | 0.0184   |\n",
      "|    value_loss         | 0.0584   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.52e+03 |\n",
      "|    ep_rew_mean        | 360      |\n",
      "| time/                 |          |\n",
      "|    fps                | 525      |\n",
      "|    iterations         | 134900   |\n",
      "|    time_elapsed       | 20523    |\n",
      "|    total_timesteps    | 10792000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.929   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 134899   |\n",
      "|    policy_loss        | 0.0159   |\n",
      "|    value_loss         | 0.0219   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10800000, episode_reward=409.60 +/- 7.17\n",
      "Episode length: 16067.40 +/- 10052.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.61e+04 |\n",
      "|    mean_reward        | 410      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10800000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.925   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 134999   |\n",
      "|    policy_loss        | 0.00418  |\n",
      "|    value_loss         | 0.132    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.48e+03 |\n",
      "|    ep_rew_mean     | 357      |\n",
      "| time/              |          |\n",
      "|    fps             | 525      |\n",
      "|    iterations      | 135000   |\n",
      "|    time_elapsed    | 20561    |\n",
      "|    total_timesteps | 10800000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.52e+03 |\n",
      "|    ep_rew_mean        | 356      |\n",
      "| time/                 |          |\n",
      "|    fps                | 525      |\n",
      "|    iterations         | 135100   |\n",
      "|    time_elapsed       | 20568    |\n",
      "|    total_timesteps    | 10808000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.861   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 135099   |\n",
      "|    policy_loss        | -0.0197  |\n",
      "|    value_loss         | 0.042    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10810000, episode_reward=433.00 +/- 115.33\n",
      "Episode length: 13174.60 +/- 5271.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.32e+04 |\n",
      "|    mean_reward        | 433      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10810000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.829   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 135124   |\n",
      "|    policy_loss        | -0.00298 |\n",
      "|    value_loss         | 0.0581   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.56e+03 |\n",
      "|    ep_rew_mean        | 357      |\n",
      "| time/                 |          |\n",
      "|    fps                | 525      |\n",
      "|    iterations         | 135200   |\n",
      "|    time_elapsed       | 20600    |\n",
      "|    total_timesteps    | 10816000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.984   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 135199   |\n",
      "|    policy_loss        | 0.0216   |\n",
      "|    value_loss         | 0.0525   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10820000, episode_reward=406.80 +/- 12.61\n",
      "Episode length: 11098.40 +/- 2712.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.11e+04 |\n",
      "|    mean_reward        | 407      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10820000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.977   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 135249   |\n",
      "|    policy_loss        | 0.0448   |\n",
      "|    value_loss         | 0.057    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.61e+03 |\n",
      "|    ep_rew_mean        | 358      |\n",
      "| time/                 |          |\n",
      "|    fps                | 524      |\n",
      "|    iterations         | 135300   |\n",
      "|    time_elapsed       | 20628    |\n",
      "|    total_timesteps    | 10824000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.918   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 135299   |\n",
      "|    policy_loss        | 0.0166   |\n",
      "|    value_loss         | 0.0136   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10830000, episode_reward=492.60 +/- 179.05\n",
      "Episode length: 13151.20 +/- 4046.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.32e+04 |\n",
      "|    mean_reward        | 493      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10830000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.853   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 135374   |\n",
      "|    policy_loss        | 0.0157   |\n",
      "|    value_loss         | 0.0284   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.69e+03 |\n",
      "|    ep_rew_mean        | 358      |\n",
      "| time/                 |          |\n",
      "|    fps                | 524      |\n",
      "|    iterations         | 135400   |\n",
      "|    time_elapsed       | 20661    |\n",
      "|    total_timesteps    | 10832000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.932   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 135399   |\n",
      "|    policy_loss        | -0.0212  |\n",
      "|    value_loss         | 0.0969   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10840000, episode_reward=424.60 +/- 4.92\n",
      "Episode length: 11236.60 +/- 1166.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 425      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10840000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.941   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 135499   |\n",
      "|    policy_loss        | 0.0323   |\n",
      "|    value_loss         | 0.116    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.7e+03  |\n",
      "|    ep_rew_mean     | 360      |\n",
      "| time/              |          |\n",
      "|    fps             | 523      |\n",
      "|    iterations      | 135500   |\n",
      "|    time_elapsed    | 20689    |\n",
      "|    total_timesteps | 10840000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.77e+03 |\n",
      "|    ep_rew_mean        | 363      |\n",
      "| time/                 |          |\n",
      "|    fps                | 524      |\n",
      "|    iterations         | 135600   |\n",
      "|    time_elapsed       | 20696    |\n",
      "|    total_timesteps    | 10848000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.854   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 135599   |\n",
      "|    policy_loss        | -0.0503  |\n",
      "|    value_loss         | 0.0323   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10850000, episode_reward=418.20 +/- 17.51\n",
      "Episode length: 12845.60 +/- 3819.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.28e+04 |\n",
      "|    mean_reward        | 418      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10850000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.847   |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 135624   |\n",
      "|    policy_loss        | 0.0613   |\n",
      "|    value_loss         | 0.104    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.73e+03 |\n",
      "|    ep_rew_mean        | 364      |\n",
      "| time/                 |          |\n",
      "|    fps                | 523      |\n",
      "|    iterations         | 135700   |\n",
      "|    time_elapsed       | 20727    |\n",
      "|    total_timesteps    | 10856000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 135699   |\n",
      "|    policy_loss        | 0.0496   |\n",
      "|    value_loss         | 0.0692   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10860000, episode_reward=395.00 +/- 53.99\n",
      "Episode length: 12493.40 +/- 5688.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.25e+04 |\n",
      "|    mean_reward        | 395      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10860000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 135749   |\n",
      "|    policy_loss        | 0.00585  |\n",
      "|    value_loss         | 0.065    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.76e+03 |\n",
      "|    ep_rew_mean        | 365      |\n",
      "| time/                 |          |\n",
      "|    fps                | 523      |\n",
      "|    iterations         | 135800   |\n",
      "|    time_elapsed       | 20758    |\n",
      "|    total_timesteps    | 10864000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.908   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 135799   |\n",
      "|    policy_loss        | 0.0276   |\n",
      "|    value_loss         | 0.0265   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10870000, episode_reward=415.40 +/- 12.24\n",
      "Episode length: 12389.40 +/- 2617.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.24e+04 |\n",
      "|    mean_reward        | 415      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10870000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.948   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 135874   |\n",
      "|    policy_loss        | 0.00504  |\n",
      "|    value_loss         | 0.0553   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.71e+03 |\n",
      "|    ep_rew_mean        | 362      |\n",
      "| time/                 |          |\n",
      "|    fps                | 522      |\n",
      "|    iterations         | 135900   |\n",
      "|    time_elapsed       | 20788    |\n",
      "|    total_timesteps    | 10872000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.85    |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 135899   |\n",
      "|    policy_loss        | 0.00362  |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10880000, episode_reward=350.60 +/- 54.96\n",
      "Episode length: 8513.80 +/- 575.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.51e+03 |\n",
      "|    mean_reward        | 351      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10880000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.928   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 135999   |\n",
      "|    policy_loss        | 0.0172   |\n",
      "|    value_loss         | 0.0251   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.79e+03 |\n",
      "|    ep_rew_mean     | 363      |\n",
      "| time/              |          |\n",
      "|    fps             | 522      |\n",
      "|    iterations      | 136000   |\n",
      "|    time_elapsed    | 20811    |\n",
      "|    total_timesteps | 10880000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.73e+03 |\n",
      "|    ep_rew_mean        | 364      |\n",
      "| time/                 |          |\n",
      "|    fps                | 522      |\n",
      "|    iterations         | 136100   |\n",
      "|    time_elapsed       | 20818    |\n",
      "|    total_timesteps    | 10888000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.858   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 136099   |\n",
      "|    policy_loss        | -0.0982  |\n",
      "|    value_loss         | 0.0731   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10890000, episode_reward=372.20 +/- 30.72\n",
      "Episode length: 8211.80 +/- 1860.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.21e+03 |\n",
      "|    mean_reward        | 372      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10890000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.937   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 136124   |\n",
      "|    policy_loss        | -0.0343  |\n",
      "|    value_loss         | 0.0186   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.81e+03 |\n",
      "|    ep_rew_mean        | 366      |\n",
      "| time/                 |          |\n",
      "|    fps                | 522      |\n",
      "|    iterations         | 136200   |\n",
      "|    time_elapsed       | 20841    |\n",
      "|    total_timesteps    | 10896000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.932   |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 136199   |\n",
      "|    policy_loss        | 0.019    |\n",
      "|    value_loss         | 0.105    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10900000, episode_reward=405.00 +/- 72.78\n",
      "Episode length: 12017.40 +/- 5250.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.2e+04  |\n",
      "|    mean_reward        | 405      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10900000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.981   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 136249   |\n",
      "|    policy_loss        | 0.0649   |\n",
      "|    value_loss         | 0.0247   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.78e+03 |\n",
      "|    ep_rew_mean        | 363      |\n",
      "| time/                 |          |\n",
      "|    fps                | 522      |\n",
      "|    iterations         | 136300   |\n",
      "|    time_elapsed       | 20871    |\n",
      "|    total_timesteps    | 10904000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.849   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 136299   |\n",
      "|    policy_loss        | -0.00269 |\n",
      "|    value_loss         | 0.0265   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10910000, episode_reward=376.40 +/- 227.26\n",
      "Episode length: 10123.20 +/- 4170.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+04 |\n",
      "|    mean_reward        | 376      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10910000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.768   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 136374   |\n",
      "|    policy_loss        | 0.0476   |\n",
      "|    value_loss         | 0.0641   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.84e+03 |\n",
      "|    ep_rew_mean        | 366      |\n",
      "| time/                 |          |\n",
      "|    fps                | 522      |\n",
      "|    iterations         | 136400   |\n",
      "|    time_elapsed       | 20897    |\n",
      "|    total_timesteps    | 10912000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.935   |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 136399   |\n",
      "|    policy_loss        | -0.0138  |\n",
      "|    value_loss         | 0.0565   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10920000, episode_reward=365.60 +/- 78.81\n",
      "Episode length: 9388.20 +/- 1896.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.39e+03 |\n",
      "|    mean_reward        | 366      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10920000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.925   |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 136499   |\n",
      "|    policy_loss        | 0.174    |\n",
      "|    value_loss         | 0.293    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.79e+03 |\n",
      "|    ep_rew_mean     | 362      |\n",
      "| time/              |          |\n",
      "|    fps             | 521      |\n",
      "|    iterations      | 136500   |\n",
      "|    time_elapsed    | 20922    |\n",
      "|    total_timesteps | 10920000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.73e+03 |\n",
      "|    ep_rew_mean        | 364      |\n",
      "| time/                 |          |\n",
      "|    fps                | 522      |\n",
      "|    iterations         | 136600   |\n",
      "|    time_elapsed       | 20930    |\n",
      "|    total_timesteps    | 10928000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 136599   |\n",
      "|    policy_loss        | 0.0911   |\n",
      "|    value_loss         | 0.112    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10930000, episode_reward=322.20 +/- 89.23\n",
      "Episode length: 7885.00 +/- 2036.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.88e+03 |\n",
      "|    mean_reward        | 322      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10930000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.89    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 136624   |\n",
      "|    policy_loss        | 0.0441   |\n",
      "|    value_loss         | 0.058    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.71e+03 |\n",
      "|    ep_rew_mean        | 362      |\n",
      "| time/                 |          |\n",
      "|    fps                | 521      |\n",
      "|    iterations         | 136700   |\n",
      "|    time_elapsed       | 20952    |\n",
      "|    total_timesteps    | 10936000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.911   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 136699   |\n",
      "|    policy_loss        | -0.00188 |\n",
      "|    value_loss         | 0.0584   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10940000, episode_reward=365.60 +/- 41.12\n",
      "Episode length: 8022.40 +/- 791.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.02e+03 |\n",
      "|    mean_reward        | 366      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10940000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.796   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 136749   |\n",
      "|    policy_loss        | 0.0502   |\n",
      "|    value_loss         | 0.036    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.66e+03 |\n",
      "|    ep_rew_mean        | 362      |\n",
      "| time/                 |          |\n",
      "|    fps                | 521      |\n",
      "|    iterations         | 136800   |\n",
      "|    time_elapsed       | 20975    |\n",
      "|    total_timesteps    | 10944000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.935   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 136799   |\n",
      "|    policy_loss        | 0.0139   |\n",
      "|    value_loss         | 0.0504   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10950000, episode_reward=359.60 +/- 40.83\n",
      "Episode length: 18439.00 +/- 19092.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.84e+04 |\n",
      "|    mean_reward        | 360      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10950000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.849   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 136874   |\n",
      "|    policy_loss        | 0.111    |\n",
      "|    value_loss         | 0.153    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.57e+03 |\n",
      "|    ep_rew_mean        | 362      |\n",
      "| time/                 |          |\n",
      "|    fps                | 521      |\n",
      "|    iterations         | 136900   |\n",
      "|    time_elapsed       | 21017    |\n",
      "|    total_timesteps    | 10952000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.909   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 136899   |\n",
      "|    policy_loss        | -0.01    |\n",
      "|    value_loss         | 0.0759   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10960000, episode_reward=387.60 +/- 33.29\n",
      "Episode length: 9592.80 +/- 2275.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.59e+03 |\n",
      "|    mean_reward        | 388      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10960000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.947   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 136999   |\n",
      "|    policy_loss        | -0.065   |\n",
      "|    value_loss         | 0.0301   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.61e+03 |\n",
      "|    ep_rew_mean     | 365      |\n",
      "| time/              |          |\n",
      "|    fps             | 520      |\n",
      "|    iterations      | 137000   |\n",
      "|    time_elapsed    | 21043    |\n",
      "|    total_timesteps | 10960000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.51e+03 |\n",
      "|    ep_rew_mean        | 366      |\n",
      "| time/                 |          |\n",
      "|    fps                | 521      |\n",
      "|    iterations         | 137100   |\n",
      "|    time_elapsed       | 21050    |\n",
      "|    total_timesteps    | 10968000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.861   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 137099   |\n",
      "|    policy_loss        | -0.0381  |\n",
      "|    value_loss         | 0.073    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10970000, episode_reward=388.20 +/- 38.74\n",
      "Episode length: 9517.60 +/- 2435.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.52e+03 |\n",
      "|    mean_reward        | 388      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10970000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.964   |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 137124   |\n",
      "|    policy_loss        | -0.0508  |\n",
      "|    value_loss         | 0.152    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.56e+03 |\n",
      "|    ep_rew_mean        | 369      |\n",
      "| time/                 |          |\n",
      "|    fps                | 520      |\n",
      "|    iterations         | 137200   |\n",
      "|    time_elapsed       | 21075    |\n",
      "|    total_timesteps    | 10976000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.87    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 137199   |\n",
      "|    policy_loss        | -0.12    |\n",
      "|    value_loss         | 0.0676   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10980000, episode_reward=408.40 +/- 14.60\n",
      "Episode length: 11728.80 +/- 5153.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.17e+04 |\n",
      "|    mean_reward        | 408      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10980000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.841   |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 137249   |\n",
      "|    policy_loss        | -0.15    |\n",
      "|    value_loss         | 0.586    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.6e+03  |\n",
      "|    ep_rew_mean        | 369      |\n",
      "| time/                 |          |\n",
      "|    fps                | 520      |\n",
      "|    iterations         | 137300   |\n",
      "|    time_elapsed       | 21105    |\n",
      "|    total_timesteps    | 10984000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.869   |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 137299   |\n",
      "|    policy_loss        | -0.0392  |\n",
      "|    value_loss         | 0.732    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10990000, episode_reward=364.00 +/- 40.97\n",
      "Episode length: 8943.20 +/- 958.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.94e+03 |\n",
      "|    mean_reward        | 364      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10990000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.901   |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 137374   |\n",
      "|    policy_loss        | -0.0122  |\n",
      "|    value_loss         | 0.15     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.66e+03 |\n",
      "|    ep_rew_mean        | 369      |\n",
      "| time/                 |          |\n",
      "|    fps                | 520      |\n",
      "|    iterations         | 137400   |\n",
      "|    time_elapsed       | 21129    |\n",
      "|    total_timesteps    | 10992000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.866   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 137399   |\n",
      "|    policy_loss        | -0.024   |\n",
      "|    value_loss         | 0.0259   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11000000, episode_reward=407.00 +/- 61.43\n",
      "Episode length: 11528.00 +/- 2643.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.15e+04 |\n",
      "|    mean_reward        | 407      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11000000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.87    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 137499   |\n",
      "|    policy_loss        | 0.0189   |\n",
      "|    value_loss         | 0.107    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.66e+03 |\n",
      "|    ep_rew_mean     | 370      |\n",
      "| time/              |          |\n",
      "|    fps             | 519      |\n",
      "|    iterations      | 137500   |\n",
      "|    time_elapsed    | 21158    |\n",
      "|    total_timesteps | 11000000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.7e+03  |\n",
      "|    ep_rew_mean        | 374      |\n",
      "| time/                 |          |\n",
      "|    fps                | 520      |\n",
      "|    iterations         | 137600   |\n",
      "|    time_elapsed       | 21165    |\n",
      "|    total_timesteps    | 11008000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 137599   |\n",
      "|    policy_loss        | -0.0107  |\n",
      "|    value_loss         | 0.0241   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11010000, episode_reward=319.20 +/- 56.40\n",
      "Episode length: 11606.00 +/- 6262.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+04 |\n",
      "|    mean_reward        | 319      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11010000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.934   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 137624   |\n",
      "|    policy_loss        | 0.03     |\n",
      "|    value_loss         | 0.0876   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.69e+03 |\n",
      "|    ep_rew_mean        | 373      |\n",
      "| time/                 |          |\n",
      "|    fps                | 519      |\n",
      "|    iterations         | 137700   |\n",
      "|    time_elapsed       | 21195    |\n",
      "|    total_timesteps    | 11016000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.905   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 137699   |\n",
      "|    policy_loss        | -0.117   |\n",
      "|    value_loss         | 0.0526   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11020000, episode_reward=383.40 +/- 29.27\n",
      "Episode length: 10110.80 +/- 1826.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+04 |\n",
      "|    mean_reward        | 383      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11020000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.943   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 137749   |\n",
      "|    policy_loss        | -0.00732 |\n",
      "|    value_loss         | 0.0438   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.74e+03 |\n",
      "|    ep_rew_mean        | 374      |\n",
      "| time/                 |          |\n",
      "|    fps                | 519      |\n",
      "|    iterations         | 137800   |\n",
      "|    time_elapsed       | 21221    |\n",
      "|    total_timesteps    | 11024000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.836   |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 137799   |\n",
      "|    policy_loss        | -0.0314  |\n",
      "|    value_loss         | 0.0873   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11030000, episode_reward=469.00 +/- 111.82\n",
      "Episode length: 10871.60 +/- 3158.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.09e+04 |\n",
      "|    mean_reward        | 469      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11030000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.873   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 137874   |\n",
      "|    policy_loss        | -0.00469 |\n",
      "|    value_loss         | 0.151    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.7e+03  |\n",
      "|    ep_rew_mean        | 373      |\n",
      "| time/                 |          |\n",
      "|    fps                | 519      |\n",
      "|    iterations         | 137900   |\n",
      "|    time_elapsed       | 21249    |\n",
      "|    total_timesteps    | 11032000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.892   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 137899   |\n",
      "|    policy_loss        | -0.0417  |\n",
      "|    value_loss         | 0.0364   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11040000, episode_reward=507.20 +/- 138.61\n",
      "Episode length: 11226.60 +/- 4110.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 507      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11040000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.932   |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 137999   |\n",
      "|    policy_loss        | -0.136   |\n",
      "|    value_loss         | 0.208    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.77e+03 |\n",
      "|    ep_rew_mean     | 377      |\n",
      "| time/              |          |\n",
      "|    fps             | 518      |\n",
      "|    iterations      | 138000   |\n",
      "|    time_elapsed    | 21277    |\n",
      "|    total_timesteps | 11040000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.71e+03 |\n",
      "|    ep_rew_mean        | 379      |\n",
      "| time/                 |          |\n",
      "|    fps                | 519      |\n",
      "|    iterations         | 138100   |\n",
      "|    time_elapsed       | 21284    |\n",
      "|    total_timesteps    | 11048000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.869   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 138099   |\n",
      "|    policy_loss        | -0.015   |\n",
      "|    value_loss         | 0.0806   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11050000, episode_reward=389.40 +/- 16.01\n",
      "Episode length: 11321.40 +/- 2593.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+04 |\n",
      "|    mean_reward        | 389      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11050000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.913   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 138124   |\n",
      "|    policy_loss        | -0.053   |\n",
      "|    value_loss         | 0.0436   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.7e+03  |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 518      |\n",
      "|    iterations         | 138200   |\n",
      "|    time_elapsed       | 21312    |\n",
      "|    total_timesteps    | 11056000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.915   |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 138199   |\n",
      "|    policy_loss        | -0.12    |\n",
      "|    value_loss         | 0.212    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11060000, episode_reward=405.20 +/- 20.60\n",
      "Episode length: 11066.60 +/- 2936.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.11e+04 |\n",
      "|    mean_reward        | 405      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11060000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 138249   |\n",
      "|    policy_loss        | -0.0584  |\n",
      "|    value_loss         | 0.0165   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.59e+03 |\n",
      "|    ep_rew_mean        | 381      |\n",
      "| time/                 |          |\n",
      "|    fps                | 518      |\n",
      "|    iterations         | 138300   |\n",
      "|    time_elapsed       | 21341    |\n",
      "|    total_timesteps    | 11064000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.836   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 138299   |\n",
      "|    policy_loss        | -0.0278  |\n",
      "|    value_loss         | 0.095    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11070000, episode_reward=383.80 +/- 25.76\n",
      "Episode length: 9249.00 +/- 1263.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.25e+03 |\n",
      "|    mean_reward        | 384      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11070000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.847   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 138374   |\n",
      "|    policy_loss        | -0.0168  |\n",
      "|    value_loss         | 0.0361   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.53e+03 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 518      |\n",
      "|    iterations         | 138400   |\n",
      "|    time_elapsed       | 21365    |\n",
      "|    total_timesteps    | 11072000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 138399   |\n",
      "|    policy_loss        | 0.0779   |\n",
      "|    value_loss         | 0.152    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11080000, episode_reward=402.00 +/- 40.64\n",
      "Episode length: 14451.80 +/- 5035.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.45e+04 |\n",
      "|    mean_reward        | 402      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11080000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.854   |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 138499   |\n",
      "|    policy_loss        | 0.0738   |\n",
      "|    value_loss         | 0.146    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.49e+03 |\n",
      "|    ep_rew_mean     | 378      |\n",
      "| time/              |          |\n",
      "|    fps             | 517      |\n",
      "|    iterations      | 138500   |\n",
      "|    time_elapsed    | 21400    |\n",
      "|    total_timesteps | 11080000 |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9.46e+03  |\n",
      "|    ep_rew_mean        | 377       |\n",
      "| time/                 |           |\n",
      "|    fps                | 517       |\n",
      "|    iterations         | 138600    |\n",
      "|    time_elapsed       | 21407     |\n",
      "|    total_timesteps    | 11088000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.905    |\n",
      "|    explained_variance | 0.906     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 138599    |\n",
      "|    policy_loss        | -0.000574 |\n",
      "|    value_loss         | 0.331     |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=11090000, episode_reward=349.60 +/- 59.24\n",
      "Episode length: 8229.00 +/- 1742.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.23e+03 |\n",
      "|    mean_reward        | 350      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11090000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.757   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 138624   |\n",
      "|    policy_loss        | 0.00658  |\n",
      "|    value_loss         | 0.0184   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.34e+03 |\n",
      "|    ep_rew_mean        | 375      |\n",
      "| time/                 |          |\n",
      "|    fps                | 517      |\n",
      "|    iterations         | 138700   |\n",
      "|    time_elapsed       | 21430    |\n",
      "|    total_timesteps    | 11096000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.831   |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 138699   |\n",
      "|    policy_loss        | 0.0392   |\n",
      "|    value_loss         | 0.073    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11100000, episode_reward=381.40 +/- 71.91\n",
      "Episode length: 11464.00 +/- 2251.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.15e+04 |\n",
      "|    mean_reward        | 381      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11100000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.886   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 138749   |\n",
      "|    policy_loss        | -0.0223  |\n",
      "|    value_loss         | 0.026    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.34e+03 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 517      |\n",
      "|    iterations         | 138800   |\n",
      "|    time_elapsed       | 21459    |\n",
      "|    total_timesteps    | 11104000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.794   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 138799   |\n",
      "|    policy_loss        | 0.0635   |\n",
      "|    value_loss         | 0.0611   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11110000, episode_reward=315.40 +/- 131.88\n",
      "Episode length: 9845.00 +/- 4987.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.84e+03 |\n",
      "|    mean_reward        | 315      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11110000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.813   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 138874   |\n",
      "|    policy_loss        | -0.0282  |\n",
      "|    value_loss         | 0.175    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.3e+03  |\n",
      "|    ep_rew_mean        | 373      |\n",
      "| time/                 |          |\n",
      "|    fps                | 517      |\n",
      "|    iterations         | 138900   |\n",
      "|    time_elapsed       | 21485    |\n",
      "|    total_timesteps    | 11112000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.81    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 138899   |\n",
      "|    policy_loss        | -0.0129  |\n",
      "|    value_loss         | 0.0335   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11120000, episode_reward=368.20 +/- 72.10\n",
      "Episode length: 8601.80 +/- 1326.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.6e+03  |\n",
      "|    mean_reward        | 368      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11120000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.778   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 138999   |\n",
      "|    policy_loss        | -0.0262  |\n",
      "|    value_loss         | 0.0214   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.3e+03  |\n",
      "|    ep_rew_mean     | 373      |\n",
      "| time/              |          |\n",
      "|    fps             | 516      |\n",
      "|    iterations      | 139000   |\n",
      "|    time_elapsed    | 21509    |\n",
      "|    total_timesteps | 11120000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.32e+03 |\n",
      "|    ep_rew_mean        | 373      |\n",
      "| time/                 |          |\n",
      "|    fps                | 517      |\n",
      "|    iterations         | 139100   |\n",
      "|    time_elapsed       | 21516    |\n",
      "|    total_timesteps    | 11128000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.875   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 139099   |\n",
      "|    policy_loss        | -0.0112  |\n",
      "|    value_loss         | 0.0222   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11130000, episode_reward=482.40 +/- 173.47\n",
      "Episode length: 13906.80 +/- 5455.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.39e+04 |\n",
      "|    mean_reward        | 482      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11130000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.801   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 139124   |\n",
      "|    policy_loss        | 0.0333   |\n",
      "|    value_loss         | 0.0171   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.33e+03 |\n",
      "|    ep_rew_mean        | 375      |\n",
      "| time/                 |          |\n",
      "|    fps                | 516      |\n",
      "|    iterations         | 139200   |\n",
      "|    time_elapsed       | 21549    |\n",
      "|    total_timesteps    | 11136000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.818   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 139199   |\n",
      "|    policy_loss        | -0.031   |\n",
      "|    value_loss         | 0.0738   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11140000, episode_reward=383.40 +/- 61.61\n",
      "Episode length: 10209.00 +/- 3154.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+04 |\n",
      "|    mean_reward        | 383      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11140000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.836   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 139249   |\n",
      "|    policy_loss        | -0.00805 |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.37e+03 |\n",
      "|    ep_rew_mean        | 375      |\n",
      "| time/                 |          |\n",
      "|    fps                | 516      |\n",
      "|    iterations         | 139300   |\n",
      "|    time_elapsed       | 21575    |\n",
      "|    total_timesteps    | 11144000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.936   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 139299   |\n",
      "|    policy_loss        | 0.0457   |\n",
      "|    value_loss         | 0.0518   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11150000, episode_reward=388.00 +/- 22.10\n",
      "Episode length: 10137.20 +/- 1470.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+04 |\n",
      "|    mean_reward        | 388      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11150000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.916   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 139374   |\n",
      "|    policy_loss        | -0.0156  |\n",
      "|    value_loss         | 0.0229   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.48e+03 |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 516      |\n",
      "|    iterations         | 139400   |\n",
      "|    time_elapsed       | 21601    |\n",
      "|    total_timesteps    | 11152000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 139399   |\n",
      "|    policy_loss        | 0.00176  |\n",
      "|    value_loss         | 0.0265   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11160000, episode_reward=475.40 +/- 184.08\n",
      "Episode length: 18278.40 +/- 11494.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.83e+04 |\n",
      "|    mean_reward        | 475      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11160000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.8     |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 139499   |\n",
      "|    policy_loss        | -0.044   |\n",
      "|    value_loss         | 0.0192   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.59e+03 |\n",
      "|    ep_rew_mean     | 378      |\n",
      "| time/              |          |\n",
      "|    fps             | 515      |\n",
      "|    iterations      | 139500   |\n",
      "|    time_elapsed    | 21642    |\n",
      "|    total_timesteps | 11160000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.65e+03 |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 515      |\n",
      "|    iterations         | 139600   |\n",
      "|    time_elapsed       | 21649    |\n",
      "|    total_timesteps    | 11168000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.816   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 139599   |\n",
      "|    policy_loss        | 0.0391   |\n",
      "|    value_loss         | 0.0767   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11170000, episode_reward=220.40 +/- 136.79\n",
      "Episode length: 8125.80 +/- 3139.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.13e+03 |\n",
      "|    mean_reward        | 220      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11170000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.892   |\n",
      "|    explained_variance | 0.773    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 139624   |\n",
      "|    policy_loss        | -0.129   |\n",
      "|    value_loss         | 0.557    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.61e+03 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 515      |\n",
      "|    iterations         | 139700   |\n",
      "|    time_elapsed       | 21672    |\n",
      "|    total_timesteps    | 11176000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.926   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 139699   |\n",
      "|    policy_loss        | 0.0173   |\n",
      "|    value_loss         | 0.0503   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11180000, episode_reward=438.20 +/- 181.89\n",
      "Episode length: 13127.40 +/- 7927.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.31e+04 |\n",
      "|    mean_reward        | 438      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11180000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.918   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 139749   |\n",
      "|    policy_loss        | 0.0181   |\n",
      "|    value_loss         | 0.0511   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.57e+03 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 515      |\n",
      "|    iterations         | 139800   |\n",
      "|    time_elapsed       | 21704    |\n",
      "|    total_timesteps    | 11184000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.859   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 139799   |\n",
      "|    policy_loss        | 0.0409   |\n",
      "|    value_loss         | 0.0229   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11190000, episode_reward=417.20 +/- 19.51\n",
      "Episode length: 10495.40 +/- 2217.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+04 |\n",
      "|    mean_reward        | 417      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11190000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.872   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 139874   |\n",
      "|    policy_loss        | 0.00274  |\n",
      "|    value_loss         | 0.0256   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.66e+03 |\n",
      "|    ep_rew_mean        | 376      |\n",
      "| time/                 |          |\n",
      "|    fps                | 514      |\n",
      "|    iterations         | 139900   |\n",
      "|    time_elapsed       | 21732    |\n",
      "|    total_timesteps    | 11192000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.913   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 139899   |\n",
      "|    policy_loss        | -0.109   |\n",
      "|    value_loss         | 0.078    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11200000, episode_reward=380.60 +/- 45.47\n",
      "Episode length: 9865.80 +/- 3292.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.87e+03 |\n",
      "|    mean_reward        | 381      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11200000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.855   |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 139999   |\n",
      "|    policy_loss        | 0.0575   |\n",
      "|    value_loss         | 0.0597   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.7e+03  |\n",
      "|    ep_rew_mean     | 375      |\n",
      "| time/              |          |\n",
      "|    fps             | 514      |\n",
      "|    iterations      | 140000   |\n",
      "|    time_elapsed    | 21757    |\n",
      "|    total_timesteps | 11200000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.72e+03 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 514      |\n",
      "|    iterations         | 140100   |\n",
      "|    time_elapsed       | 21765    |\n",
      "|    total_timesteps    | 11208000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.829   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 140099   |\n",
      "|    policy_loss        | 0.0706   |\n",
      "|    value_loss         | 0.102    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11210000, episode_reward=334.80 +/- 151.13\n",
      "Episode length: 12122.20 +/- 7948.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.21e+04 |\n",
      "|    mean_reward        | 335      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11210000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.905   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 140124   |\n",
      "|    policy_loss        | 0.0292   |\n",
      "|    value_loss         | 0.234    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.67e+03 |\n",
      "|    ep_rew_mean        | 375      |\n",
      "| time/                 |          |\n",
      "|    fps                | 514      |\n",
      "|    iterations         | 140200   |\n",
      "|    time_elapsed       | 21795    |\n",
      "|    total_timesteps    | 11216000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.904   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 140199   |\n",
      "|    policy_loss        | 0.0776   |\n",
      "|    value_loss         | 0.0512   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11220000, episode_reward=370.20 +/- 41.36\n",
      "Episode length: 9661.00 +/- 1675.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.66e+03 |\n",
      "|    mean_reward        | 370      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11220000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 140249   |\n",
      "|    policy_loss        | 0.0116   |\n",
      "|    value_loss         | 0.0247   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.74e+03 |\n",
      "|    ep_rew_mean        | 376      |\n",
      "| time/                 |          |\n",
      "|    fps                | 514      |\n",
      "|    iterations         | 140300   |\n",
      "|    time_elapsed       | 21821    |\n",
      "|    total_timesteps    | 11224000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.886   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 140299   |\n",
      "|    policy_loss        | 0.0239   |\n",
      "|    value_loss         | 0.023    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11230000, episode_reward=406.00 +/- 31.28\n",
      "Episode length: 10111.80 +/- 1200.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+04 |\n",
      "|    mean_reward        | 406      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11230000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.873   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 140374   |\n",
      "|    policy_loss        | -0.00313 |\n",
      "|    value_loss         | 0.0824   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.76e+03 |\n",
      "|    ep_rew_mean        | 376      |\n",
      "| time/                 |          |\n",
      "|    fps                | 514      |\n",
      "|    iterations         | 140400   |\n",
      "|    time_elapsed       | 21848    |\n",
      "|    total_timesteps    | 11232000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.937   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 140399   |\n",
      "|    policy_loss        | -0.063   |\n",
      "|    value_loss         | 0.0699   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11240000, episode_reward=387.20 +/- 25.90\n",
      "Episode length: 13853.00 +/- 8605.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.39e+04 |\n",
      "|    mean_reward        | 387      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11240000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.941   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 140499   |\n",
      "|    policy_loss        | 0.0268   |\n",
      "|    value_loss         | 0.0245   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.81e+03 |\n",
      "|    ep_rew_mean     | 377      |\n",
      "| time/              |          |\n",
      "|    fps             | 513      |\n",
      "|    iterations      | 140500   |\n",
      "|    time_elapsed    | 21881    |\n",
      "|    total_timesteps | 11240000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.83e+03 |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 513      |\n",
      "|    iterations         | 140600   |\n",
      "|    time_elapsed       | 21888    |\n",
      "|    total_timesteps    | 11248000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.78    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 140599   |\n",
      "|    policy_loss        | -0.0537  |\n",
      "|    value_loss         | 0.0605   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11250000, episode_reward=388.60 +/- 36.69\n",
      "Episode length: 8385.40 +/- 2590.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.39e+03 |\n",
      "|    mean_reward        | 389      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11250000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.969   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 140624   |\n",
      "|    policy_loss        | 0.0151   |\n",
      "|    value_loss         | 0.0496   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.83e+03 |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 513      |\n",
      "|    iterations         | 140700   |\n",
      "|    time_elapsed       | 21911    |\n",
      "|    total_timesteps    | 11256000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.845   |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 140699   |\n",
      "|    policy_loss        | 0.0851   |\n",
      "|    value_loss         | 0.0886   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11260000, episode_reward=404.40 +/- 12.56\n",
      "Episode length: 11623.80 +/- 3200.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+04 |\n",
      "|    mean_reward        | 404      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11260000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.959   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 140749   |\n",
      "|    policy_loss        | 0.0658   |\n",
      "|    value_loss         | 0.0196   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.81e+03 |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 513      |\n",
      "|    iterations         | 140800   |\n",
      "|    time_elapsed       | 21941    |\n",
      "|    total_timesteps    | 11264000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.783   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 140799   |\n",
      "|    policy_loss        | -0.0481  |\n",
      "|    value_loss         | 0.0406   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11270000, episode_reward=406.20 +/- 20.03\n",
      "Episode length: 12233.80 +/- 7193.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.22e+04 |\n",
      "|    mean_reward        | 406      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11270000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.829   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 140874   |\n",
      "|    policy_loss        | 0.0428   |\n",
      "|    value_loss         | 0.0598   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.75e+03 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 513      |\n",
      "|    iterations         | 140900   |\n",
      "|    time_elapsed       | 21970    |\n",
      "|    total_timesteps    | 11272000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.831   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 140899   |\n",
      "|    policy_loss        | -0.0542  |\n",
      "|    value_loss         | 0.0491   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11280000, episode_reward=391.60 +/- 33.90\n",
      "Episode length: 8929.60 +/- 1255.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.93e+03 |\n",
      "|    mean_reward        | 392      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11280000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.846   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 140999   |\n",
      "|    policy_loss        | -0.0132  |\n",
      "|    value_loss         | 0.058    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.69e+03 |\n",
      "|    ep_rew_mean     | 376      |\n",
      "| time/              |          |\n",
      "|    fps             | 512      |\n",
      "|    iterations      | 141000   |\n",
      "|    time_elapsed    | 21994    |\n",
      "|    total_timesteps | 11280000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.77e+03 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 513      |\n",
      "|    iterations         | 141100   |\n",
      "|    time_elapsed       | 22001    |\n",
      "|    total_timesteps    | 11288000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.896   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 141099   |\n",
      "|    policy_loss        | -0.013   |\n",
      "|    value_loss         | 0.0401   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11290000, episode_reward=467.00 +/- 176.78\n",
      "Episode length: 12974.80 +/- 3515.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.3e+04  |\n",
      "|    mean_reward        | 467      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11290000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.789   |\n",
      "|    explained_variance | 0.844    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 141124   |\n",
      "|    policy_loss        | -0.112   |\n",
      "|    value_loss         | 0.631    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.76e+03 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 512      |\n",
      "|    iterations         | 141200   |\n",
      "|    time_elapsed       | 22033    |\n",
      "|    total_timesteps    | 11296000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.828   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 141199   |\n",
      "|    policy_loss        | -0.0176  |\n",
      "|    value_loss         | 0.0434   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11300000, episode_reward=399.20 +/- 63.80\n",
      "Episode length: 9852.80 +/- 2106.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.85e+03 |\n",
      "|    mean_reward        | 399      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11300000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.936   |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 141249   |\n",
      "|    policy_loss        | 0.0847   |\n",
      "|    value_loss         | 0.0902   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.78e+03 |\n",
      "|    ep_rew_mean        | 375      |\n",
      "| time/                 |          |\n",
      "|    fps                | 512      |\n",
      "|    iterations         | 141300   |\n",
      "|    time_elapsed       | 22058    |\n",
      "|    total_timesteps    | 11304000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.964   |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 141299   |\n",
      "|    policy_loss        | 0.0307   |\n",
      "|    value_loss         | 0.166    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11310000, episode_reward=348.00 +/- 122.89\n",
      "Episode length: 9015.20 +/- 2922.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.02e+03 |\n",
      "|    mean_reward        | 348      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11310000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.932   |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 141374   |\n",
      "|    policy_loss        | 0.0277   |\n",
      "|    value_loss         | 0.0789   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.78e+03 |\n",
      "|    ep_rew_mean        | 375      |\n",
      "| time/                 |          |\n",
      "|    fps                | 512      |\n",
      "|    iterations         | 141400   |\n",
      "|    time_elapsed       | 22083    |\n",
      "|    total_timesteps    | 11312000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.858   |\n",
      "|    explained_variance | 0.845    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 141399   |\n",
      "|    policy_loss        | 0.0922   |\n",
      "|    value_loss         | 0.25     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11320000, episode_reward=232.40 +/- 123.79\n",
      "Episode length: 6261.00 +/- 1482.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.26e+03 |\n",
      "|    mean_reward        | 232      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11320000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.979   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 141499   |\n",
      "|    policy_loss        | 0.00783  |\n",
      "|    value_loss         | 0.15     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.85e+03 |\n",
      "|    ep_rew_mean     | 377      |\n",
      "| time/              |          |\n",
      "|    fps             | 512      |\n",
      "|    iterations      | 141500   |\n",
      "|    time_elapsed    | 22102    |\n",
      "|    total_timesteps | 11320000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.82e+03 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 512      |\n",
      "|    iterations         | 141600   |\n",
      "|    time_elapsed       | 22108    |\n",
      "|    total_timesteps    | 11328000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.926   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 141599   |\n",
      "|    policy_loss        | -0.0102  |\n",
      "|    value_loss         | 0.0583   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11330000, episode_reward=373.00 +/- 25.78\n",
      "Episode length: 9441.60 +/- 1903.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.44e+03 |\n",
      "|    mean_reward        | 373      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11330000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.939   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 141624   |\n",
      "|    policy_loss        | -0.00393 |\n",
      "|    value_loss         | 0.0446   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.96e+03 |\n",
      "|    ep_rew_mean        | 383      |\n",
      "| time/                 |          |\n",
      "|    fps                | 512      |\n",
      "|    iterations         | 141700   |\n",
      "|    time_elapsed       | 22134    |\n",
      "|    total_timesteps    | 11336000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.884   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 141699   |\n",
      "|    policy_loss        | -0.00901 |\n",
      "|    value_loss         | 0.0642   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11340000, episode_reward=387.80 +/- 22.95\n",
      "Episode length: 9124.40 +/- 1473.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.12e+03 |\n",
      "|    mean_reward        | 388      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11340000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.939   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 141749   |\n",
      "|    policy_loss        | 0.00492  |\n",
      "|    value_loss         | 0.0185   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.94e+03 |\n",
      "|    ep_rew_mean        | 383      |\n",
      "| time/                 |          |\n",
      "|    fps                | 511      |\n",
      "|    iterations         | 141800   |\n",
      "|    time_elapsed       | 22158    |\n",
      "|    total_timesteps    | 11344000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.931   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 141799   |\n",
      "|    policy_loss        | 0.0488   |\n",
      "|    value_loss         | 0.0736   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11350000, episode_reward=391.20 +/- 21.54\n",
      "Episode length: 9513.00 +/- 1264.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.51e+03 |\n",
      "|    mean_reward        | 391      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11350000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.922   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 141874   |\n",
      "|    policy_loss        | -0.0257  |\n",
      "|    value_loss         | 0.0671   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.92e+03 |\n",
      "|    ep_rew_mean        | 382      |\n",
      "| time/                 |          |\n",
      "|    fps                | 511      |\n",
      "|    iterations         | 141900   |\n",
      "|    time_elapsed       | 22183    |\n",
      "|    total_timesteps    | 11352000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.939   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 141899   |\n",
      "|    policy_loss        | 0.023    |\n",
      "|    value_loss         | 0.103    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11360000, episode_reward=207.00 +/- 148.19\n",
      "Episode length: 6307.00 +/- 1637.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.31e+03 |\n",
      "|    mean_reward        | 207      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11360000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.869   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 141999   |\n",
      "|    policy_loss        | 0.0128   |\n",
      "|    value_loss         | 0.0324   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+04    |\n",
      "|    ep_rew_mean     | 384      |\n",
      "| time/              |          |\n",
      "|    fps             | 511      |\n",
      "|    iterations      | 142000   |\n",
      "|    time_elapsed    | 22202    |\n",
      "|    total_timesteps | 11360000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+04    |\n",
      "|    ep_rew_mean        | 384      |\n",
      "| time/                 |          |\n",
      "|    fps                | 511      |\n",
      "|    iterations         | 142100   |\n",
      "|    time_elapsed       | 22209    |\n",
      "|    total_timesteps    | 11368000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.971   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 142099   |\n",
      "|    policy_loss        | -0.066   |\n",
      "|    value_loss         | 0.0266   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11370000, episode_reward=428.60 +/- 13.81\n",
      "Episode length: 11601.20 +/- 1335.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+04 |\n",
      "|    mean_reward        | 429      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11370000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.784   |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 142124   |\n",
      "|    policy_loss        | 0.0331   |\n",
      "|    value_loss         | 0.0471   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | 384      |\n",
      "| time/                 |          |\n",
      "|    fps                | 511      |\n",
      "|    iterations         | 142200   |\n",
      "|    time_elapsed       | 22238    |\n",
      "|    total_timesteps    | 11376000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.834   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 142199   |\n",
      "|    policy_loss        | -0.0092  |\n",
      "|    value_loss         | 0.0406   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11380000, episode_reward=339.40 +/- 127.61\n",
      "Episode length: 9642.20 +/- 2283.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.64e+03 |\n",
      "|    mean_reward        | 339      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11380000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.845   |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 142249   |\n",
      "|    policy_loss        | -0.0385  |\n",
      "|    value_loss         | 0.156    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | 385      |\n",
      "| time/                 |          |\n",
      "|    fps                | 511      |\n",
      "|    iterations         | 142300   |\n",
      "|    time_elapsed       | 22265    |\n",
      "|    total_timesteps    | 11384000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.834   |\n",
      "|    explained_variance | 0.758    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 142299   |\n",
      "|    policy_loss        | -0.222   |\n",
      "|    value_loss         | 0.769    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11390000, episode_reward=353.40 +/- 124.37\n",
      "Episode length: 9343.00 +/- 2246.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.34e+03 |\n",
      "|    mean_reward        | 353      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11390000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.88    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 142374   |\n",
      "|    policy_loss        | -0.0349  |\n",
      "|    value_loss         | 0.134    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | 386      |\n",
      "| time/                 |          |\n",
      "|    fps                | 510      |\n",
      "|    iterations         | 142400   |\n",
      "|    time_elapsed       | 22295    |\n",
      "|    total_timesteps    | 11392000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.924   |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 142399   |\n",
      "|    policy_loss        | 0.0223   |\n",
      "|    value_loss         | 0.125    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11400000, episode_reward=279.00 +/- 121.61\n",
      "Episode length: 7644.80 +/- 935.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.64e+03 |\n",
      "|    mean_reward        | 279      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11400000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.876   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 142499   |\n",
      "|    policy_loss        | -0.0118  |\n",
      "|    value_loss         | 0.0155   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.93e+03 |\n",
      "|    ep_rew_mean     | 386      |\n",
      "| time/              |          |\n",
      "|    fps             | 510      |\n",
      "|    iterations      | 142500   |\n",
      "|    time_elapsed    | 22320    |\n",
      "|    total_timesteps | 11400000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.94e+03 |\n",
      "|    ep_rew_mean        | 386      |\n",
      "| time/                 |          |\n",
      "|    fps                | 510      |\n",
      "|    iterations         | 142600   |\n",
      "|    time_elapsed       | 22328    |\n",
      "|    total_timesteps    | 11408000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.953   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 142599   |\n",
      "|    policy_loss        | -0.0306  |\n",
      "|    value_loss         | 0.0442   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11410000, episode_reward=248.60 +/- 118.61\n",
      "Episode length: 6465.20 +/- 1441.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.47e+03 |\n",
      "|    mean_reward        | 249      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11410000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.969   |\n",
      "|    explained_variance | 0.869    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 142624   |\n",
      "|    policy_loss        | -0.21    |\n",
      "|    value_loss         | 0.186    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.93e+03 |\n",
      "|    ep_rew_mean        | 384      |\n",
      "| time/                 |          |\n",
      "|    fps                | 510      |\n",
      "|    iterations         | 142700   |\n",
      "|    time_elapsed       | 22349    |\n",
      "|    total_timesteps    | 11416000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.936   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 142699   |\n",
      "|    policy_loss        | -0.00678 |\n",
      "|    value_loss         | 0.0802   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11420000, episode_reward=378.40 +/- 27.81\n",
      "Episode length: 11573.20 +/- 5466.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+04 |\n",
      "|    mean_reward        | 378      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11420000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.787   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 142749   |\n",
      "|    policy_loss        | -0.00227 |\n",
      "|    value_loss         | 0.0373   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.94e+03 |\n",
      "|    ep_rew_mean        | 383      |\n",
      "| time/                 |          |\n",
      "|    fps                | 510      |\n",
      "|    iterations         | 142800   |\n",
      "|    time_elapsed       | 22381    |\n",
      "|    total_timesteps    | 11424000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.891   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 142799   |\n",
      "|    policy_loss        | -0.0264  |\n",
      "|    value_loss         | 0.132    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11430000, episode_reward=405.80 +/- 9.43\n",
      "Episode length: 9183.00 +/- 1162.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.18e+03 |\n",
      "|    mean_reward        | 406      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11430000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 142874   |\n",
      "|    policy_loss        | 0.0465   |\n",
      "|    value_loss         | 0.0715   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.91e+03 |\n",
      "|    ep_rew_mean        | 382      |\n",
      "| time/                 |          |\n",
      "|    fps                | 510      |\n",
      "|    iterations         | 142900   |\n",
      "|    time_elapsed       | 22408    |\n",
      "|    total_timesteps    | 11432000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.885   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 142899   |\n",
      "|    policy_loss        | -0.0166  |\n",
      "|    value_loss         | 0.0168   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11440000, episode_reward=258.60 +/- 138.75\n",
      "Episode length: 7697.00 +/- 2828.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.7e+03  |\n",
      "|    mean_reward        | 259      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11440000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 142999   |\n",
      "|    policy_loss        | 0.0246   |\n",
      "|    value_loss         | 0.0354   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.8e+03  |\n",
      "|    ep_rew_mean     | 384      |\n",
      "| time/              |          |\n",
      "|    fps             | 509      |\n",
      "|    iterations      | 143000   |\n",
      "|    time_elapsed    | 22432    |\n",
      "|    total_timesteps | 11440000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.83e+03 |\n",
      "|    ep_rew_mean        | 384      |\n",
      "| time/                 |          |\n",
      "|    fps                | 510      |\n",
      "|    iterations         | 143100   |\n",
      "|    time_elapsed       | 22439    |\n",
      "|    total_timesteps    | 11448000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.968   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 143099   |\n",
      "|    policy_loss        | 0.0287   |\n",
      "|    value_loss         | 0.055    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11450000, episode_reward=193.80 +/- 111.84\n",
      "Episode length: 6355.80 +/- 1443.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.36e+03 |\n",
      "|    mean_reward        | 194      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11450000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.955   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 143124   |\n",
      "|    policy_loss        | 0.00675  |\n",
      "|    value_loss         | 0.0951   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.9e+03  |\n",
      "|    ep_rew_mean        | 384      |\n",
      "| time/                 |          |\n",
      "|    fps                | 510      |\n",
      "|    iterations         | 143200   |\n",
      "|    time_elapsed       | 22461    |\n",
      "|    total_timesteps    | 11456000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 143199   |\n",
      "|    policy_loss        | 0.0542   |\n",
      "|    value_loss         | 0.073    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11460000, episode_reward=394.20 +/- 15.59\n",
      "Episode length: 11236.80 +/- 2043.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 394      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11460000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.819   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 143249   |\n",
      "|    policy_loss        | -0.128   |\n",
      "|    value_loss         | 0.139    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.74e+03 |\n",
      "|    ep_rew_mean        | 382      |\n",
      "| time/                 |          |\n",
      "|    fps                | 509      |\n",
      "|    iterations         | 143300   |\n",
      "|    time_elapsed       | 22493    |\n",
      "|    total_timesteps    | 11464000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.911   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 143299   |\n",
      "|    policy_loss        | -0.00954 |\n",
      "|    value_loss         | 0.0403   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11470000, episode_reward=373.40 +/- 23.09\n",
      "Episode length: 8929.00 +/- 551.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.93e+03 |\n",
      "|    mean_reward        | 373      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11470000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 143374   |\n",
      "|    policy_loss        | 0.0422   |\n",
      "|    value_loss         | 0.196    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.74e+03 |\n",
      "|    ep_rew_mean        | 382      |\n",
      "| time/                 |          |\n",
      "|    fps                | 509      |\n",
      "|    iterations         | 143400   |\n",
      "|    time_elapsed       | 22520    |\n",
      "|    total_timesteps    | 11472000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.97    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 143399   |\n",
      "|    policy_loss        | -0.0124  |\n",
      "|    value_loss         | 0.0375   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11480000, episode_reward=367.00 +/- 59.56\n",
      "Episode length: 9485.60 +/- 1954.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.49e+03 |\n",
      "|    mean_reward        | 367      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11480000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.856   |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 143499   |\n",
      "|    policy_loss        | 0.0883   |\n",
      "|    value_loss         | 0.351    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.74e+03 |\n",
      "|    ep_rew_mean     | 382      |\n",
      "| time/              |          |\n",
      "|    fps             | 509      |\n",
      "|    iterations      | 143500   |\n",
      "|    time_elapsed    | 22547    |\n",
      "|    total_timesteps | 11480000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.74e+03 |\n",
      "|    ep_rew_mean        | 383      |\n",
      "| time/                 |          |\n",
      "|    fps                | 509      |\n",
      "|    iterations         | 143600   |\n",
      "|    time_elapsed       | 22555    |\n",
      "|    total_timesteps    | 11488000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 143599   |\n",
      "|    policy_loss        | -0.121   |\n",
      "|    value_loss         | 0.228    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11490000, episode_reward=293.40 +/- 119.68\n",
      "Episode length: 7114.60 +/- 2039.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.11e+03 |\n",
      "|    mean_reward        | 293      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11490000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.898    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 143624   |\n",
      "|    policy_loss        | -0.0498  |\n",
      "|    value_loss         | 0.0722   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.72e+03 |\n",
      "|    ep_rew_mean        | 383      |\n",
      "| time/                 |          |\n",
      "|    fps                | 509      |\n",
      "|    iterations         | 143700   |\n",
      "|    time_elapsed       | 22577    |\n",
      "|    total_timesteps    | 11496000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.864   |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 143699   |\n",
      "|    policy_loss        | 0.0606   |\n",
      "|    value_loss         | 0.0708   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11500000, episode_reward=321.20 +/- 135.42\n",
      "Episode length: 10325.80 +/- 5750.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+04 |\n",
      "|    mean_reward        | 321      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11500000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.798   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 143749   |\n",
      "|    policy_loss        | 0.0869   |\n",
      "|    value_loss         | 0.112    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.76e+03 |\n",
      "|    ep_rew_mean        | 384      |\n",
      "| time/                 |          |\n",
      "|    fps                | 508      |\n",
      "|    iterations         | 143800   |\n",
      "|    time_elapsed       | 22607    |\n",
      "|    total_timesteps    | 11504000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.852   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 143799   |\n",
      "|    policy_loss        | -0.0263  |\n",
      "|    value_loss         | 0.0219   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11510000, episode_reward=364.40 +/- 39.77\n",
      "Episode length: 9440.80 +/- 1763.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.44e+03 |\n",
      "|    mean_reward        | 364      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11510000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.925   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 143874   |\n",
      "|    policy_loss        | -0.0934  |\n",
      "|    value_loss         | 0.0464   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.82e+03 |\n",
      "|    ep_rew_mean        | 384      |\n",
      "| time/                 |          |\n",
      "|    fps                | 508      |\n",
      "|    iterations         | 143900   |\n",
      "|    time_elapsed       | 22635    |\n",
      "|    total_timesteps    | 11512000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.892   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 143899   |\n",
      "|    policy_loss        | 0.067    |\n",
      "|    value_loss         | 0.156    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11520000, episode_reward=421.20 +/- 11.41\n",
      "Episode length: 16653.80 +/- 4046.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.67e+04 |\n",
      "|    mean_reward        | 421      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11520000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.877   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 143999   |\n",
      "|    policy_loss        | -0.0634  |\n",
      "|    value_loss         | 0.0264   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.85e+03 |\n",
      "|    ep_rew_mean     | 383      |\n",
      "| time/              |          |\n",
      "|    fps             | 507      |\n",
      "|    iterations      | 144000   |\n",
      "|    time_elapsed    | 22679    |\n",
      "|    total_timesteps | 11520000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.96e+03 |\n",
      "|    ep_rew_mean        | 384      |\n",
      "| time/                 |          |\n",
      "|    fps                | 508      |\n",
      "|    iterations         | 144100   |\n",
      "|    time_elapsed       | 22686    |\n",
      "|    total_timesteps    | 11528000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 144099   |\n",
      "|    policy_loss        | -0.0406  |\n",
      "|    value_loss         | 0.144    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11530000, episode_reward=313.60 +/- 107.66\n",
      "Episode length: 8088.60 +/- 1294.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.09e+03 |\n",
      "|    mean_reward        | 314      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11530000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.875   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 144124   |\n",
      "|    policy_loss        | -0.00723 |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | 383      |\n",
      "| time/                 |          |\n",
      "|    fps                | 507      |\n",
      "|    iterations         | 144200   |\n",
      "|    time_elapsed       | 22710    |\n",
      "|    total_timesteps    | 11536000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.987   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 144199   |\n",
      "|    policy_loss        | 0.0723   |\n",
      "|    value_loss         | 0.0346   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11540000, episode_reward=406.20 +/- 9.83\n",
      "Episode length: 11637.40 +/- 3404.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+04 |\n",
      "|    mean_reward        | 406      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11540000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.922   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 144249   |\n",
      "|    policy_loss        | 0.0297   |\n",
      "|    value_loss         | 0.0603   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | 383      |\n",
      "| time/                 |          |\n",
      "|    fps                | 507      |\n",
      "|    iterations         | 144300   |\n",
      "|    time_elapsed       | 22742    |\n",
      "|    total_timesteps    | 11544000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 144299   |\n",
      "|    policy_loss        | -0.0463  |\n",
      "|    value_loss         | 0.0584   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11550000, episode_reward=412.00 +/- 13.36\n",
      "Episode length: 17240.80 +/- 8530.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.72e+04 |\n",
      "|    mean_reward        | 412      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11550000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.997   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 144374   |\n",
      "|    policy_loss        | -0.00566 |\n",
      "|    value_loss         | 0.0597   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | 382      |\n",
      "| time/                 |          |\n",
      "|    fps                | 506      |\n",
      "|    iterations         | 144400   |\n",
      "|    time_elapsed       | 22786    |\n",
      "|    total_timesteps    | 11552000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 144399   |\n",
      "|    policy_loss        | 0.037    |\n",
      "|    value_loss         | 0.0386   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11560000, episode_reward=368.80 +/- 43.60\n",
      "Episode length: 8381.60 +/- 1962.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.38e+03 |\n",
      "|    mean_reward        | 369      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11560000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.96    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 144499   |\n",
      "|    policy_loss        | 0.128    |\n",
      "|    value_loss         | 0.142    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.01e+04 |\n",
      "|    ep_rew_mean     | 382      |\n",
      "| time/              |          |\n",
      "|    fps             | 506      |\n",
      "|    iterations      | 144500   |\n",
      "|    time_elapsed    | 22812    |\n",
      "|    total_timesteps | 11560000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | 382      |\n",
      "| time/                 |          |\n",
      "|    fps                | 506      |\n",
      "|    iterations         | 144600   |\n",
      "|    time_elapsed       | 22819    |\n",
      "|    total_timesteps    | 11568000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.976   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 144599   |\n",
      "|    policy_loss        | 0.0236   |\n",
      "|    value_loss         | 0.0247   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11570000, episode_reward=399.20 +/- 19.94\n",
      "Episode length: 14389.20 +/- 6609.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.44e+04 |\n",
      "|    mean_reward        | 399      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11570000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.877   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 144624   |\n",
      "|    policy_loss        | 0.0529   |\n",
      "|    value_loss         | 0.0948   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | 382      |\n",
      "| time/                 |          |\n",
      "|    fps                | 506      |\n",
      "|    iterations         | 144700   |\n",
      "|    time_elapsed       | 22858    |\n",
      "|    total_timesteps    | 11576000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.981   |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 144699   |\n",
      "|    policy_loss        | -0.13    |\n",
      "|    value_loss         | 0.149    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11580000, episode_reward=400.80 +/- 45.68\n",
      "Episode length: 10786.60 +/- 1964.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+04 |\n",
      "|    mean_reward        | 401      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11580000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 144749   |\n",
      "|    policy_loss        | 0.0305   |\n",
      "|    value_loss         | 0.0937   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | 382      |\n",
      "| time/                 |          |\n",
      "|    fps                | 506      |\n",
      "|    iterations         | 144800   |\n",
      "|    time_elapsed       | 22888    |\n",
      "|    total_timesteps    | 11584000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 144799   |\n",
      "|    policy_loss        | -0.132   |\n",
      "|    value_loss         | 0.0689   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11590000, episode_reward=337.40 +/- 134.39\n",
      "Episode length: 8516.40 +/- 1509.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.52e+03 |\n",
      "|    mean_reward        | 337      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11590000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 144874   |\n",
      "|    policy_loss        | 0.0481   |\n",
      "|    value_loss         | 0.0559   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+04    |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 505      |\n",
      "|    iterations         | 144900   |\n",
      "|    time_elapsed       | 22914    |\n",
      "|    total_timesteps    | 11592000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.879   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 144899   |\n",
      "|    policy_loss        | 0.0132   |\n",
      "|    value_loss         | 0.036    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11600000, episode_reward=403.00 +/- 38.92\n",
      "Episode length: 12586.60 +/- 3734.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.26e+04 |\n",
      "|    mean_reward        | 403      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11600000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.962   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 144999   |\n",
      "|    policy_loss        | -0.0317  |\n",
      "|    value_loss         | 0.174    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.01e+04 |\n",
      "|    ep_rew_mean     | 376      |\n",
      "| time/              |          |\n",
      "|    fps             | 505      |\n",
      "|    iterations      | 145000   |\n",
      "|    time_elapsed    | 22948    |\n",
      "|    total_timesteps | 11600000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 505      |\n",
      "|    iterations         | 145100   |\n",
      "|    time_elapsed       | 22956    |\n",
      "|    total_timesteps    | 11608000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 145099   |\n",
      "|    policy_loss        | -0.184   |\n",
      "|    value_loss         | 0.174    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11610000, episode_reward=368.00 +/- 92.01\n",
      "Episode length: 9826.40 +/- 2328.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.83e+03 |\n",
      "|    mean_reward        | 368      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11610000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.929   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 145124   |\n",
      "|    policy_loss        | -0.192   |\n",
      "|    value_loss         | 0.13     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+04 |\n",
      "|    ep_rew_mean        | 374      |\n",
      "| time/                 |          |\n",
      "|    fps                | 505      |\n",
      "|    iterations         | 145200   |\n",
      "|    time_elapsed       | 22984    |\n",
      "|    total_timesteps    | 11616000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.984   |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 145199   |\n",
      "|    policy_loss        | 0.0891   |\n",
      "|    value_loss         | 0.922    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11620000, episode_reward=316.20 +/- 118.21\n",
      "Episode length: 8448.40 +/- 2244.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.45e+03 |\n",
      "|    mean_reward        | 316      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11620000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.94    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 145249   |\n",
      "|    policy_loss        | 0.00589  |\n",
      "|    value_loss         | 0.0742   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | 369      |\n",
      "| time/                 |          |\n",
      "|    fps                | 505      |\n",
      "|    iterations         | 145300   |\n",
      "|    time_elapsed       | 23010    |\n",
      "|    total_timesteps    | 11624000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.961   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 145299   |\n",
      "|    policy_loss        | -0.0348  |\n",
      "|    value_loss         | 0.0734   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11630000, episode_reward=384.60 +/- 74.15\n",
      "Episode length: 10925.20 +/- 3986.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.09e+04 |\n",
      "|    mean_reward        | 385      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11630000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.953   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 145374   |\n",
      "|    policy_loss        | -0.0702  |\n",
      "|    value_loss         | 0.0323   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | 369      |\n",
      "| time/                 |          |\n",
      "|    fps                | 504      |\n",
      "|    iterations         | 145400   |\n",
      "|    time_elapsed       | 23040    |\n",
      "|    total_timesteps    | 11632000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.927   |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 145399   |\n",
      "|    policy_loss        | 0.0625   |\n",
      "|    value_loss         | 0.064    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11640000, episode_reward=401.20 +/- 47.36\n",
      "Episode length: 11855.20 +/- 5535.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.19e+04 |\n",
      "|    mean_reward        | 401      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11640000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.861   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 145499   |\n",
      "|    policy_loss        | 0.126    |\n",
      "|    value_loss         | 0.0936   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.02e+04 |\n",
      "|    ep_rew_mean     | 371      |\n",
      "| time/              |          |\n",
      "|    fps             | 504      |\n",
      "|    iterations      | 145500   |\n",
      "|    time_elapsed    | 23073    |\n",
      "|    total_timesteps | 11640000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+04 |\n",
      "|    ep_rew_mean        | 373      |\n",
      "| time/                 |          |\n",
      "|    fps                | 504      |\n",
      "|    iterations         | 145600   |\n",
      "|    time_elapsed       | 23080    |\n",
      "|    total_timesteps    | 11648000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.912   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 145599   |\n",
      "|    policy_loss        | -0.0931  |\n",
      "|    value_loss         | 0.0952   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11650000, episode_reward=460.60 +/- 149.93\n",
      "Episode length: 10138.00 +/- 2340.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+04 |\n",
      "|    mean_reward        | 461      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11650000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.926   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 145624   |\n",
      "|    policy_loss        | -0.012   |\n",
      "|    value_loss         | 0.0382   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+04 |\n",
      "|    ep_rew_mean        | 373      |\n",
      "| time/                 |          |\n",
      "|    fps                | 504      |\n",
      "|    iterations         | 145700   |\n",
      "|    time_elapsed       | 23110    |\n",
      "|    total_timesteps    | 11656000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.929   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 145699   |\n",
      "|    policy_loss        | -0.0141  |\n",
      "|    value_loss         | 0.0377   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11660000, episode_reward=451.80 +/- 138.57\n",
      "Episode length: 10596.60 +/- 3886.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+04 |\n",
      "|    mean_reward        | 452      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11660000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 145749   |\n",
      "|    policy_loss        | 0.0976   |\n",
      "|    value_loss         | 0.153    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+04 |\n",
      "|    ep_rew_mean        | 373      |\n",
      "| time/                 |          |\n",
      "|    fps                | 504      |\n",
      "|    iterations         | 145800   |\n",
      "|    time_elapsed       | 23141    |\n",
      "|    total_timesteps    | 11664000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.905   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 145799   |\n",
      "|    policy_loss        | -0.0216  |\n",
      "|    value_loss         | 0.0249   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11670000, episode_reward=382.80 +/- 20.21\n",
      "Episode length: 7999.60 +/- 1140.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8e+03    |\n",
      "|    mean_reward        | 383      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11670000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.864   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 145874   |\n",
      "|    policy_loss        | 0.0472   |\n",
      "|    value_loss         | 0.0375   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02e+04 |\n",
      "|    ep_rew_mean        | 374      |\n",
      "| time/                 |          |\n",
      "|    fps                | 503      |\n",
      "|    iterations         | 145900   |\n",
      "|    time_elapsed       | 23165    |\n",
      "|    total_timesteps    | 11672000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.985   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 145899   |\n",
      "|    policy_loss        | -0.0102  |\n",
      "|    value_loss         | 0.0386   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11680000, episode_reward=361.60 +/- 38.20\n",
      "Episode length: 8813.20 +/- 1336.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.81e+03 |\n",
      "|    mean_reward        | 362      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11680000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.972   |\n",
      "|    explained_variance | 0.701    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 145999   |\n",
      "|    policy_loss        | -0.16    |\n",
      "|    value_loss         | 1.89     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.03e+04 |\n",
      "|    ep_rew_mean     | 378      |\n",
      "| time/              |          |\n",
      "|    fps             | 503      |\n",
      "|    iterations      | 146000   |\n",
      "|    time_elapsed    | 23192    |\n",
      "|    total_timesteps | 11680000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+04 |\n",
      "|    ep_rew_mean        | 379      |\n",
      "| time/                 |          |\n",
      "|    fps                | 503      |\n",
      "|    iterations         | 146100   |\n",
      "|    time_elapsed       | 23199    |\n",
      "|    total_timesteps    | 11688000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 146099   |\n",
      "|    policy_loss        | 0.0127   |\n",
      "|    value_loss         | 0.0553   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11690000, episode_reward=348.20 +/- 104.96\n",
      "Episode length: 10012.60 +/- 2578.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1e+04    |\n",
      "|    mean_reward        | 348      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11690000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.947   |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 146124   |\n",
      "|    policy_loss        | 0.0533   |\n",
      "|    value_loss         | 0.0967   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05e+04 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 503      |\n",
      "|    iterations         | 146200   |\n",
      "|    time_elapsed       | 23228    |\n",
      "|    total_timesteps    | 11696000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.935   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 146199   |\n",
      "|    policy_loss        | -0.0604  |\n",
      "|    value_loss         | 0.0419   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11700000, episode_reward=384.20 +/- 34.82\n",
      "Episode length: 11684.00 +/- 3526.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.17e+04 |\n",
      "|    mean_reward        | 384      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11700000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.918   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 146249   |\n",
      "|    policy_loss        | -0.00632 |\n",
      "|    value_loss         | 0.0585   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+04 |\n",
      "|    ep_rew_mean        | 372      |\n",
      "| time/                 |          |\n",
      "|    fps                | 503      |\n",
      "|    iterations         | 146300   |\n",
      "|    time_elapsed       | 23257    |\n",
      "|    total_timesteps    | 11704000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.821   |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 146299   |\n",
      "|    policy_loss        | -0.115   |\n",
      "|    value_loss         | 0.294    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11710000, episode_reward=373.60 +/- 62.87\n",
      "Episode length: 9874.80 +/- 1344.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.87e+03 |\n",
      "|    mean_reward        | 374      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11710000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.905   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 146374   |\n",
      "|    policy_loss        | 0.0723   |\n",
      "|    value_loss         | 0.177    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+04 |\n",
      "|    ep_rew_mean        | 372      |\n",
      "| time/                 |          |\n",
      "|    fps                | 503      |\n",
      "|    iterations         | 146400   |\n",
      "|    time_elapsed       | 23283    |\n",
      "|    total_timesteps    | 11712000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.86    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 146399   |\n",
      "|    policy_loss        | 0.0639   |\n",
      "|    value_loss         | 0.164    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11720000, episode_reward=402.60 +/- 28.38\n",
      "Episode length: 10133.80 +/- 957.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+04 |\n",
      "|    mean_reward        | 403      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11720000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.913   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 146499   |\n",
      "|    policy_loss        | 0.031    |\n",
      "|    value_loss         | 0.0462   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.04e+04 |\n",
      "|    ep_rew_mean     | 372      |\n",
      "| time/              |          |\n",
      "|    fps             | 502      |\n",
      "|    iterations      | 146500   |\n",
      "|    time_elapsed    | 23309    |\n",
      "|    total_timesteps | 11720000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+04 |\n",
      "|    ep_rew_mean        | 372      |\n",
      "| time/                 |          |\n",
      "|    fps                | 502      |\n",
      "|    iterations         | 146600   |\n",
      "|    time_elapsed       | 23316    |\n",
      "|    total_timesteps    | 11728000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.956   |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 146599   |\n",
      "|    policy_loss        | 0.0257   |\n",
      "|    value_loss         | 0.122    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11730000, episode_reward=418.20 +/- 10.98\n",
      "Episode length: 10825.20 +/- 1525.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+04 |\n",
      "|    mean_reward        | 418      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11730000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.814   |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 146624   |\n",
      "|    policy_loss        | 0.0123   |\n",
      "|    value_loss         | 0.0871   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+04 |\n",
      "|    ep_rew_mean        | 372      |\n",
      "| time/                 |          |\n",
      "|    fps                | 502      |\n",
      "|    iterations         | 146700   |\n",
      "|    time_elapsed       | 23344    |\n",
      "|    total_timesteps    | 11736000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.987   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 146699   |\n",
      "|    policy_loss        | 0.00319  |\n",
      "|    value_loss         | 0.0387   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11740000, episode_reward=418.60 +/- 16.57\n",
      "Episode length: 11756.60 +/- 2840.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.18e+04 |\n",
      "|    mean_reward        | 419      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11740000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 146749   |\n",
      "|    policy_loss        | -0.0245  |\n",
      "|    value_loss         | 0.0461   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+04 |\n",
      "|    ep_rew_mean        | 371      |\n",
      "| time/                 |          |\n",
      "|    fps                | 502      |\n",
      "|    iterations         | 146800   |\n",
      "|    time_elapsed       | 23373    |\n",
      "|    total_timesteps    | 11744000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.891   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 146799   |\n",
      "|    policy_loss        | -0.022   |\n",
      "|    value_loss         | 0.0635   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11750000, episode_reward=382.20 +/- 34.08\n",
      "Episode length: 9150.20 +/- 1053.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.15e+03 |\n",
      "|    mean_reward        | 382      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11750000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.834   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 146874   |\n",
      "|    policy_loss        | -0.0808  |\n",
      "|    value_loss         | 0.0366   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+04 |\n",
      "|    ep_rew_mean        | 371      |\n",
      "| time/                 |          |\n",
      "|    fps                | 502      |\n",
      "|    iterations         | 146900   |\n",
      "|    time_elapsed       | 23398    |\n",
      "|    total_timesteps    | 11752000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.945   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 146899   |\n",
      "|    policy_loss        | 0.0615   |\n",
      "|    value_loss         | 0.0774   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11760000, episode_reward=392.20 +/- 37.25\n",
      "Episode length: 9640.00 +/- 1271.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.64e+03 |\n",
      "|    mean_reward        | 392      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11760000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.963   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 146999   |\n",
      "|    policy_loss        | -0.0277  |\n",
      "|    value_loss         | 0.0387   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.04e+04 |\n",
      "|    ep_rew_mean     | 372      |\n",
      "| time/              |          |\n",
      "|    fps             | 502      |\n",
      "|    iterations      | 147000   |\n",
      "|    time_elapsed    | 23423    |\n",
      "|    total_timesteps | 11760000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+04 |\n",
      "|    ep_rew_mean        | 374      |\n",
      "| time/                 |          |\n",
      "|    fps                | 502      |\n",
      "|    iterations         | 147100   |\n",
      "|    time_elapsed       | 23430    |\n",
      "|    total_timesteps    | 11768000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.866   |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 147099   |\n",
      "|    policy_loss        | -0.0783  |\n",
      "|    value_loss         | 0.357    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11770000, episode_reward=376.00 +/- 26.57\n",
      "Episode length: 13115.00 +/- 5224.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.31e+04 |\n",
      "|    mean_reward        | 376      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11770000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.918   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 147124   |\n",
      "|    policy_loss        | -0.0845  |\n",
      "|    value_loss         | 0.0659   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+04 |\n",
      "|    ep_rew_mean        | 374      |\n",
      "| time/                 |          |\n",
      "|    fps                | 501      |\n",
      "|    iterations         | 147200   |\n",
      "|    time_elapsed       | 23462    |\n",
      "|    total_timesteps    | 11776000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.94    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 147199   |\n",
      "|    policy_loss        | -0.00317 |\n",
      "|    value_loss         | 0.0655   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11780000, episode_reward=385.60 +/- 23.85\n",
      "Episode length: 10416.80 +/- 2841.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+04 |\n",
      "|    mean_reward        | 386      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11780000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.825   |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 147249   |\n",
      "|    policy_loss        | 0.0296   |\n",
      "|    value_loss         | 0.205    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02e+04 |\n",
      "|    ep_rew_mean        | 374      |\n",
      "| time/                 |          |\n",
      "|    fps                | 501      |\n",
      "|    iterations         | 147300   |\n",
      "|    time_elapsed       | 23489    |\n",
      "|    total_timesteps    | 11784000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.882   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 147299   |\n",
      "|    policy_loss        | 0.159    |\n",
      "|    value_loss         | 0.364    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11790000, episode_reward=388.80 +/- 18.35\n",
      "Episode length: 12892.20 +/- 5803.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.29e+04 |\n",
      "|    mean_reward        | 389      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11790000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.927   |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 147374   |\n",
      "|    policy_loss        | 0.0339   |\n",
      "|    value_loss         | 0.451    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02e+04 |\n",
      "|    ep_rew_mean        | 375      |\n",
      "| time/                 |          |\n",
      "|    fps                | 501      |\n",
      "|    iterations         | 147400   |\n",
      "|    time_elapsed       | 23521    |\n",
      "|    total_timesteps    | 11792000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.887   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 147399   |\n",
      "|    policy_loss        | 0.0531   |\n",
      "|    value_loss         | 0.0517   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11800000, episode_reward=392.80 +/- 29.06\n",
      "Episode length: 10176.80 +/- 880.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+04 |\n",
      "|    mean_reward        | 393      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11800000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.995   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 147499   |\n",
      "|    policy_loss        | -0.0417  |\n",
      "|    value_loss         | 0.0354   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.01e+04 |\n",
      "|    ep_rew_mean     | 376      |\n",
      "| time/              |          |\n",
      "|    fps             | 501      |\n",
      "|    iterations      | 147500   |\n",
      "|    time_elapsed    | 23547    |\n",
      "|    total_timesteps | 11800000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02e+04 |\n",
      "|    ep_rew_mean        | 376      |\n",
      "| time/                 |          |\n",
      "|    fps                | 501      |\n",
      "|    iterations         | 147600   |\n",
      "|    time_elapsed       | 23554    |\n",
      "|    total_timesteps    | 11808000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.963   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 147599   |\n",
      "|    policy_loss        | -0.00905 |\n",
      "|    value_loss         | 0.0323   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11810000, episode_reward=359.80 +/- 46.99\n",
      "Episode length: 10106.00 +/- 2879.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+04 |\n",
      "|    mean_reward        | 360      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11810000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 147624   |\n",
      "|    policy_loss        | -0.0604  |\n",
      "|    value_loss         | 0.0992   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02e+04 |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 501      |\n",
      "|    iterations         | 147700   |\n",
      "|    time_elapsed       | 23581    |\n",
      "|    total_timesteps    | 11816000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 147699   |\n",
      "|    policy_loss        | 0.0626   |\n",
      "|    value_loss         | 0.0915   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11820000, episode_reward=404.20 +/- 23.42\n",
      "Episode length: 10675.00 +/- 2823.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.07e+04 |\n",
      "|    mean_reward        | 404      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11820000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.955   |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 147749   |\n",
      "|    policy_loss        | -0.0237  |\n",
      "|    value_loss         | 0.162    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+04 |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 500      |\n",
      "|    iterations         | 147800   |\n",
      "|    time_elapsed       | 23608    |\n",
      "|    total_timesteps    | 11824000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 147799   |\n",
      "|    policy_loss        | -0.0246  |\n",
      "|    value_loss         | 0.0475   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11830000, episode_reward=398.80 +/- 25.71\n",
      "Episode length: 10244.40 +/- 1799.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+04 |\n",
      "|    mean_reward        | 399      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11830000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.935   |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 147874   |\n",
      "|    policy_loss        | -0.0985  |\n",
      "|    value_loss         | 0.186    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02e+04 |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 500      |\n",
      "|    iterations         | 147900   |\n",
      "|    time_elapsed       | 23635    |\n",
      "|    total_timesteps    | 11832000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.984   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 147899   |\n",
      "|    policy_loss        | -0.0678  |\n",
      "|    value_loss         | 0.0567   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11840000, episode_reward=375.40 +/- 45.83\n",
      "Episode length: 10314.80 +/- 3327.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+04 |\n",
      "|    mean_reward        | 375      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11840000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.913   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 147999   |\n",
      "|    policy_loss        | 0.0372   |\n",
      "|    value_loss         | 0.0398   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.02e+04 |\n",
      "|    ep_rew_mean     | 377      |\n",
      "| time/              |          |\n",
      "|    fps             | 500      |\n",
      "|    iterations      | 148000   |\n",
      "|    time_elapsed    | 23661    |\n",
      "|    total_timesteps | 11840000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+04    |\n",
      "|    ep_rew_mean        | 376      |\n",
      "| time/                 |          |\n",
      "|    fps                | 500      |\n",
      "|    iterations         | 148100   |\n",
      "|    time_elapsed       | 23668    |\n",
      "|    total_timesteps    | 11848000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.862   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 148099   |\n",
      "|    policy_loss        | -0.0842  |\n",
      "|    value_loss         | 0.175    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11850000, episode_reward=398.80 +/- 129.77\n",
      "Episode length: 12217.80 +/- 3437.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.22e+04 |\n",
      "|    mean_reward        | 399      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11850000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.987   |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 148124   |\n",
      "|    policy_loss        | 0.0954   |\n",
      "|    value_loss         | 0.134    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+04    |\n",
      "|    ep_rew_mean        | 375      |\n",
      "| time/                 |          |\n",
      "|    fps                | 500      |\n",
      "|    iterations         | 148200   |\n",
      "|    time_elapsed       | 23699    |\n",
      "|    total_timesteps    | 11856000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.959   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 148199   |\n",
      "|    policy_loss        | 0.115    |\n",
      "|    value_loss         | 0.0995   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11860000, episode_reward=365.20 +/- 81.71\n",
      "Episode length: 8784.60 +/- 1456.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.78e+03 |\n",
      "|    mean_reward        | 365      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11860000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.827   |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 148249   |\n",
      "|    policy_loss        | 0.0277   |\n",
      "|    value_loss         | 0.181    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.95e+03 |\n",
      "|    ep_rew_mean        | 379      |\n",
      "| time/                 |          |\n",
      "|    fps                | 500      |\n",
      "|    iterations         | 148300   |\n",
      "|    time_elapsed       | 23723    |\n",
      "|    total_timesteps    | 11864000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.868   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 148299   |\n",
      "|    policy_loss        | -0.0347  |\n",
      "|    value_loss         | 0.0234   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11870000, episode_reward=337.40 +/- 107.39\n",
      "Episode length: 9846.20 +/- 2527.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.85e+03 |\n",
      "|    mean_reward        | 337      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11870000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.931   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 148374   |\n",
      "|    policy_loss        | 0.0276   |\n",
      "|    value_loss         | 0.124    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+04    |\n",
      "|    ep_rew_mean        | 381      |\n",
      "| time/                 |          |\n",
      "|    fps                | 499      |\n",
      "|    iterations         | 148400   |\n",
      "|    time_elapsed       | 23748    |\n",
      "|    total_timesteps    | 11872000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.94    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 148399   |\n",
      "|    policy_loss        | 0.0278   |\n",
      "|    value_loss         | 0.0935   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11880000, episode_reward=411.60 +/- 9.41\n",
      "Episode length: 10759.80 +/- 1322.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+04 |\n",
      "|    mean_reward        | 412      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11880000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.89    |\n",
      "|    explained_variance | 0.793    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 148499   |\n",
      "|    policy_loss        | -0.36    |\n",
      "|    value_loss         | 1.11     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.01e+04 |\n",
      "|    ep_rew_mean     | 384      |\n",
      "| time/              |          |\n",
      "|    fps             | 499      |\n",
      "|    iterations      | 148500   |\n",
      "|    time_elapsed    | 23776    |\n",
      "|    total_timesteps | 11880000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | 384      |\n",
      "| time/                 |          |\n",
      "|    fps                | 499      |\n",
      "|    iterations         | 148600   |\n",
      "|    time_elapsed       | 23783    |\n",
      "|    total_timesteps    | 11888000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.917   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 148599   |\n",
      "|    policy_loss        | 0.0224   |\n",
      "|    value_loss         | 0.0275   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11890000, episode_reward=308.20 +/- 106.75\n",
      "Episode length: 9358.80 +/- 1829.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.36e+03 |\n",
      "|    mean_reward        | 308      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11890000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 148624   |\n",
      "|    policy_loss        | -0.06    |\n",
      "|    value_loss         | 0.0479   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | 385      |\n",
      "| time/                 |          |\n",
      "|    fps                | 499      |\n",
      "|    iterations         | 148700   |\n",
      "|    time_elapsed       | 23807    |\n",
      "|    total_timesteps    | 11896000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.969   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 148699   |\n",
      "|    policy_loss        | -0.0195  |\n",
      "|    value_loss         | 0.0689   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11900000, episode_reward=384.60 +/- 44.48\n",
      "Episode length: 9713.20 +/- 673.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.71e+03 |\n",
      "|    mean_reward        | 385      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11900000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 148749   |\n",
      "|    policy_loss        | -0.0825  |\n",
      "|    value_loss         | 0.0573   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+04    |\n",
      "|    ep_rew_mean        | 383      |\n",
      "| time/                 |          |\n",
      "|    fps                | 499      |\n",
      "|    iterations         | 148800   |\n",
      "|    time_elapsed       | 23833    |\n",
      "|    total_timesteps    | 11904000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.913   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 148799   |\n",
      "|    policy_loss        | -0.0315  |\n",
      "|    value_loss         | 0.0195   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11910000, episode_reward=329.60 +/- 97.35\n",
      "Episode length: 8332.40 +/- 1987.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.33e+03 |\n",
      "|    mean_reward        | 330      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11910000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 148874   |\n",
      "|    policy_loss        | -0.0808  |\n",
      "|    value_loss         | 0.0367   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | 380      |\n",
      "| time/                 |          |\n",
      "|    fps                | 499      |\n",
      "|    iterations         | 148900   |\n",
      "|    time_elapsed       | 23856    |\n",
      "|    total_timesteps    | 11912000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.974   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 148899   |\n",
      "|    policy_loss        | -0.0777  |\n",
      "|    value_loss         | 0.0561   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11920000, episode_reward=341.20 +/- 103.29\n",
      "Episode length: 9825.40 +/- 2675.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.83e+03 |\n",
      "|    mean_reward        | 341      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11920000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.912   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 148999   |\n",
      "|    policy_loss        | 0.0973   |\n",
      "|    value_loss         | 0.0368   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.98e+03 |\n",
      "|    ep_rew_mean     | 379      |\n",
      "| time/              |          |\n",
      "|    fps             | 499      |\n",
      "|    iterations      | 149000   |\n",
      "|    time_elapsed    | 23882    |\n",
      "|    total_timesteps | 11920000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.93e+03 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 499      |\n",
      "|    iterations         | 149100   |\n",
      "|    time_elapsed       | 23889    |\n",
      "|    total_timesteps    | 11928000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.944   |\n",
      "|    explained_variance | 0.839    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 149099   |\n",
      "|    policy_loss        | -0.126   |\n",
      "|    value_loss         | 0.592    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11930000, episode_reward=376.60 +/- 33.04\n",
      "Episode length: 8070.80 +/- 965.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.07e+03 |\n",
      "|    mean_reward        | 377      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11930000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.869   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 149124   |\n",
      "|    policy_loss        | 0.0719   |\n",
      "|    value_loss         | 0.178    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.91e+03 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 499      |\n",
      "|    iterations         | 149200   |\n",
      "|    time_elapsed       | 23911    |\n",
      "|    total_timesteps    | 11936000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.927   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 149199   |\n",
      "|    policy_loss        | -0.0503  |\n",
      "|    value_loss         | 0.14     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11940000, episode_reward=364.00 +/- 48.99\n",
      "Episode length: 10760.00 +/- 2434.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+04 |\n",
      "|    mean_reward        | 364      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11940000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 149249   |\n",
      "|    policy_loss        | -0.0125  |\n",
      "|    value_loss         | 0.0837   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.86e+03 |\n",
      "|    ep_rew_mean        | 380      |\n",
      "| time/                 |          |\n",
      "|    fps                | 498      |\n",
      "|    iterations         | 149300   |\n",
      "|    time_elapsed       | 23939    |\n",
      "|    total_timesteps    | 11944000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.857   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 149299   |\n",
      "|    policy_loss        | -0.021   |\n",
      "|    value_loss         | 0.0154   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11950000, episode_reward=380.60 +/- 37.99\n",
      "Episode length: 9210.60 +/- 1927.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.21e+03 |\n",
      "|    mean_reward        | 381      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11950000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.899   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 149374   |\n",
      "|    policy_loss        | 0.0218   |\n",
      "|    value_loss         | 0.145    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+04    |\n",
      "|    ep_rew_mean        | 388      |\n",
      "| time/                 |          |\n",
      "|    fps                | 498      |\n",
      "|    iterations         | 149400   |\n",
      "|    time_elapsed       | 23963    |\n",
      "|    total_timesteps    | 11952000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.897   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 149399   |\n",
      "|    policy_loss        | 0.00207  |\n",
      "|    value_loss         | 0.0477   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11960000, episode_reward=413.40 +/- 11.29\n",
      "Episode length: 10338.60 +/- 1562.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+04 |\n",
      "|    mean_reward        | 413      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11960000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.908   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 149499   |\n",
      "|    policy_loss        | 0.0745   |\n",
      "|    value_loss         | 0.0387   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.99e+03 |\n",
      "|    ep_rew_mean     | 386      |\n",
      "| time/              |          |\n",
      "|    fps             | 498      |\n",
      "|    iterations      | 149500   |\n",
      "|    time_elapsed    | 23990    |\n",
      "|    total_timesteps | 11960000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.9e+03  |\n",
      "|    ep_rew_mean        | 382      |\n",
      "| time/                 |          |\n",
      "|    fps                | 498      |\n",
      "|    iterations         | 149600   |\n",
      "|    time_elapsed       | 23997    |\n",
      "|    total_timesteps    | 11968000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.964   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 149599   |\n",
      "|    policy_loss        | 0.0715   |\n",
      "|    value_loss         | 0.0757   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11970000, episode_reward=410.20 +/- 43.85\n",
      "Episode length: 10419.80 +/- 1539.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+04 |\n",
      "|    mean_reward        | 410      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11970000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 149624   |\n",
      "|    policy_loss        | 0.0823   |\n",
      "|    value_loss         | 0.0797   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.92e+03 |\n",
      "|    ep_rew_mean        | 382      |\n",
      "| time/                 |          |\n",
      "|    fps                | 498      |\n",
      "|    iterations         | 149700   |\n",
      "|    time_elapsed       | 24024    |\n",
      "|    total_timesteps    | 11976000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.973   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 149699   |\n",
      "|    policy_loss        | 0.151    |\n",
      "|    value_loss         | 0.139    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11980000, episode_reward=392.00 +/- 29.70\n",
      "Episode length: 12916.40 +/- 3658.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.29e+04 |\n",
      "|    mean_reward        | 392      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11980000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 149749   |\n",
      "|    policy_loss        | -0.0458  |\n",
      "|    value_loss         | 0.0444   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.86e+03 |\n",
      "|    ep_rew_mean        | 382      |\n",
      "| time/                 |          |\n",
      "|    fps                | 498      |\n",
      "|    iterations         | 149800   |\n",
      "|    time_elapsed       | 24055    |\n",
      "|    total_timesteps    | 11984000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.995   |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 149799   |\n",
      "|    policy_loss        | 0.0497   |\n",
      "|    value_loss         | 0.258    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11990000, episode_reward=412.00 +/- 40.15\n",
      "Episode length: 11720.60 +/- 2043.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.17e+04 |\n",
      "|    mean_reward        | 412      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11990000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 149874   |\n",
      "|    policy_loss        | 0.0361   |\n",
      "|    value_loss         | 0.0818   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.71e+03 |\n",
      "|    ep_rew_mean        | 379      |\n",
      "| time/                 |          |\n",
      "|    fps                | 497      |\n",
      "|    iterations         | 149900   |\n",
      "|    time_elapsed       | 24085    |\n",
      "|    total_timesteps    | 11992000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.943   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 149899   |\n",
      "|    policy_loss        | 0.0163   |\n",
      "|    value_loss         | 0.0239   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12000000, episode_reward=411.80 +/- 11.36\n",
      "Episode length: 12736.20 +/- 2332.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.27e+04 |\n",
      "|    mean_reward        | 412      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12000000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.421    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 149999   |\n",
      "|    policy_loss        | -0.086   |\n",
      "|    value_loss         | 1.27     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.72e+03 |\n",
      "|    ep_rew_mean     | 377      |\n",
      "| time/              |          |\n",
      "|    fps             | 497      |\n",
      "|    iterations      | 150000   |\n",
      "|    time_elapsed    | 24117    |\n",
      "|    total_timesteps | 12000000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.71e+03 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 497      |\n",
      "|    iterations         | 150100   |\n",
      "|    time_elapsed       | 24125    |\n",
      "|    total_timesteps    | 12008000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.998   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 150099   |\n",
      "|    policy_loss        | 0.03     |\n",
      "|    value_loss         | 0.0427   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12010000, episode_reward=322.80 +/- 139.46\n",
      "Episode length: 8353.80 +/- 2674.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.35e+03 |\n",
      "|    mean_reward        | 323      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12010000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.986   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 150124   |\n",
      "|    policy_loss        | 0.0161   |\n",
      "|    value_loss         | 0.106    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.75e+03 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 497      |\n",
      "|    iterations         | 150200   |\n",
      "|    time_elapsed       | 24153    |\n",
      "|    total_timesteps    | 12016000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.928   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 150199   |\n",
      "|    policy_loss        | -0.0402  |\n",
      "|    value_loss         | 0.0542   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12020000, episode_reward=357.40 +/- 50.27\n",
      "Episode length: 10267.80 +/- 1827.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+04 |\n",
      "|    mean_reward        | 357      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12020000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.962   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 150249   |\n",
      "|    policy_loss        | -0.0946  |\n",
      "|    value_loss         | 0.244    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.68e+03 |\n",
      "|    ep_rew_mean        | 372      |\n",
      "| time/                 |          |\n",
      "|    fps                | 497      |\n",
      "|    iterations         | 150300   |\n",
      "|    time_elapsed       | 24184    |\n",
      "|    total_timesteps    | 12024000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.83    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 150299   |\n",
      "|    policy_loss        | 0.0149   |\n",
      "|    value_loss         | 0.0216   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12030000, episode_reward=393.80 +/- 14.47\n",
      "Episode length: 9594.80 +/- 2350.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.59e+03 |\n",
      "|    mean_reward        | 394      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12030000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.937   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 150374   |\n",
      "|    policy_loss        | -0.014   |\n",
      "|    value_loss         | 0.0472   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.72e+03 |\n",
      "|    ep_rew_mean        | 373      |\n",
      "| time/                 |          |\n",
      "|    fps                | 496      |\n",
      "|    iterations         | 150400   |\n",
      "|    time_elapsed       | 24214    |\n",
      "|    total_timesteps    | 12032000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.937   |\n",
      "|    explained_variance | 0.592    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 150399   |\n",
      "|    policy_loss        | -0.285   |\n",
      "|    value_loss         | 1.65     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12040000, episode_reward=369.40 +/- 41.67\n",
      "Episode length: 9322.60 +/- 2054.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.32e+03 |\n",
      "|    mean_reward        | 369      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12040000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.855   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 150499   |\n",
      "|    policy_loss        | 0.00389  |\n",
      "|    value_loss         | 0.0398   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.73e+03 |\n",
      "|    ep_rew_mean     | 373      |\n",
      "| time/              |          |\n",
      "|    fps             | 496      |\n",
      "|    iterations      | 150500   |\n",
      "|    time_elapsed    | 24245    |\n",
      "|    total_timesteps | 12040000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.77e+03 |\n",
      "|    ep_rew_mean        | 371      |\n",
      "| time/                 |          |\n",
      "|    fps                | 496      |\n",
      "|    iterations         | 150600   |\n",
      "|    time_elapsed       | 24252    |\n",
      "|    total_timesteps    | 12048000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.953   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 150599   |\n",
      "|    policy_loss        | 0.0423   |\n",
      "|    value_loss         | 0.0732   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12050000, episode_reward=466.80 +/- 184.11\n",
      "Episode length: 12659.40 +/- 4141.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.27e+04 |\n",
      "|    mean_reward        | 467      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12050000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.858   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 150624   |\n",
      "|    policy_loss        | -0.0454  |\n",
      "|    value_loss         | 0.0701   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.7e+03  |\n",
      "|    ep_rew_mean        | 368      |\n",
      "| time/                 |          |\n",
      "|    fps                | 496      |\n",
      "|    iterations         | 150700   |\n",
      "|    time_elapsed       | 24289    |\n",
      "|    total_timesteps    | 12056000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.925   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 150699   |\n",
      "|    policy_loss        | -0.0109  |\n",
      "|    value_loss         | 0.027    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12060000, episode_reward=287.00 +/- 111.07\n",
      "Episode length: 8007.20 +/- 3303.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.01e+03 |\n",
      "|    mean_reward        | 287      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12060000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 150749   |\n",
      "|    policy_loss        | 0.0421   |\n",
      "|    value_loss         | 0.00811  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.62e+03 |\n",
      "|    ep_rew_mean        | 365      |\n",
      "| time/                 |          |\n",
      "|    fps                | 496      |\n",
      "|    iterations         | 150800   |\n",
      "|    time_elapsed       | 24316    |\n",
      "|    total_timesteps    | 12064000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.892   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 150799   |\n",
      "|    policy_loss        | -0.013   |\n",
      "|    value_loss         | 0.0401   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12070000, episode_reward=395.00 +/- 43.02\n",
      "Episode length: 10484.60 +/- 2881.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+04 |\n",
      "|    mean_reward        | 395      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12070000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.987   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 150874   |\n",
      "|    policy_loss        | -0.0131  |\n",
      "|    value_loss         | 0.0225   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.65e+03 |\n",
      "|    ep_rew_mean        | 362      |\n",
      "| time/                 |          |\n",
      "|    fps                | 495      |\n",
      "|    iterations         | 150900   |\n",
      "|    time_elapsed       | 24348    |\n",
      "|    total_timesteps    | 12072000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.993   |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 150899   |\n",
      "|    policy_loss        | 0.0326   |\n",
      "|    value_loss         | 0.314    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12080000, episode_reward=495.00 +/- 155.20\n",
      "Episode length: 14247.20 +/- 6656.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.42e+04 |\n",
      "|    mean_reward        | 495      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12080000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.944   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 150999   |\n",
      "|    policy_loss        | -0.0369  |\n",
      "|    value_loss         | 0.0301   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.67e+03 |\n",
      "|    ep_rew_mean     | 361      |\n",
      "| time/              |          |\n",
      "|    fps             | 495      |\n",
      "|    iterations      | 151000   |\n",
      "|    time_elapsed    | 24390    |\n",
      "|    total_timesteps | 12080000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.63e+03 |\n",
      "|    ep_rew_mean        | 358      |\n",
      "| time/                 |          |\n",
      "|    fps                | 495      |\n",
      "|    iterations         | 151100   |\n",
      "|    time_elapsed       | 24397    |\n",
      "|    total_timesteps    | 12088000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.851   |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 151099   |\n",
      "|    policy_loss        | 0.075    |\n",
      "|    value_loss         | 0.311    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12090000, episode_reward=346.80 +/- 120.78\n",
      "Episode length: 10756.20 +/- 3458.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+04 |\n",
      "|    mean_reward        | 347      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12090000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.997   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 151124   |\n",
      "|    policy_loss        | 0.0633   |\n",
      "|    value_loss         | 0.0641   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.72e+03 |\n",
      "|    ep_rew_mean        | 362      |\n",
      "| time/                 |          |\n",
      "|    fps                | 495      |\n",
      "|    iterations         | 151200   |\n",
      "|    time_elapsed       | 24430    |\n",
      "|    total_timesteps    | 12096000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.842   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 151199   |\n",
      "|    policy_loss        | 0.00659  |\n",
      "|    value_loss         | 0.034    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12100000, episode_reward=414.00 +/- 6.81\n",
      "Episode length: 11064.20 +/- 1497.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.11e+04 |\n",
      "|    mean_reward        | 414      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12100000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.86    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 151249   |\n",
      "|    policy_loss        | -0.0433  |\n",
      "|    value_loss         | 0.0408   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.73e+03 |\n",
      "|    ep_rew_mean        | 361      |\n",
      "| time/                 |          |\n",
      "|    fps                | 494      |\n",
      "|    iterations         | 151300   |\n",
      "|    time_elapsed       | 24465    |\n",
      "|    total_timesteps    | 12104000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.88    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 151299   |\n",
      "|    policy_loss        | -0.0403  |\n",
      "|    value_loss         | 0.0229   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12110000, episode_reward=381.80 +/- 30.63\n",
      "Episode length: 13466.40 +/- 3974.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.35e+04 |\n",
      "|    mean_reward        | 382      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12110000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 151374   |\n",
      "|    policy_loss        | -0.0381  |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.65e+03 |\n",
      "|    ep_rew_mean        | 359      |\n",
      "| time/                 |          |\n",
      "|    fps                | 494      |\n",
      "|    iterations         | 151400   |\n",
      "|    time_elapsed       | 24512    |\n",
      "|    total_timesteps    | 12112000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.885   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 151399   |\n",
      "|    policy_loss        | 0.0313   |\n",
      "|    value_loss         | 0.0413   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12120000, episode_reward=494.40 +/- 129.87\n",
      "Episode length: 15541.40 +/- 2828.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.55e+04 |\n",
      "|    mean_reward        | 494      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12120000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.947   |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 151499   |\n",
      "|    policy_loss        | 0.11     |\n",
      "|    value_loss         | 0.443    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.73e+03 |\n",
      "|    ep_rew_mean     | 359      |\n",
      "| time/              |          |\n",
      "|    fps             | 493      |\n",
      "|    iterations      | 151500   |\n",
      "|    time_elapsed    | 24570    |\n",
      "|    total_timesteps | 12120000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.75e+03 |\n",
      "|    ep_rew_mean        | 359      |\n",
      "| time/                 |          |\n",
      "|    fps                | 493      |\n",
      "|    iterations         | 151600   |\n",
      "|    time_elapsed       | 24579    |\n",
      "|    total_timesteps    | 12128000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.953   |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 151599   |\n",
      "|    policy_loss        | 0.0239   |\n",
      "|    value_loss         | 0.144    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12130000, episode_reward=352.80 +/- 128.96\n",
      "Episode length: 9765.80 +/- 2329.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.77e+03 |\n",
      "|    mean_reward        | 353      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12130000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.895   |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 151624   |\n",
      "|    policy_loss        | 0.0767   |\n",
      "|    value_loss         | 0.19     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.61e+03 |\n",
      "|    ep_rew_mean        | 358      |\n",
      "| time/                 |          |\n",
      "|    fps                | 492      |\n",
      "|    iterations         | 151700   |\n",
      "|    time_elapsed       | 24618    |\n",
      "|    total_timesteps    | 12136000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.884   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 151699   |\n",
      "|    policy_loss        | 0.0435   |\n",
      "|    value_loss         | 0.0648   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12140000, episode_reward=342.40 +/- 107.46\n",
      "Episode length: 9629.40 +/- 4435.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.63e+03 |\n",
      "|    mean_reward        | 342      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12140000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.988   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 151749   |\n",
      "|    policy_loss        | -0.0443  |\n",
      "|    value_loss         | 0.0398   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.64e+03 |\n",
      "|    ep_rew_mean        | 358      |\n",
      "| time/                 |          |\n",
      "|    fps                | 492      |\n",
      "|    iterations         | 151800   |\n",
      "|    time_elapsed       | 24656    |\n",
      "|    total_timesteps    | 12144000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.887   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 151799   |\n",
      "|    policy_loss        | -0.00849 |\n",
      "|    value_loss         | 0.0164   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12150000, episode_reward=398.00 +/- 28.71\n",
      "Episode length: 10821.80 +/- 1594.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+04 |\n",
      "|    mean_reward        | 398      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12150000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.861   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 151874   |\n",
      "|    policy_loss        | -0.0865  |\n",
      "|    value_loss         | 0.0711   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.69e+03 |\n",
      "|    ep_rew_mean        | 359      |\n",
      "| time/                 |          |\n",
      "|    fps                | 492      |\n",
      "|    iterations         | 151900   |\n",
      "|    time_elapsed       | 24698    |\n",
      "|    total_timesteps    | 12152000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.855   |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 151899   |\n",
      "|    policy_loss        | 0.0797   |\n",
      "|    value_loss         | 0.0929   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12160000, episode_reward=397.00 +/- 26.10\n",
      "Episode length: 9066.20 +/- 1472.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.07e+03 |\n",
      "|    mean_reward        | 397      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12160000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.969   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 151999   |\n",
      "|    policy_loss        | -0.151   |\n",
      "|    value_loss         | 0.143    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.76e+03 |\n",
      "|    ep_rew_mean     | 362      |\n",
      "| time/              |          |\n",
      "|    fps             | 491      |\n",
      "|    iterations      | 152000   |\n",
      "|    time_elapsed    | 24735    |\n",
      "|    total_timesteps | 12160000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.74e+03 |\n",
      "|    ep_rew_mean        | 362      |\n",
      "| time/                 |          |\n",
      "|    fps                | 491      |\n",
      "|    iterations         | 152100   |\n",
      "|    time_elapsed       | 24743    |\n",
      "|    total_timesteps    | 12168000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.975   |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 152099   |\n",
      "|    policy_loss        | -0.0541  |\n",
      "|    value_loss         | 0.271    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12170000, episode_reward=379.60 +/- 71.58\n",
      "Episode length: 9972.80 +/- 2937.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.97e+03 |\n",
      "|    mean_reward        | 380      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12170000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.902   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 152124   |\n",
      "|    policy_loss        | 0.0638   |\n",
      "|    value_loss         | 0.077    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.83e+03 |\n",
      "|    ep_rew_mean        | 365      |\n",
      "| time/                 |          |\n",
      "|    fps                | 491      |\n",
      "|    iterations         | 152200   |\n",
      "|    time_elapsed       | 24784    |\n",
      "|    total_timesteps    | 12176000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 152199   |\n",
      "|    policy_loss        | -0.0707  |\n",
      "|    value_loss         | 0.0696   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12180000, episode_reward=402.20 +/- 27.00\n",
      "Episode length: 14569.00 +/- 8431.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.46e+04 |\n",
      "|    mean_reward        | 402      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12180000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.988   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 152249   |\n",
      "|    policy_loss        | 0.0357   |\n",
      "|    value_loss         | 0.0383   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.83e+03 |\n",
      "|    ep_rew_mean        | 367      |\n",
      "| time/                 |          |\n",
      "|    fps                | 490      |\n",
      "|    iterations         | 152300   |\n",
      "|    time_elapsed       | 24840    |\n",
      "|    total_timesteps    | 12184000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.85    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 152299   |\n",
      "|    policy_loss        | -0.00181 |\n",
      "|    value_loss         | 0.0648   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12190000, episode_reward=411.00 +/- 9.44\n",
      "Episode length: 14115.80 +/- 8553.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.41e+04 |\n",
      "|    mean_reward        | 411      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12190000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.902   |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 152374   |\n",
      "|    policy_loss        | 0.0219   |\n",
      "|    value_loss         | 0.12     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.97e+03 |\n",
      "|    ep_rew_mean        | 372      |\n",
      "| time/                 |          |\n",
      "|    fps                | 489      |\n",
      "|    iterations         | 152400   |\n",
      "|    time_elapsed       | 24894    |\n",
      "|    total_timesteps    | 12192000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.991   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 152399   |\n",
      "|    policy_loss        | -0.00961 |\n",
      "|    value_loss         | 0.105    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12200000, episode_reward=362.00 +/- 57.90\n",
      "Episode length: 10904.80 +/- 4656.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.09e+04 |\n",
      "|    mean_reward        | 362      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12200000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.975   |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 152499   |\n",
      "|    policy_loss        | -0.111   |\n",
      "|    value_loss         | 0.17     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.97e+03 |\n",
      "|    ep_rew_mean     | 372      |\n",
      "| time/              |          |\n",
      "|    fps             | 489      |\n",
      "|    iterations      | 152500   |\n",
      "|    time_elapsed    | 24939    |\n",
      "|    total_timesteps | 12200000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.88e+03 |\n",
      "|    ep_rew_mean        | 368      |\n",
      "| time/                 |          |\n",
      "|    fps                | 489      |\n",
      "|    iterations         | 152600   |\n",
      "|    time_elapsed       | 24946    |\n",
      "|    total_timesteps    | 12208000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.922   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 152599   |\n",
      "|    policy_loss        | 0.0248   |\n",
      "|    value_loss         | 0.0631   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12210000, episode_reward=430.00 +/- 219.71\n",
      "Episode length: 11544.80 +/- 5462.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.15e+04 |\n",
      "|    mean_reward        | 430      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12210000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.925   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 152624   |\n",
      "|    policy_loss        | -0.0228  |\n",
      "|    value_loss         | 0.0622   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.9e+03  |\n",
      "|    ep_rew_mean        | 369      |\n",
      "| time/                 |          |\n",
      "|    fps                | 488      |\n",
      "|    iterations         | 152700   |\n",
      "|    time_elapsed       | 24994    |\n",
      "|    total_timesteps    | 12216000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.968   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 152699   |\n",
      "|    policy_loss        | -0.0199  |\n",
      "|    value_loss         | 0.0314   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12220000, episode_reward=222.40 +/- 161.77\n",
      "Episode length: 7458.60 +/- 3135.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.46e+03 |\n",
      "|    mean_reward        | 222      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12220000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.979   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 152749   |\n",
      "|    policy_loss        | 0.00502  |\n",
      "|    value_loss         | 0.0679   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.91e+03 |\n",
      "|    ep_rew_mean        | 370      |\n",
      "| time/                 |          |\n",
      "|    fps                | 488      |\n",
      "|    iterations         | 152800   |\n",
      "|    time_elapsed       | 25027    |\n",
      "|    total_timesteps    | 12224000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 152799   |\n",
      "|    policy_loss        | 0.163    |\n",
      "|    value_loss         | 0.135    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12230000, episode_reward=383.60 +/- 40.71\n",
      "Episode length: 11061.80 +/- 5132.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.11e+04 |\n",
      "|    mean_reward        | 384      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12230000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.875   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 152874   |\n",
      "|    policy_loss        | 0.0215   |\n",
      "|    value_loss         | 0.0334   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | 374      |\n",
      "| time/                 |          |\n",
      "|    fps                | 487      |\n",
      "|    iterations         | 152900   |\n",
      "|    time_elapsed       | 25071    |\n",
      "|    total_timesteps    | 12232000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 152899   |\n",
      "|    policy_loss        | 0.0401   |\n",
      "|    value_loss         | 0.042    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12240000, episode_reward=407.60 +/- 15.23\n",
      "Episode length: 9450.60 +/- 1803.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.45e+03 |\n",
      "|    mean_reward        | 408      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12240000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.958   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 152999   |\n",
      "|    policy_loss        | -0.0783  |\n",
      "|    value_loss         | 0.0544   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.01e+04 |\n",
      "|    ep_rew_mean     | 373      |\n",
      "| time/              |          |\n",
      "|    fps             | 487      |\n",
      "|    iterations      | 153000   |\n",
      "|    time_elapsed    | 25107    |\n",
      "|    total_timesteps | 12240000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+04 |\n",
      "|    ep_rew_mean        | 376      |\n",
      "| time/                 |          |\n",
      "|    fps                | 487      |\n",
      "|    iterations         | 153100   |\n",
      "|    time_elapsed       | 25115    |\n",
      "|    total_timesteps    | 12248000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 153099   |\n",
      "|    policy_loss        | -0.00457 |\n",
      "|    value_loss         | 0.0245   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12250000, episode_reward=339.80 +/- 97.93\n",
      "Episode length: 9708.60 +/- 1814.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.71e+03 |\n",
      "|    mean_reward        | 340      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12250000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 153124   |\n",
      "|    policy_loss        | -0.0542  |\n",
      "|    value_loss         | 0.0954   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+04 |\n",
      "|    ep_rew_mean        | 377      |\n",
      "| time/                 |          |\n",
      "|    fps                | 487      |\n",
      "|    iterations         | 153200   |\n",
      "|    time_elapsed       | 25153    |\n",
      "|    total_timesteps    | 12256000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.927   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 153199   |\n",
      "|    policy_loss        | -0.0169  |\n",
      "|    value_loss         | 0.0192   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12260000, episode_reward=393.20 +/- 21.14\n",
      "Episode length: 9571.80 +/- 1473.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.57e+03 |\n",
      "|    mean_reward        | 393      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12260000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.951   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 153249   |\n",
      "|    policy_loss        | -0.00389 |\n",
      "|    value_loss         | 0.0704   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+04 |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 486      |\n",
      "|    iterations         | 153300   |\n",
      "|    time_elapsed       | 25191    |\n",
      "|    total_timesteps    | 12264000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 153299   |\n",
      "|    policy_loss        | -0.00604 |\n",
      "|    value_loss         | 0.0411   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12270000, episode_reward=361.20 +/- 33.59\n",
      "Episode length: 8398.60 +/- 1045.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.4e+03  |\n",
      "|    mean_reward        | 361      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12270000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.933   |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 153374   |\n",
      "|    policy_loss        | -0.0438  |\n",
      "|    value_loss         | 0.569    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+04 |\n",
      "|    ep_rew_mean        | 378      |\n",
      "| time/                 |          |\n",
      "|    fps                | 486      |\n",
      "|    iterations         | 153400   |\n",
      "|    time_elapsed       | 25224    |\n",
      "|    total_timesteps    | 12272000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 153399   |\n",
      "|    policy_loss        | 0.00083  |\n",
      "|    value_loss         | 0.0244   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12280000, episode_reward=391.80 +/- 31.36\n",
      "Episode length: 8916.80 +/- 1991.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.92e+03 |\n",
      "|    mean_reward        | 392      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12280000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 153499   |\n",
      "|    policy_loss        | -0.0535  |\n",
      "|    value_loss         | 0.082    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.03e+04 |\n",
      "|    ep_rew_mean     | 377      |\n",
      "| time/              |          |\n",
      "|    fps             | 486      |\n",
      "|    iterations      | 153500   |\n",
      "|    time_elapsed    | 25259    |\n",
      "|    total_timesteps | 12280000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+04 |\n",
      "|    ep_rew_mean        | 382      |\n",
      "| time/                 |          |\n",
      "|    fps                | 486      |\n",
      "|    iterations         | 153600   |\n",
      "|    time_elapsed       | 25267    |\n",
      "|    total_timesteps    | 12288000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.954   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 153599   |\n",
      "|    policy_loss        | 0.0211   |\n",
      "|    value_loss         | 0.0321   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12290000, episode_reward=348.80 +/- 132.35\n",
      "Episode length: 10289.20 +/- 2717.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+04 |\n",
      "|    mean_reward        | 349      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12290000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.9     |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 153624   |\n",
      "|    policy_loss        | -0.0443  |\n",
      "|    value_loss         | 0.0337   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05e+04 |\n",
      "|    ep_rew_mean        | 381      |\n",
      "| time/                 |          |\n",
      "|    fps                | 485      |\n",
      "|    iterations         | 153700   |\n",
      "|    time_elapsed       | 25307    |\n",
      "|    total_timesteps    | 12296000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.831   |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 153699   |\n",
      "|    policy_loss        | 0.0161   |\n",
      "|    value_loss         | 0.457    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12300000, episode_reward=321.40 +/- 144.71\n",
      "Episode length: 8677.80 +/- 2597.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.68e+03 |\n",
      "|    mean_reward        | 321      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12300000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.991   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 153749   |\n",
      "|    policy_loss        | 0.0162   |\n",
      "|    value_loss         | 0.0862   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05e+04 |\n",
      "|    ep_rew_mean        | 382      |\n",
      "| time/                 |          |\n",
      "|    fps                | 485      |\n",
      "|    iterations         | 153800   |\n",
      "|    time_elapsed       | 25342    |\n",
      "|    total_timesteps    | 12304000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.934   |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 153799   |\n",
      "|    policy_loss        | -0.168   |\n",
      "|    value_loss         | 0.289    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12310000, episode_reward=413.40 +/- 16.23\n",
      "Episode length: 12197.80 +/- 3942.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.22e+04 |\n",
      "|    mean_reward        | 413      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12310000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 153874   |\n",
      "|    policy_loss        | 0.0414   |\n",
      "|    value_loss         | 0.0633   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+04 |\n",
      "|    ep_rew_mean        | 381      |\n",
      "| time/                 |          |\n",
      "|    fps                | 484      |\n",
      "|    iterations         | 153900   |\n",
      "|    time_elapsed       | 25388    |\n",
      "|    total_timesteps    | 12312000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 153899   |\n",
      "|    policy_loss        | -0.0401  |\n",
      "|    value_loss         | 0.0396   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12320000, episode_reward=413.80 +/- 14.23\n",
      "Episode length: 10291.40 +/- 806.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+04 |\n",
      "|    mean_reward        | 414      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12320000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 153999   |\n",
      "|    policy_loss        | -0.00159 |\n",
      "|    value_loss         | 0.0876   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.04e+04 |\n",
      "|    ep_rew_mean     | 387      |\n",
      "| time/              |          |\n",
      "|    fps             | 484      |\n",
      "|    iterations      | 154000   |\n",
      "|    time_elapsed    | 25427    |\n",
      "|    total_timesteps | 12320000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05e+04 |\n",
      "|    ep_rew_mean        | 390      |\n",
      "| time/                 |          |\n",
      "|    fps                | 484      |\n",
      "|    iterations         | 154100   |\n",
      "|    time_elapsed       | 25435    |\n",
      "|    total_timesteps    | 12328000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.93    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 154099   |\n",
      "|    policy_loss        | 0.0783   |\n",
      "|    value_loss         | 0.052    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12330000, episode_reward=406.40 +/- 20.79\n",
      "Episode length: 15915.60 +/- 6534.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.59e+04 |\n",
      "|    mean_reward        | 406      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12330000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.841   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 154124   |\n",
      "|    policy_loss        | 0.0372   |\n",
      "|    value_loss         | 0.0719   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 394      |\n",
      "| time/                 |          |\n",
      "|    fps                | 483      |\n",
      "|    iterations         | 154200   |\n",
      "|    time_elapsed       | 25492    |\n",
      "|    total_timesteps    | 12336000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.982   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 154199   |\n",
      "|    policy_loss        | -0.107   |\n",
      "|    value_loss         | 0.0532   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12340000, episode_reward=400.60 +/- 54.48\n",
      "Episode length: 12184.40 +/- 3349.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.22e+04 |\n",
      "|    mean_reward        | 401      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12340000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.878   |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 154249   |\n",
      "|    policy_loss        | -0.144   |\n",
      "|    value_loss         | 0.279    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 395      |\n",
      "| time/                 |          |\n",
      "|    fps                | 483      |\n",
      "|    iterations         | 154300   |\n",
      "|    time_elapsed       | 25536    |\n",
      "|    total_timesteps    | 12344000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.998   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 154299   |\n",
      "|    policy_loss        | 0.00559  |\n",
      "|    value_loss         | 0.0466   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12350000, episode_reward=387.20 +/- 43.06\n",
      "Episode length: 15322.20 +/- 5878.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.53e+04 |\n",
      "|    mean_reward        | 387      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12350000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.791   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 154374   |\n",
      "|    policy_loss        | -0.00055 |\n",
      "|    value_loss         | 0.0301   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 398      |\n",
      "| time/                 |          |\n",
      "|    fps                | 482      |\n",
      "|    iterations         | 154400   |\n",
      "|    time_elapsed       | 25590    |\n",
      "|    total_timesteps    | 12352000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.855   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 154399   |\n",
      "|    policy_loss        | 0.0127   |\n",
      "|    value_loss         | 0.0336   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12360000, episode_reward=360.60 +/- 94.60\n",
      "Episode length: 9314.40 +/- 1471.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.31e+03 |\n",
      "|    mean_reward        | 361      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12360000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 154499   |\n",
      "|    policy_loss        | -0.0476  |\n",
      "|    value_loss         | 0.445    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.08e+04 |\n",
      "|    ep_rew_mean     | 398      |\n",
      "| time/              |          |\n",
      "|    fps             | 482      |\n",
      "|    iterations      | 154500   |\n",
      "|    time_elapsed    | 25626    |\n",
      "|    total_timesteps | 12360000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 398      |\n",
      "| time/                 |          |\n",
      "|    fps                | 482      |\n",
      "|    iterations         | 154600   |\n",
      "|    time_elapsed       | 25634    |\n",
      "|    total_timesteps    | 12368000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.985   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 154599   |\n",
      "|    policy_loss        | -0.0413  |\n",
      "|    value_loss         | 0.0352   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12370000, episode_reward=407.60 +/- 11.32\n",
      "Episode length: 9171.20 +/- 1391.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.17e+03 |\n",
      "|    mean_reward        | 408      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12370000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.966   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 154624   |\n",
      "|    policy_loss        | 0.0485   |\n",
      "|    value_loss         | 0.153    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 399      |\n",
      "| time/                 |          |\n",
      "|    fps                | 482      |\n",
      "|    iterations         | 154700   |\n",
      "|    time_elapsed       | 25670    |\n",
      "|    total_timesteps    | 12376000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 154699   |\n",
      "|    policy_loss        | 0.11     |\n",
      "|    value_loss         | 0.156    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12380000, episode_reward=352.20 +/- 56.20\n",
      "Episode length: 8562.40 +/- 642.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.56e+03 |\n",
      "|    mean_reward        | 352      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12380000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.975   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 154749   |\n",
      "|    policy_loss        | -0.00849 |\n",
      "|    value_loss         | 0.0549   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+04 |\n",
      "|    ep_rew_mean        | 402      |\n",
      "| time/                 |          |\n",
      "|    fps                | 481      |\n",
      "|    iterations         | 154800   |\n",
      "|    time_elapsed       | 25705    |\n",
      "|    total_timesteps    | 12384000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.945   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 154799   |\n",
      "|    policy_loss        | -0.0421  |\n",
      "|    value_loss         | 0.0176   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12390000, episode_reward=428.00 +/- 32.70\n",
      "Episode length: 13026.60 +/- 3145.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.3e+04  |\n",
      "|    mean_reward        | 428      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12390000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.941   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 154874   |\n",
      "|    policy_loss        | -0.0324  |\n",
      "|    value_loss         | 0.0535   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 403      |\n",
      "| time/                 |          |\n",
      "|    fps                | 481      |\n",
      "|    iterations         | 154900   |\n",
      "|    time_elapsed       | 25753    |\n",
      "|    total_timesteps    | 12392000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.814   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 154899   |\n",
      "|    policy_loss        | -0.0149  |\n",
      "|    value_loss         | 0.0565   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12400000, episode_reward=411.40 +/- 12.66\n",
      "Episode length: 9720.20 +/- 1819.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.72e+03 |\n",
      "|    mean_reward        | 411      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12400000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 154999   |\n",
      "|    policy_loss        | 0.0184   |\n",
      "|    value_loss         | 0.0272   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.09e+04 |\n",
      "|    ep_rew_mean     | 401      |\n",
      "| time/              |          |\n",
      "|    fps             | 480      |\n",
      "|    iterations      | 155000   |\n",
      "|    time_elapsed    | 25791    |\n",
      "|    total_timesteps | 12400000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 402      |\n",
      "| time/                 |          |\n",
      "|    fps                | 480      |\n",
      "|    iterations         | 155100   |\n",
      "|    time_elapsed       | 25799    |\n",
      "|    total_timesteps    | 12408000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.855   |\n",
      "|    explained_variance | 0.785    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 155099   |\n",
      "|    policy_loss        | -0.105   |\n",
      "|    value_loss         | 0.699    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12410000, episode_reward=374.60 +/- 72.80\n",
      "Episode length: 16887.00 +/- 14840.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.69e+04 |\n",
      "|    mean_reward        | 375      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12410000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.917   |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 155124   |\n",
      "|    policy_loss        | 0.145    |\n",
      "|    value_loss         | 0.452    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 402      |\n",
      "| time/                 |          |\n",
      "|    fps                | 480      |\n",
      "|    iterations         | 155200   |\n",
      "|    time_elapsed       | 25858    |\n",
      "|    total_timesteps    | 12416000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.842   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 155199   |\n",
      "|    policy_loss        | -0.00983 |\n",
      "|    value_loss         | 0.0312   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12420000, episode_reward=441.60 +/- 19.68\n",
      "Episode length: 13585.80 +/- 3517.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.36e+04 |\n",
      "|    mean_reward        | 442      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12420000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.998   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 155249   |\n",
      "|    policy_loss        | 0.0513   |\n",
      "|    value_loss         | 0.0508   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.1e+04   |\n",
      "|    ep_rew_mean        | 400       |\n",
      "| time/                 |           |\n",
      "|    fps                | 479       |\n",
      "|    iterations         | 155300    |\n",
      "|    time_elapsed       | 25907     |\n",
      "|    total_timesteps    | 12424000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.08     |\n",
      "|    explained_variance | 0.994     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 155299    |\n",
      "|    policy_loss        | -5.48e-05 |\n",
      "|    value_loss         | 0.0275    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=12430000, episode_reward=391.00 +/- 34.04\n",
      "Episode length: 9176.20 +/- 2070.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.18e+03 |\n",
      "|    mean_reward        | 391      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12430000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.947   |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 155374   |\n",
      "|    policy_loss        | 0.132    |\n",
      "|    value_loss         | 0.235    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11e+04 |\n",
      "|    ep_rew_mean        | 399      |\n",
      "| time/                 |          |\n",
      "|    fps                | 479      |\n",
      "|    iterations         | 155400   |\n",
      "|    time_elapsed       | 25942    |\n",
      "|    total_timesteps    | 12432000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.784   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 155399   |\n",
      "|    policy_loss        | -0.00313 |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12440000, episode_reward=393.00 +/- 39.28\n",
      "Episode length: 32395.40 +/- 38181.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.24e+04 |\n",
      "|    mean_reward        | 393      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12440000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.991   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 155499   |\n",
      "|    policy_loss        | 0.0354   |\n",
      "|    value_loss         | 0.0921   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.1e+04  |\n",
      "|    ep_rew_mean     | 398      |\n",
      "| time/              |          |\n",
      "|    fps             | 477      |\n",
      "|    iterations      | 155500   |\n",
      "|    time_elapsed    | 26036    |\n",
      "|    total_timesteps | 12440000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | 401      |\n",
      "| time/                 |          |\n",
      "|    fps                | 477      |\n",
      "|    iterations         | 155600   |\n",
      "|    time_elapsed       | 26044    |\n",
      "|    total_timesteps    | 12448000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.972   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 155599   |\n",
      "|    policy_loss        | -0.00943 |\n",
      "|    value_loss         | 0.0571   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12450000, episode_reward=442.60 +/- 99.03\n",
      "Episode length: 11583.80 +/- 3026.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+04 |\n",
      "|    mean_reward        | 443      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12450000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.928   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 155624   |\n",
      "|    policy_loss        | -0.0146  |\n",
      "|    value_loss         | 0.0262   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.13e+04 |\n",
      "|    ep_rew_mean        | 403      |\n",
      "| time/                 |          |\n",
      "|    fps                | 477      |\n",
      "|    iterations         | 155700   |\n",
      "|    time_elapsed       | 26082    |\n",
      "|    total_timesteps    | 12456000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.932   |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 155699   |\n",
      "|    policy_loss        | 0.119    |\n",
      "|    value_loss         | 0.257    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12460000, episode_reward=390.00 +/- 33.26\n",
      "Episode length: 9285.20 +/- 712.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.29e+03 |\n",
      "|    mean_reward        | 390      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12460000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.952   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 155749   |\n",
      "|    policy_loss        | 0.0202   |\n",
      "|    value_loss         | 0.0403   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15e+04 |\n",
      "|    ep_rew_mean        | 405      |\n",
      "| time/                 |          |\n",
      "|    fps                | 477      |\n",
      "|    iterations         | 155800   |\n",
      "|    time_elapsed       | 26114    |\n",
      "|    total_timesteps    | 12464000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.936   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 155799   |\n",
      "|    policy_loss        | -0.0148  |\n",
      "|    value_loss         | 0.0514   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12470000, episode_reward=420.00 +/- 15.66\n",
      "Episode length: 12566.80 +/- 2500.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.26e+04 |\n",
      "|    mean_reward        | 420      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12470000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.753   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 155874   |\n",
      "|    policy_loss        | 0.0267   |\n",
      "|    value_loss         | 0.0484   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15e+04 |\n",
      "|    ep_rew_mean        | 405      |\n",
      "| time/                 |          |\n",
      "|    fps                | 476      |\n",
      "|    iterations         | 155900   |\n",
      "|    time_elapsed       | 26156    |\n",
      "|    total_timesteps    | 12472000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.84    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 155899   |\n",
      "|    policy_loss        | 0.147    |\n",
      "|    value_loss         | 0.379    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12480000, episode_reward=355.80 +/- 90.34\n",
      "Episode length: 12263.00 +/- 7418.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.23e+04 |\n",
      "|    mean_reward        | 356      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12480000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.935   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 155999   |\n",
      "|    policy_loss        | -0.016   |\n",
      "|    value_loss         | 0.048    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.14e+04 |\n",
      "|    ep_rew_mean     | 400      |\n",
      "| time/              |          |\n",
      "|    fps             | 476      |\n",
      "|    iterations      | 156000   |\n",
      "|    time_elapsed    | 26196    |\n",
      "|    total_timesteps | 12480000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14e+04 |\n",
      "|    ep_rew_mean        | 400      |\n",
      "| time/                 |          |\n",
      "|    fps                | 476      |\n",
      "|    iterations         | 156100   |\n",
      "|    time_elapsed       | 26203    |\n",
      "|    total_timesteps    | 12488000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 156099   |\n",
      "|    policy_loss        | 0.0279   |\n",
      "|    value_loss         | 0.0233   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12490000, episode_reward=402.20 +/- 16.18\n",
      "Episode length: 12851.00 +/- 7159.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.29e+04 |\n",
      "|    mean_reward        | 402      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12490000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.87    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 156124   |\n",
      "|    policy_loss        | -0.0119  |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15e+04 |\n",
      "|    ep_rew_mean        | 405      |\n",
      "| time/                 |          |\n",
      "|    fps                | 476      |\n",
      "|    iterations         | 156200   |\n",
      "|    time_elapsed       | 26244    |\n",
      "|    total_timesteps    | 12496000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.942   |\n",
      "|    explained_variance | 0.895    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 156199   |\n",
      "|    policy_loss        | 0.0271   |\n",
      "|    value_loss         | 0.227    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12500000, episode_reward=424.20 +/- 20.87\n",
      "Episode length: 12579.60 +/- 1644.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.26e+04 |\n",
      "|    mean_reward        | 424      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12500000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.92    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 156249   |\n",
      "|    policy_loss        | 0.00129  |\n",
      "|    value_loss         | 0.0367   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.16e+04 |\n",
      "|    ep_rew_mean        | 405      |\n",
      "| time/                 |          |\n",
      "|    fps                | 475      |\n",
      "|    iterations         | 156300   |\n",
      "|    time_elapsed       | 26284    |\n",
      "|    total_timesteps    | 12504000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.952   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 156299   |\n",
      "|    policy_loss        | 0.0382   |\n",
      "|    value_loss         | 0.0346   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12510000, episode_reward=441.60 +/- 38.86\n",
      "Episode length: 11427.80 +/- 1495.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+04 |\n",
      "|    mean_reward        | 442      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12510000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.927   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 156374   |\n",
      "|    policy_loss        | -0.0221  |\n",
      "|    value_loss         | 0.0845   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.16e+04 |\n",
      "|    ep_rew_mean        | 405      |\n",
      "| time/                 |          |\n",
      "|    fps                | 475      |\n",
      "|    iterations         | 156400   |\n",
      "|    time_elapsed       | 26321    |\n",
      "|    total_timesteps    | 12512000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 156399   |\n",
      "|    policy_loss        | -0.00711 |\n",
      "|    value_loss         | 0.0457   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12520000, episode_reward=414.20 +/- 19.55\n",
      "Episode length: 12001.80 +/- 1464.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.2e+04  |\n",
      "|    mean_reward        | 414      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12520000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.984   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 156499   |\n",
      "|    policy_loss        | 0.00502  |\n",
      "|    value_loss         | 0.0534   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.16e+04 |\n",
      "|    ep_rew_mean     | 405      |\n",
      "| time/              |          |\n",
      "|    fps             | 474      |\n",
      "|    iterations      | 156500   |\n",
      "|    time_elapsed    | 26359    |\n",
      "|    total_timesteps | 12520000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.16e+04 |\n",
      "|    ep_rew_mean        | 404      |\n",
      "| time/                 |          |\n",
      "|    fps                | 475      |\n",
      "|    iterations         | 156600   |\n",
      "|    time_elapsed       | 26367    |\n",
      "|    total_timesteps    | 12528000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 156599   |\n",
      "|    policy_loss        | 0.0548   |\n",
      "|    value_loss         | 0.0441   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12530000, episode_reward=408.60 +/- 15.30\n",
      "Episode length: 12825.20 +/- 3584.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.28e+04 |\n",
      "|    mean_reward        | 409      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12530000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.84    |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 156624   |\n",
      "|    policy_loss        | 0.00792  |\n",
      "|    value_loss         | 0.00753  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15e+04 |\n",
      "|    ep_rew_mean        | 404      |\n",
      "| time/                 |          |\n",
      "|    fps                | 474      |\n",
      "|    iterations         | 156700   |\n",
      "|    time_elapsed       | 26408    |\n",
      "|    total_timesteps    | 12536000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.874   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 156699   |\n",
      "|    policy_loss        | 0.034    |\n",
      "|    value_loss         | 0.0751   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12540000, episode_reward=413.20 +/- 7.39\n",
      "Episode length: 11426.60 +/- 1092.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+04 |\n",
      "|    mean_reward        | 413      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12540000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 156749   |\n",
      "|    policy_loss        | 0.0736   |\n",
      "|    value_loss         | 0.0857   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15e+04 |\n",
      "|    ep_rew_mean        | 404      |\n",
      "| time/                 |          |\n",
      "|    fps                | 474      |\n",
      "|    iterations         | 156800   |\n",
      "|    time_elapsed       | 26447    |\n",
      "|    total_timesteps    | 12544000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 156799   |\n",
      "|    policy_loss        | 0.0357   |\n",
      "|    value_loss         | 0.109    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12550000, episode_reward=332.00 +/- 149.32\n",
      "Episode length: 9496.60 +/- 2964.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.5e+03  |\n",
      "|    mean_reward        | 332      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12550000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 156874   |\n",
      "|    policy_loss        | -0.0421  |\n",
      "|    value_loss         | 0.0455   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15e+04 |\n",
      "|    ep_rew_mean        | 405      |\n",
      "| time/                 |          |\n",
      "|    fps                | 474      |\n",
      "|    iterations         | 156900   |\n",
      "|    time_elapsed       | 26479    |\n",
      "|    total_timesteps    | 12552000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.883   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 156899   |\n",
      "|    policy_loss        | -0.0118  |\n",
      "|    value_loss         | 0.0209   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12560000, episode_reward=299.60 +/- 115.63\n",
      "Episode length: 7996.00 +/- 1459.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8e+03    |\n",
      "|    mean_reward        | 300      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12560000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 156999   |\n",
      "|    policy_loss        | 0.0268   |\n",
      "|    value_loss         | 0.0395   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.14e+04 |\n",
      "|    ep_rew_mean     | 405      |\n",
      "| time/              |          |\n",
      "|    fps             | 473      |\n",
      "|    iterations      | 157000   |\n",
      "|    time_elapsed    | 26507    |\n",
      "|    total_timesteps | 12560000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14e+04 |\n",
      "|    ep_rew_mean        | 405      |\n",
      "| time/                 |          |\n",
      "|    fps                | 474      |\n",
      "|    iterations         | 157100   |\n",
      "|    time_elapsed       | 26514    |\n",
      "|    total_timesteps    | 12568000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.954   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 157099   |\n",
      "|    policy_loss        | 0.0137   |\n",
      "|    value_loss         | 0.0504   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12570000, episode_reward=393.40 +/- 16.01\n",
      "Episode length: 10367.20 +/- 2733.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+04 |\n",
      "|    mean_reward        | 393      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12570000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.916   |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 157124   |\n",
      "|    policy_loss        | -0.048   |\n",
      "|    value_loss         | 0.0169   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14e+04 |\n",
      "|    ep_rew_mean        | 405      |\n",
      "| time/                 |          |\n",
      "|    fps                | 473      |\n",
      "|    iterations         | 157200   |\n",
      "|    time_elapsed       | 26550    |\n",
      "|    total_timesteps    | 12576000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.935   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 157199   |\n",
      "|    policy_loss        | -0.0155  |\n",
      "|    value_loss         | 0.017    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12580000, episode_reward=362.00 +/- 78.37\n",
      "Episode length: 8643.20 +/- 1506.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.64e+03 |\n",
      "|    mean_reward        | 362      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12580000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 157249   |\n",
      "|    policy_loss        | -0.0247  |\n",
      "|    value_loss         | 0.228    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14e+04 |\n",
      "|    ep_rew_mean        | 405      |\n",
      "| time/                 |          |\n",
      "|    fps                | 473      |\n",
      "|    iterations         | 157300   |\n",
      "|    time_elapsed       | 26581    |\n",
      "|    total_timesteps    | 12584000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 157299   |\n",
      "|    policy_loss        | -0.0844  |\n",
      "|    value_loss         | 0.0627   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12590000, episode_reward=416.00 +/- 26.28\n",
      "Episode length: 13276.80 +/- 4821.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.33e+04 |\n",
      "|    mean_reward        | 416      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12590000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.891   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 157374   |\n",
      "|    policy_loss        | 0.0826   |\n",
      "|    value_loss         | 0.148    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14e+04 |\n",
      "|    ep_rew_mean        | 404      |\n",
      "| time/                 |          |\n",
      "|    fps                | 472      |\n",
      "|    iterations         | 157400   |\n",
      "|    time_elapsed       | 26623    |\n",
      "|    total_timesteps    | 12592000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.911   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 157399   |\n",
      "|    policy_loss        | 0.0268   |\n",
      "|    value_loss         | 0.018    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12600000, episode_reward=489.40 +/- 125.00\n",
      "Episode length: 12230.60 +/- 2886.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.22e+04 |\n",
      "|    mean_reward        | 489      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12600000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.959   |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 157499   |\n",
      "|    policy_loss        | -0.0834  |\n",
      "|    value_loss         | 0.343    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.14e+04 |\n",
      "|    ep_rew_mean     | 403      |\n",
      "| time/              |          |\n",
      "|    fps             | 472      |\n",
      "|    iterations      | 157500   |\n",
      "|    time_elapsed    | 26662    |\n",
      "|    total_timesteps | 12600000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | 402      |\n",
      "| time/                 |          |\n",
      "|    fps                | 472      |\n",
      "|    iterations         | 157600   |\n",
      "|    time_elapsed       | 26669    |\n",
      "|    total_timesteps    | 12608000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 157599   |\n",
      "|    policy_loss        | -0.0187  |\n",
      "|    value_loss         | 0.0276   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12610000, episode_reward=420.00 +/- 7.48\n",
      "Episode length: 11459.40 +/- 2353.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.15e+04 |\n",
      "|    mean_reward        | 420      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12610000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.986   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 157624   |\n",
      "|    policy_loss        | 0.0633   |\n",
      "|    value_loss         | 0.0776   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | 402      |\n",
      "| time/                 |          |\n",
      "|    fps                | 472      |\n",
      "|    iterations         | 157700   |\n",
      "|    time_elapsed       | 26707    |\n",
      "|    total_timesteps    | 12616000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.939   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 157699   |\n",
      "|    policy_loss        | 0.0776   |\n",
      "|    value_loss         | 0.0591   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12620000, episode_reward=370.80 +/- 77.24\n",
      "Episode length: 9205.40 +/- 1244.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.21e+03 |\n",
      "|    mean_reward        | 371      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12620000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.993   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 157749   |\n",
      "|    policy_loss        | -0.0183  |\n",
      "|    value_loss         | 0.0282   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | 401      |\n",
      "| time/                 |          |\n",
      "|    fps                | 472      |\n",
      "|    iterations         | 157800   |\n",
      "|    time_elapsed       | 26739    |\n",
      "|    total_timesteps    | 12624000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 157799   |\n",
      "|    policy_loss        | -0.0435  |\n",
      "|    value_loss         | 0.0622   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12630000, episode_reward=378.40 +/- 57.40\n",
      "Episode length: 9049.00 +/- 1279.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.05e+03 |\n",
      "|    mean_reward        | 378      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12630000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.918   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 157874   |\n",
      "|    policy_loss        | 0.0224   |\n",
      "|    value_loss         | 0.047    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | 402      |\n",
      "| time/                 |          |\n",
      "|    fps                | 471      |\n",
      "|    iterations         | 157900   |\n",
      "|    time_elapsed       | 26776    |\n",
      "|    total_timesteps    | 12632000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 157899   |\n",
      "|    policy_loss        | -0.0535  |\n",
      "|    value_loss         | 0.0488   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12640000, episode_reward=382.40 +/- 37.30\n",
      "Episode length: 11549.20 +/- 5995.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.15e+04 |\n",
      "|    mean_reward        | 382      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12640000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 157999   |\n",
      "|    policy_loss        | -0.0141  |\n",
      "|    value_loss         | 0.0398   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.12e+04 |\n",
      "|    ep_rew_mean     | 402      |\n",
      "| time/              |          |\n",
      "|    fps             | 471      |\n",
      "|    iterations      | 158000   |\n",
      "|    time_elapsed    | 26820    |\n",
      "|    total_timesteps | 12640000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | 401      |\n",
      "| time/                 |          |\n",
      "|    fps                | 471      |\n",
      "|    iterations         | 158100   |\n",
      "|    time_elapsed       | 26828    |\n",
      "|    total_timesteps    | 12648000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 158099   |\n",
      "|    policy_loss        | 0.00805  |\n",
      "|    value_loss         | 0.146    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12650000, episode_reward=395.40 +/- 26.26\n",
      "Episode length: 10006.00 +/- 1891.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1e+04    |\n",
      "|    mean_reward        | 395      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12650000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 158124   |\n",
      "|    policy_loss        | -0.00283 |\n",
      "|    value_loss         | 0.039    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | 400      |\n",
      "| time/                 |          |\n",
      "|    fps                | 471      |\n",
      "|    iterations         | 158200   |\n",
      "|    time_elapsed       | 26866    |\n",
      "|    total_timesteps    | 12656000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 158199   |\n",
      "|    policy_loss        | 0.0924   |\n",
      "|    value_loss         | 0.061    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12660000, episode_reward=400.40 +/- 59.81\n",
      "Episode length: 18653.80 +/- 13274.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.87e+04 |\n",
      "|    mean_reward        | 400      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12660000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.943   |\n",
      "|    explained_variance | 0.491    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 158249   |\n",
      "|    policy_loss        | -0.52    |\n",
      "|    value_loss         | 2.58     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | 402      |\n",
      "| time/                 |          |\n",
      "|    fps                | 470      |\n",
      "|    iterations         | 158300   |\n",
      "|    time_elapsed       | 26932    |\n",
      "|    total_timesteps    | 12664000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 158299   |\n",
      "|    policy_loss        | -0.0196  |\n",
      "|    value_loss         | 0.0494   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12670000, episode_reward=451.00 +/- 207.52\n",
      "Episode length: 12284.20 +/- 4021.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.23e+04 |\n",
      "|    mean_reward        | 451      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12670000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 158374   |\n",
      "|    policy_loss        | -0.0431  |\n",
      "|    value_loss         | 0.15     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | 402      |\n",
      "| time/                 |          |\n",
      "|    fps                | 469      |\n",
      "|    iterations         | 158400   |\n",
      "|    time_elapsed       | 26978    |\n",
      "|    total_timesteps    | 12672000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 158399   |\n",
      "|    policy_loss        | -0.0806  |\n",
      "|    value_loss         | 0.106    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12680000, episode_reward=404.20 +/- 38.91\n",
      "Episode length: 11572.80 +/- 3595.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+04 |\n",
      "|    mean_reward        | 404      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12680000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.867   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 158499   |\n",
      "|    policy_loss        | 0.081    |\n",
      "|    value_loss         | 0.0581   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.12e+04 |\n",
      "|    ep_rew_mean     | 401      |\n",
      "| time/              |          |\n",
      "|    fps             | 469      |\n",
      "|    iterations      | 158500   |\n",
      "|    time_elapsed    | 27021    |\n",
      "|    total_timesteps | 12680000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | 401      |\n",
      "| time/                 |          |\n",
      "|    fps                | 469      |\n",
      "|    iterations         | 158600   |\n",
      "|    time_elapsed       | 27029    |\n",
      "|    total_timesteps    | 12688000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.928   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 158599   |\n",
      "|    policy_loss        | -0.0942  |\n",
      "|    value_loss         | 0.231    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12690000, episode_reward=339.00 +/- 130.97\n",
      "Episode length: 10370.60 +/- 2584.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+04 |\n",
      "|    mean_reward        | 339      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12690000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.953   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 158624   |\n",
      "|    policy_loss        | 0.0213   |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11e+04 |\n",
      "|    ep_rew_mean        | 403      |\n",
      "| time/                 |          |\n",
      "|    fps                | 469      |\n",
      "|    iterations         | 158700   |\n",
      "|    time_elapsed       | 27070    |\n",
      "|    total_timesteps    | 12696000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.948   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 158699   |\n",
      "|    policy_loss        | 0.00639  |\n",
      "|    value_loss         | 0.0488   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12700000, episode_reward=400.00 +/- 45.47\n",
      "Episode length: 10395.80 +/- 1777.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+04 |\n",
      "|    mean_reward        | 400      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12700000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 158749   |\n",
      "|    policy_loss        | -0.0694  |\n",
      "|    value_loss         | 0.0927   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 402      |\n",
      "| time/                 |          |\n",
      "|    fps                | 468      |\n",
      "|    iterations         | 158800   |\n",
      "|    time_elapsed       | 27111    |\n",
      "|    total_timesteps    | 12704000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 158799   |\n",
      "|    policy_loss        | 0.123    |\n",
      "|    value_loss         | 0.119    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12710000, episode_reward=419.80 +/- 22.45\n",
      "Episode length: 11401.80 +/- 2215.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+04 |\n",
      "|    mean_reward        | 420      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12710000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.99    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 158874   |\n",
      "|    policy_loss        | -0.0445  |\n",
      "|    value_loss         | 0.0327   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+04 |\n",
      "|    ep_rew_mean        | 401      |\n",
      "| time/                 |          |\n",
      "|    fps                | 468      |\n",
      "|    iterations         | 158900   |\n",
      "|    time_elapsed       | 27154    |\n",
      "|    total_timesteps    | 12712000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.946   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 158899   |\n",
      "|    policy_loss        | -0.0162  |\n",
      "|    value_loss         | 0.0261   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12720000, episode_reward=407.60 +/- 14.91\n",
      "Episode length: 12546.00 +/- 3376.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.25e+04 |\n",
      "|    mean_reward        | 408      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12720000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.959   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 158999   |\n",
      "|    policy_loss        | 0.0305   |\n",
      "|    value_loss         | 0.0263   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.07e+04 |\n",
      "|    ep_rew_mean     | 395      |\n",
      "| time/              |          |\n",
      "|    fps             | 467      |\n",
      "|    iterations      | 159000   |\n",
      "|    time_elapsed    | 27199    |\n",
      "|    total_timesteps | 12720000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 395      |\n",
      "| time/                 |          |\n",
      "|    fps                | 467      |\n",
      "|    iterations         | 159100   |\n",
      "|    time_elapsed       | 27207    |\n",
      "|    total_timesteps    | 12728000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.906    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 159099   |\n",
      "|    policy_loss        | -0.161   |\n",
      "|    value_loss         | 0.345    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12730000, episode_reward=464.80 +/- 120.79\n",
      "Episode length: 14329.80 +/- 5394.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.43e+04 |\n",
      "|    mean_reward        | 465      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12730000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 159124   |\n",
      "|    policy_loss        | -0.0713  |\n",
      "|    value_loss         | 0.0284   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06e+04 |\n",
      "|    ep_rew_mean        | 394      |\n",
      "| time/                 |          |\n",
      "|    fps                | 467      |\n",
      "|    iterations         | 159200   |\n",
      "|    time_elapsed       | 27260    |\n",
      "|    total_timesteps    | 12736000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.891   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 159199   |\n",
      "|    policy_loss        | 0.0378   |\n",
      "|    value_loss         | 0.0545   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12740000, episode_reward=489.20 +/- 184.01\n",
      "Episode length: 14826.80 +/- 5767.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.48e+04 |\n",
      "|    mean_reward        | 489      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12740000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 159249   |\n",
      "|    policy_loss        | 0.0877   |\n",
      "|    value_loss         | 0.082    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 394      |\n",
      "| time/                 |          |\n",
      "|    fps                | 466      |\n",
      "|    iterations         | 159300   |\n",
      "|    time_elapsed       | 27312    |\n",
      "|    total_timesteps    | 12744000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.96    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 159299   |\n",
      "|    policy_loss        | -0.139   |\n",
      "|    value_loss         | 0.0805   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12750000, episode_reward=388.40 +/- 34.90\n",
      "Episode length: 10835.20 +/- 3144.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+04 |\n",
      "|    mean_reward        | 388      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12750000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 159374   |\n",
      "|    policy_loss        | 0.0179   |\n",
      "|    value_loss         | 0.304    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 394      |\n",
      "| time/                 |          |\n",
      "|    fps                | 466      |\n",
      "|    iterations         | 159400   |\n",
      "|    time_elapsed       | 27353    |\n",
      "|    total_timesteps    | 12752000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 159399   |\n",
      "|    policy_loss        | 0.14     |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12760000, episode_reward=394.60 +/- 30.32\n",
      "Episode length: 11791.20 +/- 2484.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.18e+04 |\n",
      "|    mean_reward        | 395      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12760000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 159499   |\n",
      "|    policy_loss        | 0.0505   |\n",
      "|    value_loss         | 0.0567   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.07e+04 |\n",
      "|    ep_rew_mean     | 394      |\n",
      "| time/              |          |\n",
      "|    fps             | 465      |\n",
      "|    iterations      | 159500   |\n",
      "|    time_elapsed    | 27398    |\n",
      "|    total_timesteps | 12760000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 394      |\n",
      "| time/                 |          |\n",
      "|    fps                | 465      |\n",
      "|    iterations         | 159600   |\n",
      "|    time_elapsed       | 27405    |\n",
      "|    total_timesteps    | 12768000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.988   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 159599   |\n",
      "|    policy_loss        | -0.00175 |\n",
      "|    value_loss         | 0.0329   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12770000, episode_reward=422.60 +/- 9.39\n",
      "Episode length: 10492.00 +/- 578.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+04 |\n",
      "|    mean_reward        | 423      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12770000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 159624   |\n",
      "|    policy_loss        | -0.03    |\n",
      "|    value_loss         | 0.0319   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 394      |\n",
      "| time/                 |          |\n",
      "|    fps                | 465      |\n",
      "|    iterations         | 159700   |\n",
      "|    time_elapsed       | 27447    |\n",
      "|    total_timesteps    | 12776000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.715   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 159699   |\n",
      "|    policy_loss        | 0.00786  |\n",
      "|    value_loss         | 0.0285   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12780000, episode_reward=496.40 +/- 146.65\n",
      "Episode length: 15096.00 +/- 7326.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.51e+04 |\n",
      "|    mean_reward        | 496      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12780000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.908   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 159749   |\n",
      "|    policy_loss        | 0.0344   |\n",
      "|    value_loss         | 0.0163   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06e+04 |\n",
      "|    ep_rew_mean        | 394      |\n",
      "| time/                 |          |\n",
      "|    fps                | 464      |\n",
      "|    iterations         | 159800   |\n",
      "|    time_elapsed       | 27502    |\n",
      "|    total_timesteps    | 12784000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.948   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 159799   |\n",
      "|    policy_loss        | -0.00188 |\n",
      "|    value_loss         | 0.0204   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12790000, episode_reward=349.40 +/- 105.33\n",
      "Episode length: 16342.20 +/- 14589.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.63e+04 |\n",
      "|    mean_reward        | 349      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12790000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 159874   |\n",
      "|    policy_loss        | -0.0982  |\n",
      "|    value_loss         | 0.148    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06e+04 |\n",
      "|    ep_rew_mean        | 394      |\n",
      "| time/                 |          |\n",
      "|    fps                | 464      |\n",
      "|    iterations         | 159900   |\n",
      "|    time_elapsed       | 27560    |\n",
      "|    total_timesteps    | 12792000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.944   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 159899   |\n",
      "|    policy_loss        | -0.00882 |\n",
      "|    value_loss         | 0.0348   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12800000, episode_reward=379.40 +/- 48.11\n",
      "Episode length: 10184.80 +/- 1725.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+04 |\n",
      "|    mean_reward        | 379      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12800000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 159999   |\n",
      "|    policy_loss        | -0.0659  |\n",
      "|    value_loss         | 0.0742   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.07e+04 |\n",
      "|    ep_rew_mean     | 397      |\n",
      "| time/              |          |\n",
      "|    fps             | 463      |\n",
      "|    iterations      | 160000   |\n",
      "|    time_elapsed    | 27601    |\n",
      "|    total_timesteps | 12800000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 394      |\n",
      "| time/                 |          |\n",
      "|    fps                | 463      |\n",
      "|    iterations         | 160100   |\n",
      "|    time_elapsed       | 27609    |\n",
      "|    total_timesteps    | 12808000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.961   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 160099   |\n",
      "|    policy_loss        | -0.0214  |\n",
      "|    value_loss         | 0.0157   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12810000, episode_reward=351.60 +/- 136.88\n",
      "Episode length: 10311.20 +/- 2607.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+04 |\n",
      "|    mean_reward        | 352      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12810000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 160124   |\n",
      "|    policy_loss        | -0.0431  |\n",
      "|    value_loss         | 0.0446   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 391      |\n",
      "| time/                 |          |\n",
      "|    fps                | 463      |\n",
      "|    iterations         | 160200   |\n",
      "|    time_elapsed       | 27648    |\n",
      "|    total_timesteps    | 12816000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.933   |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 160199   |\n",
      "|    policy_loss        | -0.0573  |\n",
      "|    value_loss         | 0.109    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12820000, episode_reward=369.20 +/- 30.26\n",
      "Episode length: 8430.80 +/- 1297.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.43e+03 |\n",
      "|    mean_reward        | 369      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12820000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 160249   |\n",
      "|    policy_loss        | 0.0385   |\n",
      "|    value_loss         | 0.0312   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 390      |\n",
      "| time/                 |          |\n",
      "|    fps                | 463      |\n",
      "|    iterations         | 160300   |\n",
      "|    time_elapsed       | 27681    |\n",
      "|    total_timesteps    | 12824000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.943   |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 160299   |\n",
      "|    policy_loss        | -0.187   |\n",
      "|    value_loss         | 0.254    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12830000, episode_reward=418.80 +/- 6.97\n",
      "Episode length: 14447.40 +/- 4812.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.44e+04 |\n",
      "|    mean_reward        | 419      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12830000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.98    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 160374   |\n",
      "|    policy_loss        | 0.0634   |\n",
      "|    value_loss         | 0.0694   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+04 |\n",
      "|    ep_rew_mean        | 390      |\n",
      "| time/                 |          |\n",
      "|    fps                | 462      |\n",
      "|    iterations         | 160400   |\n",
      "|    time_elapsed       | 27735    |\n",
      "|    total_timesteps    | 12832000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.89    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 160399   |\n",
      "|    policy_loss        | 0.00302  |\n",
      "|    value_loss         | 0.0442   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12840000, episode_reward=417.20 +/- 23.75\n",
      "Episode length: 12674.20 +/- 2303.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.27e+04 |\n",
      "|    mean_reward        | 417      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12840000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 160499   |\n",
      "|    policy_loss        | -0.224   |\n",
      "|    value_loss         | 0.364    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.09e+04 |\n",
      "|    ep_rew_mean     | 391      |\n",
      "| time/              |          |\n",
      "|    fps             | 462      |\n",
      "|    iterations      | 160500   |\n",
      "|    time_elapsed    | 27785    |\n",
      "|    total_timesteps | 12840000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 395      |\n",
      "| time/                 |          |\n",
      "|    fps                | 462      |\n",
      "|    iterations         | 160600   |\n",
      "|    time_elapsed       | 27793    |\n",
      "|    total_timesteps    | 12848000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 160599   |\n",
      "|    policy_loss        | 0.0477   |\n",
      "|    value_loss         | 0.0649   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12850000, episode_reward=422.20 +/- 12.19\n",
      "Episode length: 11067.40 +/- 1299.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.11e+04 |\n",
      "|    mean_reward        | 422      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12850000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.865   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 160624   |\n",
      "|    policy_loss        | -0.0116  |\n",
      "|    value_loss         | 0.234    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 396      |\n",
      "| time/                 |          |\n",
      "|    fps                | 461      |\n",
      "|    iterations         | 160700   |\n",
      "|    time_elapsed       | 27835    |\n",
      "|    total_timesteps    | 12856000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 160699   |\n",
      "|    policy_loss        | 0.0496   |\n",
      "|    value_loss         | 0.0278   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12860000, episode_reward=469.40 +/- 142.73\n",
      "Episode length: 32611.80 +/- 37893.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.26e+04 |\n",
      "|    mean_reward        | 469      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12860000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 160749   |\n",
      "|    policy_loss        | 0.0132   |\n",
      "|    value_loss         | 0.0201   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 394      |\n",
      "| time/                 |          |\n",
      "|    fps                | 460      |\n",
      "|    iterations         | 160800   |\n",
      "|    time_elapsed       | 27938    |\n",
      "|    total_timesteps    | 12864000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.977   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 160799   |\n",
      "|    policy_loss        | -0.183   |\n",
      "|    value_loss         | 0.204    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12870000, episode_reward=410.60 +/- 13.17\n",
      "Episode length: 10537.00 +/- 2105.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+04 |\n",
      "|    mean_reward        | 411      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12870000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.909   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 160874   |\n",
      "|    policy_loss        | 0.00479  |\n",
      "|    value_loss         | 0.0365   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11e+04 |\n",
      "|    ep_rew_mean        | 398      |\n",
      "| time/                 |          |\n",
      "|    fps                | 460      |\n",
      "|    iterations         | 160900   |\n",
      "|    time_elapsed       | 27976    |\n",
      "|    total_timesteps    | 12872000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 160899   |\n",
      "|    policy_loss        | -0.111   |\n",
      "|    value_loss         | 0.0615   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12880000, episode_reward=332.20 +/- 147.28\n",
      "Episode length: 9304.40 +/- 4631.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.3e+03  |\n",
      "|    mean_reward        | 332      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12880000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.932   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 160999   |\n",
      "|    policy_loss        | -0.0117  |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.11e+04 |\n",
      "|    ep_rew_mean     | 399      |\n",
      "| time/              |          |\n",
      "|    fps             | 459      |\n",
      "|    iterations      | 161000   |\n",
      "|    time_elapsed    | 28011    |\n",
      "|    total_timesteps | 12880000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11e+04 |\n",
      "|    ep_rew_mean        | 398      |\n",
      "| time/                 |          |\n",
      "|    fps                | 459      |\n",
      "|    iterations         | 161100   |\n",
      "|    time_elapsed       | 28019    |\n",
      "|    total_timesteps    | 12888000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.908   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 161099   |\n",
      "|    policy_loss        | 0.0285   |\n",
      "|    value_loss         | 0.0247   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12890000, episode_reward=415.00 +/- 12.31\n",
      "Episode length: 12984.80 +/- 6642.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.3e+04  |\n",
      "|    mean_reward        | 415      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12890000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 161124   |\n",
      "|    policy_loss        | 0.0269   |\n",
      "|    value_loss         | 0.0895   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11e+04 |\n",
      "|    ep_rew_mean        | 398      |\n",
      "| time/                 |          |\n",
      "|    fps                | 459      |\n",
      "|    iterations         | 161200   |\n",
      "|    time_elapsed       | 28067    |\n",
      "|    total_timesteps    | 12896000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.855   |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 161199   |\n",
      "|    policy_loss        | -0.0341  |\n",
      "|    value_loss         | 0.117    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12900000, episode_reward=408.20 +/- 24.42\n",
      "Episode length: 11351.40 +/- 2659.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+04 |\n",
      "|    mean_reward        | 408      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12900000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.997   |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 161249   |\n",
      "|    policy_loss        | 0.0615   |\n",
      "|    value_loss         | 0.11     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11e+04 |\n",
      "|    ep_rew_mean        | 398      |\n",
      "| time/                 |          |\n",
      "|    fps                | 459      |\n",
      "|    iterations         | 161300   |\n",
      "|    time_elapsed       | 28111    |\n",
      "|    total_timesteps    | 12904000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 161299   |\n",
      "|    policy_loss        | 0.000777 |\n",
      "|    value_loss         | 0.0561   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12910000, episode_reward=474.80 +/- 112.21\n",
      "Episode length: 12022.00 +/- 2097.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.2e+04  |\n",
      "|    mean_reward        | 475      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12910000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 161374   |\n",
      "|    policy_loss        | -0.0218  |\n",
      "|    value_loss         | 0.00731  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11e+04 |\n",
      "|    ep_rew_mean        | 396      |\n",
      "| time/                 |          |\n",
      "|    fps                | 458      |\n",
      "|    iterations         | 161400   |\n",
      "|    time_elapsed       | 28155    |\n",
      "|    total_timesteps    | 12912000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 161399   |\n",
      "|    policy_loss        | -0.0651  |\n",
      "|    value_loss         | 0.0576   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12920000, episode_reward=415.40 +/- 23.23\n",
      "Episode length: 12140.20 +/- 3318.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.21e+04 |\n",
      "|    mean_reward        | 415      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12920000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 161499   |\n",
      "|    policy_loss        | 0.0464   |\n",
      "|    value_loss         | 0.0282   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.11e+04 |\n",
      "|    ep_rew_mean     | 397      |\n",
      "| time/              |          |\n",
      "|    fps             | 458      |\n",
      "|    iterations      | 161500   |\n",
      "|    time_elapsed    | 28200    |\n",
      "|    total_timesteps | 12920000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | 396      |\n",
      "| time/                 |          |\n",
      "|    fps                | 458      |\n",
      "|    iterations         | 161600   |\n",
      "|    time_elapsed       | 28209    |\n",
      "|    total_timesteps    | 12928000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.948   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 161599   |\n",
      "|    policy_loss        | -0.0257  |\n",
      "|    value_loss         | 0.058    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12930000, episode_reward=401.80 +/- 26.13\n",
      "Episode length: 14208.40 +/- 7779.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.42e+04 |\n",
      "|    mean_reward        | 402      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12930000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 161624   |\n",
      "|    policy_loss        | 0.0444   |\n",
      "|    value_loss         | 0.031    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11e+04 |\n",
      "|    ep_rew_mean        | 387      |\n",
      "| time/                 |          |\n",
      "|    fps                | 457      |\n",
      "|    iterations         | 161700   |\n",
      "|    time_elapsed       | 28261    |\n",
      "|    total_timesteps    | 12936000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.991   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 161699   |\n",
      "|    policy_loss        | 0.11     |\n",
      "|    value_loss         | 0.131    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12940000, episode_reward=354.20 +/- 126.21\n",
      "Episode length: 10153.20 +/- 4456.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+04 |\n",
      "|    mean_reward        | 354      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12940000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 161749   |\n",
      "|    policy_loss        | 0.015    |\n",
      "|    value_loss         | 0.369    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 384      |\n",
      "| time/                 |          |\n",
      "|    fps                | 457      |\n",
      "|    iterations         | 161800   |\n",
      "|    time_elapsed       | 28303    |\n",
      "|    total_timesteps    | 12944000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 161799   |\n",
      "|    policy_loss        | -0.00599 |\n",
      "|    value_loss         | 0.0274   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12950000, episode_reward=423.20 +/- 19.72\n",
      "Episode length: 11722.20 +/- 969.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.17e+04 |\n",
      "|    mean_reward        | 423      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12950000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 161874   |\n",
      "|    policy_loss        | -0.0329  |\n",
      "|    value_loss         | 0.0312   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11e+04 |\n",
      "|    ep_rew_mean        | 386      |\n",
      "| time/                 |          |\n",
      "|    fps                | 456      |\n",
      "|    iterations         | 161900   |\n",
      "|    time_elapsed       | 28350    |\n",
      "|    total_timesteps    | 12952000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 161899   |\n",
      "|    policy_loss        | 0.0402   |\n",
      "|    value_loss         | 0.12     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12960000, episode_reward=412.20 +/- 37.61\n",
      "Episode length: 13875.00 +/- 5150.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.39e+04 |\n",
      "|    mean_reward        | 412      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12960000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 161999   |\n",
      "|    policy_loss        | 0.0343   |\n",
      "|    value_loss         | 0.0377   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.1e+04  |\n",
      "|    ep_rew_mean     | 385      |\n",
      "| time/              |          |\n",
      "|    fps             | 456      |\n",
      "|    iterations      | 162000   |\n",
      "|    time_elapsed    | 28399    |\n",
      "|    total_timesteps | 12960000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 385      |\n",
      "| time/                 |          |\n",
      "|    fps                | 456      |\n",
      "|    iterations         | 162100   |\n",
      "|    time_elapsed       | 28407    |\n",
      "|    total_timesteps    | 12968000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 162099   |\n",
      "|    policy_loss        | 0.0122   |\n",
      "|    value_loss         | 0.332    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12970000, episode_reward=329.20 +/- 136.00\n",
      "Episode length: 10816.40 +/- 4745.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+04 |\n",
      "|    mean_reward        | 329      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12970000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.834    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 162124   |\n",
      "|    policy_loss        | 0.00261  |\n",
      "|    value_loss         | 0.255    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11e+04 |\n",
      "|    ep_rew_mean        | 386      |\n",
      "| time/                 |          |\n",
      "|    fps                | 456      |\n",
      "|    iterations         | 162200   |\n",
      "|    time_elapsed       | 28446    |\n",
      "|    total_timesteps    | 12976000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.945   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 162199   |\n",
      "|    policy_loss        | -0.00904 |\n",
      "|    value_loss         | 0.0219   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12980000, episode_reward=441.40 +/- 223.85\n",
      "Episode length: 10550.60 +/- 4301.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+04 |\n",
      "|    mean_reward        | 441      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12980000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 162249   |\n",
      "|    policy_loss        | -0.00739 |\n",
      "|    value_loss         | 0.0343   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11e+04 |\n",
      "|    ep_rew_mean        | 383      |\n",
      "| time/                 |          |\n",
      "|    fps                | 455      |\n",
      "|    iterations         | 162300   |\n",
      "|    time_elapsed       | 28484    |\n",
      "|    total_timesteps    | 12984000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 162299   |\n",
      "|    policy_loss        | -0.153   |\n",
      "|    value_loss         | 0.0704   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12990000, episode_reward=424.20 +/- 13.48\n",
      "Episode length: 13157.00 +/- 3485.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.32e+04 |\n",
      "|    mean_reward        | 424      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12990000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 162374   |\n",
      "|    policy_loss        | -0.08    |\n",
      "|    value_loss         | 0.345    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11e+04 |\n",
      "|    ep_rew_mean        | 386      |\n",
      "| time/                 |          |\n",
      "|    fps                | 455      |\n",
      "|    iterations         | 162400   |\n",
      "|    time_elapsed       | 28533    |\n",
      "|    total_timesteps    | 12992000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.906   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 162399   |\n",
      "|    policy_loss        | -0.00703 |\n",
      "|    value_loss         | 0.0468   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13000000, episode_reward=337.20 +/- 140.09\n",
      "Episode length: 9676.60 +/- 2963.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.68e+03 |\n",
      "|    mean_reward        | 337      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13000000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 162499   |\n",
      "|    policy_loss        | 0.0616   |\n",
      "|    value_loss         | 0.0368   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.11e+04 |\n",
      "|    ep_rew_mean     | 386      |\n",
      "| time/              |          |\n",
      "|    fps             | 454      |\n",
      "|    iterations      | 162500   |\n",
      "|    time_elapsed    | 28573    |\n",
      "|    total_timesteps | 13000000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 385      |\n",
      "| time/                 |          |\n",
      "|    fps                | 455      |\n",
      "|    iterations         | 162600   |\n",
      "|    time_elapsed       | 28581    |\n",
      "|    total_timesteps    | 13008000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 162599   |\n",
      "|    policy_loss        | 0.126    |\n",
      "|    value_loss         | 0.17     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13010000, episode_reward=408.20 +/- 27.62\n",
      "Episode length: 10887.20 +/- 3545.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.09e+04 |\n",
      "|    mean_reward        | 408      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13010000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 162624   |\n",
      "|    policy_loss        | 0.146    |\n",
      "|    value_loss         | 0.185    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 385      |\n",
      "| time/                 |          |\n",
      "|    fps                | 454      |\n",
      "|    iterations         | 162700   |\n",
      "|    time_elapsed       | 28623    |\n",
      "|    total_timesteps    | 13016000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 162699   |\n",
      "|    policy_loss        | -0.0549  |\n",
      "|    value_loss         | 0.0344   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13020000, episode_reward=350.80 +/- 128.98\n",
      "Episode length: 11755.40 +/- 5230.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.18e+04 |\n",
      "|    mean_reward        | 351      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13020000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 162749   |\n",
      "|    policy_loss        | -0.0146  |\n",
      "|    value_loss         | 0.0352   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+04 |\n",
      "|    ep_rew_mean        | 384      |\n",
      "| time/                 |          |\n",
      "|    fps                | 454      |\n",
      "|    iterations         | 162800   |\n",
      "|    time_elapsed       | 28667    |\n",
      "|    total_timesteps    | 13024000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.823   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 162799   |\n",
      "|    policy_loss        | 0.00325  |\n",
      "|    value_loss         | 0.0213   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13030000, episode_reward=407.00 +/- 33.44\n",
      "Episode length: 12249.80 +/- 3683.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.22e+04 |\n",
      "|    mean_reward        | 407      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13030000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 162874   |\n",
      "|    policy_loss        | -0.0207  |\n",
      "|    value_loss         | 0.0356   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 385      |\n",
      "| time/                 |          |\n",
      "|    fps                | 453      |\n",
      "|    iterations         | 162900   |\n",
      "|    time_elapsed       | 28712    |\n",
      "|    total_timesteps    | 13032000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 162899   |\n",
      "|    policy_loss        | 0.000662 |\n",
      "|    value_loss         | 0.046    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13040000, episode_reward=374.00 +/- 58.65\n",
      "Episode length: 9894.00 +/- 2448.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.89e+03 |\n",
      "|    mean_reward        | 374      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13040000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 162999   |\n",
      "|    policy_loss        | 0.0179   |\n",
      "|    value_loss         | 0.0234   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.09e+04 |\n",
      "|    ep_rew_mean     | 383      |\n",
      "| time/              |          |\n",
      "|    fps             | 453      |\n",
      "|    iterations      | 163000   |\n",
      "|    time_elapsed    | 28750    |\n",
      "|    total_timesteps | 13040000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+04 |\n",
      "|    ep_rew_mean        | 383      |\n",
      "| time/                 |          |\n",
      "|    fps                | 453      |\n",
      "|    iterations         | 163100   |\n",
      "|    time_elapsed       | 28758    |\n",
      "|    total_timesteps    | 13048000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.879   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 163099   |\n",
      "|    policy_loss        | -0.0239  |\n",
      "|    value_loss         | 0.0831   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13050000, episode_reward=428.60 +/- 12.99\n",
      "Episode length: 11060.40 +/- 1900.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.11e+04 |\n",
      "|    mean_reward        | 429      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13050000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.996   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 163124   |\n",
      "|    policy_loss        | -0.0159  |\n",
      "|    value_loss         | 0.0714   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 382      |\n",
      "| time/                 |          |\n",
      "|    fps                | 453      |\n",
      "|    iterations         | 163200   |\n",
      "|    time_elapsed       | 28798    |\n",
      "|    total_timesteps    | 13056000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 163199   |\n",
      "|    policy_loss        | -0.00294 |\n",
      "|    value_loss         | 0.0213   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13060000, episode_reward=410.20 +/- 6.88\n",
      "Episode length: 16670.40 +/- 13180.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.67e+04 |\n",
      "|    mean_reward        | 410      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13060000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.915   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 163249   |\n",
      "|    policy_loss        | -0.0805  |\n",
      "|    value_loss         | 0.031    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 382      |\n",
      "| time/                 |          |\n",
      "|    fps                | 452      |\n",
      "|    iterations         | 163300   |\n",
      "|    time_elapsed       | 28855    |\n",
      "|    total_timesteps    | 13064000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.947   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 163299   |\n",
      "|    policy_loss        | 0.0134   |\n",
      "|    value_loss         | 0.0286   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13070000, episode_reward=331.00 +/- 84.39\n",
      "Episode length: 8368.00 +/- 1133.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.37e+03 |\n",
      "|    mean_reward        | 331      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13070000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.995   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 163374   |\n",
      "|    policy_loss        | -0.0578  |\n",
      "|    value_loss         | 0.0255   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 386      |\n",
      "| time/                 |          |\n",
      "|    fps                | 452      |\n",
      "|    iterations         | 163400   |\n",
      "|    time_elapsed       | 28888    |\n",
      "|    total_timesteps    | 13072000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.951   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 163399   |\n",
      "|    policy_loss        | 0.0141   |\n",
      "|    value_loss         | 0.0535   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13080000, episode_reward=405.40 +/- 16.22\n",
      "Episode length: 10682.00 +/- 3080.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.07e+04 |\n",
      "|    mean_reward        | 405      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13080000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 163499   |\n",
      "|    policy_loss        | -0.00607 |\n",
      "|    value_loss         | 0.04     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.08e+04 |\n",
      "|    ep_rew_mean     | 390      |\n",
      "| time/              |          |\n",
      "|    fps             | 452      |\n",
      "|    iterations      | 163500   |\n",
      "|    time_elapsed    | 28931    |\n",
      "|    total_timesteps | 13080000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 387      |\n",
      "| time/                 |          |\n",
      "|    fps                | 452      |\n",
      "|    iterations         | 163600   |\n",
      "|    time_elapsed       | 28939    |\n",
      "|    total_timesteps    | 13088000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.959   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 163599   |\n",
      "|    policy_loss        | -0.0348  |\n",
      "|    value_loss         | 0.0204   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13090000, episode_reward=402.60 +/- 54.72\n",
      "Episode length: 12417.20 +/- 1280.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.24e+04 |\n",
      "|    mean_reward        | 403      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13090000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 163624   |\n",
      "|    policy_loss        | -0.0688  |\n",
      "|    value_loss         | 0.0341   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 387      |\n",
      "| time/                 |          |\n",
      "|    fps                | 451      |\n",
      "|    iterations         | 163700   |\n",
      "|    time_elapsed       | 28988    |\n",
      "|    total_timesteps    | 13096000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.996   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 163699   |\n",
      "|    policy_loss        | 0.019    |\n",
      "|    value_loss         | 0.0251   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13100000, episode_reward=404.80 +/- 18.21\n",
      "Episode length: 13293.00 +/- 3071.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.33e+04 |\n",
      "|    mean_reward        | 405      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13100000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.952   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 163749   |\n",
      "|    policy_loss        | -0.209   |\n",
      "|    value_loss         | 0.536    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 385      |\n",
      "| time/                 |          |\n",
      "|    fps                | 451      |\n",
      "|    iterations         | 163800   |\n",
      "|    time_elapsed       | 29037    |\n",
      "|    total_timesteps    | 13104000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.962   |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 163799   |\n",
      "|    policy_loss        | -0.0507  |\n",
      "|    value_loss         | 0.394    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13110000, episode_reward=400.60 +/- 15.87\n",
      "Episode length: 29112.60 +/- 39471.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.91e+04 |\n",
      "|    mean_reward        | 401      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13110000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 163874   |\n",
      "|    policy_loss        | -0.0373  |\n",
      "|    value_loss         | 0.0934   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06e+04 |\n",
      "|    ep_rew_mean        | 381      |\n",
      "| time/                 |          |\n",
      "|    fps                | 450      |\n",
      "|    iterations         | 163900   |\n",
      "|    time_elapsed       | 29134    |\n",
      "|    total_timesteps    | 13112000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.981   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 163899   |\n",
      "|    policy_loss        | -0.0481  |\n",
      "|    value_loss         | 0.193    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13120000, episode_reward=562.20 +/- 188.09\n",
      "Episode length: 15899.80 +/- 5151.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.59e+04 |\n",
      "|    mean_reward        | 562      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13120000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 163999   |\n",
      "|    policy_loss        | 0.0379   |\n",
      "|    value_loss         | 0.0476   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.07e+04 |\n",
      "|    ep_rew_mean     | 381      |\n",
      "| time/              |          |\n",
      "|    fps             | 449      |\n",
      "|    iterations      | 164000   |\n",
      "|    time_elapsed    | 29190    |\n",
      "|    total_timesteps | 13120000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06e+04 |\n",
      "|    ep_rew_mean        | 381      |\n",
      "| time/                 |          |\n",
      "|    fps                | 449      |\n",
      "|    iterations         | 164100   |\n",
      "|    time_elapsed       | 29198    |\n",
      "|    total_timesteps    | 13128000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 164099   |\n",
      "|    policy_loss        | -0.0318  |\n",
      "|    value_loss         | 0.0594   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13130000, episode_reward=357.00 +/- 94.82\n",
      "Episode length: 10149.00 +/- 3175.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+04 |\n",
      "|    mean_reward        | 357      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13130000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 164124   |\n",
      "|    policy_loss        | 0.0488   |\n",
      "|    value_loss         | 0.0859   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06e+04 |\n",
      "|    ep_rew_mean        | 380      |\n",
      "| time/                 |          |\n",
      "|    fps                | 449      |\n",
      "|    iterations         | 164200   |\n",
      "|    time_elapsed       | 29236    |\n",
      "|    total_timesteps    | 13136000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.954   |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 164199   |\n",
      "|    policy_loss        | -0.0925  |\n",
      "|    value_loss         | 0.415    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13140000, episode_reward=398.00 +/- 22.79\n",
      "Episode length: 15126.60 +/- 5317.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.51e+04 |\n",
      "|    mean_reward        | 398      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13140000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 164249   |\n",
      "|    policy_loss        | -0.0296  |\n",
      "|    value_loss         | 0.0396   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06e+04 |\n",
      "|    ep_rew_mean        | 380      |\n",
      "| time/                 |          |\n",
      "|    fps                | 448      |\n",
      "|    iterations         | 164300   |\n",
      "|    time_elapsed       | 29287    |\n",
      "|    total_timesteps    | 13144000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 164299   |\n",
      "|    policy_loss        | 0.0156   |\n",
      "|    value_loss         | 0.536    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13150000, episode_reward=506.60 +/- 161.64\n",
      "Episode length: 20428.80 +/- 7470.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.04e+04 |\n",
      "|    mean_reward        | 507      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13150000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.919   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 164374   |\n",
      "|    policy_loss        | 0.00694  |\n",
      "|    value_loss         | 0.0488   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06e+04 |\n",
      "|    ep_rew_mean        | 380      |\n",
      "| time/                 |          |\n",
      "|    fps                | 448      |\n",
      "|    iterations         | 164400   |\n",
      "|    time_elapsed       | 29355    |\n",
      "|    total_timesteps    | 13152000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 164399   |\n",
      "|    policy_loss        | -0.0224  |\n",
      "|    value_loss         | 0.13     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13160000, episode_reward=403.80 +/- 20.51\n",
      "Episode length: 15423.60 +/- 13239.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.54e+04 |\n",
      "|    mean_reward        | 404      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13160000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.995   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 164499   |\n",
      "|    policy_loss        | -0.0435  |\n",
      "|    value_loss         | 0.037    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.07e+04 |\n",
      "|    ep_rew_mean     | 381      |\n",
      "| time/              |          |\n",
      "|    fps             | 447      |\n",
      "|    iterations      | 164500   |\n",
      "|    time_elapsed    | 29408    |\n",
      "|    total_timesteps | 13160000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06e+04 |\n",
      "|    ep_rew_mean        | 381      |\n",
      "| time/                 |          |\n",
      "|    fps                | 447      |\n",
      "|    iterations         | 164600   |\n",
      "|    time_elapsed       | 29416    |\n",
      "|    total_timesteps    | 13168000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 164599   |\n",
      "|    policy_loss        | -0.0266  |\n",
      "|    value_loss         | 0.0088   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13170000, episode_reward=404.20 +/- 21.05\n",
      "Episode length: 16990.20 +/- 5802.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.7e+04  |\n",
      "|    mean_reward        | 404      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13170000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.971   |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 164624   |\n",
      "|    policy_loss        | -0.0438  |\n",
      "|    value_loss         | 0.174    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 382      |\n",
      "| time/                 |          |\n",
      "|    fps                | 447      |\n",
      "|    iterations         | 164700   |\n",
      "|    time_elapsed       | 29474    |\n",
      "|    total_timesteps    | 13176000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.977   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 164699   |\n",
      "|    policy_loss        | -0.0756  |\n",
      "|    value_loss         | 0.234    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13180000, episode_reward=429.60 +/- 169.01\n",
      "Episode length: 9736.40 +/- 2718.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.74e+03 |\n",
      "|    mean_reward        | 430      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13180000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.988   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 164749   |\n",
      "|    policy_loss        | -0.00846 |\n",
      "|    value_loss         | 0.0196   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 383      |\n",
      "| time/                 |          |\n",
      "|    fps                | 446      |\n",
      "|    iterations         | 164800   |\n",
      "|    time_elapsed       | 29511    |\n",
      "|    total_timesteps    | 13184000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 164799   |\n",
      "|    policy_loss        | 0.0892   |\n",
      "|    value_loss         | 0.0437   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13190000, episode_reward=386.40 +/- 79.92\n",
      "Episode length: 9689.00 +/- 1558.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.69e+03 |\n",
      "|    mean_reward        | 386      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13190000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 164874   |\n",
      "|    policy_loss        | -0.0123  |\n",
      "|    value_loss         | 0.138    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 386      |\n",
      "| time/                 |          |\n",
      "|    fps                | 446      |\n",
      "|    iterations         | 164900   |\n",
      "|    time_elapsed       | 29549    |\n",
      "|    total_timesteps    | 13192000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.868   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 164899   |\n",
      "|    policy_loss        | -0.0623  |\n",
      "|    value_loss         | 0.0526   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13200000, episode_reward=352.00 +/- 116.93\n",
      "Episode length: 12731.20 +/- 8424.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.27e+04 |\n",
      "|    mean_reward        | 352      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13200000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 164999   |\n",
      "|    policy_loss        | -0.0211  |\n",
      "|    value_loss         | 0.0219   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.07e+04 |\n",
      "|    ep_rew_mean     | 388      |\n",
      "| time/              |          |\n",
      "|    fps             | 446      |\n",
      "|    iterations      | 165000   |\n",
      "|    time_elapsed    | 29594    |\n",
      "|    total_timesteps | 13200000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 388      |\n",
      "| time/                 |          |\n",
      "|    fps                | 446      |\n",
      "|    iterations         | 165100   |\n",
      "|    time_elapsed       | 29602    |\n",
      "|    total_timesteps    | 13208000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 165099   |\n",
      "|    policy_loss        | 0.00946  |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13210000, episode_reward=495.40 +/- 173.95\n",
      "Episode length: 13446.20 +/- 3537.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.34e+04 |\n",
      "|    mean_reward        | 495      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13210000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 165124   |\n",
      "|    policy_loss        | -0.0573  |\n",
      "|    value_loss         | 0.0457   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 392      |\n",
      "| time/                 |          |\n",
      "|    fps                | 445      |\n",
      "|    iterations         | 165200   |\n",
      "|    time_elapsed       | 29649    |\n",
      "|    total_timesteps    | 13216000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 165199   |\n",
      "|    policy_loss        | -0.00635 |\n",
      "|    value_loss         | 0.0278   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13220000, episode_reward=428.60 +/- 6.74\n",
      "Episode length: 14711.60 +/- 6865.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.47e+04 |\n",
      "|    mean_reward        | 429      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13220000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.884   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 165249   |\n",
      "|    policy_loss        | 0.151    |\n",
      "|    value_loss         | 0.142    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 393      |\n",
      "| time/                 |          |\n",
      "|    fps                | 445      |\n",
      "|    iterations         | 165300   |\n",
      "|    time_elapsed       | 29699    |\n",
      "|    total_timesteps    | 13224000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 165299   |\n",
      "|    policy_loss        | 0.0609   |\n",
      "|    value_loss         | 0.0812   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13230000, episode_reward=381.00 +/- 41.17\n",
      "Episode length: 10876.40 +/- 3256.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.09e+04 |\n",
      "|    mean_reward        | 381      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13230000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 165374   |\n",
      "|    policy_loss        | 0.0282   |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08e+04  |\n",
      "|    ep_rew_mean        | 394       |\n",
      "| time/                 |           |\n",
      "|    fps                | 444       |\n",
      "|    iterations         | 165400    |\n",
      "|    time_elapsed       | 29740     |\n",
      "|    total_timesteps    | 13232000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.97     |\n",
      "|    explained_variance | 0.993     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 165399    |\n",
      "|    policy_loss        | -0.000586 |\n",
      "|    value_loss         | 0.0303    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=13240000, episode_reward=335.40 +/- 127.10\n",
      "Episode length: 9272.20 +/- 2640.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.27e+03 |\n",
      "|    mean_reward        | 335      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13240000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.995   |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 165499   |\n",
      "|    policy_loss        | 0.0434   |\n",
      "|    value_loss         | 0.344    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.09e+04 |\n",
      "|    ep_rew_mean     | 396      |\n",
      "| time/              |          |\n",
      "|    fps             | 444      |\n",
      "|    iterations      | 165500   |\n",
      "|    time_elapsed    | 29777    |\n",
      "|    total_timesteps | 13240000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+04 |\n",
      "|    ep_rew_mean        | 392      |\n",
      "| time/                 |          |\n",
      "|    fps                | 444      |\n",
      "|    iterations         | 165600   |\n",
      "|    time_elapsed       | 29785    |\n",
      "|    total_timesteps    | 13248000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 165599   |\n",
      "|    policy_loss        | -0.0416  |\n",
      "|    value_loss         | 0.0268   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13250000, episode_reward=352.40 +/- 84.38\n",
      "Episode length: 9925.80 +/- 1790.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.93e+03 |\n",
      "|    mean_reward        | 352      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13250000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.947   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 165624   |\n",
      "|    policy_loss        | -0.0455  |\n",
      "|    value_loss         | 0.0441   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 396      |\n",
      "| time/                 |          |\n",
      "|    fps                | 444      |\n",
      "|    iterations         | 165700   |\n",
      "|    time_elapsed       | 29821    |\n",
      "|    total_timesteps    | 13256000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 165699   |\n",
      "|    policy_loss        | 0.0156   |\n",
      "|    value_loss         | 0.0219   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13260000, episode_reward=443.40 +/- 52.48\n",
      "Episode length: 14754.80 +/- 5183.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.48e+04 |\n",
      "|    mean_reward        | 443      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13260000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 165749   |\n",
      "|    policy_loss        | 0.0249   |\n",
      "|    value_loss         | 0.0435   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11e+04 |\n",
      "|    ep_rew_mean        | 396      |\n",
      "| time/                 |          |\n",
      "|    fps                | 444      |\n",
      "|    iterations         | 165800   |\n",
      "|    time_elapsed       | 29871    |\n",
      "|    total_timesteps    | 13264000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.996   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 165799   |\n",
      "|    policy_loss        | -0.0585  |\n",
      "|    value_loss         | 0.0258   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13270000, episode_reward=415.00 +/- 13.08\n",
      "Episode length: 11150.40 +/- 1487.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 415      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13270000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 165874   |\n",
      "|    policy_loss        | 0.0296   |\n",
      "|    value_loss         | 0.0894   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 393      |\n",
      "| time/                 |          |\n",
      "|    fps                | 443      |\n",
      "|    iterations         | 165900   |\n",
      "|    time_elapsed       | 29914    |\n",
      "|    total_timesteps    | 13272000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.898   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 165899   |\n",
      "|    policy_loss        | 0.086    |\n",
      "|    value_loss         | 0.0877   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13280000, episode_reward=417.80 +/- 6.68\n",
      "Episode length: 10387.40 +/- 1266.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+04 |\n",
      "|    mean_reward        | 418      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13280000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 165999   |\n",
      "|    policy_loss        | 0.0257   |\n",
      "|    value_loss         | 0.0623   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.1e+04  |\n",
      "|    ep_rew_mean     | 392      |\n",
      "| time/              |          |\n",
      "|    fps             | 443      |\n",
      "|    iterations      | 166000   |\n",
      "|    time_elapsed    | 29954    |\n",
      "|    total_timesteps | 13280000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 393      |\n",
      "| time/                 |          |\n",
      "|    fps                | 443      |\n",
      "|    iterations         | 166100   |\n",
      "|    time_elapsed       | 29962    |\n",
      "|    total_timesteps    | 13288000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 166099   |\n",
      "|    policy_loss        | 0.0638   |\n",
      "|    value_loss         | 0.0956   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13290000, episode_reward=401.20 +/- 17.15\n",
      "Episode length: 10115.80 +/- 2218.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+04 |\n",
      "|    mean_reward        | 401      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13290000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 166124   |\n",
      "|    policy_loss        | 0.0164   |\n",
      "|    value_loss         | 0.0902   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+04 |\n",
      "|    ep_rew_mean        | 390      |\n",
      "| time/                 |          |\n",
      "|    fps                | 443      |\n",
      "|    iterations         | 166200   |\n",
      "|    time_elapsed       | 29998    |\n",
      "|    total_timesteps    | 13296000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.972   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 166199   |\n",
      "|    policy_loss        | -0.126   |\n",
      "|    value_loss         | 0.0595   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13300000, episode_reward=421.20 +/- 79.11\n",
      "Episode length: 12610.60 +/- 3757.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.26e+04 |\n",
      "|    mean_reward        | 421      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13300000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 166249   |\n",
      "|    policy_loss        | -0.0282  |\n",
      "|    value_loss         | 0.0192   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+04 |\n",
      "|    ep_rew_mean        | 391      |\n",
      "| time/                 |          |\n",
      "|    fps                | 442      |\n",
      "|    iterations         | 166300   |\n",
      "|    time_elapsed       | 30043    |\n",
      "|    total_timesteps    | 13304000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.927   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 166299   |\n",
      "|    policy_loss        | 0.0729   |\n",
      "|    value_loss         | 0.107    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13310000, episode_reward=476.60 +/- 115.89\n",
      "Episode length: 13907.80 +/- 4354.69\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 1.39e+04  |\n",
      "|    mean_reward        | 477       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 13310000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.999    |\n",
      "|    explained_variance | 0.991     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 166374    |\n",
      "|    policy_loss        | -0.000334 |\n",
      "|    value_loss         | 0.0232    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 387      |\n",
      "| time/                 |          |\n",
      "|    fps                | 442      |\n",
      "|    iterations         | 166400   |\n",
      "|    time_elapsed       | 30090    |\n",
      "|    total_timesteps    | 13312000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.985   |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 166399   |\n",
      "|    policy_loss        | -0.0418  |\n",
      "|    value_loss         | 0.119    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13320000, episode_reward=406.80 +/- 23.03\n",
      "Episode length: 11199.00 +/- 4169.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 407      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13320000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 166499   |\n",
      "|    policy_loss        | 0.101    |\n",
      "|    value_loss         | 0.274    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.08e+04 |\n",
      "|    ep_rew_mean     | 388      |\n",
      "| time/              |          |\n",
      "|    fps             | 442      |\n",
      "|    iterations      | 166500   |\n",
      "|    time_elapsed    | 30129    |\n",
      "|    total_timesteps | 13320000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 388      |\n",
      "| time/                 |          |\n",
      "|    fps                | 442      |\n",
      "|    iterations         | 166600   |\n",
      "|    time_elapsed       | 30136    |\n",
      "|    total_timesteps    | 13328000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 166599   |\n",
      "|    policy_loss        | -0.0256  |\n",
      "|    value_loss         | 0.0409   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13330000, episode_reward=401.60 +/- 26.06\n",
      "Episode length: 9559.80 +/- 1491.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.56e+03 |\n",
      "|    mean_reward        | 402      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13330000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.962   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 166624   |\n",
      "|    policy_loss        | 0.121    |\n",
      "|    value_loss         | 0.26     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 388      |\n",
      "| time/                 |          |\n",
      "|    fps                | 442      |\n",
      "|    iterations         | 166700   |\n",
      "|    time_elapsed       | 30169    |\n",
      "|    total_timesteps    | 13336000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 166699   |\n",
      "|    policy_loss        | 0.0195   |\n",
      "|    value_loss         | 0.0619   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13340000, episode_reward=371.60 +/- 67.17\n",
      "Episode length: 8985.80 +/- 1878.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.99e+03 |\n",
      "|    mean_reward        | 372      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13340000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 166749   |\n",
      "|    policy_loss        | -0.017   |\n",
      "|    value_loss         | 0.089    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 387      |\n",
      "| time/                 |          |\n",
      "|    fps                | 441      |\n",
      "|    iterations         | 166800   |\n",
      "|    time_elapsed       | 30200    |\n",
      "|    total_timesteps    | 13344000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.928   |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 166799   |\n",
      "|    policy_loss        | 0.0751   |\n",
      "|    value_loss         | 0.0457   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13350000, episode_reward=386.00 +/- 50.58\n",
      "Episode length: 9194.00 +/- 1413.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.19e+03 |\n",
      "|    mean_reward        | 386      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13350000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 166874   |\n",
      "|    policy_loss        | 0.0517   |\n",
      "|    value_loss         | 0.0392   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+04 |\n",
      "|    ep_rew_mean        | 391      |\n",
      "| time/                 |          |\n",
      "|    fps                | 441      |\n",
      "|    iterations         | 166900   |\n",
      "|    time_elapsed       | 30234    |\n",
      "|    total_timesteps    | 13352000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 166899   |\n",
      "|    policy_loss        | 0.00223  |\n",
      "|    value_loss         | 0.0322   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13360000, episode_reward=360.80 +/- 168.02\n",
      "Episode length: 16710.60 +/- 12930.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.67e+04 |\n",
      "|    mean_reward        | 361      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13360000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 166999   |\n",
      "|    policy_loss        | -0.0574  |\n",
      "|    value_loss         | 0.0466   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.08e+04 |\n",
      "|    ep_rew_mean     | 391      |\n",
      "| time/              |          |\n",
      "|    fps             | 441      |\n",
      "|    iterations      | 167000   |\n",
      "|    time_elapsed    | 30286    |\n",
      "|    total_timesteps | 13360000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 389      |\n",
      "| time/                 |          |\n",
      "|    fps                | 441      |\n",
      "|    iterations         | 167100   |\n",
      "|    time_elapsed       | 30293    |\n",
      "|    total_timesteps    | 13368000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.917   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 167099   |\n",
      "|    policy_loss        | 0.0376   |\n",
      "|    value_loss         | 0.11     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13370000, episode_reward=411.40 +/- 15.32\n",
      "Episode length: 15166.80 +/- 8901.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.52e+04 |\n",
      "|    mean_reward        | 411      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13370000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.917   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 167124   |\n",
      "|    policy_loss        | -0.0464  |\n",
      "|    value_loss         | 0.0333   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 390      |\n",
      "| time/                 |          |\n",
      "|    fps                | 440      |\n",
      "|    iterations         | 167200   |\n",
      "|    time_elapsed       | 30341    |\n",
      "|    total_timesteps    | 13376000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 167199   |\n",
      "|    policy_loss        | -0.0924  |\n",
      "|    value_loss         | 0.123    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13380000, episode_reward=373.20 +/- 43.70\n",
      "Episode length: 10060.40 +/- 3418.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+04 |\n",
      "|    mean_reward        | 373      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13380000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.906   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 167249   |\n",
      "|    policy_loss        | -0.046   |\n",
      "|    value_loss         | 0.0292   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 391      |\n",
      "| time/                 |          |\n",
      "|    fps                | 440      |\n",
      "|    iterations         | 167300   |\n",
      "|    time_elapsed       | 30374    |\n",
      "|    total_timesteps    | 13384000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.968   |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 167299   |\n",
      "|    policy_loss        | -0.146   |\n",
      "|    value_loss         | 0.251    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13390000, episode_reward=386.20 +/- 20.46\n",
      "Episode length: 17726.40 +/- 18394.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.77e+04 |\n",
      "|    mean_reward        | 386      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13390000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.978   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 167374   |\n",
      "|    policy_loss        | -0.0464  |\n",
      "|    value_loss         | 0.0316   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 393      |\n",
      "| time/                 |          |\n",
      "|    fps                | 440      |\n",
      "|    iterations         | 167400   |\n",
      "|    time_elapsed       | 30429    |\n",
      "|    total_timesteps    | 13392000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.903   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 167399   |\n",
      "|    policy_loss        | -0.108   |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13400000, episode_reward=433.80 +/- 16.95\n",
      "Episode length: 13229.00 +/- 2525.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.32e+04 |\n",
      "|    mean_reward        | 434      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13400000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.956   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 167499   |\n",
      "|    policy_loss        | -0.00704 |\n",
      "|    value_loss         | 0.0198   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.07e+04 |\n",
      "|    ep_rew_mean     | 392      |\n",
      "| time/              |          |\n",
      "|    fps             | 439      |\n",
      "|    iterations      | 167500   |\n",
      "|    time_elapsed    | 30473    |\n",
      "|    total_timesteps | 13400000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 392      |\n",
      "| time/                 |          |\n",
      "|    fps                | 439      |\n",
      "|    iterations         | 167600   |\n",
      "|    time_elapsed       | 30481    |\n",
      "|    total_timesteps    | 13408000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 167599   |\n",
      "|    policy_loss        | -0.0481  |\n",
      "|    value_loss         | 0.0998   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13410000, episode_reward=418.40 +/- 8.62\n",
      "Episode length: 11544.00 +/- 1878.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.15e+04 |\n",
      "|    mean_reward        | 418      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13410000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.968   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 167624   |\n",
      "|    policy_loss        | -0.0281  |\n",
      "|    value_loss         | 0.0264   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+04 |\n",
      "|    ep_rew_mean        | 393      |\n",
      "| time/                 |          |\n",
      "|    fps                | 439      |\n",
      "|    iterations         | 167700   |\n",
      "|    time_elapsed       | 30521    |\n",
      "|    total_timesteps    | 13416000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 167699   |\n",
      "|    policy_loss        | -0.0254  |\n",
      "|    value_loss         | 0.0361   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13420000, episode_reward=291.60 +/- 145.76\n",
      "Episode length: 9100.60 +/- 4792.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.1e+03  |\n",
      "|    mean_reward        | 292      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13420000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 167749   |\n",
      "|    policy_loss        | 0.0394   |\n",
      "|    value_loss         | 0.136    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 393      |\n",
      "| time/                 |          |\n",
      "|    fps                | 439      |\n",
      "|    iterations         | 167800   |\n",
      "|    time_elapsed       | 30552    |\n",
      "|    total_timesteps    | 13424000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.968   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 167799   |\n",
      "|    policy_loss        | 0.023    |\n",
      "|    value_loss         | 0.0501   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13430000, episode_reward=405.80 +/- 15.28\n",
      "Episode length: 8872.80 +/- 1361.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.87e+03 |\n",
      "|    mean_reward        | 406      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13430000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 167874   |\n",
      "|    policy_loss        | 0.0254   |\n",
      "|    value_loss         | 0.0691   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+04 |\n",
      "|    ep_rew_mean        | 393      |\n",
      "| time/                 |          |\n",
      "|    fps                | 439      |\n",
      "|    iterations         | 167900   |\n",
      "|    time_elapsed       | 30586    |\n",
      "|    total_timesteps    | 13432000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.917   |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 167899   |\n",
      "|    policy_loss        | -0.149   |\n",
      "|    value_loss         | 0.517    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13440000, episode_reward=386.00 +/- 55.78\n",
      "Episode length: 9708.20 +/- 1715.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.71e+03 |\n",
      "|    mean_reward        | 386      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13440000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.681    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 167999   |\n",
      "|    policy_loss        | -0.341   |\n",
      "|    value_loss         | 1.99     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.09e+04 |\n",
      "|    ep_rew_mean     | 393      |\n",
      "| time/              |          |\n",
      "|    fps             | 438      |\n",
      "|    iterations      | 168000   |\n",
      "|    time_elapsed    | 30624    |\n",
      "|    total_timesteps | 13440000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 396      |\n",
      "| time/                 |          |\n",
      "|    fps                | 439      |\n",
      "|    iterations         | 168100   |\n",
      "|    time_elapsed       | 30632    |\n",
      "|    total_timesteps    | 13448000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 168099   |\n",
      "|    policy_loss        | -0.00503 |\n",
      "|    value_loss         | 0.0371   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13450000, episode_reward=416.40 +/- 13.26\n",
      "Episode length: 11425.60 +/- 1552.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+04 |\n",
      "|    mean_reward        | 416      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13450000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 168124   |\n",
      "|    policy_loss        | -0.055   |\n",
      "|    value_loss         | 0.0614   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 396      |\n",
      "| time/                 |          |\n",
      "|    fps                | 438      |\n",
      "|    iterations         | 168200   |\n",
      "|    time_elapsed       | 30673    |\n",
      "|    total_timesteps    | 13456000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.953   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 168199   |\n",
      "|    policy_loss        | 0.042    |\n",
      "|    value_loss         | 0.0713   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13460000, episode_reward=424.40 +/- 11.55\n",
      "Episode length: 12683.60 +/- 1594.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.27e+04 |\n",
      "|    mean_reward        | 424      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13460000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.973   |\n",
      "|    explained_variance | 0.642    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 168249   |\n",
      "|    policy_loss        | -0.0886  |\n",
      "|    value_loss         | 0.692    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 391      |\n",
      "| time/                 |          |\n",
      "|    fps                | 438      |\n",
      "|    iterations         | 168300   |\n",
      "|    time_elapsed       | 30722    |\n",
      "|    total_timesteps    | 13464000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.976   |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 168299   |\n",
      "|    policy_loss        | 0.0318   |\n",
      "|    value_loss         | 0.0544   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13470000, episode_reward=418.80 +/- 20.30\n",
      "Episode length: 16953.40 +/- 10418.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.7e+04  |\n",
      "|    mean_reward        | 419      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13470000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.835   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 168374   |\n",
      "|    policy_loss        | -0.00263 |\n",
      "|    value_loss         | 0.0275   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 392      |\n",
      "| time/                 |          |\n",
      "|    fps                | 437      |\n",
      "|    iterations         | 168400   |\n",
      "|    time_elapsed       | 30785    |\n",
      "|    total_timesteps    | 13472000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.906   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 168399   |\n",
      "|    policy_loss        | -0.0551  |\n",
      "|    value_loss         | 0.0323   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13480000, episode_reward=337.20 +/- 153.57\n",
      "Episode length: 11188.60 +/- 4877.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 337      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13480000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.971   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 168499   |\n",
      "|    policy_loss        | -0.00558 |\n",
      "|    value_loss         | 0.0651   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.1e+04  |\n",
      "|    ep_rew_mean     | 392      |\n",
      "| time/              |          |\n",
      "|    fps             | 437      |\n",
      "|    iterations      | 168500   |\n",
      "|    time_elapsed    | 30830    |\n",
      "|    total_timesteps | 13480000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11e+04 |\n",
      "|    ep_rew_mean        | 396      |\n",
      "| time/                 |          |\n",
      "|    fps                | 437      |\n",
      "|    iterations         | 168600   |\n",
      "|    time_elapsed       | 30838    |\n",
      "|    total_timesteps    | 13488000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.972   |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 168599   |\n",
      "|    policy_loss        | 0.161    |\n",
      "|    value_loss         | 0.227    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13490000, episode_reward=426.60 +/- 20.22\n",
      "Episode length: 12798.20 +/- 2318.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.28e+04 |\n",
      "|    mean_reward        | 427      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13490000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.931   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 168624   |\n",
      "|    policy_loss        | 0.0208   |\n",
      "|    value_loss         | 0.0292   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | 399      |\n",
      "| time/                 |          |\n",
      "|    fps                | 437      |\n",
      "|    iterations         | 168700   |\n",
      "|    time_elapsed       | 30879    |\n",
      "|    total_timesteps    | 13496000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 168699   |\n",
      "|    policy_loss        | -0.103   |\n",
      "|    value_loss         | 0.0427   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13500000, episode_reward=415.00 +/- 41.00\n",
      "Episode length: 15581.40 +/- 5543.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.56e+04 |\n",
      "|    mean_reward        | 415      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13500000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 168749   |\n",
      "|    policy_loss        | -0.0394  |\n",
      "|    value_loss         | 0.0363   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11e+04 |\n",
      "|    ep_rew_mean        | 395      |\n",
      "| time/                 |          |\n",
      "|    fps                | 436      |\n",
      "|    iterations         | 168800   |\n",
      "|    time_elapsed       | 30927    |\n",
      "|    total_timesteps    | 13504000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 168799   |\n",
      "|    policy_loss        | -0.0661  |\n",
      "|    value_loss         | 0.0275   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13510000, episode_reward=399.00 +/- 39.68\n",
      "Episode length: 9529.60 +/- 1697.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.53e+03 |\n",
      "|    mean_reward        | 399      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13510000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 168874   |\n",
      "|    policy_loss        | 0.0151   |\n",
      "|    value_loss         | 0.0217   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 395      |\n",
      "| time/                 |          |\n",
      "|    fps                | 436      |\n",
      "|    iterations         | 168900   |\n",
      "|    time_elapsed       | 30960    |\n",
      "|    total_timesteps    | 13512000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 168899   |\n",
      "|    policy_loss        | 0.00495  |\n",
      "|    value_loss         | 0.0326   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13520000, episode_reward=402.20 +/- 22.83\n",
      "Episode length: 8328.40 +/- 667.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.33e+03 |\n",
      "|    mean_reward        | 402      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13520000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.977   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 168999   |\n",
      "|    policy_loss        | -0.0187  |\n",
      "|    value_loss         | 0.0648   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.1e+04  |\n",
      "|    ep_rew_mean     | 395      |\n",
      "| time/              |          |\n",
      "|    fps             | 436      |\n",
      "|    iterations      | 169000   |\n",
      "|    time_elapsed    | 30991    |\n",
      "|    total_timesteps | 13520000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+04 |\n",
      "|    ep_rew_mean        | 394      |\n",
      "| time/                 |          |\n",
      "|    fps                | 436      |\n",
      "|    iterations         | 169100   |\n",
      "|    time_elapsed       | 30999    |\n",
      "|    total_timesteps    | 13528000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 169099   |\n",
      "|    policy_loss        | -0.00743 |\n",
      "|    value_loss         | 0.0359   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13530000, episode_reward=473.20 +/- 114.63\n",
      "Episode length: 11425.60 +/- 2879.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+04 |\n",
      "|    mean_reward        | 473      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13530000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 169124   |\n",
      "|    policy_loss        | 0.02     |\n",
      "|    value_loss         | 0.0403   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 391      |\n",
      "| time/                 |          |\n",
      "|    fps                | 436      |\n",
      "|    iterations         | 169200   |\n",
      "|    time_elapsed       | 31035    |\n",
      "|    total_timesteps    | 13536000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 169199   |\n",
      "|    policy_loss        | 0.078    |\n",
      "|    value_loss         | 0.0356   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13540000, episode_reward=379.20 +/- 47.96\n",
      "Episode length: 10970.80 +/- 5771.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.1e+04  |\n",
      "|    mean_reward        | 379      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13540000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 169249   |\n",
      "|    policy_loss        | 0.083    |\n",
      "|    value_loss         | 0.084    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 394      |\n",
      "| time/                 |          |\n",
      "|    fps                | 435      |\n",
      "|    iterations         | 169300   |\n",
      "|    time_elapsed       | 31073    |\n",
      "|    total_timesteps    | 13544000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 169299   |\n",
      "|    policy_loss        | 0.0213   |\n",
      "|    value_loss         | 0.0202   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13550000, episode_reward=373.60 +/- 70.21\n",
      "Episode length: 11369.80 +/- 2770.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+04 |\n",
      "|    mean_reward        | 374      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13550000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 169374   |\n",
      "|    policy_loss        | 0.0849   |\n",
      "|    value_loss         | 0.0537   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | 393      |\n",
      "| time/                 |          |\n",
      "|    fps                | 435      |\n",
      "|    iterations         | 169400   |\n",
      "|    time_elapsed       | 31112    |\n",
      "|    total_timesteps    | 13552000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 169399   |\n",
      "|    policy_loss        | -0.0172  |\n",
      "|    value_loss         | 0.0464   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13560000, episode_reward=326.20 +/- 116.92\n",
      "Episode length: 8145.40 +/- 1821.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.15e+03 |\n",
      "|    mean_reward        | 326      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13560000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 169499   |\n",
      "|    policy_loss        | -0.0037  |\n",
      "|    value_loss         | 0.0595   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.09e+04 |\n",
      "|    ep_rew_mean     | 394      |\n",
      "| time/              |          |\n",
      "|    fps             | 435      |\n",
      "|    iterations      | 169500   |\n",
      "|    time_elapsed    | 31140    |\n",
      "|    total_timesteps | 13560000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+04 |\n",
      "|    ep_rew_mean        | 394      |\n",
      "| time/                 |          |\n",
      "|    fps                | 435      |\n",
      "|    iterations         | 169600   |\n",
      "|    time_elapsed       | 31147    |\n",
      "|    total_timesteps    | 13568000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 169599   |\n",
      "|    policy_loss        | 0.0345   |\n",
      "|    value_loss         | 0.0404   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13570000, episode_reward=417.80 +/- 4.92\n",
      "Episode length: 10884.80 +/- 658.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.09e+04 |\n",
      "|    mean_reward        | 418      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13570000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.936   |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 169624   |\n",
      "|    policy_loss        | 0.209    |\n",
      "|    value_loss         | 0.221    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+04 |\n",
      "|    ep_rew_mean        | 396      |\n",
      "| time/                 |          |\n",
      "|    fps                | 435      |\n",
      "|    iterations         | 169700   |\n",
      "|    time_elapsed       | 31185    |\n",
      "|    total_timesteps    | 13576000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.95    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 169699   |\n",
      "|    policy_loss        | -0.0178  |\n",
      "|    value_loss         | 0.0242   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13580000, episode_reward=517.40 +/- 177.67\n",
      "Episode length: 16167.80 +/- 4614.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.62e+04 |\n",
      "|    mean_reward        | 517      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13580000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 169749   |\n",
      "|    policy_loss        | -0.0955  |\n",
      "|    value_loss         | 0.0879   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 397      |\n",
      "| time/                 |          |\n",
      "|    fps                | 434      |\n",
      "|    iterations         | 169800   |\n",
      "|    time_elapsed       | 31237    |\n",
      "|    total_timesteps    | 13584000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 169799   |\n",
      "|    policy_loss        | -0.0226  |\n",
      "|    value_loss         | 0.0127   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13590000, episode_reward=500.80 +/- 171.26\n",
      "Episode length: 13210.00 +/- 6545.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.32e+04 |\n",
      "|    mean_reward        | 501      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13590000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 169874   |\n",
      "|    policy_loss        | 0.0317   |\n",
      "|    value_loss         | 0.0566   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | 401      |\n",
      "| time/                 |          |\n",
      "|    fps                | 434      |\n",
      "|    iterations         | 169900   |\n",
      "|    time_elapsed       | 31281    |\n",
      "|    total_timesteps    | 13592000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.847   |\n",
      "|    explained_variance | 0.801    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 169899   |\n",
      "|    policy_loss        | 0.146    |\n",
      "|    value_loss         | 0.511    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13600000, episode_reward=477.40 +/- 137.22\n",
      "Episode length: 17029.40 +/- 7715.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.7e+04  |\n",
      "|    mean_reward        | 477      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13600000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.753   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 169999   |\n",
      "|    policy_loss        | 0.0348   |\n",
      "|    value_loss         | 0.0244   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.13e+04 |\n",
      "|    ep_rew_mean     | 402      |\n",
      "| time/              |          |\n",
      "|    fps             | 434      |\n",
      "|    iterations      | 170000   |\n",
      "|    time_elapsed    | 31333    |\n",
      "|    total_timesteps | 13600000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.13e+04 |\n",
      "|    ep_rew_mean        | 402      |\n",
      "| time/                 |          |\n",
      "|    fps                | 434      |\n",
      "|    iterations         | 170100   |\n",
      "|    time_elapsed       | 31340    |\n",
      "|    total_timesteps    | 13608000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 170099   |\n",
      "|    policy_loss        | 0.00185  |\n",
      "|    value_loss         | 0.125    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13610000, episode_reward=381.60 +/- 43.55\n",
      "Episode length: 11286.20 +/- 4776.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+04 |\n",
      "|    mean_reward        | 382      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13610000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 170124   |\n",
      "|    policy_loss        | -0.103   |\n",
      "|    value_loss         | 0.12     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14e+04 |\n",
      "|    ep_rew_mean        | 403      |\n",
      "| time/                 |          |\n",
      "|    fps                | 433      |\n",
      "|    iterations         | 170200   |\n",
      "|    time_elapsed       | 31376    |\n",
      "|    total_timesteps    | 13616000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.996   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 170199   |\n",
      "|    policy_loss        | -0.0496  |\n",
      "|    value_loss         | 0.0315   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13620000, episode_reward=423.40 +/- 11.99\n",
      "Episode length: 11499.60 +/- 1207.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.15e+04 |\n",
      "|    mean_reward        | 423      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13620000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 170249   |\n",
      "|    policy_loss        | -0.0292  |\n",
      "|    value_loss         | 0.0228   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.13e+04 |\n",
      "|    ep_rew_mean        | 402      |\n",
      "| time/                 |          |\n",
      "|    fps                | 433      |\n",
      "|    iterations         | 170300   |\n",
      "|    time_elapsed       | 31416    |\n",
      "|    total_timesteps    | 13624000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 170299   |\n",
      "|    policy_loss        | 0.0182   |\n",
      "|    value_loss         | 0.0233   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13630000, episode_reward=424.00 +/- 13.58\n",
      "Episode length: 11852.40 +/- 1475.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.19e+04 |\n",
      "|    mean_reward        | 424      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13630000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 170374   |\n",
      "|    policy_loss        | 0.0228   |\n",
      "|    value_loss         | 0.0382   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.13e+04 |\n",
      "|    ep_rew_mean        | 402      |\n",
      "| time/                 |          |\n",
      "|    fps                | 433      |\n",
      "|    iterations         | 170400   |\n",
      "|    time_elapsed       | 31456    |\n",
      "|    total_timesteps    | 13632000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 170399   |\n",
      "|    policy_loss        | 0.0121   |\n",
      "|    value_loss         | 0.0217   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13640000, episode_reward=366.40 +/- 43.56\n",
      "Episode length: 9559.00 +/- 2264.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.56e+03 |\n",
      "|    mean_reward        | 366      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13640000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 170499   |\n",
      "|    policy_loss        | 0.0891   |\n",
      "|    value_loss         | 0.0854   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.12e+04 |\n",
      "|    ep_rew_mean     | 403      |\n",
      "| time/              |          |\n",
      "|    fps             | 433      |\n",
      "|    iterations      | 170500   |\n",
      "|    time_elapsed    | 31487    |\n",
      "|    total_timesteps | 13640000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | 402      |\n",
      "| time/                 |          |\n",
      "|    fps                | 433      |\n",
      "|    iterations         | 170600   |\n",
      "|    time_elapsed       | 31494    |\n",
      "|    total_timesteps    | 13648000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 170599   |\n",
      "|    policy_loss        | -0.00237 |\n",
      "|    value_loss         | 0.011    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13650000, episode_reward=403.80 +/- 22.30\n",
      "Episode length: 13997.80 +/- 3549.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.4e+04  |\n",
      "|    mean_reward        | 404      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13650000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 170624   |\n",
      "|    policy_loss        | 0.0663   |\n",
      "|    value_loss         | 0.0366   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | 401      |\n",
      "| time/                 |          |\n",
      "|    fps                | 432      |\n",
      "|    iterations         | 170700   |\n",
      "|    time_elapsed       | 31541    |\n",
      "|    total_timesteps    | 13656000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.982   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 170699   |\n",
      "|    policy_loss        | -0.0523  |\n",
      "|    value_loss         | 0.0397   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13660000, episode_reward=339.20 +/- 103.20\n",
      "Episode length: 10908.20 +/- 3451.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.09e+04 |\n",
      "|    mean_reward        | 339      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13660000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.953   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 170749   |\n",
      "|    policy_loss        | -0.0131  |\n",
      "|    value_loss         | 0.157    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | 401      |\n",
      "| time/                 |          |\n",
      "|    fps                | 432      |\n",
      "|    iterations         | 170800   |\n",
      "|    time_elapsed       | 31578    |\n",
      "|    total_timesteps    | 13664000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.92    |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 170799   |\n",
      "|    policy_loss        | 0.0361   |\n",
      "|    value_loss         | 0.0197   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13670000, episode_reward=419.80 +/- 13.50\n",
      "Episode length: 13258.60 +/- 3374.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.33e+04 |\n",
      "|    mean_reward        | 420      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13670000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 170874   |\n",
      "|    policy_loss        | 0.00961  |\n",
      "|    value_loss         | 0.0224   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.13e+04 |\n",
      "|    ep_rew_mean        | 404      |\n",
      "| time/                 |          |\n",
      "|    fps                | 432      |\n",
      "|    iterations         | 170900   |\n",
      "|    time_elapsed       | 31618    |\n",
      "|    total_timesteps    | 13672000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 170899   |\n",
      "|    policy_loss        | 0.0529   |\n",
      "|    value_loss         | 0.104    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13680000, episode_reward=402.80 +/- 16.63\n",
      "Episode length: 12350.60 +/- 4069.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.24e+04 |\n",
      "|    mean_reward        | 403      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13680000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 170999   |\n",
      "|    policy_loss        | -0.0719  |\n",
      "|    value_loss         | 0.0775   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.13e+04 |\n",
      "|    ep_rew_mean     | 405      |\n",
      "| time/              |          |\n",
      "|    fps             | 432      |\n",
      "|    iterations      | 171000   |\n",
      "|    time_elapsed    | 31660    |\n",
      "|    total_timesteps | 13680000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15e+04 |\n",
      "|    ep_rew_mean        | 406      |\n",
      "| time/                 |          |\n",
      "|    fps                | 432      |\n",
      "|    iterations         | 171100   |\n",
      "|    time_elapsed       | 31667    |\n",
      "|    total_timesteps    | 13688000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.979   |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 171099   |\n",
      "|    policy_loss        | 0.0342   |\n",
      "|    value_loss         | 0.185    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13690000, episode_reward=333.20 +/- 89.94\n",
      "Episode length: 8914.40 +/- 1746.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.91e+03 |\n",
      "|    mean_reward        | 333      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13690000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 171124   |\n",
      "|    policy_loss        | 0.00756  |\n",
      "|    value_loss         | 0.0251   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15e+04 |\n",
      "|    ep_rew_mean        | 406      |\n",
      "| time/                 |          |\n",
      "|    fps                | 432      |\n",
      "|    iterations         | 171200   |\n",
      "|    time_elapsed       | 31699    |\n",
      "|    total_timesteps    | 13696000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 171199   |\n",
      "|    policy_loss        | -0.0325  |\n",
      "|    value_loss         | 0.018    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13700000, episode_reward=416.40 +/- 18.42\n",
      "Episode length: 14908.80 +/- 6221.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.49e+04 |\n",
      "|    mean_reward        | 416      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13700000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 171249   |\n",
      "|    policy_loss        | -0.0143  |\n",
      "|    value_loss         | 0.0459   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | 406      |\n",
      "| time/                 |          |\n",
      "|    fps                | 431      |\n",
      "|    iterations         | 171300   |\n",
      "|    time_elapsed       | 31744    |\n",
      "|    total_timesteps    | 13704000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 171299   |\n",
      "|    policy_loss        | 0.15     |\n",
      "|    value_loss         | 0.0949   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13710000, episode_reward=401.60 +/- 27.98\n",
      "Episode length: 10773.60 +/- 1697.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+04 |\n",
      "|    mean_reward        | 402      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13710000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.952   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 171374   |\n",
      "|    policy_loss        | -0.0476  |\n",
      "|    value_loss         | 0.0291   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | 406      |\n",
      "| time/                 |          |\n",
      "|    fps                | 431      |\n",
      "|    iterations         | 171400   |\n",
      "|    time_elapsed       | 31781    |\n",
      "|    total_timesteps    | 13712000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.908   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 171399   |\n",
      "|    policy_loss        | 0.0213   |\n",
      "|    value_loss         | 0.0448   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13720000, episode_reward=375.60 +/- 98.37\n",
      "Episode length: 10282.80 +/- 3329.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+04 |\n",
      "|    mean_reward        | 376      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13720000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.956   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 171499   |\n",
      "|    policy_loss        | 0.199    |\n",
      "|    value_loss         | 0.141    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.25e+04 |\n",
      "|    ep_rew_mean     | 405      |\n",
      "| time/              |          |\n",
      "|    fps             | 431      |\n",
      "|    iterations      | 171500   |\n",
      "|    time_elapsed    | 31816    |\n",
      "|    total_timesteps | 13720000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | 404      |\n",
      "| time/                 |          |\n",
      "|    fps                | 431      |\n",
      "|    iterations         | 171600   |\n",
      "|    time_elapsed       | 31824    |\n",
      "|    total_timesteps    | 13728000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 171599   |\n",
      "|    policy_loss        | 0.0314   |\n",
      "|    value_loss         | 0.0189   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13730000, episode_reward=326.40 +/- 153.38\n",
      "Episode length: 8581.20 +/- 3181.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.58e+03 |\n",
      "|    mean_reward        | 326      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13730000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.953   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 171624   |\n",
      "|    policy_loss        | 0.0256   |\n",
      "|    value_loss         | 0.0188   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+04 |\n",
      "|    ep_rew_mean        | 404      |\n",
      "| time/                 |          |\n",
      "|    fps                | 431      |\n",
      "|    iterations         | 171700   |\n",
      "|    time_elapsed       | 31852    |\n",
      "|    total_timesteps    | 13736000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 171699   |\n",
      "|    policy_loss        | -0.111   |\n",
      "|    value_loss         | 0.0954   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13740000, episode_reward=354.80 +/- 127.91\n",
      "Episode length: 10050.40 +/- 2978.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+04 |\n",
      "|    mean_reward        | 355      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13740000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 171749   |\n",
      "|    policy_loss        | 0.0964   |\n",
      "|    value_loss         | 0.136    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+04 |\n",
      "|    ep_rew_mean        | 403      |\n",
      "| time/                 |          |\n",
      "|    fps                | 431      |\n",
      "|    iterations         | 171800   |\n",
      "|    time_elapsed       | 31888    |\n",
      "|    total_timesteps    | 13744000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 171799   |\n",
      "|    policy_loss        | 0.0254   |\n",
      "|    value_loss         | 0.0399   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13750000, episode_reward=491.60 +/- 176.04\n",
      "Episode length: 11682.80 +/- 2807.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.17e+04 |\n",
      "|    mean_reward        | 492      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13750000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.956   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 171874   |\n",
      "|    policy_loss        | -0.0355  |\n",
      "|    value_loss         | 0.0279   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 401      |\n",
      "| time/                 |          |\n",
      "|    fps                | 430      |\n",
      "|    iterations         | 171900   |\n",
      "|    time_elapsed       | 31924    |\n",
      "|    total_timesteps    | 13752000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 171899   |\n",
      "|    policy_loss        | 0.00902  |\n",
      "|    value_loss         | 0.0209   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13760000, episode_reward=416.60 +/- 20.67\n",
      "Episode length: 11561.80 +/- 2536.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+04 |\n",
      "|    mean_reward        | 417      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13760000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 171999   |\n",
      "|    policy_loss        | 0.000309 |\n",
      "|    value_loss         | 0.0687   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.24e+04 |\n",
      "|    ep_rew_mean     | 404      |\n",
      "| time/              |          |\n",
      "|    fps             | 430      |\n",
      "|    iterations      | 172000   |\n",
      "|    time_elapsed    | 31963    |\n",
      "|    total_timesteps | 13760000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 404      |\n",
      "| time/                 |          |\n",
      "|    fps                | 430      |\n",
      "|    iterations         | 172100   |\n",
      "|    time_elapsed       | 31970    |\n",
      "|    total_timesteps    | 13768000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.968   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 172099   |\n",
      "|    policy_loss        | -0.0595  |\n",
      "|    value_loss         | 0.0502   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13770000, episode_reward=398.80 +/- 23.94\n",
      "Episode length: 9244.60 +/- 1370.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.24e+03 |\n",
      "|    mean_reward        | 399      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13770000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.977   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 172124   |\n",
      "|    policy_loss        | -0.022   |\n",
      "|    value_loss         | 0.0135   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | 403      |\n",
      "| time/                 |          |\n",
      "|    fps                | 430      |\n",
      "|    iterations         | 172200   |\n",
      "|    time_elapsed       | 32003    |\n",
      "|    total_timesteps    | 13776000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.971   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 172199   |\n",
      "|    policy_loss        | 0.0903   |\n",
      "|    value_loss         | 0.0832   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13780000, episode_reward=376.40 +/- 47.55\n",
      "Episode length: 9676.40 +/- 1493.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.68e+03 |\n",
      "|    mean_reward        | 376      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13780000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.99    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 172249   |\n",
      "|    policy_loss        | 0.0371   |\n",
      "|    value_loss         | 0.076    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | 400      |\n",
      "| time/                 |          |\n",
      "|    fps                | 430      |\n",
      "|    iterations         | 172300   |\n",
      "|    time_elapsed       | 32035    |\n",
      "|    total_timesteps    | 13784000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 172299   |\n",
      "|    policy_loss        | 0.0243   |\n",
      "|    value_loss         | 0.0369   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13790000, episode_reward=359.20 +/- 125.18\n",
      "Episode length: 15713.60 +/- 11916.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.57e+04 |\n",
      "|    mean_reward        | 359      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13790000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 172374   |\n",
      "|    policy_loss        | 0.046    |\n",
      "|    value_loss         | 0.0375   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.21e+04 |\n",
      "|    ep_rew_mean        | 399      |\n",
      "| time/                 |          |\n",
      "|    fps                | 429      |\n",
      "|    iterations         | 172400   |\n",
      "|    time_elapsed       | 32087    |\n",
      "|    total_timesteps    | 13792000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.959   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 172399   |\n",
      "|    policy_loss        | 0.0532   |\n",
      "|    value_loss         | 0.0596   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13800000, episode_reward=408.40 +/- 20.21\n",
      "Episode length: 13969.00 +/- 2465.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.4e+04  |\n",
      "|    mean_reward        | 408      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13800000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.926   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 172499   |\n",
      "|    policy_loss        | 0.0158   |\n",
      "|    value_loss         | 0.0473   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.22e+04 |\n",
      "|    ep_rew_mean     | 400      |\n",
      "| time/              |          |\n",
      "|    fps             | 429      |\n",
      "|    iterations      | 172500   |\n",
      "|    time_elapsed    | 32132    |\n",
      "|    total_timesteps | 13800000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | 402      |\n",
      "| time/                 |          |\n",
      "|    fps                | 429      |\n",
      "|    iterations         | 172600   |\n",
      "|    time_elapsed       | 32140    |\n",
      "|    total_timesteps    | 13808000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.929   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 172599   |\n",
      "|    policy_loss        | 0.00662  |\n",
      "|    value_loss         | 0.061    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13810000, episode_reward=447.60 +/- 69.30\n",
      "Episode length: 12305.60 +/- 2035.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.23e+04 |\n",
      "|    mean_reward        | 448      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13810000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 172624   |\n",
      "|    policy_loss        | -0.0731  |\n",
      "|    value_loss         | 0.0482   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | 401      |\n",
      "| time/                 |          |\n",
      "|    fps                | 429      |\n",
      "|    iterations         | 172700   |\n",
      "|    time_elapsed       | 32177    |\n",
      "|    total_timesteps    | 13816000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.878   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 172699   |\n",
      "|    policy_loss        | 0.00688  |\n",
      "|    value_loss         | 0.0248   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13820000, episode_reward=319.80 +/- 148.33\n",
      "Episode length: 10356.60 +/- 4211.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+04 |\n",
      "|    mean_reward        | 320      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13820000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.87    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 172749   |\n",
      "|    policy_loss        | 0.0261   |\n",
      "|    value_loss         | 0.0325   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 402      |\n",
      "| time/                 |          |\n",
      "|    fps                | 429      |\n",
      "|    iterations         | 172800   |\n",
      "|    time_elapsed       | 32212    |\n",
      "|    total_timesteps    | 13824000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.999   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 172799   |\n",
      "|    policy_loss        | 0.0616   |\n",
      "|    value_loss         | 0.0781   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13830000, episode_reward=477.60 +/- 118.30\n",
      "Episode length: 18908.20 +/- 8627.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.89e+04 |\n",
      "|    mean_reward        | 478      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13830000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 172874   |\n",
      "|    policy_loss        | -0.0196  |\n",
      "|    value_loss         | 0.0482   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 406      |\n",
      "| time/                 |          |\n",
      "|    fps                | 428      |\n",
      "|    iterations         | 172900   |\n",
      "|    time_elapsed       | 32272    |\n",
      "|    total_timesteps    | 13832000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.961   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 172899   |\n",
      "|    policy_loss        | 0.045    |\n",
      "|    value_loss         | 0.0392   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13840000, episode_reward=500.40 +/- 180.26\n",
      "Episode length: 12768.40 +/- 4304.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.28e+04 |\n",
      "|    mean_reward        | 500      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13840000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.901   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 172999   |\n",
      "|    policy_loss        | 0.0227   |\n",
      "|    value_loss         | 0.0195   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+04 |\n",
      "|    ep_rew_mean     | 406      |\n",
      "| time/              |          |\n",
      "|    fps             | 428      |\n",
      "|    iterations      | 173000   |\n",
      "|    time_elapsed    | 32313    |\n",
      "|    total_timesteps | 13840000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | 406      |\n",
      "| time/                 |          |\n",
      "|    fps                | 428      |\n",
      "|    iterations         | 173100   |\n",
      "|    time_elapsed       | 32320    |\n",
      "|    total_timesteps    | 13848000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 173099   |\n",
      "|    policy_loss        | 0.0131   |\n",
      "|    value_loss         | 0.0287   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13850000, episode_reward=393.00 +/- 29.17\n",
      "Episode length: 9038.80 +/- 960.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.04e+03 |\n",
      "|    mean_reward        | 393      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13850000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.94    |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 173124   |\n",
      "|    policy_loss        | -0.0204  |\n",
      "|    value_loss         | 0.0125   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | 406      |\n",
      "| time/                 |          |\n",
      "|    fps                | 428      |\n",
      "|    iterations         | 173200   |\n",
      "|    time_elapsed       | 32349    |\n",
      "|    total_timesteps    | 13856000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 173199   |\n",
      "|    policy_loss        | -0.0448  |\n",
      "|    value_loss         | 0.0313   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13860000, episode_reward=521.80 +/- 171.54\n",
      "Episode length: 21291.40 +/- 9865.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.13e+04 |\n",
      "|    mean_reward        | 522      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13860000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.882   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 173249   |\n",
      "|    policy_loss        | -0.053   |\n",
      "|    value_loss         | 0.0437   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 409      |\n",
      "| time/                 |          |\n",
      "|    fps                | 427      |\n",
      "|    iterations         | 173300   |\n",
      "|    time_elapsed       | 32412    |\n",
      "|    total_timesteps    | 13864000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.829   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 173299   |\n",
      "|    policy_loss        | 0.00401  |\n",
      "|    value_loss         | 0.0262   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13870000, episode_reward=391.40 +/- 42.83\n",
      "Episode length: 9409.60 +/- 1229.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.41e+03 |\n",
      "|    mean_reward        | 391      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13870000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 173374   |\n",
      "|    policy_loss        | 0.0932   |\n",
      "|    value_loss         | 0.162    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 409      |\n",
      "| time/                 |          |\n",
      "|    fps                | 427      |\n",
      "|    iterations         | 173400   |\n",
      "|    time_elapsed       | 32447    |\n",
      "|    total_timesteps    | 13872000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.865   |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 173399   |\n",
      "|    policy_loss        | 0.0104   |\n",
      "|    value_loss         | 0.00856  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13880000, episode_reward=358.60 +/- 155.80\n",
      "Episode length: 10987.40 +/- 5352.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.1e+04  |\n",
      "|    mean_reward        | 359      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13880000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 173499   |\n",
      "|    policy_loss        | 0.0915   |\n",
      "|    value_loss         | 0.0515   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+04 |\n",
      "|    ep_rew_mean     | 409      |\n",
      "| time/              |          |\n",
      "|    fps             | 427      |\n",
      "|    iterations      | 173500   |\n",
      "|    time_elapsed    | 32481    |\n",
      "|    total_timesteps | 13880000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 412      |\n",
      "| time/                 |          |\n",
      "|    fps                | 427      |\n",
      "|    iterations         | 173600   |\n",
      "|    time_elapsed       | 32488    |\n",
      "|    total_timesteps    | 13888000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.969   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 173599   |\n",
      "|    policy_loss        | 0.0118   |\n",
      "|    value_loss         | 0.075    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13890000, episode_reward=410.60 +/- 85.67\n",
      "Episode length: 12626.00 +/- 4422.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.26e+04 |\n",
      "|    mean_reward        | 411      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13890000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 173624   |\n",
      "|    policy_loss        | 0.0129   |\n",
      "|    value_loss         | 0.0415   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 412      |\n",
      "| time/                 |          |\n",
      "|    fps                | 427      |\n",
      "|    iterations         | 173700   |\n",
      "|    time_elapsed       | 32529    |\n",
      "|    total_timesteps    | 13896000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.747   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 173699   |\n",
      "|    policy_loss        | -0.0156  |\n",
      "|    value_loss         | 0.0254   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13900000, episode_reward=488.20 +/- 163.31\n",
      "Episode length: 19293.40 +/- 8730.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.93e+04 |\n",
      "|    mean_reward        | 488      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13900000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 173749   |\n",
      "|    policy_loss        | 0.047    |\n",
      "|    value_loss         | 0.0442   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 411      |\n",
      "| time/                 |          |\n",
      "|    fps                | 426      |\n",
      "|    iterations         | 173800   |\n",
      "|    time_elapsed       | 32588    |\n",
      "|    total_timesteps    | 13904000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 173799   |\n",
      "|    policy_loss        | -0.0411  |\n",
      "|    value_loss         | 0.0461   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13910000, episode_reward=568.00 +/- 182.26\n",
      "Episode length: 18168.40 +/- 6105.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.82e+04 |\n",
      "|    mean_reward        | 568      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13910000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.866   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 173874   |\n",
      "|    policy_loss        | 0.0954   |\n",
      "|    value_loss         | 0.0741   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | 411      |\n",
      "| time/                 |          |\n",
      "|    fps                | 426      |\n",
      "|    iterations         | 173900   |\n",
      "|    time_elapsed       | 32646    |\n",
      "|    total_timesteps    | 13912000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 173899   |\n",
      "|    policy_loss        | -0.00729 |\n",
      "|    value_loss         | 0.032    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13920000, episode_reward=406.80 +/- 7.47\n",
      "Episode length: 11075.60 +/- 1864.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.11e+04 |\n",
      "|    mean_reward        | 407      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13920000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.97    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 173999   |\n",
      "|    policy_loss        | 0.0601   |\n",
      "|    value_loss         | 0.0472   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+04 |\n",
      "|    ep_rew_mean     | 418      |\n",
      "| time/              |          |\n",
      "|    fps             | 425      |\n",
      "|    iterations      | 174000   |\n",
      "|    time_elapsed    | 32682    |\n",
      "|    total_timesteps | 13920000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 419      |\n",
      "| time/                 |          |\n",
      "|    fps                | 426      |\n",
      "|    iterations         | 174100   |\n",
      "|    time_elapsed       | 32690    |\n",
      "|    total_timesteps    | 13928000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.859   |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 174099   |\n",
      "|    policy_loss        | 0.0028   |\n",
      "|    value_loss         | 0.0853   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13930000, episode_reward=409.20 +/- 24.64\n",
      "Episode length: 9686.40 +/- 1546.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.69e+03 |\n",
      "|    mean_reward        | 409      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13930000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.982   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 174124   |\n",
      "|    policy_loss        | 0.0733   |\n",
      "|    value_loss         | 0.0825   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 420      |\n",
      "| time/                 |          |\n",
      "|    fps                | 425      |\n",
      "|    iterations         | 174200   |\n",
      "|    time_elapsed       | 32721    |\n",
      "|    total_timesteps    | 13936000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.901   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 174199   |\n",
      "|    policy_loss        | -0.124   |\n",
      "|    value_loss         | 0.106    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13940000, episode_reward=404.80 +/- 33.03\n",
      "Episode length: 10513.60 +/- 1460.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+04 |\n",
      "|    mean_reward        | 405      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13940000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 174249   |\n",
      "|    policy_loss        | -0.0926  |\n",
      "|    value_loss         | 0.0597   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.28e+04 |\n",
      "|    ep_rew_mean        | 420      |\n",
      "| time/                 |          |\n",
      "|    fps                | 425      |\n",
      "|    iterations         | 174300   |\n",
      "|    time_elapsed       | 32758    |\n",
      "|    total_timesteps    | 13944000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 174299   |\n",
      "|    policy_loss        | 0.32     |\n",
      "|    value_loss         | 0.584    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13950000, episode_reward=409.60 +/- 29.02\n",
      "Episode length: 9449.00 +/- 1207.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.45e+03 |\n",
      "|    mean_reward        | 410      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13950000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 174374   |\n",
      "|    policy_loss        | 0.0536   |\n",
      "|    value_loss         | 0.117    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.29e+04 |\n",
      "|    ep_rew_mean        | 421      |\n",
      "| time/                 |          |\n",
      "|    fps                | 425      |\n",
      "|    iterations         | 174400   |\n",
      "|    time_elapsed       | 32789    |\n",
      "|    total_timesteps    | 13952000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 174399   |\n",
      "|    policy_loss        | -0.0287  |\n",
      "|    value_loss         | 0.038    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13960000, episode_reward=347.80 +/- 71.63\n",
      "Episode length: 8913.80 +/- 2096.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.91e+03 |\n",
      "|    mean_reward        | 348      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13960000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 174499   |\n",
      "|    policy_loss        | -0.067   |\n",
      "|    value_loss         | 0.0697   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.29e+04 |\n",
      "|    ep_rew_mean     | 421      |\n",
      "| time/              |          |\n",
      "|    fps             | 425      |\n",
      "|    iterations      | 174500   |\n",
      "|    time_elapsed    | 32821    |\n",
      "|    total_timesteps | 13960000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.29e+04 |\n",
      "|    ep_rew_mean        | 421      |\n",
      "| time/                 |          |\n",
      "|    fps                | 425      |\n",
      "|    iterations         | 174600   |\n",
      "|    time_elapsed       | 32829    |\n",
      "|    total_timesteps    | 13968000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.954   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 174599   |\n",
      "|    policy_loss        | -0.0165  |\n",
      "|    value_loss         | 0.0214   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13970000, episode_reward=419.00 +/- 9.63\n",
      "Episode length: 11033.20 +/- 2669.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.1e+04  |\n",
      "|    mean_reward        | 419      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13970000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.936   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 174624   |\n",
      "|    policy_loss        | -0.0893  |\n",
      "|    value_loss         | 0.0448   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.29e+04 |\n",
      "|    ep_rew_mean        | 420      |\n",
      "| time/                 |          |\n",
      "|    fps                | 425      |\n",
      "|    iterations         | 174700   |\n",
      "|    time_elapsed       | 32863    |\n",
      "|    total_timesteps    | 13976000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 174699   |\n",
      "|    policy_loss        | -0.0347  |\n",
      "|    value_loss         | 0.0462   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13980000, episode_reward=477.40 +/- 123.62\n",
      "Episode length: 20131.20 +/- 10989.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.01e+04 |\n",
      "|    mean_reward        | 477      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13980000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 174749   |\n",
      "|    policy_loss        | -0.0106  |\n",
      "|    value_loss         | 0.383    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.29e+04 |\n",
      "|    ep_rew_mean        | 419      |\n",
      "| time/                 |          |\n",
      "|    fps                | 424      |\n",
      "|    iterations         | 174800   |\n",
      "|    time_elapsed       | 32921    |\n",
      "|    total_timesteps    | 13984000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.926   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 174799   |\n",
      "|    policy_loss        | -0.00857 |\n",
      "|    value_loss         | 0.0577   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13990000, episode_reward=478.00 +/- 178.34\n",
      "Episode length: 10969.60 +/- 4002.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.1e+04  |\n",
      "|    mean_reward        | 478      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13990000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.914   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 174874   |\n",
      "|    policy_loss        | -0.0355  |\n",
      "|    value_loss         | 0.0439   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.29e+04 |\n",
      "|    ep_rew_mean        | 419      |\n",
      "| time/                 |          |\n",
      "|    fps                | 424      |\n",
      "|    iterations         | 174900   |\n",
      "|    time_elapsed       | 32959    |\n",
      "|    total_timesteps    | 13992000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.862   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 174899   |\n",
      "|    policy_loss        | -0.00253 |\n",
      "|    value_loss         | 0.0394   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14000000, episode_reward=401.00 +/- 43.47\n",
      "Episode length: 10473.60 +/- 1432.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+04 |\n",
      "|    mean_reward        | 401      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14000000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 174999   |\n",
      "|    policy_loss        | -0.105   |\n",
      "|    value_loss         | 0.246    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.28e+04 |\n",
      "|    ep_rew_mean     | 418      |\n",
      "| time/              |          |\n",
      "|    fps             | 424      |\n",
      "|    iterations      | 175000   |\n",
      "|    time_elapsed    | 32994    |\n",
      "|    total_timesteps | 14000000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.29e+04 |\n",
      "|    ep_rew_mean        | 418      |\n",
      "| time/                 |          |\n",
      "|    fps                | 424      |\n",
      "|    iterations         | 175100   |\n",
      "|    time_elapsed       | 33002    |\n",
      "|    total_timesteps    | 14008000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.961   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 175099   |\n",
      "|    policy_loss        | 0.0152   |\n",
      "|    value_loss         | 0.0386   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14010000, episode_reward=419.20 +/- 12.86\n",
      "Episode length: 14662.00 +/- 8921.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.47e+04 |\n",
      "|    mean_reward        | 419      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14010000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 175124   |\n",
      "|    policy_loss        | 0.0126   |\n",
      "|    value_loss         | 0.0308   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.19e+04 |\n",
      "|    ep_rew_mean        | 419      |\n",
      "| time/                 |          |\n",
      "|    fps                | 424      |\n",
      "|    iterations         | 175200   |\n",
      "|    time_elapsed       | 33046    |\n",
      "|    total_timesteps    | 14016000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 175199   |\n",
      "|    policy_loss        | 0.0312   |\n",
      "|    value_loss         | 0.107    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14020000, episode_reward=431.40 +/- 236.15\n",
      "Episode length: 10766.80 +/- 5069.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+04 |\n",
      "|    mean_reward        | 431      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14020000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.933   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 175249   |\n",
      "|    policy_loss        | 0.00648  |\n",
      "|    value_loss         | 0.0275   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.19e+04 |\n",
      "|    ep_rew_mean        | 419      |\n",
      "| time/                 |          |\n",
      "|    fps                | 423      |\n",
      "|    iterations         | 175300   |\n",
      "|    time_elapsed       | 33083    |\n",
      "|    total_timesteps    | 14024000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 175299   |\n",
      "|    policy_loss        | 0.0267   |\n",
      "|    value_loss         | 0.241    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14030000, episode_reward=381.00 +/- 38.74\n",
      "Episode length: 9210.40 +/- 1176.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.21e+03 |\n",
      "|    mean_reward        | 381      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14030000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 175374   |\n",
      "|    policy_loss        | -0.0606  |\n",
      "|    value_loss         | 0.104    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.19e+04 |\n",
      "|    ep_rew_mean        | 419      |\n",
      "| time/                 |          |\n",
      "|    fps                | 423      |\n",
      "|    iterations         | 175400   |\n",
      "|    time_elapsed       | 33114    |\n",
      "|    total_timesteps    | 14032000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.973   |\n",
      "|    explained_variance | 0.811    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 175399   |\n",
      "|    policy_loss        | -0.183   |\n",
      "|    value_loss         | 0.752    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14040000, episode_reward=407.40 +/- 26.15\n",
      "Episode length: 13886.80 +/- 1401.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.39e+04 |\n",
      "|    mean_reward        | 407      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14040000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 175499   |\n",
      "|    policy_loss        | -0.13    |\n",
      "|    value_loss         | 0.27     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.19e+04 |\n",
      "|    ep_rew_mean     | 420      |\n",
      "| time/              |          |\n",
      "|    fps             | 423      |\n",
      "|    iterations      | 175500   |\n",
      "|    time_elapsed    | 33157    |\n",
      "|    total_timesteps | 14040000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.18e+04 |\n",
      "|    ep_rew_mean        | 418      |\n",
      "| time/                 |          |\n",
      "|    fps                | 423      |\n",
      "|    iterations         | 175600   |\n",
      "|    time_elapsed       | 33165    |\n",
      "|    total_timesteps    | 14048000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 175599   |\n",
      "|    policy_loss        | 0.0994   |\n",
      "|    value_loss         | 0.0774   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14050000, episode_reward=411.00 +/- 31.55\n",
      "Episode length: 16357.60 +/- 4947.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.64e+04 |\n",
      "|    mean_reward        | 411      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14050000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.974   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 175624   |\n",
      "|    policy_loss        | -0.0133  |\n",
      "|    value_loss         | 0.0168   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.18e+04 |\n",
      "|    ep_rew_mean        | 417      |\n",
      "| time/                 |          |\n",
      "|    fps                | 423      |\n",
      "|    iterations         | 175700   |\n",
      "|    time_elapsed       | 33215    |\n",
      "|    total_timesteps    | 14056000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 175699   |\n",
      "|    policy_loss        | 0.00719  |\n",
      "|    value_loss         | 0.0649   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14060000, episode_reward=390.20 +/- 29.59\n",
      "Episode length: 10770.60 +/- 5322.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+04 |\n",
      "|    mean_reward        | 390      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14060000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 175749   |\n",
      "|    policy_loss        | -0.0708  |\n",
      "|    value_loss         | 0.044    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.18e+04 |\n",
      "|    ep_rew_mean        | 417      |\n",
      "| time/                 |          |\n",
      "|    fps                | 422      |\n",
      "|    iterations         | 175800   |\n",
      "|    time_elapsed       | 33253    |\n",
      "|    total_timesteps    | 14064000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 175799   |\n",
      "|    policy_loss        | -0.0439  |\n",
      "|    value_loss         | 0.038    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14070000, episode_reward=411.40 +/- 17.56\n",
      "Episode length: 11011.60 +/- 2352.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.1e+04  |\n",
      "|    mean_reward        | 411      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14070000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 175874   |\n",
      "|    policy_loss        | 0.0514   |\n",
      "|    value_loss         | 0.0223   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.18e+04 |\n",
      "|    ep_rew_mean        | 417      |\n",
      "| time/                 |          |\n",
      "|    fps                | 422      |\n",
      "|    iterations         | 175900   |\n",
      "|    time_elapsed       | 33286    |\n",
      "|    total_timesteps    | 14072000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 175899   |\n",
      "|    policy_loss        | -0.0361  |\n",
      "|    value_loss         | 0.0254   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14080000, episode_reward=401.00 +/- 28.54\n",
      "Episode length: 11280.80 +/- 2057.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+04 |\n",
      "|    mean_reward        | 401      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14080000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 175999   |\n",
      "|    policy_loss        | 0.021    |\n",
      "|    value_loss         | 0.0694   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.18e+04 |\n",
      "|    ep_rew_mean     | 416      |\n",
      "| time/              |          |\n",
      "|    fps             | 422      |\n",
      "|    iterations      | 176000   |\n",
      "|    time_elapsed    | 33325    |\n",
      "|    total_timesteps | 14080000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.19e+04 |\n",
      "|    ep_rew_mean        | 420      |\n",
      "| time/                 |          |\n",
      "|    fps                | 422      |\n",
      "|    iterations         | 176100   |\n",
      "|    time_elapsed       | 33333    |\n",
      "|    total_timesteps    | 14088000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 176099   |\n",
      "|    policy_loss        | 0.0641   |\n",
      "|    value_loss         | 0.0692   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14090000, episode_reward=486.80 +/- 133.69\n",
      "Episode length: 13734.80 +/- 2664.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.37e+04 |\n",
      "|    mean_reward        | 487      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14090000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.895   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 176124   |\n",
      "|    policy_loss        | -0.036   |\n",
      "|    value_loss         | 0.0173   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.19e+04 |\n",
      "|    ep_rew_mean        | 420      |\n",
      "| time/                 |          |\n",
      "|    fps                | 422      |\n",
      "|    iterations         | 176200   |\n",
      "|    time_elapsed       | 33376    |\n",
      "|    total_timesteps    | 14096000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 176199   |\n",
      "|    policy_loss        | -0.012   |\n",
      "|    value_loss         | 0.02     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14100000, episode_reward=419.60 +/- 17.87\n",
      "Episode length: 10030.80 +/- 862.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1e+04    |\n",
      "|    mean_reward        | 420      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14100000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 176249   |\n",
      "|    policy_loss        | -0.143   |\n",
      "|    value_loss         | 0.123    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.3e+04  |\n",
      "|    ep_rew_mean        | 421      |\n",
      "| time/                 |          |\n",
      "|    fps                | 422      |\n",
      "|    iterations         | 176300   |\n",
      "|    time_elapsed       | 33407    |\n",
      "|    total_timesteps    | 14104000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 176299   |\n",
      "|    policy_loss        | 0.0182   |\n",
      "|    value_loss         | 0.0422   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14110000, episode_reward=432.80 +/- 79.35\n",
      "Episode length: 11617.80 +/- 3488.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+04 |\n",
      "|    mean_reward        | 433      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14110000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 176374   |\n",
      "|    policy_loss        | 0.0152   |\n",
      "|    value_loss         | 0.0439   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.3e+04  |\n",
      "|    ep_rew_mean        | 421      |\n",
      "| time/                 |          |\n",
      "|    fps                | 421      |\n",
      "|    iterations         | 176400   |\n",
      "|    time_elapsed       | 33447    |\n",
      "|    total_timesteps    | 14112000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 176399   |\n",
      "|    policy_loss        | -0.0333  |\n",
      "|    value_loss         | 0.027    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14120000, episode_reward=493.00 +/- 152.33\n",
      "Episode length: 13161.80 +/- 2701.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.32e+04 |\n",
      "|    mean_reward        | 493      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14120000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.904   |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 176499   |\n",
      "|    policy_loss        | 0.00773  |\n",
      "|    value_loss         | 0.0168   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.29e+04 |\n",
      "|    ep_rew_mean     | 420      |\n",
      "| time/              |          |\n",
      "|    fps             | 421      |\n",
      "|    iterations      | 176500   |\n",
      "|    time_elapsed    | 33488    |\n",
      "|    total_timesteps | 14120000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.3e+04  |\n",
      "|    ep_rew_mean        | 420      |\n",
      "| time/                 |          |\n",
      "|    fps                | 421      |\n",
      "|    iterations         | 176600   |\n",
      "|    time_elapsed       | 33495    |\n",
      "|    total_timesteps    | 14128000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.945   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 176599   |\n",
      "|    policy_loss        | 0.0995   |\n",
      "|    value_loss         | 0.0919   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14130000, episode_reward=492.40 +/- 168.01\n",
      "Episode length: 13415.60 +/- 7811.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.34e+04 |\n",
      "|    mean_reward        | 492      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14130000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 176624   |\n",
      "|    policy_loss        | 0.0126   |\n",
      "|    value_loss         | 0.0387   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.3e+04  |\n",
      "|    ep_rew_mean        | 420      |\n",
      "| time/                 |          |\n",
      "|    fps                | 421      |\n",
      "|    iterations         | 176700   |\n",
      "|    time_elapsed       | 33538    |\n",
      "|    total_timesteps    | 14136000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 176699   |\n",
      "|    policy_loss        | -0.06    |\n",
      "|    value_loss         | 0.0566   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14140000, episode_reward=440.20 +/- 35.33\n",
      "Episode length: 15239.60 +/- 2121.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.52e+04 |\n",
      "|    mean_reward        | 440      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14140000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 176749   |\n",
      "|    policy_loss        | 0.0393   |\n",
      "|    value_loss         | 0.0643   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 418      |\n",
      "| time/                 |          |\n",
      "|    fps                | 421      |\n",
      "|    iterations         | 176800   |\n",
      "|    time_elapsed       | 33586    |\n",
      "|    total_timesteps    | 14144000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 176799   |\n",
      "|    policy_loss        | -0.0636  |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14150000, episode_reward=415.60 +/- 4.32\n",
      "Episode length: 10932.00 +/- 1384.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.09e+04 |\n",
      "|    mean_reward        | 416      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14150000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.993   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 176874   |\n",
      "|    policy_loss        | -0.0137  |\n",
      "|    value_loss         | 0.105    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 418      |\n",
      "| time/                 |          |\n",
      "|    fps                | 420      |\n",
      "|    iterations         | 176900   |\n",
      "|    time_elapsed       | 33624    |\n",
      "|    total_timesteps    | 14152000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.966   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 176899   |\n",
      "|    policy_loss        | -0.116   |\n",
      "|    value_loss         | 0.0985   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14160000, episode_reward=417.80 +/- 39.76\n",
      "Episode length: 11992.20 +/- 1622.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.2e+04  |\n",
      "|    mean_reward        | 418      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14160000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 176999   |\n",
      "|    policy_loss        | 0.0191   |\n",
      "|    value_loss         | 0.0269   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.28e+04 |\n",
      "|    ep_rew_mean     | 418      |\n",
      "| time/              |          |\n",
      "|    fps             | 420      |\n",
      "|    iterations      | 177000   |\n",
      "|    time_elapsed    | 33661    |\n",
      "|    total_timesteps | 14160000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.28e+04 |\n",
      "|    ep_rew_mean        | 420      |\n",
      "| time/                 |          |\n",
      "|    fps                | 420      |\n",
      "|    iterations         | 177100   |\n",
      "|    time_elapsed       | 33668    |\n",
      "|    total_timesteps    | 14168000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 177099   |\n",
      "|    policy_loss        | 0.0951   |\n",
      "|    value_loss         | 0.0645   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14170000, episode_reward=361.00 +/- 108.58\n",
      "Episode length: 11782.40 +/- 5301.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.18e+04 |\n",
      "|    mean_reward        | 361      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14170000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 177124   |\n",
      "|    policy_loss        | 0.0343   |\n",
      "|    value_loss         | 0.0657   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.28e+04 |\n",
      "|    ep_rew_mean        | 420      |\n",
      "| time/                 |          |\n",
      "|    fps                | 420      |\n",
      "|    iterations         | 177200   |\n",
      "|    time_elapsed       | 33708    |\n",
      "|    total_timesteps    | 14176000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 177199   |\n",
      "|    policy_loss        | 0.0828   |\n",
      "|    value_loss         | 0.0976   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14180000, episode_reward=350.00 +/- 51.29\n",
      "Episode length: 8499.60 +/- 929.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.5e+03  |\n",
      "|    mean_reward        | 350      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14180000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.955   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 177249   |\n",
      "|    policy_loss        | 0.0771   |\n",
      "|    value_loss         | 0.0867   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.28e+04 |\n",
      "|    ep_rew_mean        | 420      |\n",
      "| time/                 |          |\n",
      "|    fps                | 420      |\n",
      "|    iterations         | 177300   |\n",
      "|    time_elapsed       | 33738    |\n",
      "|    total_timesteps    | 14184000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 177299   |\n",
      "|    policy_loss        | 0.0226   |\n",
      "|    value_loss         | 0.0298   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14190000, episode_reward=338.80 +/- 28.69\n",
      "Episode length: 7988.60 +/- 901.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.99e+03 |\n",
      "|    mean_reward        | 339      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14190000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 177374   |\n",
      "|    policy_loss        | -0.041   |\n",
      "|    value_loss         | 0.0801   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.29e+04 |\n",
      "|    ep_rew_mean        | 424      |\n",
      "| time/                 |          |\n",
      "|    fps                | 420      |\n",
      "|    iterations         | 177400   |\n",
      "|    time_elapsed       | 33767    |\n",
      "|    total_timesteps    | 14192000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.923   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 177399   |\n",
      "|    policy_loss        | -0.0444  |\n",
      "|    value_loss         | 0.045    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14200000, episode_reward=397.00 +/- 13.61\n",
      "Episode length: 10299.60 +/- 1666.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+04 |\n",
      "|    mean_reward        | 397      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14200000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 177499   |\n",
      "|    policy_loss        | 0.0179   |\n",
      "|    value_loss         | 0.0265   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.28e+04 |\n",
      "|    ep_rew_mean     | 423      |\n",
      "| time/              |          |\n",
      "|    fps             | 420      |\n",
      "|    iterations      | 177500   |\n",
      "|    time_elapsed    | 33802    |\n",
      "|    total_timesteps | 14200000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 422      |\n",
      "| time/                 |          |\n",
      "|    fps                | 420      |\n",
      "|    iterations         | 177600   |\n",
      "|    time_elapsed       | 33810    |\n",
      "|    total_timesteps    | 14208000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 177599   |\n",
      "|    policy_loss        | 0.0885   |\n",
      "|    value_loss         | 0.0686   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14210000, episode_reward=384.20 +/- 81.98\n",
      "Episode length: 9023.20 +/- 1913.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.02e+03 |\n",
      "|    mean_reward        | 384      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14210000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.998   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 177624   |\n",
      "|    policy_loss        | 0.0921   |\n",
      "|    value_loss         | 0.0396   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.28e+04 |\n",
      "|    ep_rew_mean        | 416      |\n",
      "| time/                 |          |\n",
      "|    fps                | 420      |\n",
      "|    iterations         | 177700   |\n",
      "|    time_elapsed       | 33840    |\n",
      "|    total_timesteps    | 14216000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 177699   |\n",
      "|    policy_loss        | -0.0608  |\n",
      "|    value_loss         | 0.0513   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14220000, episode_reward=378.00 +/- 72.60\n",
      "Episode length: 10037.60 +/- 2329.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1e+04    |\n",
      "|    mean_reward        | 378      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14220000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 177749   |\n",
      "|    policy_loss        | 0.0878   |\n",
      "|    value_loss         | 0.121    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.28e+04 |\n",
      "|    ep_rew_mean        | 419      |\n",
      "| time/                 |          |\n",
      "|    fps                | 419      |\n",
      "|    iterations         | 177800   |\n",
      "|    time_elapsed       | 33876    |\n",
      "|    total_timesteps    | 14224000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.997   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 177799   |\n",
      "|    policy_loss        | 0.0152   |\n",
      "|    value_loss         | 0.0324   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14230000, episode_reward=418.20 +/- 12.91\n",
      "Episode length: 12099.80 +/- 1061.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.21e+04 |\n",
      "|    mean_reward        | 418      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14230000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.981   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 177874   |\n",
      "|    policy_loss        | 0.0246   |\n",
      "|    value_loss         | 0.0564   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.28e+04 |\n",
      "|    ep_rew_mean        | 418      |\n",
      "| time/                 |          |\n",
      "|    fps                | 419      |\n",
      "|    iterations         | 177900   |\n",
      "|    time_elapsed       | 33912    |\n",
      "|    total_timesteps    | 14232000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.972   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 177899   |\n",
      "|    policy_loss        | 0.069    |\n",
      "|    value_loss         | 0.0678   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14240000, episode_reward=411.80 +/- 22.79\n",
      "Episode length: 11571.40 +/- 3438.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+04 |\n",
      "|    mean_reward        | 412      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14240000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 177999   |\n",
      "|    policy_loss        | 0.0273   |\n",
      "|    value_loss         | 0.0748   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.27e+04 |\n",
      "|    ep_rew_mean     | 417      |\n",
      "| time/              |          |\n",
      "|    fps             | 419      |\n",
      "|    iterations      | 178000   |\n",
      "|    time_elapsed    | 33952    |\n",
      "|    total_timesteps | 14240000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 416      |\n",
      "| time/                 |          |\n",
      "|    fps                | 419      |\n",
      "|    iterations         | 178100   |\n",
      "|    time_elapsed       | 33959    |\n",
      "|    total_timesteps    | 14248000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.974   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 178099   |\n",
      "|    policy_loss        | -0.0202  |\n",
      "|    value_loss         | 0.0152   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14250000, episode_reward=326.60 +/- 109.47\n",
      "Episode length: 8297.80 +/- 2141.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.3e+03  |\n",
      "|    mean_reward        | 327      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14250000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 178124   |\n",
      "|    policy_loss        | -0.0242  |\n",
      "|    value_loss         | 0.0275   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 415      |\n",
      "| time/                 |          |\n",
      "|    fps                | 419      |\n",
      "|    iterations         | 178200   |\n",
      "|    time_elapsed       | 33987    |\n",
      "|    total_timesteps    | 14256000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 178199   |\n",
      "|    policy_loss        | -0.0337  |\n",
      "|    value_loss         | 0.027    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14260000, episode_reward=391.20 +/- 28.95\n",
      "Episode length: 11940.80 +/- 4968.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.19e+04 |\n",
      "|    mean_reward        | 391      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14260000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 178249   |\n",
      "|    policy_loss        | -0.017   |\n",
      "|    value_loss         | 0.0443   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 415      |\n",
      "| time/                 |          |\n",
      "|    fps                | 419      |\n",
      "|    iterations         | 178300   |\n",
      "|    time_elapsed       | 34026    |\n",
      "|    total_timesteps    | 14264000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 178299   |\n",
      "|    policy_loss        | -0.109   |\n",
      "|    value_loss         | 0.0859   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14270000, episode_reward=343.00 +/- 98.88\n",
      "Episode length: 8802.80 +/- 1435.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.8e+03  |\n",
      "|    mean_reward        | 343      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14270000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 178374   |\n",
      "|    policy_loss        | 0.0301   |\n",
      "|    value_loss         | 0.0659   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 412      |\n",
      "| time/                 |          |\n",
      "|    fps                | 419      |\n",
      "|    iterations         | 178400   |\n",
      "|    time_elapsed       | 34058    |\n",
      "|    total_timesteps    | 14272000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.963   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 178399   |\n",
      "|    policy_loss        | -0.042   |\n",
      "|    value_loss         | 0.0253   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14280000, episode_reward=393.40 +/- 48.47\n",
      "Episode length: 10840.80 +/- 1869.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+04 |\n",
      "|    mean_reward        | 393      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14280000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 178499   |\n",
      "|    policy_loss        | -0.0589  |\n",
      "|    value_loss         | 0.0685   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.27e+04 |\n",
      "|    ep_rew_mean     | 416      |\n",
      "| time/              |          |\n",
      "|    fps             | 418      |\n",
      "|    iterations      | 178500   |\n",
      "|    time_elapsed    | 34093    |\n",
      "|    total_timesteps | 14280000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 416      |\n",
      "| time/                 |          |\n",
      "|    fps                | 418      |\n",
      "|    iterations         | 178600   |\n",
      "|    time_elapsed       | 34100    |\n",
      "|    total_timesteps    | 14288000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 178599   |\n",
      "|    policy_loss        | 0.0148   |\n",
      "|    value_loss         | 0.0542   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14290000, episode_reward=391.00 +/- 32.30\n",
      "Episode length: 9833.20 +/- 2223.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.83e+03 |\n",
      "|    mean_reward        | 391      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14290000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 178624   |\n",
      "|    policy_loss        | 0.105    |\n",
      "|    value_loss         | 0.406    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | 418      |\n",
      "| time/                 |          |\n",
      "|    fps                | 418      |\n",
      "|    iterations         | 178700   |\n",
      "|    time_elapsed       | 34136    |\n",
      "|    total_timesteps    | 14296000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.986   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 178699   |\n",
      "|    policy_loss        | -0.00916 |\n",
      "|    value_loss         | 0.0203   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14300000, episode_reward=412.80 +/- 29.44\n",
      "Episode length: 30223.80 +/- 38927.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.02e+04 |\n",
      "|    mean_reward        | 413      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14300000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 178749   |\n",
      "|    policy_loss        | 0.0979   |\n",
      "|    value_loss         | 0.0584   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | 418      |\n",
      "| time/                 |          |\n",
      "|    fps                | 417      |\n",
      "|    iterations         | 178800   |\n",
      "|    time_elapsed       | 34222    |\n",
      "|    total_timesteps    | 14304000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 178799   |\n",
      "|    policy_loss        | 0.0522   |\n",
      "|    value_loss         | 0.271    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14310000, episode_reward=565.40 +/- 192.29\n",
      "Episode length: 13585.60 +/- 4579.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.36e+04 |\n",
      "|    mean_reward        | 565      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14310000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.966   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 178874   |\n",
      "|    policy_loss        | -0.0272  |\n",
      "|    value_loss         | 0.0462   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | 418      |\n",
      "| time/                 |          |\n",
      "|    fps                | 417      |\n",
      "|    iterations         | 178900   |\n",
      "|    time_elapsed       | 34268    |\n",
      "|    total_timesteps    | 14312000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.873   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 178899   |\n",
      "|    policy_loss        | 0.00352  |\n",
      "|    value_loss         | 0.0241   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14320000, episode_reward=384.40 +/- 31.16\n",
      "Episode length: 9765.80 +/- 709.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.77e+03 |\n",
      "|    mean_reward        | 384      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14320000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.999   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 178999   |\n",
      "|    policy_loss        | -0.0478  |\n",
      "|    value_loss         | 0.151    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.24e+04 |\n",
      "|    ep_rew_mean     | 418      |\n",
      "| time/              |          |\n",
      "|    fps             | 417      |\n",
      "|    iterations      | 179000   |\n",
      "|    time_elapsed    | 34301    |\n",
      "|    total_timesteps | 14320000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 418      |\n",
      "| time/                 |          |\n",
      "|    fps                | 417      |\n",
      "|    iterations         | 179100   |\n",
      "|    time_elapsed       | 34309    |\n",
      "|    total_timesteps    | 14328000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.893    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 179099   |\n",
      "|    policy_loss        | -0.33    |\n",
      "|    value_loss         | 0.462    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14330000, episode_reward=412.00 +/- 13.61\n",
      "Episode length: 16968.00 +/- 6746.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.7e+04  |\n",
      "|    mean_reward        | 412      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14330000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.972   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 179124   |\n",
      "|    policy_loss        | -0.0301  |\n",
      "|    value_loss         | 0.056    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 419      |\n",
      "| time/                 |          |\n",
      "|    fps                | 417      |\n",
      "|    iterations         | 179200   |\n",
      "|    time_elapsed       | 34360    |\n",
      "|    total_timesteps    | 14336000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.985   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 179199   |\n",
      "|    policy_loss        | 0.0473   |\n",
      "|    value_loss         | 0.0551   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14340000, episode_reward=395.60 +/- 245.60\n",
      "Episode length: 13631.40 +/- 9251.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.36e+04 |\n",
      "|    mean_reward        | 396      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14340000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 179249   |\n",
      "|    policy_loss        | 0.0366   |\n",
      "|    value_loss         | 0.0515   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 423      |\n",
      "| time/                 |          |\n",
      "|    fps                | 416      |\n",
      "|    iterations         | 179300   |\n",
      "|    time_elapsed       | 34404    |\n",
      "|    total_timesteps    | 14344000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.986   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 179299   |\n",
      "|    policy_loss        | -0.0822  |\n",
      "|    value_loss         | 0.144    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14350000, episode_reward=386.00 +/- 39.60\n",
      "Episode length: 10693.20 +/- 828.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.07e+04 |\n",
      "|    mean_reward        | 386      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14350000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.926   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 179374   |\n",
      "|    policy_loss        | -0.0302  |\n",
      "|    value_loss         | 0.0587   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 423      |\n",
      "| time/                 |          |\n",
      "|    fps                | 416      |\n",
      "|    iterations         | 179400   |\n",
      "|    time_elapsed       | 34442    |\n",
      "|    total_timesteps    | 14352000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.965   |\n",
      "|    explained_variance | 0.72     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 179399   |\n",
      "|    policy_loss        | -0.415   |\n",
      "|    value_loss         | 1.53     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14360000, episode_reward=305.40 +/- 116.89\n",
      "Episode length: 7526.20 +/- 1492.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.53e+03 |\n",
      "|    mean_reward        | 305      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14360000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.911   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 179499   |\n",
      "|    policy_loss        | 0.0371   |\n",
      "|    value_loss         | 0.0669   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.28e+04 |\n",
      "|    ep_rew_mean     | 426      |\n",
      "| time/              |          |\n",
      "|    fps             | 416      |\n",
      "|    iterations      | 179500   |\n",
      "|    time_elapsed    | 34466    |\n",
      "|    total_timesteps | 14360000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 427      |\n",
      "| time/                 |          |\n",
      "|    fps                | 416      |\n",
      "|    iterations         | 179600   |\n",
      "|    time_elapsed       | 34474    |\n",
      "|    total_timesteps    | 14368000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.991   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 179599   |\n",
      "|    policy_loss        | 0.00773  |\n",
      "|    value_loss         | 0.0152   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14370000, episode_reward=410.20 +/- 246.47\n",
      "Episode length: 13300.40 +/- 5129.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.33e+04 |\n",
      "|    mean_reward        | 410      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14370000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 179624   |\n",
      "|    policy_loss        | -0.0108  |\n",
      "|    value_loss         | 0.0281   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 427      |\n",
      "| time/                 |          |\n",
      "|    fps                | 416      |\n",
      "|    iterations         | 179700   |\n",
      "|    time_elapsed       | 34519    |\n",
      "|    total_timesteps    | 14376000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.903   |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 179699   |\n",
      "|    policy_loss        | -0.0277  |\n",
      "|    value_loss         | 0.0247   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14380000, episode_reward=508.40 +/- 177.46\n",
      "Episode length: 16929.00 +/- 4025.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.69e+04 |\n",
      "|    mean_reward        | 508      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14380000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.929   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 179749   |\n",
      "|    policy_loss        | 0.0435   |\n",
      "|    value_loss         | 0.027    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 430      |\n",
      "| time/                 |          |\n",
      "|    fps                | 416      |\n",
      "|    iterations         | 179800   |\n",
      "|    time_elapsed       | 34573    |\n",
      "|    total_timesteps    | 14384000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.993   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 179799   |\n",
      "|    policy_loss        | 0.0441   |\n",
      "|    value_loss         | 0.0235   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14390000, episode_reward=427.00 +/- 28.87\n",
      "Episode length: 14010.60 +/- 6039.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.4e+04  |\n",
      "|    mean_reward        | 427      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14390000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 179874   |\n",
      "|    policy_loss        | -0.0729  |\n",
      "|    value_loss         | 0.0284   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 427      |\n",
      "| time/                 |          |\n",
      "|    fps                | 415      |\n",
      "|    iterations         | 179900   |\n",
      "|    time_elapsed       | 34618    |\n",
      "|    total_timesteps    | 14392000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.966   |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 179899   |\n",
      "|    policy_loss        | -0.109   |\n",
      "|    value_loss         | 0.504    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14400000, episode_reward=421.20 +/- 11.65\n",
      "Episode length: 11518.00 +/- 1489.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.15e+04 |\n",
      "|    mean_reward        | 421      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14400000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.872   |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 179999   |\n",
      "|    policy_loss        | 0.0762   |\n",
      "|    value_loss         | 0.147    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+04 |\n",
      "|    ep_rew_mean     | 426      |\n",
      "| time/              |          |\n",
      "|    fps             | 415      |\n",
      "|    iterations      | 180000   |\n",
      "|    time_elapsed    | 34654    |\n",
      "|    total_timesteps | 14400000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.16e+04 |\n",
      "|    ep_rew_mean        | 427      |\n",
      "| time/                 |          |\n",
      "|    fps                | 415      |\n",
      "|    iterations         | 180100   |\n",
      "|    time_elapsed       | 34662    |\n",
      "|    total_timesteps    | 14408000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.994   |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 180099   |\n",
      "|    policy_loss        | -0.158   |\n",
      "|    value_loss         | 0.297    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14410000, episode_reward=407.20 +/- 30.29\n",
      "Episode length: 13225.40 +/- 7792.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.32e+04 |\n",
      "|    mean_reward        | 407      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14410000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.867   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 180124   |\n",
      "|    policy_loss        | 0.0722   |\n",
      "|    value_loss         | 0.0269   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.17e+04 |\n",
      "|    ep_rew_mean        | 428      |\n",
      "| time/                 |          |\n",
      "|    fps                | 415      |\n",
      "|    iterations         | 180200   |\n",
      "|    time_elapsed       | 34705    |\n",
      "|    total_timesteps    | 14416000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 180199   |\n",
      "|    policy_loss        | 0.0108   |\n",
      "|    value_loss         | 0.0289   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14420000, episode_reward=382.00 +/- 53.82\n",
      "Episode length: 10501.20 +/- 2903.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+04 |\n",
      "|    mean_reward        | 382      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14420000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.881   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 180249   |\n",
      "|    policy_loss        | 0.0226   |\n",
      "|    value_loss         | 0.0238   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.17e+04 |\n",
      "|    ep_rew_mean        | 428      |\n",
      "| time/                 |          |\n",
      "|    fps                | 415      |\n",
      "|    iterations         | 180300   |\n",
      "|    time_elapsed       | 34742    |\n",
      "|    total_timesteps    | 14424000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.856   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 180299   |\n",
      "|    policy_loss        | 0.023    |\n",
      "|    value_loss         | 0.048    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14430000, episode_reward=420.40 +/- 15.42\n",
      "Episode length: 11231.40 +/- 1597.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 420      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14430000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 180374   |\n",
      "|    policy_loss        | 0.0241   |\n",
      "|    value_loss         | 0.032    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.17e+04 |\n",
      "|    ep_rew_mean        | 428      |\n",
      "| time/                 |          |\n",
      "|    fps                | 414      |\n",
      "|    iterations         | 180400   |\n",
      "|    time_elapsed       | 34777    |\n",
      "|    total_timesteps    | 14432000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 180399   |\n",
      "|    policy_loss        | -0.0308  |\n",
      "|    value_loss         | 0.0932   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14440000, episode_reward=420.80 +/- 5.00\n",
      "Episode length: 12472.60 +/- 1568.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.25e+04 |\n",
      "|    mean_reward        | 421      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14440000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.86    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 180499   |\n",
      "|    policy_loss        | -0.0182  |\n",
      "|    value_loss         | 0.0255   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.16e+04 |\n",
      "|    ep_rew_mean     | 427      |\n",
      "| time/              |          |\n",
      "|    fps             | 414      |\n",
      "|    iterations      | 180500   |\n",
      "|    time_elapsed    | 34819    |\n",
      "|    total_timesteps | 14440000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.17e+04 |\n",
      "|    ep_rew_mean        | 429      |\n",
      "| time/                 |          |\n",
      "|    fps                | 414      |\n",
      "|    iterations         | 180600   |\n",
      "|    time_elapsed       | 34826    |\n",
      "|    total_timesteps    | 14448000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 180599   |\n",
      "|    policy_loss        | -0.0584  |\n",
      "|    value_loss         | 0.0552   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14450000, episode_reward=472.80 +/- 182.11\n",
      "Episode length: 17480.00 +/- 7193.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.75e+04 |\n",
      "|    mean_reward        | 473      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14450000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 180624   |\n",
      "|    policy_loss        | -0.0526  |\n",
      "|    value_loss         | 0.0174   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.17e+04 |\n",
      "|    ep_rew_mean        | 425      |\n",
      "| time/                 |          |\n",
      "|    fps                | 414      |\n",
      "|    iterations         | 180700   |\n",
      "|    time_elapsed       | 34883    |\n",
      "|    total_timesteps    | 14456000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 180699   |\n",
      "|    policy_loss        | 0.114    |\n",
      "|    value_loss         | 0.191    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14460000, episode_reward=430.20 +/- 3.19\n",
      "Episode length: 16303.80 +/- 4835.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.63e+04 |\n",
      "|    mean_reward        | 430      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14460000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 180749   |\n",
      "|    policy_loss        | -0.162   |\n",
      "|    value_loss         | 0.176    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.17e+04 |\n",
      "|    ep_rew_mean        | 425      |\n",
      "| time/                 |          |\n",
      "|    fps                | 414      |\n",
      "|    iterations         | 180800   |\n",
      "|    time_elapsed       | 34935    |\n",
      "|    total_timesteps    | 14464000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 180799   |\n",
      "|    policy_loss        | -0.0417  |\n",
      "|    value_loss         | 0.0576   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14470000, episode_reward=470.40 +/- 139.96\n",
      "Episode length: 11245.20 +/- 3458.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 470      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14470000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 180874   |\n",
      "|    policy_loss        | -0.0984  |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.18e+04 |\n",
      "|    ep_rew_mean        | 425      |\n",
      "| time/                 |          |\n",
      "|    fps                | 413      |\n",
      "|    iterations         | 180900   |\n",
      "|    time_elapsed       | 34973    |\n",
      "|    total_timesteps    | 14472000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 180899   |\n",
      "|    policy_loss        | 0.0659   |\n",
      "|    value_loss         | 0.0906   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14480000, episode_reward=270.00 +/- 140.16\n",
      "Episode length: 7079.80 +/- 2061.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.08e+03 |\n",
      "|    mean_reward        | 270      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14480000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 180999   |\n",
      "|    policy_loss        | 0.0316   |\n",
      "|    value_loss         | 0.014    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.18e+04 |\n",
      "|    ep_rew_mean     | 425      |\n",
      "| time/              |          |\n",
      "|    fps             | 413      |\n",
      "|    iterations      | 181000   |\n",
      "|    time_elapsed    | 34999    |\n",
      "|    total_timesteps | 14480000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.16e+04 |\n",
      "|    ep_rew_mean        | 421      |\n",
      "| time/                 |          |\n",
      "|    fps                | 413      |\n",
      "|    iterations         | 181100   |\n",
      "|    time_elapsed       | 35006    |\n",
      "|    total_timesteps    | 14488000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 181099   |\n",
      "|    policy_loss        | 0.0278   |\n",
      "|    value_loss         | 0.0084   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14490000, episode_reward=509.20 +/- 158.21\n",
      "Episode length: 12811.80 +/- 3362.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.28e+04 |\n",
      "|    mean_reward        | 509      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14490000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.926   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 181124   |\n",
      "|    policy_loss        | 0.00179  |\n",
      "|    value_loss         | 0.0625   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.17e+04 |\n",
      "|    ep_rew_mean        | 421      |\n",
      "| time/                 |          |\n",
      "|    fps                | 413      |\n",
      "|    iterations         | 181200   |\n",
      "|    time_elapsed       | 35050    |\n",
      "|    total_timesteps    | 14496000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.948   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 181199   |\n",
      "|    policy_loss        | 0.0532   |\n",
      "|    value_loss         | 0.247    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14500000, episode_reward=422.60 +/- 9.20\n",
      "Episode length: 11266.00 +/- 1366.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+04 |\n",
      "|    mean_reward        | 423      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14500000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.848   |\n",
      "|    explained_variance | 0.886    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 181249   |\n",
      "|    policy_loss        | -0.0927  |\n",
      "|    value_loss         | 0.616    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.2e+04  |\n",
      "|    ep_rew_mean        | 420      |\n",
      "| time/                 |          |\n",
      "|    fps                | 413      |\n",
      "|    iterations         | 181300   |\n",
      "|    time_elapsed       | 35086    |\n",
      "|    total_timesteps    | 14504000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 181299   |\n",
      "|    policy_loss        | 0.0593   |\n",
      "|    value_loss         | 0.0806   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14510000, episode_reward=195.80 +/- 108.99\n",
      "Episode length: 11418.40 +/- 10439.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+04 |\n",
      "|    mean_reward        | 196      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14510000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 181374   |\n",
      "|    policy_loss        | 0.00873  |\n",
      "|    value_loss         | 0.0265   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.2e+04  |\n",
      "|    ep_rew_mean        | 420      |\n",
      "| time/                 |          |\n",
      "|    fps                | 413      |\n",
      "|    iterations         | 181400   |\n",
      "|    time_elapsed       | 35124    |\n",
      "|    total_timesteps    | 14512000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 181399   |\n",
      "|    policy_loss        | -0.124   |\n",
      "|    value_loss         | 0.103    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14520000, episode_reward=427.80 +/- 3.66\n",
      "Episode length: 11649.00 +/- 2249.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+04 |\n",
      "|    mean_reward        | 428      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14520000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 181499   |\n",
      "|    policy_loss        | 0.0332   |\n",
      "|    value_loss         | 0.0689   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.19e+04 |\n",
      "|    ep_rew_mean     | 420      |\n",
      "| time/              |          |\n",
      "|    fps             | 412      |\n",
      "|    iterations      | 181500   |\n",
      "|    time_elapsed    | 35164    |\n",
      "|    total_timesteps | 14520000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.19e+04 |\n",
      "|    ep_rew_mean        | 417      |\n",
      "| time/                 |          |\n",
      "|    fps                | 413      |\n",
      "|    iterations         | 181600   |\n",
      "|    time_elapsed       | 35172    |\n",
      "|    total_timesteps    | 14528000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 181599   |\n",
      "|    policy_loss        | 0.0238   |\n",
      "|    value_loss         | 0.0147   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14530000, episode_reward=383.80 +/- 75.25\n",
      "Episode length: 10194.40 +/- 2020.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+04 |\n",
      "|    mean_reward        | 384      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14530000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 181624   |\n",
      "|    policy_loss        | -0.0126  |\n",
      "|    value_loss         | 0.0838   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.2e+04  |\n",
      "|    ep_rew_mean        | 417      |\n",
      "| time/                 |          |\n",
      "|    fps                | 412      |\n",
      "|    iterations         | 181700   |\n",
      "|    time_elapsed       | 35205    |\n",
      "|    total_timesteps    | 14536000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 181699   |\n",
      "|    policy_loss        | 0.038    |\n",
      "|    value_loss         | 0.0308   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14540000, episode_reward=416.80 +/- 8.40\n",
      "Episode length: 11313.00 +/- 1317.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+04 |\n",
      "|    mean_reward        | 417      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14540000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 181749   |\n",
      "|    policy_loss        | -0.0204  |\n",
      "|    value_loss         | 0.0871   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.2e+04  |\n",
      "|    ep_rew_mean        | 417      |\n",
      "| time/                 |          |\n",
      "|    fps                | 412      |\n",
      "|    iterations         | 181800   |\n",
      "|    time_elapsed       | 35244    |\n",
      "|    total_timesteps    | 14544000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 181799   |\n",
      "|    policy_loss        | 0.0929   |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14550000, episode_reward=502.00 +/- 175.26\n",
      "Episode length: 18196.80 +/- 9227.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.82e+04 |\n",
      "|    mean_reward        | 502      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14550000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.963   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 181874   |\n",
      "|    policy_loss        | 0.0024   |\n",
      "|    value_loss         | 0.0257   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.21e+04 |\n",
      "|    ep_rew_mean        | 419      |\n",
      "| time/                 |          |\n",
      "|    fps                | 412      |\n",
      "|    iterations         | 181900   |\n",
      "|    time_elapsed       | 35302    |\n",
      "|    total_timesteps    | 14552000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.998   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 181899   |\n",
      "|    policy_loss        | -0.0485  |\n",
      "|    value_loss         | 0.0354   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14560000, episode_reward=470.80 +/- 124.75\n",
      "Episode length: 12597.80 +/- 5554.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.26e+04 |\n",
      "|    mean_reward        | 471      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14560000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.969   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 181999   |\n",
      "|    policy_loss        | -0.053   |\n",
      "|    value_loss         | 0.0408   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.22e+04 |\n",
      "|    ep_rew_mean     | 424      |\n",
      "| time/              |          |\n",
      "|    fps             | 411      |\n",
      "|    iterations      | 182000   |\n",
      "|    time_elapsed    | 35346    |\n",
      "|    total_timesteps | 14560000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | 424      |\n",
      "| time/                 |          |\n",
      "|    fps                | 412      |\n",
      "|    iterations         | 182100   |\n",
      "|    time_elapsed       | 35353    |\n",
      "|    total_timesteps    | 14568000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 182099   |\n",
      "|    policy_loss        | -0.0146  |\n",
      "|    value_loss         | 0.023    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14570000, episode_reward=422.20 +/- 31.53\n",
      "Episode length: 11759.00 +/- 2374.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.18e+04 |\n",
      "|    mean_reward        | 422      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14570000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 182124   |\n",
      "|    policy_loss        | -0.0664  |\n",
      "|    value_loss         | 0.0254   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | 424      |\n",
      "| time/                 |          |\n",
      "|    fps                | 411      |\n",
      "|    iterations         | 182200   |\n",
      "|    time_elapsed       | 35391    |\n",
      "|    total_timesteps    | 14576000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.944   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 182199   |\n",
      "|    policy_loss        | 0.0759   |\n",
      "|    value_loss         | 0.0965   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14580000, episode_reward=429.60 +/- 23.22\n",
      "Episode length: 10580.20 +/- 891.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+04 |\n",
      "|    mean_reward        | 430      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14580000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 182249   |\n",
      "|    policy_loss        | 0.087    |\n",
      "|    value_loss         | 0.0395   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+04 |\n",
      "|    ep_rew_mean        | 430      |\n",
      "| time/                 |          |\n",
      "|    fps                | 411      |\n",
      "|    iterations         | 182300   |\n",
      "|    time_elapsed       | 35426    |\n",
      "|    total_timesteps    | 14584000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 182299   |\n",
      "|    policy_loss        | -0.00257 |\n",
      "|    value_loss         | 0.165    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14590000, episode_reward=395.00 +/- 34.22\n",
      "Episode length: 10168.60 +/- 1637.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+04 |\n",
      "|    mean_reward        | 395      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14590000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 182374   |\n",
      "|    policy_loss        | -0.00808 |\n",
      "|    value_loss         | 0.0484   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | 428      |\n",
      "| time/                 |          |\n",
      "|    fps                | 411      |\n",
      "|    iterations         | 182400   |\n",
      "|    time_elapsed       | 35463    |\n",
      "|    total_timesteps    | 14592000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.956   |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 182399   |\n",
      "|    policy_loss        | -0.0117  |\n",
      "|    value_loss         | 0.433    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14600000, episode_reward=471.00 +/- 118.83\n",
      "Episode length: 12244.40 +/- 2735.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.22e+04 |\n",
      "|    mean_reward        | 471      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14600000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 182499   |\n",
      "|    policy_loss        | 0.0315   |\n",
      "|    value_loss         | 0.0718   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.22e+04 |\n",
      "|    ep_rew_mean     | 428      |\n",
      "| time/              |          |\n",
      "|    fps             | 411      |\n",
      "|    iterations      | 182500   |\n",
      "|    time_elapsed    | 35502    |\n",
      "|    total_timesteps | 14600000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | 425      |\n",
      "| time/                 |          |\n",
      "|    fps                | 411      |\n",
      "|    iterations         | 182600   |\n",
      "|    time_elapsed       | 35509    |\n",
      "|    total_timesteps    | 14608000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.879   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 182599   |\n",
      "|    policy_loss        | 0.0149   |\n",
      "|    value_loss         | 0.0212   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14610000, episode_reward=337.80 +/- 147.11\n",
      "Episode length: 15694.00 +/- 12651.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.57e+04 |\n",
      "|    mean_reward        | 338      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14610000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 182624   |\n",
      "|    policy_loss        | 0.0158   |\n",
      "|    value_loss         | 0.179    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 429      |\n",
      "| time/                 |          |\n",
      "|    fps                | 411      |\n",
      "|    iterations         | 182700   |\n",
      "|    time_elapsed       | 35558    |\n",
      "|    total_timesteps    | 14616000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.887   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 182699   |\n",
      "|    policy_loss        | -0.0783  |\n",
      "|    value_loss         | 0.075    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14620000, episode_reward=394.80 +/- 31.39\n",
      "Episode length: 12291.00 +/- 6062.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.23e+04 |\n",
      "|    mean_reward        | 395      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14620000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.964   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 182749   |\n",
      "|    policy_loss        | -0.0188  |\n",
      "|    value_loss         | 0.0407   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | 428      |\n",
      "| time/                 |          |\n",
      "|    fps                | 410      |\n",
      "|    iterations         | 182800   |\n",
      "|    time_elapsed       | 35598    |\n",
      "|    total_timesteps    | 14624000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 182799   |\n",
      "|    policy_loss        | -0.151   |\n",
      "|    value_loss         | 0.0824   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14630000, episode_reward=506.00 +/- 167.88\n",
      "Episode length: 17730.00 +/- 5755.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.77e+04 |\n",
      "|    mean_reward        | 506      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14630000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 182874   |\n",
      "|    policy_loss        | -0.0271  |\n",
      "|    value_loss         | 0.0509   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 428      |\n",
      "| time/                 |          |\n",
      "|    fps                | 410      |\n",
      "|    iterations         | 182900   |\n",
      "|    time_elapsed       | 35653    |\n",
      "|    total_timesteps    | 14632000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 182899   |\n",
      "|    policy_loss        | 0.0545   |\n",
      "|    value_loss         | 0.0311   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14640000, episode_reward=509.00 +/- 178.26\n",
      "Episode length: 12933.40 +/- 3638.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.29e+04 |\n",
      "|    mean_reward        | 509      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14640000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.919   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 182999   |\n",
      "|    policy_loss        | -0.109   |\n",
      "|    value_loss         | 0.119    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.23e+04 |\n",
      "|    ep_rew_mean     | 426      |\n",
      "| time/              |          |\n",
      "|    fps             | 410      |\n",
      "|    iterations      | 183000   |\n",
      "|    time_elapsed    | 35698    |\n",
      "|    total_timesteps | 14640000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+04 |\n",
      "|    ep_rew_mean        | 427      |\n",
      "| time/                 |          |\n",
      "|    fps                | 410      |\n",
      "|    iterations         | 183100   |\n",
      "|    time_elapsed       | 35706    |\n",
      "|    total_timesteps    | 14648000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 183099   |\n",
      "|    policy_loss        | -0.0269  |\n",
      "|    value_loss         | 0.0364   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14650000, episode_reward=478.20 +/- 181.10\n",
      "Episode length: 14375.20 +/- 5083.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.44e+04 |\n",
      "|    mean_reward        | 478      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14650000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 183124   |\n",
      "|    policy_loss        | 0.00567  |\n",
      "|    value_loss         | 0.0231   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 423      |\n",
      "| time/                 |          |\n",
      "|    fps                | 409      |\n",
      "|    iterations         | 183200   |\n",
      "|    time_elapsed       | 35752    |\n",
      "|    total_timesteps    | 14656000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 183199   |\n",
      "|    policy_loss        | 0.026    |\n",
      "|    value_loss         | 0.0502   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14660000, episode_reward=409.60 +/- 18.46\n",
      "Episode length: 13434.60 +/- 5485.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.34e+04 |\n",
      "|    mean_reward        | 410      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14660000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.991   |\n",
      "|    explained_variance | 0.824    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 183249   |\n",
      "|    policy_loss        | -0.176   |\n",
      "|    value_loss         | 0.639    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+04 |\n",
      "|    ep_rew_mean        | 428      |\n",
      "| time/                 |          |\n",
      "|    fps                | 409      |\n",
      "|    iterations         | 183300   |\n",
      "|    time_elapsed       | 35793    |\n",
      "|    total_timesteps    | 14664000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.881   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 183299   |\n",
      "|    policy_loss        | -0.0726  |\n",
      "|    value_loss         | 0.0159   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14670000, episode_reward=408.60 +/- 34.34\n",
      "Episode length: 9561.80 +/- 1849.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.56e+03 |\n",
      "|    mean_reward        | 409      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14670000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.946   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 183374   |\n",
      "|    policy_loss        | 0.0896   |\n",
      "|    value_loss         | 0.0696   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+04 |\n",
      "|    ep_rew_mean        | 428      |\n",
      "| time/                 |          |\n",
      "|    fps                | 409      |\n",
      "|    iterations         | 183400   |\n",
      "|    time_elapsed       | 35826    |\n",
      "|    total_timesteps    | 14672000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.92    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 183399   |\n",
      "|    policy_loss        | -0.0719  |\n",
      "|    value_loss         | 0.0306   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14680000, episode_reward=429.00 +/- 15.28\n",
      "Episode length: 11252.00 +/- 2079.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+04 |\n",
      "|    mean_reward        | 429      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14680000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.881   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 183499   |\n",
      "|    policy_loss        | -0.0389  |\n",
      "|    value_loss         | 0.0236   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+04 |\n",
      "|    ep_rew_mean     | 429      |\n",
      "| time/              |          |\n",
      "|    fps             | 409      |\n",
      "|    iterations      | 183500   |\n",
      "|    time_elapsed    | 35866    |\n",
      "|    total_timesteps | 14680000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 429      |\n",
      "| time/                 |          |\n",
      "|    fps                | 409      |\n",
      "|    iterations         | 183600   |\n",
      "|    time_elapsed       | 35874    |\n",
      "|    total_timesteps    | 14688000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 183599   |\n",
      "|    policy_loss        | -0.22    |\n",
      "|    value_loss         | 0.199    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14690000, episode_reward=425.60 +/- 15.55\n",
      "Episode length: 12406.00 +/- 1647.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.24e+04 |\n",
      "|    mean_reward        | 426      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14690000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 183624   |\n",
      "|    policy_loss        | -0.00757 |\n",
      "|    value_loss         | 0.077    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 429      |\n",
      "| time/                 |          |\n",
      "|    fps                | 409      |\n",
      "|    iterations         | 183700   |\n",
      "|    time_elapsed       | 35913    |\n",
      "|    total_timesteps    | 14696000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.964   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 183699   |\n",
      "|    policy_loss        | 0.0219   |\n",
      "|    value_loss         | 0.0242   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14700000, episode_reward=390.40 +/- 30.34\n",
      "Episode length: 9437.60 +/- 1047.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.44e+03 |\n",
      "|    mean_reward        | 390      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14700000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.979   |\n",
      "|    explained_variance | 0.826    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 183749   |\n",
      "|    policy_loss        | -0.0571  |\n",
      "|    value_loss         | 1.24     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 429      |\n",
      "| time/                 |          |\n",
      "|    fps                | 409      |\n",
      "|    iterations         | 183800   |\n",
      "|    time_elapsed       | 35945    |\n",
      "|    total_timesteps    | 14704000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 183799   |\n",
      "|    policy_loss        | -0.154   |\n",
      "|    value_loss         | 0.062    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14710000, episode_reward=477.40 +/- 136.45\n",
      "Episode length: 12438.80 +/- 4050.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.24e+04 |\n",
      "|    mean_reward        | 477      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14710000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 183874   |\n",
      "|    policy_loss        | -0.0497  |\n",
      "|    value_loss         | 0.0101   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 429      |\n",
      "| time/                 |          |\n",
      "|    fps                | 408      |\n",
      "|    iterations         | 183900   |\n",
      "|    time_elapsed       | 35988    |\n",
      "|    total_timesteps    | 14712000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 183899   |\n",
      "|    policy_loss        | -0.0129  |\n",
      "|    value_loss         | 0.0229   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14720000, episode_reward=388.20 +/- 77.87\n",
      "Episode length: 10794.20 +/- 2337.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+04 |\n",
      "|    mean_reward        | 388      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14720000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 183999   |\n",
      "|    policy_loss        | -0.00661 |\n",
      "|    value_loss         | 0.0628   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.28e+04 |\n",
      "|    ep_rew_mean     | 431      |\n",
      "| time/              |          |\n",
      "|    fps             | 408      |\n",
      "|    iterations      | 184000   |\n",
      "|    time_elapsed    | 36023    |\n",
      "|    total_timesteps | 14720000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 431      |\n",
      "| time/                 |          |\n",
      "|    fps                | 408      |\n",
      "|    iterations         | 184100   |\n",
      "|    time_elapsed       | 36031    |\n",
      "|    total_timesteps    | 14728000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 184099   |\n",
      "|    policy_loss        | -0.0245  |\n",
      "|    value_loss         | 0.0389   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14730000, episode_reward=397.60 +/- 27.70\n",
      "Episode length: 12859.80 +/- 6041.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.29e+04 |\n",
      "|    mean_reward        | 398      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14730000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.699    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 184124   |\n",
      "|    policy_loss        | -0.131   |\n",
      "|    value_loss         | 1.34     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 431      |\n",
      "| time/                 |          |\n",
      "|    fps                | 408      |\n",
      "|    iterations         | 184200   |\n",
      "|    time_elapsed       | 36071    |\n",
      "|    total_timesteps    | 14736000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.971   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 184199   |\n",
      "|    policy_loss        | 0.0114   |\n",
      "|    value_loss         | 0.0277   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14740000, episode_reward=477.80 +/- 127.24\n",
      "Episode length: 12095.40 +/- 2915.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.21e+04 |\n",
      "|    mean_reward        | 478      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14740000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 184249   |\n",
      "|    policy_loss        | -0.027   |\n",
      "|    value_loss         | 0.0916   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 431      |\n",
      "| time/                 |          |\n",
      "|    fps                | 408      |\n",
      "|    iterations         | 184300   |\n",
      "|    time_elapsed       | 36114    |\n",
      "|    total_timesteps    | 14744000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.991   |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 184299   |\n",
      "|    policy_loss        | 0.217    |\n",
      "|    value_loss         | 0.745    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14750000, episode_reward=392.60 +/- 24.48\n",
      "Episode length: 11526.80 +/- 4863.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.15e+04 |\n",
      "|    mean_reward        | 393      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14750000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 184374   |\n",
      "|    policy_loss        | -0.0149  |\n",
      "|    value_loss         | 0.0266   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 431      |\n",
      "| time/                 |          |\n",
      "|    fps                | 408      |\n",
      "|    iterations         | 184400   |\n",
      "|    time_elapsed       | 36153    |\n",
      "|    total_timesteps    | 14752000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.865   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 184399   |\n",
      "|    policy_loss        | -0.0429  |\n",
      "|    value_loss         | 0.00996  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14760000, episode_reward=421.40 +/- 8.40\n",
      "Episode length: 10205.60 +/- 1212.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+04 |\n",
      "|    mean_reward        | 421      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14760000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 184499   |\n",
      "|    policy_loss        | -0.00935 |\n",
      "|    value_loss         | 0.019    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.28e+04 |\n",
      "|    ep_rew_mean     | 432      |\n",
      "| time/              |          |\n",
      "|    fps             | 407      |\n",
      "|    iterations      | 184500   |\n",
      "|    time_elapsed    | 36186    |\n",
      "|    total_timesteps | 14760000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.28e+04 |\n",
      "|    ep_rew_mean        | 433      |\n",
      "| time/                 |          |\n",
      "|    fps                | 408      |\n",
      "|    iterations         | 184600   |\n",
      "|    time_elapsed       | 36194    |\n",
      "|    total_timesteps    | 14768000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.926   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 184599   |\n",
      "|    policy_loss        | -0.0389  |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14770000, episode_reward=376.40 +/- 39.32\n",
      "Episode length: 9404.40 +/- 1982.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.4e+03  |\n",
      "|    mean_reward        | 376      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14770000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 184624   |\n",
      "|    policy_loss        | -0.108   |\n",
      "|    value_loss         | 0.0508   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.29e+04 |\n",
      "|    ep_rew_mean        | 435      |\n",
      "| time/                 |          |\n",
      "|    fps                | 407      |\n",
      "|    iterations         | 184700   |\n",
      "|    time_elapsed       | 36228    |\n",
      "|    total_timesteps    | 14776000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 184699   |\n",
      "|    policy_loss        | 0.0487   |\n",
      "|    value_loss         | 0.0995   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14780000, episode_reward=417.60 +/- 11.59\n",
      "Episode length: 14931.20 +/- 6562.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.49e+04 |\n",
      "|    mean_reward        | 418      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14780000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.994   |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 184749   |\n",
      "|    policy_loss        | 0.0433   |\n",
      "|    value_loss         | 0.172    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.3e+04  |\n",
      "|    ep_rew_mean        | 435      |\n",
      "| time/                 |          |\n",
      "|    fps                | 407      |\n",
      "|    iterations         | 184800   |\n",
      "|    time_elapsed       | 36276    |\n",
      "|    total_timesteps    | 14784000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 184799   |\n",
      "|    policy_loss        | -0.11    |\n",
      "|    value_loss         | 0.0319   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14790000, episode_reward=403.40 +/- 27.88\n",
      "Episode length: 10578.00 +/- 3540.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+04 |\n",
      "|    mean_reward        | 403      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14790000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.916   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 184874   |\n",
      "|    policy_loss        | 0.00015  |\n",
      "|    value_loss         | 0.0402   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.3e+04  |\n",
      "|    ep_rew_mean        | 435      |\n",
      "| time/                 |          |\n",
      "|    fps                | 407      |\n",
      "|    iterations         | 184900   |\n",
      "|    time_elapsed       | 36310    |\n",
      "|    total_timesteps    | 14792000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.982   |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 184899   |\n",
      "|    policy_loss        | 0.0971   |\n",
      "|    value_loss         | 0.6      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14800000, episode_reward=480.40 +/- 112.58\n",
      "Episode length: 15835.40 +/- 4483.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.58e+04 |\n",
      "|    mean_reward        | 480      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14800000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 184999   |\n",
      "|    policy_loss        | -0.0221  |\n",
      "|    value_loss         | 0.0294   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+04  |\n",
      "|    ep_rew_mean     | 434      |\n",
      "| time/              |          |\n",
      "|    fps             | 407      |\n",
      "|    iterations      | 185000   |\n",
      "|    time_elapsed    | 36358    |\n",
      "|    total_timesteps | 14800000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.3e+04  |\n",
      "|    ep_rew_mean        | 435      |\n",
      "| time/                 |          |\n",
      "|    fps                | 407      |\n",
      "|    iterations         | 185100   |\n",
      "|    time_elapsed       | 36366    |\n",
      "|    total_timesteps    | 14808000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.961   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 185099   |\n",
      "|    policy_loss        | 0.0648   |\n",
      "|    value_loss         | 0.0673   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14810000, episode_reward=588.60 +/- 199.59\n",
      "Episode length: 15081.20 +/- 6064.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.51e+04 |\n",
      "|    mean_reward        | 589      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14810000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 185124   |\n",
      "|    policy_loss        | -0.013   |\n",
      "|    value_loss         | 0.0125   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.29e+04 |\n",
      "|    ep_rew_mean        | 435      |\n",
      "| time/                 |          |\n",
      "|    fps                | 406      |\n",
      "|    iterations         | 185200   |\n",
      "|    time_elapsed       | 36414    |\n",
      "|    total_timesteps    | 14816000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.781    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 185199   |\n",
      "|    policy_loss        | -0.175   |\n",
      "|    value_loss         | 0.877    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14820000, episode_reward=421.00 +/- 13.67\n",
      "Episode length: 15609.60 +/- 8221.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.56e+04 |\n",
      "|    mean_reward        | 421      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14820000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 185249   |\n",
      "|    policy_loss        | 0.02     |\n",
      "|    value_loss         | 0.04     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 436      |\n",
      "| time/                 |          |\n",
      "|    fps                | 406      |\n",
      "|    iterations         | 185300   |\n",
      "|    time_elapsed       | 36465    |\n",
      "|    total_timesteps    | 14824000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 185299   |\n",
      "|    policy_loss        | 0.0287   |\n",
      "|    value_loss         | 0.0659   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14830000, episode_reward=508.20 +/- 151.01\n",
      "Episode length: 17429.80 +/- 4611.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.74e+04 |\n",
      "|    mean_reward        | 508      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14830000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.92    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 185374   |\n",
      "|    policy_loss        | 0.0272   |\n",
      "|    value_loss         | 0.0392   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 436      |\n",
      "| time/                 |          |\n",
      "|    fps                | 406      |\n",
      "|    iterations         | 185400   |\n",
      "|    time_elapsed       | 36521    |\n",
      "|    total_timesteps    | 14832000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 185399   |\n",
      "|    policy_loss        | -0.0689  |\n",
      "|    value_loss         | 0.313    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14840000, episode_reward=334.40 +/- 112.79\n",
      "Episode length: 8652.60 +/- 2877.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.65e+03 |\n",
      "|    mean_reward        | 334      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14840000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 185499   |\n",
      "|    policy_loss        | -0.0654  |\n",
      "|    value_loss         | 0.0525   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.27e+04 |\n",
      "|    ep_rew_mean     | 439      |\n",
      "| time/              |          |\n",
      "|    fps             | 405      |\n",
      "|    iterations      | 185500   |\n",
      "|    time_elapsed    | 36553    |\n",
      "|    total_timesteps | 14840000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 439      |\n",
      "| time/                 |          |\n",
      "|    fps                | 406      |\n",
      "|    iterations         | 185600   |\n",
      "|    time_elapsed       | 36561    |\n",
      "|    total_timesteps    | 14848000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 185599   |\n",
      "|    policy_loss        | 0.0822   |\n",
      "|    value_loss         | 0.0934   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14850000, episode_reward=410.20 +/- 19.05\n",
      "Episode length: 9604.60 +/- 1525.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.6e+03  |\n",
      "|    mean_reward        | 410      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14850000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 185624   |\n",
      "|    policy_loss        | -0.045   |\n",
      "|    value_loss         | 0.0308   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 438      |\n",
      "| time/                 |          |\n",
      "|    fps                | 405      |\n",
      "|    iterations         | 185700   |\n",
      "|    time_elapsed       | 36592    |\n",
      "|    total_timesteps    | 14856000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.96    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 185699   |\n",
      "|    policy_loss        | -0.0762  |\n",
      "|    value_loss         | 0.0844   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14860000, episode_reward=408.60 +/- 18.47\n",
      "Episode length: 10803.60 +/- 2127.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+04 |\n",
      "|    mean_reward        | 409      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14860000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.997   |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 185749   |\n",
      "|    policy_loss        | -0.0732  |\n",
      "|    value_loss         | 0.241    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 439      |\n",
      "| time/                 |          |\n",
      "|    fps                | 405      |\n",
      "|    iterations         | 185800   |\n",
      "|    time_elapsed       | 36631    |\n",
      "|    total_timesteps    | 14864000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 185799   |\n",
      "|    policy_loss        | -0.131   |\n",
      "|    value_loss         | 0.0616   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14870000, episode_reward=406.80 +/- 18.95\n",
      "Episode length: 10841.60 +/- 1415.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+04 |\n",
      "|    mean_reward        | 407      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14870000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 185874   |\n",
      "|    policy_loss        | -0.144   |\n",
      "|    value_loss         | 0.155    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 439      |\n",
      "| time/                 |          |\n",
      "|    fps                | 405      |\n",
      "|    iterations         | 185900   |\n",
      "|    time_elapsed       | 36666    |\n",
      "|    total_timesteps    | 14872000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.965   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 185899   |\n",
      "|    policy_loss        | 0.101    |\n",
      "|    value_loss         | 0.16     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14880000, episode_reward=352.20 +/- 137.38\n",
      "Episode length: 9479.00 +/- 2616.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.48e+03 |\n",
      "|    mean_reward        | 352      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14880000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 185999   |\n",
      "|    policy_loss        | 0.0392   |\n",
      "|    value_loss         | 0.0468   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.24e+04 |\n",
      "|    ep_rew_mean     | 433      |\n",
      "| time/              |          |\n",
      "|    fps             | 405      |\n",
      "|    iterations      | 186000   |\n",
      "|    time_elapsed    | 36699    |\n",
      "|    total_timesteps | 14880000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 429      |\n",
      "| time/                 |          |\n",
      "|    fps                | 405      |\n",
      "|    iterations         | 186100   |\n",
      "|    time_elapsed       | 36706    |\n",
      "|    total_timesteps    | 14888000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 186099   |\n",
      "|    policy_loss        | 0.0076   |\n",
      "|    value_loss         | 0.0956   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14890000, episode_reward=389.60 +/- 52.96\n",
      "Episode length: 9242.20 +/- 2218.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.24e+03 |\n",
      "|    mean_reward        | 390      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14890000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.971   |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 186124   |\n",
      "|    policy_loss        | -0.0105  |\n",
      "|    value_loss         | 0.0149   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 430      |\n",
      "| time/                 |          |\n",
      "|    fps                | 405      |\n",
      "|    iterations         | 186200   |\n",
      "|    time_elapsed       | 36740    |\n",
      "|    total_timesteps    | 14896000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 186199   |\n",
      "|    policy_loss        | 0.0285   |\n",
      "|    value_loss         | 0.0149   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14900000, episode_reward=400.40 +/- 40.03\n",
      "Episode length: 10494.40 +/- 2240.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+04 |\n",
      "|    mean_reward        | 400      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14900000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 186249   |\n",
      "|    policy_loss        | 0.0425   |\n",
      "|    value_loss         | 0.0456   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 430      |\n",
      "| time/                 |          |\n",
      "|    fps                | 405      |\n",
      "|    iterations         | 186300   |\n",
      "|    time_elapsed       | 36774    |\n",
      "|    total_timesteps    | 14904000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 186299   |\n",
      "|    policy_loss        | -0.0822  |\n",
      "|    value_loss         | 0.0548   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14910000, episode_reward=488.60 +/- 156.04\n",
      "Episode length: 13502.00 +/- 4386.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.35e+04 |\n",
      "|    mean_reward        | 489      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14910000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 186374   |\n",
      "|    policy_loss        | 0.0181   |\n",
      "|    value_loss         | 0.0335   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 430      |\n",
      "| time/                 |          |\n",
      "|    fps                | 405      |\n",
      "|    iterations         | 186400   |\n",
      "|    time_elapsed       | 36817    |\n",
      "|    total_timesteps    | 14912000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.925   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 186399   |\n",
      "|    policy_loss        | -0.0463  |\n",
      "|    value_loss         | 0.0455   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14920000, episode_reward=411.20 +/- 20.67\n",
      "Episode length: 11781.20 +/- 4216.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.18e+04 |\n",
      "|    mean_reward        | 411      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14920000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 186499   |\n",
      "|    policy_loss        | 0.0173   |\n",
      "|    value_loss         | 0.0597   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.23e+04 |\n",
      "|    ep_rew_mean     | 431      |\n",
      "| time/              |          |\n",
      "|    fps             | 404      |\n",
      "|    iterations      | 186500   |\n",
      "|    time_elapsed    | 36858    |\n",
      "|    total_timesteps | 14920000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 426      |\n",
      "| time/                 |          |\n",
      "|    fps                | 404      |\n",
      "|    iterations         | 186600   |\n",
      "|    time_elapsed       | 36866    |\n",
      "|    total_timesteps    | 14928000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 186599   |\n",
      "|    policy_loss        | -0.00875 |\n",
      "|    value_loss         | 0.0413   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14930000, episode_reward=409.80 +/- 20.61\n",
      "Episode length: 30989.80 +/- 38519.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.1e+04  |\n",
      "|    mean_reward        | 410      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14930000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.98    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 186624   |\n",
      "|    policy_loss        | -0.0298  |\n",
      "|    value_loss         | 0.0134   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 427      |\n",
      "| time/                 |          |\n",
      "|    fps                | 404      |\n",
      "|    iterations         | 186700   |\n",
      "|    time_elapsed       | 36956    |\n",
      "|    total_timesteps    | 14936000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 186699   |\n",
      "|    policy_loss        | -0.00869 |\n",
      "|    value_loss         | 0.0313   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14940000, episode_reward=489.60 +/- 181.09\n",
      "Episode length: 12511.40 +/- 5236.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.25e+04 |\n",
      "|    mean_reward        | 490      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14940000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.925   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 186749   |\n",
      "|    policy_loss        | 0.0599   |\n",
      "|    value_loss         | 0.0367   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | 427      |\n",
      "| time/                 |          |\n",
      "|    fps                | 403      |\n",
      "|    iterations         | 186800   |\n",
      "|    time_elapsed       | 36996    |\n",
      "|    total_timesteps    | 14944000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.956   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 186799   |\n",
      "|    policy_loss        | 0.0144   |\n",
      "|    value_loss         | 0.0209   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14950000, episode_reward=437.00 +/- 78.45\n",
      "Episode length: 30760.20 +/- 38751.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.08e+04 |\n",
      "|    mean_reward        | 437      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14950000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 186874   |\n",
      "|    policy_loss        | 0.0873   |\n",
      "|    value_loss         | 0.0485   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | 427      |\n",
      "| time/                 |          |\n",
      "|    fps                | 403      |\n",
      "|    iterations         | 186900   |\n",
      "|    time_elapsed       | 37086    |\n",
      "|    total_timesteps    | 14952000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.991   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 186899   |\n",
      "|    policy_loss        | -0.0629  |\n",
      "|    value_loss         | 0.0274   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14960000, episode_reward=419.80 +/- 8.89\n",
      "Episode length: 15926.60 +/- 3678.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.59e+04 |\n",
      "|    mean_reward        | 420      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14960000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 186999   |\n",
      "|    policy_loss        | 0.072    |\n",
      "|    value_loss         | 0.0402   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.22e+04 |\n",
      "|    ep_rew_mean     | 427      |\n",
      "| time/              |          |\n",
      "|    fps             | 402      |\n",
      "|    iterations      | 187000   |\n",
      "|    time_elapsed    | 37137    |\n",
      "|    total_timesteps | 14960000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+04 |\n",
      "|    ep_rew_mean        | 429      |\n",
      "| time/                 |          |\n",
      "|    fps                | 402      |\n",
      "|    iterations         | 187100   |\n",
      "|    time_elapsed       | 37144    |\n",
      "|    total_timesteps    | 14968000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 187099   |\n",
      "|    policy_loss        | 0.0569   |\n",
      "|    value_loss         | 0.0406   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14970000, episode_reward=251.00 +/- 185.13\n",
      "Episode length: 6956.00 +/- 3424.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.96e+03 |\n",
      "|    mean_reward        | 251      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14970000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 187124   |\n",
      "|    policy_loss        | -0.0395  |\n",
      "|    value_loss         | 0.36     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+04 |\n",
      "|    ep_rew_mean        | 429      |\n",
      "| time/                 |          |\n",
      "|    fps                | 402      |\n",
      "|    iterations         | 187200   |\n",
      "|    time_elapsed       | 37169    |\n",
      "|    total_timesteps    | 14976000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.957   |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 187199   |\n",
      "|    policy_loss        | -0.0331  |\n",
      "|    value_loss         | 0.0241   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14980000, episode_reward=417.80 +/- 13.11\n",
      "Episode length: 11629.40 +/- 1648.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+04 |\n",
      "|    mean_reward        | 418      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14980000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 187249   |\n",
      "|    policy_loss        | 0.0895   |\n",
      "|    value_loss         | 0.126    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.23e+04  |\n",
      "|    ep_rew_mean        | 426       |\n",
      "| time/                 |           |\n",
      "|    fps                | 402       |\n",
      "|    iterations         | 187300    |\n",
      "|    time_elapsed       | 37210     |\n",
      "|    total_timesteps    | 14984000  |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.05     |\n",
      "|    explained_variance | 0.997     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 187299    |\n",
      "|    policy_loss        | -0.000458 |\n",
      "|    value_loss         | 0.0144    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=14990000, episode_reward=407.60 +/- 18.05\n",
      "Episode length: 12407.40 +/- 2067.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.24e+04 |\n",
      "|    mean_reward        | 408      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14990000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.935   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 187374   |\n",
      "|    policy_loss        | 0.068    |\n",
      "|    value_loss         | 0.0435   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 425      |\n",
      "| time/                 |          |\n",
      "|    fps                | 402      |\n",
      "|    iterations         | 187400   |\n",
      "|    time_elapsed       | 37250    |\n",
      "|    total_timesteps    | 14992000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 187399   |\n",
      "|    policy_loss        | -0.0249  |\n",
      "|    value_loss         | 0.0449   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15000000, episode_reward=340.80 +/- 155.11\n",
      "Episode length: 9036.00 +/- 2517.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.04e+03 |\n",
      "|    mean_reward        | 341      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15000000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.989   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 187499   |\n",
      "|    policy_loss        | -0.0138  |\n",
      "|    value_loss         | 0.0206   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.22e+04 |\n",
      "|    ep_rew_mean     | 421      |\n",
      "| time/              |          |\n",
      "|    fps             | 402      |\n",
      "|    iterations      | 187500   |\n",
      "|    time_elapsed    | 37280    |\n",
      "|    total_timesteps | 15000000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+04 |\n",
      "|    ep_rew_mean        | 421      |\n",
      "| time/                 |          |\n",
      "|    fps                | 402      |\n",
      "|    iterations         | 187600   |\n",
      "|    time_elapsed       | 37288    |\n",
      "|    total_timesteps    | 15008000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 187599   |\n",
      "|    policy_loss        | 0.0444   |\n",
      "|    value_loss         | 0.0586   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15010000, episode_reward=486.60 +/- 179.01\n",
      "Episode length: 12732.20 +/- 5706.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.27e+04 |\n",
      "|    mean_reward        | 487      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15010000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 187624   |\n",
      "|    policy_loss        | 0.0129   |\n",
      "|    value_loss         | 0.0418   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | 421      |\n",
      "| time/                 |          |\n",
      "|    fps                | 402      |\n",
      "|    iterations         | 187700   |\n",
      "|    time_elapsed       | 37332    |\n",
      "|    total_timesteps    | 15016000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.982   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 187699   |\n",
      "|    policy_loss        | -0.0482  |\n",
      "|    value_loss         | 0.0333   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15020000, episode_reward=362.20 +/- 36.15\n",
      "Episode length: 9011.80 +/- 1392.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.01e+03 |\n",
      "|    mean_reward        | 362      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15020000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 187749   |\n",
      "|    policy_loss        | 0.0777   |\n",
      "|    value_loss         | 0.115    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | 424      |\n",
      "| time/                 |          |\n",
      "|    fps                | 402      |\n",
      "|    iterations         | 187800   |\n",
      "|    time_elapsed       | 37364    |\n",
      "|    total_timesteps    | 15024000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 187799   |\n",
      "|    policy_loss        | -0.0798  |\n",
      "|    value_loss         | 0.0903   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15030000, episode_reward=410.40 +/- 24.58\n",
      "Episode length: 9705.20 +/- 1115.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.71e+03 |\n",
      "|    mean_reward        | 410      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15030000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.962   |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 187874   |\n",
      "|    policy_loss        | 0.00641  |\n",
      "|    value_loss         | 0.00862  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | 427      |\n",
      "| time/                 |          |\n",
      "|    fps                | 401      |\n",
      "|    iterations         | 187900   |\n",
      "|    time_elapsed       | 37397    |\n",
      "|    total_timesteps    | 15032000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.875   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 187899   |\n",
      "|    policy_loss        | -0.0263  |\n",
      "|    value_loss         | 0.0433   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15040000, episode_reward=296.80 +/- 262.58\n",
      "Episode length: 8758.20 +/- 4924.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.76e+03 |\n",
      "|    mean_reward        | 297      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15040000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.996   |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 187999   |\n",
      "|    policy_loss        | -0.00435 |\n",
      "|    value_loss         | 0.115    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.25e+04 |\n",
      "|    ep_rew_mean     | 427      |\n",
      "| time/              |          |\n",
      "|    fps             | 401      |\n",
      "|    iterations      | 188000   |\n",
      "|    time_elapsed    | 37429    |\n",
      "|    total_timesteps | 15040000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+04 |\n",
      "|    ep_rew_mean        | 425      |\n",
      "| time/                 |          |\n",
      "|    fps                | 401      |\n",
      "|    iterations         | 188100   |\n",
      "|    time_elapsed       | 37437    |\n",
      "|    total_timesteps    | 15048000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 188099   |\n",
      "|    policy_loss        | -0.00507 |\n",
      "|    value_loss         | 0.0761   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15050000, episode_reward=368.00 +/- 79.51\n",
      "Episode length: 7957.60 +/- 2379.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.96e+03 |\n",
      "|    mean_reward        | 368      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15050000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.867   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 188124   |\n",
      "|    policy_loss        | -0.00631 |\n",
      "|    value_loss         | 0.0281   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 424      |\n",
      "| time/                 |          |\n",
      "|    fps                | 401      |\n",
      "|    iterations         | 188200   |\n",
      "|    time_elapsed       | 37463    |\n",
      "|    total_timesteps    | 15056000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.861    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 188199   |\n",
      "|    policy_loss        | -0.215   |\n",
      "|    value_loss         | 0.844    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15060000, episode_reward=401.40 +/- 56.90\n",
      "Episode length: 14510.00 +/- 3793.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.45e+04 |\n",
      "|    mean_reward        | 401      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15060000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.979   |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 188249   |\n",
      "|    policy_loss        | -0.115   |\n",
      "|    value_loss         | 0.204    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 424      |\n",
      "| time/                 |          |\n",
      "|    fps                | 401      |\n",
      "|    iterations         | 188300   |\n",
      "|    time_elapsed       | 37511    |\n",
      "|    total_timesteps    | 15064000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.953   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 188299   |\n",
      "|    policy_loss        | -0.022   |\n",
      "|    value_loss         | 0.0605   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15070000, episode_reward=514.20 +/- 171.56\n",
      "Episode length: 19493.60 +/- 8231.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.95e+04 |\n",
      "|    mean_reward        | 514      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15070000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.972   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 188374   |\n",
      "|    policy_loss        | -0.024   |\n",
      "|    value_loss         | 0.0274   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 423      |\n",
      "| time/                 |          |\n",
      "|    fps                | 401      |\n",
      "|    iterations         | 188400   |\n",
      "|    time_elapsed       | 37571    |\n",
      "|    total_timesteps    | 15072000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 188399   |\n",
      "|    policy_loss        | -0.068   |\n",
      "|    value_loss         | 0.0202   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15080000, episode_reward=411.20 +/- 11.70\n",
      "Episode length: 16142.00 +/- 11241.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.61e+04 |\n",
      "|    mean_reward        | 411      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15080000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 188499   |\n",
      "|    policy_loss        | 0.137    |\n",
      "|    value_loss         | 0.109    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.24e+04 |\n",
      "|    ep_rew_mean     | 424      |\n",
      "| time/              |          |\n",
      "|    fps             | 400      |\n",
      "|    iterations      | 188500   |\n",
      "|    time_elapsed    | 37622    |\n",
      "|    total_timesteps | 15080000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 423      |\n",
      "| time/                 |          |\n",
      "|    fps                | 400      |\n",
      "|    iterations         | 188600   |\n",
      "|    time_elapsed       | 37630    |\n",
      "|    total_timesteps    | 15088000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 188599   |\n",
      "|    policy_loss        | 0.00529  |\n",
      "|    value_loss         | 0.0349   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15090000, episode_reward=399.40 +/- 32.62\n",
      "Episode length: 9967.40 +/- 1278.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.97e+03 |\n",
      "|    mean_reward        | 399      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15090000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 188624   |\n",
      "|    policy_loss        | -0.0491  |\n",
      "|    value_loss         | 0.0264   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 424      |\n",
      "| time/                 |          |\n",
      "|    fps                | 400      |\n",
      "|    iterations         | 188700   |\n",
      "|    time_elapsed       | 37666    |\n",
      "|    total_timesteps    | 15096000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 188699   |\n",
      "|    policy_loss        | -0.0015  |\n",
      "|    value_loss         | 0.0162   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15100000, episode_reward=410.00 +/- 21.59\n",
      "Episode length: 11651.80 +/- 3464.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.17e+04 |\n",
      "|    mean_reward        | 410      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15100000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.954   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 188749   |\n",
      "|    policy_loss        | -0.108   |\n",
      "|    value_loss         | 0.043    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+04 |\n",
      "|    ep_rew_mean        | 425      |\n",
      "| time/                 |          |\n",
      "|    fps                | 400      |\n",
      "|    iterations         | 188800   |\n",
      "|    time_elapsed       | 37703    |\n",
      "|    total_timesteps    | 15104000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.969   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 188799   |\n",
      "|    policy_loss        | 0.00753  |\n",
      "|    value_loss         | 0.0285   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15110000, episode_reward=408.20 +/- 27.47\n",
      "Episode length: 11676.60 +/- 1970.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.17e+04 |\n",
      "|    mean_reward        | 408      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15110000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.909   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 188874   |\n",
      "|    policy_loss        | -0.0401  |\n",
      "|    value_loss         | 0.11     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+04 |\n",
      "|    ep_rew_mean        | 425      |\n",
      "| time/                 |          |\n",
      "|    fps                | 400      |\n",
      "|    iterations         | 188900   |\n",
      "|    time_elapsed       | 37742    |\n",
      "|    total_timesteps    | 15112000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 188899   |\n",
      "|    policy_loss        | -0.0501  |\n",
      "|    value_loss         | 0.0391   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15120000, episode_reward=492.60 +/- 139.31\n",
      "Episode length: 13163.00 +/- 3743.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.32e+04 |\n",
      "|    mean_reward        | 493      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15120000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 188999   |\n",
      "|    policy_loss        | -0.0294  |\n",
      "|    value_loss         | 0.0314   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.24e+04 |\n",
      "|    ep_rew_mean     | 425      |\n",
      "| time/              |          |\n",
      "|    fps             | 400      |\n",
      "|    iterations      | 189000   |\n",
      "|    time_elapsed    | 37787    |\n",
      "|    total_timesteps | 15120000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 424      |\n",
      "| time/                 |          |\n",
      "|    fps                | 400      |\n",
      "|    iterations         | 189100   |\n",
      "|    time_elapsed       | 37794    |\n",
      "|    total_timesteps    | 15128000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.985   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 189099   |\n",
      "|    policy_loss        | 0.0359   |\n",
      "|    value_loss         | 0.034    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15130000, episode_reward=421.00 +/- 8.49\n",
      "Episode length: 11071.80 +/- 2042.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.11e+04 |\n",
      "|    mean_reward        | 421      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15130000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.906   |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 189124   |\n",
      "|    policy_loss        | -0.0987  |\n",
      "|    value_loss         | 0.296    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 425      |\n",
      "| time/                 |          |\n",
      "|    fps                | 400      |\n",
      "|    iterations         | 189200   |\n",
      "|    time_elapsed       | 37830    |\n",
      "|    total_timesteps    | 15136000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 189199   |\n",
      "|    policy_loss        | -0.0171  |\n",
      "|    value_loss         | 0.0879   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15140000, episode_reward=480.80 +/- 138.85\n",
      "Episode length: 11525.20 +/- 2640.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.15e+04 |\n",
      "|    mean_reward        | 481      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15140000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 189249   |\n",
      "|    policy_loss        | 0.0191   |\n",
      "|    value_loss         | 0.0438   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | 421      |\n",
      "| time/                 |          |\n",
      "|    fps                | 399      |\n",
      "|    iterations         | 189300   |\n",
      "|    time_elapsed       | 37867    |\n",
      "|    total_timesteps    | 15144000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.927   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 189299   |\n",
      "|    policy_loss        | 0.00323  |\n",
      "|    value_loss         | 0.0211   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15150000, episode_reward=460.20 +/- 88.91\n",
      "Episode length: 13193.20 +/- 4047.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.32e+04 |\n",
      "|    mean_reward        | 460      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15150000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.992   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 189374   |\n",
      "|    policy_loss        | 0.0727   |\n",
      "|    value_loss         | 0.0632   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | 421      |\n",
      "| time/                 |          |\n",
      "|    fps                | 399      |\n",
      "|    iterations         | 189400   |\n",
      "|    time_elapsed       | 37911    |\n",
      "|    total_timesteps    | 15152000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.872   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 189399   |\n",
      "|    policy_loss        | -0.0154  |\n",
      "|    value_loss         | 0.0602   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15160000, episode_reward=591.80 +/- 206.33\n",
      "Episode length: 24958.40 +/- 11890.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.5e+04  |\n",
      "|    mean_reward        | 592      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15160000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.934   |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 189499   |\n",
      "|    policy_loss        | -0.00722 |\n",
      "|    value_loss         | 0.0117   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.22e+04 |\n",
      "|    ep_rew_mean     | 422      |\n",
      "| time/              |          |\n",
      "|    fps             | 399      |\n",
      "|    iterations      | 189500   |\n",
      "|    time_elapsed    | 37984    |\n",
      "|    total_timesteps | 15160000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 426      |\n",
      "| time/                 |          |\n",
      "|    fps                | 399      |\n",
      "|    iterations         | 189600   |\n",
      "|    time_elapsed       | 37992    |\n",
      "|    total_timesteps    | 15168000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.795    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 189599   |\n",
      "|    policy_loss        | -0.284   |\n",
      "|    value_loss         | 0.944    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15170000, episode_reward=410.80 +/- 16.23\n",
      "Episode length: 33152.40 +/- 37802.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.32e+04 |\n",
      "|    mean_reward        | 411      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15170000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.988   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 189624   |\n",
      "|    policy_loss        | -0.00445 |\n",
      "|    value_loss         | 0.0555   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | 427      |\n",
      "| time/                 |          |\n",
      "|    fps                | 398      |\n",
      "|    iterations         | 189700   |\n",
      "|    time_elapsed       | 38089    |\n",
      "|    total_timesteps    | 15176000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 189699   |\n",
      "|    policy_loss        | -0.0307  |\n",
      "|    value_loss         | 0.021    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15180000, episode_reward=433.00 +/- 40.36\n",
      "Episode length: 16944.00 +/- 9825.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.69e+04 |\n",
      "|    mean_reward        | 433      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15180000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.847   |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 189749   |\n",
      "|    policy_loss        | 0.000614 |\n",
      "|    value_loss         | 0.00774  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+04 |\n",
      "|    ep_rew_mean        | 426      |\n",
      "| time/                 |          |\n",
      "|    fps                | 398      |\n",
      "|    iterations         | 189800   |\n",
      "|    time_elapsed       | 38144    |\n",
      "|    total_timesteps    | 15184000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.889   |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 189799   |\n",
      "|    policy_loss        | -0.00906 |\n",
      "|    value_loss         | 0.0134   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15190000, episode_reward=411.80 +/- 10.17\n",
      "Episode length: 12109.20 +/- 5943.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.21e+04 |\n",
      "|    mean_reward        | 412      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15190000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 189874   |\n",
      "|    policy_loss        | -0.0353  |\n",
      "|    value_loss         | 0.0209   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+04 |\n",
      "|    ep_rew_mean        | 426      |\n",
      "| time/                 |          |\n",
      "|    fps                | 397      |\n",
      "|    iterations         | 189900   |\n",
      "|    time_elapsed       | 38185    |\n",
      "|    total_timesteps    | 15192000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 189899   |\n",
      "|    policy_loss        | -0.00887 |\n",
      "|    value_loss         | 0.0509   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15200000, episode_reward=414.60 +/- 9.13\n",
      "Episode length: 11919.00 +/- 5021.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.19e+04 |\n",
      "|    mean_reward        | 415      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15200000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 189999   |\n",
      "|    policy_loss        | -0.0634  |\n",
      "|    value_loss         | 0.0215   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+04 |\n",
      "|    ep_rew_mean     | 426      |\n",
      "| time/              |          |\n",
      "|    fps             | 397      |\n",
      "|    iterations      | 190000   |\n",
      "|    time_elapsed    | 38223    |\n",
      "|    total_timesteps | 15200000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.28e+04 |\n",
      "|    ep_rew_mean        | 426      |\n",
      "| time/                 |          |\n",
      "|    fps                | 397      |\n",
      "|    iterations         | 190100   |\n",
      "|    time_elapsed       | 38230    |\n",
      "|    total_timesteps    | 15208000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 190099   |\n",
      "|    policy_loss        | -0.0325  |\n",
      "|    value_loss         | 0.023    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15210000, episode_reward=383.40 +/- 29.71\n",
      "Episode length: 9173.60 +/- 1600.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.17e+03 |\n",
      "|    mean_reward        | 383      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15210000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 190124   |\n",
      "|    policy_loss        | 0.0387   |\n",
      "|    value_loss         | 0.05     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.28e+04 |\n",
      "|    ep_rew_mean        | 428      |\n",
      "| time/                 |          |\n",
      "|    fps                | 397      |\n",
      "|    iterations         | 190200   |\n",
      "|    time_elapsed       | 38263    |\n",
      "|    total_timesteps    | 15216000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 190199   |\n",
      "|    policy_loss        | -0.149   |\n",
      "|    value_loss         | 0.29     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15220000, episode_reward=420.40 +/- 7.23\n",
      "Episode length: 9890.40 +/- 940.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.89e+03 |\n",
      "|    mean_reward        | 420      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15220000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.92    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 190249   |\n",
      "|    policy_loss        | -0.0297  |\n",
      "|    value_loss         | 0.0321   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 427      |\n",
      "| time/                 |          |\n",
      "|    fps                | 397      |\n",
      "|    iterations         | 190300   |\n",
      "|    time_elapsed       | 38299    |\n",
      "|    total_timesteps    | 15224000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 190299   |\n",
      "|    policy_loss        | -0.0231  |\n",
      "|    value_loss         | 0.0694   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15230000, episode_reward=366.60 +/- 117.18\n",
      "Episode length: 9264.20 +/- 2751.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.26e+03 |\n",
      "|    mean_reward        | 367      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15230000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.925   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 190374   |\n",
      "|    policy_loss        | -0.0438  |\n",
      "|    value_loss         | 0.038    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 431      |\n",
      "| time/                 |          |\n",
      "|    fps                | 397      |\n",
      "|    iterations         | 190400   |\n",
      "|    time_elapsed       | 38329    |\n",
      "|    total_timesteps    | 15232000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.889    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 190399   |\n",
      "|    policy_loss        | -0.135   |\n",
      "|    value_loss         | 0.406    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15240000, episode_reward=494.60 +/- 183.32\n",
      "Episode length: 13502.00 +/- 4303.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.35e+04 |\n",
      "|    mean_reward        | 495      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15240000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 190499   |\n",
      "|    policy_loss        | 0.0557   |\n",
      "|    value_loss         | 0.0918   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.34e+04 |\n",
      "|    ep_rew_mean     | 431      |\n",
      "| time/              |          |\n",
      "|    fps             | 397      |\n",
      "|    iterations      | 190500   |\n",
      "|    time_elapsed    | 38375    |\n",
      "|    total_timesteps | 15240000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 431      |\n",
      "| time/                 |          |\n",
      "|    fps                | 397      |\n",
      "|    iterations         | 190600   |\n",
      "|    time_elapsed       | 38382    |\n",
      "|    total_timesteps    | 15248000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.858   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 190599   |\n",
      "|    policy_loss        | -0.0218  |\n",
      "|    value_loss         | 0.0808   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15250000, episode_reward=345.60 +/- 145.84\n",
      "Episode length: 8132.40 +/- 2683.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.13e+03 |\n",
      "|    mean_reward        | 346      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15250000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.887   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 190624   |\n",
      "|    policy_loss        | -0.0273  |\n",
      "|    value_loss         | 0.0461   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 430      |\n",
      "| time/                 |          |\n",
      "|    fps                | 397      |\n",
      "|    iterations         | 190700   |\n",
      "|    time_elapsed       | 38413    |\n",
      "|    total_timesteps    | 15256000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.969   |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 190699   |\n",
      "|    policy_loss        | 0.146    |\n",
      "|    value_loss         | 0.314    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15260000, episode_reward=427.40 +/- 41.01\n",
      "Episode length: 13005.80 +/- 4292.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.3e+04  |\n",
      "|    mean_reward        | 427      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15260000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.915   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 190749   |\n",
      "|    policy_loss        | 0.0548   |\n",
      "|    value_loss         | 0.169    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.34e+04 |\n",
      "|    ep_rew_mean        | 430      |\n",
      "| time/                 |          |\n",
      "|    fps                | 396      |\n",
      "|    iterations         | 190800   |\n",
      "|    time_elapsed       | 38455    |\n",
      "|    total_timesteps    | 15264000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.906   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 190799   |\n",
      "|    policy_loss        | 0.00726  |\n",
      "|    value_loss         | 0.0248   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15270000, episode_reward=381.80 +/- 58.37\n",
      "Episode length: 32236.00 +/- 37939.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.22e+04 |\n",
      "|    mean_reward        | 382      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15270000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 190874   |\n",
      "|    policy_loss        | 0.013    |\n",
      "|    value_loss         | 0.0513   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.31e+04 |\n",
      "|    ep_rew_mean        | 430      |\n",
      "| time/                 |          |\n",
      "|    fps                | 396      |\n",
      "|    iterations         | 190900   |\n",
      "|    time_elapsed       | 38549    |\n",
      "|    total_timesteps    | 15272000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 190899   |\n",
      "|    policy_loss        | -0.00894 |\n",
      "|    value_loss         | 0.0303   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15280000, episode_reward=332.20 +/- 101.16\n",
      "Episode length: 9747.00 +/- 2599.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.75e+03 |\n",
      "|    mean_reward        | 332      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15280000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.19    |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 190999   |\n",
      "|    policy_loss        | 0.184    |\n",
      "|    value_loss         | 0.198    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.31e+04 |\n",
      "|    ep_rew_mean     | 430      |\n",
      "| time/              |          |\n",
      "|    fps             | 396      |\n",
      "|    iterations      | 191000   |\n",
      "|    time_elapsed    | 38584    |\n",
      "|    total_timesteps | 15280000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.3e+04  |\n",
      "|    ep_rew_mean        | 428      |\n",
      "| time/                 |          |\n",
      "|    fps                | 396      |\n",
      "|    iterations         | 191100   |\n",
      "|    time_elapsed       | 38592    |\n",
      "|    total_timesteps    | 15288000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.99    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 191099   |\n",
      "|    policy_loss        | 0.0321   |\n",
      "|    value_loss         | 0.046    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15290000, episode_reward=494.40 +/- 161.34\n",
      "Episode length: 12595.80 +/- 3757.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.26e+04 |\n",
      "|    mean_reward        | 494      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15290000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 191124   |\n",
      "|    policy_loss        | -0.0647  |\n",
      "|    value_loss         | 0.0585   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.31e+04 |\n",
      "|    ep_rew_mean        | 428      |\n",
      "| time/                 |          |\n",
      "|    fps                | 395      |\n",
      "|    iterations         | 191200   |\n",
      "|    time_elapsed       | 38631    |\n",
      "|    total_timesteps    | 15296000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 191199   |\n",
      "|    policy_loss        | 0.0432   |\n",
      "|    value_loss         | 0.093    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15300000, episode_reward=410.00 +/- 21.14\n",
      "Episode length: 19446.40 +/- 11598.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.94e+04 |\n",
      "|    mean_reward        | 410      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15300000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 191249   |\n",
      "|    policy_loss        | -0.111   |\n",
      "|    value_loss         | 0.0813   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.3e+04  |\n",
      "|    ep_rew_mean        | 428      |\n",
      "| time/                 |          |\n",
      "|    fps                | 395      |\n",
      "|    iterations         | 191300   |\n",
      "|    time_elapsed       | 38690    |\n",
      "|    total_timesteps    | 15304000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 191299   |\n",
      "|    policy_loss        | 0.09     |\n",
      "|    value_loss         | 0.0577   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15310000, episode_reward=428.00 +/- 10.24\n",
      "Episode length: 15636.40 +/- 9421.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.56e+04 |\n",
      "|    mean_reward        | 428      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15310000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.984   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 191374   |\n",
      "|    policy_loss        | 0.0112   |\n",
      "|    value_loss         | 0.0212   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.3e+04  |\n",
      "|    ep_rew_mean        | 426      |\n",
      "| time/                 |          |\n",
      "|    fps                | 395      |\n",
      "|    iterations         | 191400   |\n",
      "|    time_elapsed       | 38739    |\n",
      "|    total_timesteps    | 15312000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.986   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 191399   |\n",
      "|    policy_loss        | 0.0154   |\n",
      "|    value_loss         | 0.0163   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15320000, episode_reward=375.40 +/- 33.30\n",
      "Episode length: 10719.20 +/- 2456.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.07e+04 |\n",
      "|    mean_reward        | 375      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15320000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 191499   |\n",
      "|    policy_loss        | 0.0135   |\n",
      "|    value_loss         | 0.0353   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.27e+04 |\n",
      "|    ep_rew_mean     | 426      |\n",
      "| time/              |          |\n",
      "|    fps             | 395      |\n",
      "|    iterations      | 191500   |\n",
      "|    time_elapsed    | 38777    |\n",
      "|    total_timesteps | 15320000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 423      |\n",
      "| time/                 |          |\n",
      "|    fps                | 395      |\n",
      "|    iterations         | 191600   |\n",
      "|    time_elapsed       | 38784    |\n",
      "|    total_timesteps    | 15328000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.895   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 191599   |\n",
      "|    policy_loss        | 0.0705   |\n",
      "|    value_loss         | 0.0399   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15330000, episode_reward=365.20 +/- 47.75\n",
      "Episode length: 8635.40 +/- 1856.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.64e+03 |\n",
      "|    mean_reward        | 365      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15330000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 191624   |\n",
      "|    policy_loss        | 0.1      |\n",
      "|    value_loss         | 0.0826   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | 423      |\n",
      "| time/                 |          |\n",
      "|    fps                | 395      |\n",
      "|    iterations         | 191700   |\n",
      "|    time_elapsed       | 38816    |\n",
      "|    total_timesteps    | 15336000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.943   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 191699   |\n",
      "|    policy_loss        | 0.00312  |\n",
      "|    value_loss         | 0.0236   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15340000, episode_reward=257.20 +/- 161.27\n",
      "Episode length: 8964.40 +/- 4701.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.96e+03 |\n",
      "|    mean_reward        | 257      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15340000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.899   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 191749   |\n",
      "|    policy_loss        | 0.0107   |\n",
      "|    value_loss         | 0.0312   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | 418      |\n",
      "| time/                 |          |\n",
      "|    fps                | 394      |\n",
      "|    iterations         | 191800   |\n",
      "|    time_elapsed       | 38847    |\n",
      "|    total_timesteps    | 15344000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.979   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 191799   |\n",
      "|    policy_loss        | -0.0906  |\n",
      "|    value_loss         | 0.109    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15350000, episode_reward=484.40 +/- 172.45\n",
      "Episode length: 14907.80 +/- 7254.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.49e+04 |\n",
      "|    mean_reward        | 484      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15350000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 191874   |\n",
      "|    policy_loss        | -0.00396 |\n",
      "|    value_loss         | 0.0868   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | 417      |\n",
      "| time/                 |          |\n",
      "|    fps                | 394      |\n",
      "|    iterations         | 191900   |\n",
      "|    time_elapsed       | 38897    |\n",
      "|    total_timesteps    | 15352000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 191899   |\n",
      "|    policy_loss        | -0.025   |\n",
      "|    value_loss         | 0.0174   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15360000, episode_reward=390.00 +/- 21.17\n",
      "Episode length: 10622.40 +/- 1736.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+04 |\n",
      "|    mean_reward        | 390      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15360000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.874   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 191999   |\n",
      "|    policy_loss        | -0.0143  |\n",
      "|    value_loss         | 0.0264   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.25e+04 |\n",
      "|    ep_rew_mean     | 419      |\n",
      "| time/              |          |\n",
      "|    fps             | 394      |\n",
      "|    iterations      | 192000   |\n",
      "|    time_elapsed    | 38934    |\n",
      "|    total_timesteps | 15360000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | 419      |\n",
      "| time/                 |          |\n",
      "|    fps                | 394      |\n",
      "|    iterations         | 192100   |\n",
      "|    time_elapsed       | 38942    |\n",
      "|    total_timesteps    | 15368000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.927   |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 192099   |\n",
      "|    policy_loss        | 0.00913  |\n",
      "|    value_loss         | 0.0212   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15370000, episode_reward=365.60 +/- 111.36\n",
      "Episode length: 11041.00 +/- 3444.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.1e+04  |\n",
      "|    mean_reward        | 366      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15370000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.991   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 192124   |\n",
      "|    policy_loss        | 0.00376  |\n",
      "|    value_loss         | 0.0547   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+04 |\n",
      "|    ep_rew_mean        | 418      |\n",
      "| time/                 |          |\n",
      "|    fps                | 394      |\n",
      "|    iterations         | 192200   |\n",
      "|    time_elapsed       | 38977    |\n",
      "|    total_timesteps    | 15376000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.913   |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 192199   |\n",
      "|    policy_loss        | -0.17    |\n",
      "|    value_loss         | 0.355    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15380000, episode_reward=483.20 +/- 188.14\n",
      "Episode length: 14539.80 +/- 6315.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.45e+04 |\n",
      "|    mean_reward        | 483      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15380000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.948   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 192249   |\n",
      "|    policy_loss        | 0.00612  |\n",
      "|    value_loss         | 0.0842   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 418      |\n",
      "| time/                 |          |\n",
      "|    fps                | 394      |\n",
      "|    iterations         | 192300   |\n",
      "|    time_elapsed       | 39022    |\n",
      "|    total_timesteps    | 15384000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.98    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 192299   |\n",
      "|    policy_loss        | 0.0128   |\n",
      "|    value_loss         | 0.083    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15390000, episode_reward=510.80 +/- 176.80\n",
      "Episode length: 14451.00 +/- 4906.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.45e+04 |\n",
      "|    mean_reward        | 511      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15390000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 192374   |\n",
      "|    policy_loss        | -0.0327  |\n",
      "|    value_loss         | 0.00877  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | 417      |\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 192400   |\n",
      "|    time_elapsed       | 39071    |\n",
      "|    total_timesteps    | 15392000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.976   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 192399   |\n",
      "|    policy_loss        | -0.0516  |\n",
      "|    value_loss         | 0.0296   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15400000, episode_reward=385.00 +/- 33.27\n",
      "Episode length: 14562.80 +/- 9174.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.46e+04 |\n",
      "|    mean_reward        | 385      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15400000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 192499   |\n",
      "|    policy_loss        | -0.0907  |\n",
      "|    value_loss         | 0.0416   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.22e+04 |\n",
      "|    ep_rew_mean     | 416      |\n",
      "| time/              |          |\n",
      "|    fps             | 393      |\n",
      "|    iterations      | 192500   |\n",
      "|    time_elapsed    | 39129    |\n",
      "|    total_timesteps | 15400000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.2e+04  |\n",
      "|    ep_rew_mean        | 413      |\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 192600   |\n",
      "|    time_elapsed       | 39137    |\n",
      "|    total_timesteps    | 15408000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.963   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 192599   |\n",
      "|    policy_loss        | 0.0149   |\n",
      "|    value_loss         | 0.0161   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15410000, episode_reward=487.40 +/- 171.35\n",
      "Episode length: 12423.40 +/- 2805.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.24e+04 |\n",
      "|    mean_reward        | 487      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15410000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 192624   |\n",
      "|    policy_loss        | -0.0451  |\n",
      "|    value_loss         | 0.0273   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | 413      |\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 192700   |\n",
      "|    time_elapsed       | 39184    |\n",
      "|    total_timesteps    | 15416000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.976   |\n",
      "|    explained_variance | 0.855    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 192699   |\n",
      "|    policy_loss        | -0.11    |\n",
      "|    value_loss         | 0.283    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15420000, episode_reward=311.40 +/- 126.26\n",
      "Episode length: 8455.60 +/- 2082.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.46e+03 |\n",
      "|    mean_reward        | 311      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15420000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.996   |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 192749   |\n",
      "|    policy_loss        | -0.199   |\n",
      "|    value_loss         | 0.227    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 414      |\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 192800   |\n",
      "|    time_elapsed       | 39218    |\n",
      "|    total_timesteps    | 15424000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 192799   |\n",
      "|    policy_loss        | 0.0877   |\n",
      "|    value_loss         | 0.085    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15430000, episode_reward=415.40 +/- 12.78\n",
      "Episode length: 10974.40 +/- 1244.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.1e+04  |\n",
      "|    mean_reward        | 415      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15430000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.924   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 192874   |\n",
      "|    policy_loss        | -0.0489  |\n",
      "|    value_loss         | 0.0345   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 413      |\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 192900   |\n",
      "|    time_elapsed       | 39261    |\n",
      "|    total_timesteps    | 15432000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.99    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 192899   |\n",
      "|    policy_loss        | 0.0755   |\n",
      "|    value_loss         | 0.103    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15440000, episode_reward=400.40 +/- 16.52\n",
      "Episode length: 12145.60 +/- 4361.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.21e+04 |\n",
      "|    mean_reward        | 400      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15440000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 192999   |\n",
      "|    policy_loss        | 0.0401   |\n",
      "|    value_loss         | 0.0259   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.22e+04 |\n",
      "|    ep_rew_mean     | 412      |\n",
      "| time/              |          |\n",
      "|    fps             | 392      |\n",
      "|    iterations      | 193000   |\n",
      "|    time_elapsed    | 39304    |\n",
      "|    total_timesteps | 15440000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | 411      |\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 193100   |\n",
      "|    time_elapsed       | 39312    |\n",
      "|    total_timesteps    | 15448000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 193099   |\n",
      "|    policy_loss        | -0.0833  |\n",
      "|    value_loss         | 0.0481   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15450000, episode_reward=489.20 +/- 133.05\n",
      "Episode length: 14341.20 +/- 1777.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.43e+04 |\n",
      "|    mean_reward        | 489      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15450000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 193124   |\n",
      "|    policy_loss        | 0.00034  |\n",
      "|    value_loss         | 0.0266   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.21e+04 |\n",
      "|    ep_rew_mean        | 404      |\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 193200   |\n",
      "|    time_elapsed       | 39361    |\n",
      "|    total_timesteps    | 15456000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.927   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 193199   |\n",
      "|    policy_loss        | -0.00428 |\n",
      "|    value_loss         | 0.0096   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15460000, episode_reward=311.20 +/- 130.39\n",
      "Episode length: 8693.00 +/- 3176.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.69e+03 |\n",
      "|    mean_reward        | 311      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15460000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.949   |\n",
      "|    explained_variance | 0.895    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 193249   |\n",
      "|    policy_loss        | -0.164   |\n",
      "|    value_loss         | 0.286    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.19e+04 |\n",
      "|    ep_rew_mean        | 401      |\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 193300   |\n",
      "|    time_elapsed       | 39398    |\n",
      "|    total_timesteps    | 15464000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 193299   |\n",
      "|    policy_loss        | -0.00102 |\n",
      "|    value_loss         | 0.0233   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15470000, episode_reward=516.60 +/- 132.88\n",
      "Episode length: 20079.00 +/- 11561.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.01e+04 |\n",
      "|    mean_reward        | 517      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15470000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.906   |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 193374   |\n",
      "|    policy_loss        | -0.0559  |\n",
      "|    value_loss         | 0.029    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15e+04 |\n",
      "|    ep_rew_mean        | 399      |\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 193400   |\n",
      "|    time_elapsed       | 39471    |\n",
      "|    total_timesteps    | 15472000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 193399   |\n",
      "|    policy_loss        | -0.107   |\n",
      "|    value_loss         | 0.0719   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15480000, episode_reward=419.00 +/- 10.71\n",
      "Episode length: 9478.60 +/- 662.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.48e+03 |\n",
      "|    mean_reward        | 419      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15480000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 193499   |\n",
      "|    policy_loss        | 0.00931  |\n",
      "|    value_loss         | 0.191    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.15e+04 |\n",
      "|    ep_rew_mean     | 399      |\n",
      "| time/              |          |\n",
      "|    fps             | 391      |\n",
      "|    iterations      | 193500   |\n",
      "|    time_elapsed    | 39510    |\n",
      "|    total_timesteps | 15480000 |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14e+04 |\n",
      "|    ep_rew_mean        | 402      |\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 193600   |\n",
      "|    time_elapsed       | 39518    |\n",
      "|    total_timesteps    | 15488000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.919   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 193599   |\n",
      "|    policy_loss        | -0.00897 |\n",
      "|    value_loss         | 0.0258   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15490000, episode_reward=409.40 +/- 44.30\n",
      "Episode length: 11732.20 +/- 3437.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.17e+04 |\n",
      "|    mean_reward        | 409      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15490000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.97    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 193624   |\n",
      "|    policy_loss        | -0.00298 |\n",
      "|    value_loss         | 0.0319   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14e+04 |\n",
      "|    ep_rew_mean        | 403      |\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 193700   |\n",
      "|    time_elapsed       | 39562    |\n",
      "|    total_timesteps    | 15496000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.966   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 193699   |\n",
      "|    policy_loss        | -0.0166  |\n",
      "|    value_loss         | 0.101    |\n",
      "------------------------------------\n",
      "^C\n",
      "Saving to /home/sequenzia/dev/rl-project/trained-agents/a2c/BreakoutNoFrameskip-v4_1\n"
     ]
    }
   ],
   "source": [
    "ALGO = \"a2c\"\n",
    "TAGS = f\"{ROM} {ALGO.upper()}\"\n",
    "CONFIG_PATH = f\"{CONFIG_DIR}/{ALGO}.yml\"\n",
    "CMD = f\"{CMD} --algo {ALGO} --conf {CONFIG_PATH} -tags {TAGS}\"\n",
    "\n",
    "!{CMD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== BreakoutNoFrameskip-v4 ==========\n",
      "Seed: 43\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mappliedtheta\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sequenzia/dev/rl-project/logs/wandb/run-20231218_114818-jm0ojx1y\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mBreakoutNoFrameskip-v4__ppo__1702918097\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/appliedtheta/Solen-RL-Project\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/appliedtheta/Solen-RL-Project/runs/jm0ojx1y\u001b[0m\n",
      "Loading hyperparameters from: /home/sequenzia/dev/rl-project/configs/ppo.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 256),\n",
      "             ('clip_range', 'lin_0.1'),\n",
      "             ('ent_coef', 0.01),\n",
      "             ('env_wrapper',\n",
      "              ['stable_baselines3.common.atari_wrappers.AtariWrapper']),\n",
      "             ('frame_stack', 4),\n",
      "             ('learning_rate', 'lin_2.5e-4'),\n",
      "             ('n_envs', 16),\n",
      "             ('n_epochs', 5),\n",
      "             ('n_steps', 128),\n",
      "             ('n_timesteps', 10000000.0),\n",
      "             ('policy', 'CnnPolicy'),\n",
      "             ('vf_coef', 0.5)])\n",
      "Using 16 environments\n",
      "Creating test environment\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "Stacking 4 frames\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Stacking 4 frames\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Using cuda device\n",
      "Log path: /home/sequenzia/dev/rl-project/trained-agents/ppo/BreakoutNoFrameskip-v4_1\n",
      "Logging to runs/BreakoutNoFrameskip-v4__ppo__1702918097/BreakoutNoFrameskip-v4/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 516      |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 431      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 614          |\n",
      "|    ep_rew_mean          | 0.611        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 548          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011714264 |\n",
      "|    clip_fraction        | 0.00127      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -0.0462      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00829      |\n",
      "|    n_updates            | 5            |\n",
      "|    policy_gradient_loss | -0.000265    |\n",
      "|    value_loss           | 0.0842       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 692          |\n",
      "|    ep_rew_mean          | 1            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 606          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009881562 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.0329       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0153       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000362    |\n",
      "|    value_loss           | 0.0722       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 718          |\n",
      "|    ep_rew_mean          | 1.19         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 640          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020574245 |\n",
      "|    clip_fraction        | 0.0152       |\n",
      "|    clip_range           | 0.0999       |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.123        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0135       |\n",
      "|    n_updates            | 15           |\n",
      "|    policy_gradient_loss | -0.000844    |\n",
      "|    value_loss           | 0.0675       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=2.80 +/- 3.43\n",
      "Episode length: 883.80 +/- 416.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 884          |\n",
      "|    mean_reward          | 2.8          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032814438 |\n",
      "|    clip_fraction        | 0.0371       |\n",
      "|    clip_range           | 0.0999       |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.301        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00386      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    value_loss           | 0.0477       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 700      |\n",
      "|    ep_rew_mean     | 1.09     |\n",
      "| time/              |          |\n",
      "|    fps             | 566      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 701         |\n",
      "|    ep_rew_mean          | 1.14        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 589         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001249541 |\n",
      "|    clip_fraction        | 0.00205     |\n",
      "|    clip_range           | 0.0999      |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.604       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0038     |\n",
      "|    n_updates            | 25          |\n",
      "|    policy_gradient_loss | 7.81e-07    |\n",
      "|    value_loss           | 0.0393      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 717          |\n",
      "|    ep_rew_mean          | 1.23         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 609          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021067539 |\n",
      "|    clip_fraction        | 0.0135       |\n",
      "|    clip_range           | 0.0999       |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.726        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -1.93e-05    |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000759    |\n",
      "|    value_loss           | 0.0362       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 749          |\n",
      "|    ep_rew_mean          | 1.44         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 622          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027219495 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.0999       |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 0.675        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00276      |\n",
      "|    n_updates            | 35           |\n",
      "|    policy_gradient_loss | -0.0013      |\n",
      "|    value_loss           | 0.0498       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 740          |\n",
      "|    ep_rew_mean          | 1.37         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 635          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034219248 |\n",
      "|    clip_fraction        | 0.0979       |\n",
      "|    clip_range           | 0.0998       |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.812        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0153      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00274     |\n",
      "|    value_loss           | 0.023        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=2.40 +/- 3.83\n",
      "Episode length: 848.20 +/- 500.58\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 848          |\n",
      "|    mean_reward          | 2.4          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017874397 |\n",
      "|    clip_fraction        | 0.0083       |\n",
      "|    clip_range           | 0.0998       |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.799        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00705     |\n",
      "|    n_updates            | 45           |\n",
      "|    policy_gradient_loss | -0.00106     |\n",
      "|    value_loss           | 0.0307       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 771      |\n",
      "|    ep_rew_mean     | 1.59     |\n",
      "| time/              |          |\n",
      "|    fps             | 603      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 768          |\n",
      "|    ep_rew_mean          | 1.57         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 613          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024566052 |\n",
      "|    clip_fraction        | 0.0335       |\n",
      "|    clip_range           | 0.0998       |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.808        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.00675     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    value_loss           | 0.0238       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 769        |\n",
      "|    ep_rew_mean          | 1.59       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 620        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 39         |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00199221 |\n",
      "|    clip_fraction        | 0.0249     |\n",
      "|    clip_range           | 0.0998     |\n",
      "|    entropy_loss         | -1.37      |\n",
      "|    explained_variance   | 0.84       |\n",
      "|    learning_rate        | 0.000249   |\n",
      "|    loss                 | -0.011     |\n",
      "|    n_updates            | 55         |\n",
      "|    policy_gradient_loss | -0.00153   |\n",
      "|    value_loss           | 0.0284     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 762          |\n",
      "|    ep_rew_mean          | 1.55         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 629          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 42           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021703627 |\n",
      "|    clip_fraction        | 0.0782       |\n",
      "|    clip_range           | 0.0998       |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.0115      |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00368     |\n",
      "|    value_loss           | 0.0148       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 781          |\n",
      "|    ep_rew_mean          | 1.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 638          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021099653 |\n",
      "|    clip_fraction        | 0.0673       |\n",
      "|    clip_range           | 0.0997       |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.00927     |\n",
      "|    n_updates            | 65           |\n",
      "|    policy_gradient_loss | -0.00295     |\n",
      "|    value_loss           | 0.0148       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=1.40 +/- 1.96\n",
      "Episode length: 763.60 +/- 349.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 764          |\n",
      "|    mean_reward          | 1.4          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018538515 |\n",
      "|    clip_fraction        | 0.064        |\n",
      "|    clip_range           | 0.0997       |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.0111      |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00212     |\n",
      "|    value_loss           | 0.0168       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 804      |\n",
      "|    ep_rew_mean     | 1.84     |\n",
      "| time/              |          |\n",
      "|    fps             | 613      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 818          |\n",
      "|    ep_rew_mean          | 1.93         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 620          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 52           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012287853 |\n",
      "|    clip_fraction        | 0.0358       |\n",
      "|    clip_range           | 0.0997       |\n",
      "|    entropy_loss         | -1.31        |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.0103      |\n",
      "|    n_updates            | 75           |\n",
      "|    policy_gradient_loss | -0.00157     |\n",
      "|    value_loss           | 0.0206       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 786          |\n",
      "|    ep_rew_mean          | 1.74         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 627          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031879512 |\n",
      "|    clip_fraction        | 0.0936       |\n",
      "|    clip_range           | 0.0997       |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.932        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.0128      |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00287     |\n",
      "|    value_loss           | 0.0141       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 794          |\n",
      "|    ep_rew_mean          | 1.79         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 633          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023224717 |\n",
      "|    clip_fraction        | 0.0626       |\n",
      "|    clip_range           | 0.0997       |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.964        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.0153      |\n",
      "|    n_updates            | 85           |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    value_loss           | 0.00959      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 822          |\n",
      "|    ep_rew_mean          | 2.02         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 637          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019061939 |\n",
      "|    clip_fraction        | 0.0907       |\n",
      "|    clip_range           | 0.0996       |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.957        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.0133      |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0034      |\n",
      "|    value_loss           | 0.0108       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=2.60 +/- 3.32\n",
      "Episode length: 862.40 +/- 404.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 862          |\n",
      "|    mean_reward          | 2.6          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038194275 |\n",
      "|    clip_fraction        | 0.16         |\n",
      "|    clip_range           | 0.0996       |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.973        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.0211      |\n",
      "|    n_updates            | 95           |\n",
      "|    policy_gradient_loss | -0.005       |\n",
      "|    value_loss           | 0.00932      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 2.1      |\n",
      "| time/              |          |\n",
      "|    fps             | 621      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 65       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 825          |\n",
      "|    ep_rew_mean          | 2.04         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 627          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 68           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015715105 |\n",
      "|    clip_fraction        | 0.066        |\n",
      "|    clip_range           | 0.0996       |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 0.949        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.0125      |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00191     |\n",
      "|    value_loss           | 0.00911      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 848          |\n",
      "|    ep_rew_mean          | 2.21         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 631          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022509173 |\n",
      "|    clip_fraction        | 0.0956       |\n",
      "|    clip_range           | 0.0996       |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.967        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.0214      |\n",
      "|    n_updates            | 105          |\n",
      "|    policy_gradient_loss | -0.00439     |\n",
      "|    value_loss           | 0.00739      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 861          |\n",
      "|    ep_rew_mean          | 2.28         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 636          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022011716 |\n",
      "|    clip_fraction        | 0.113        |\n",
      "|    clip_range           | 0.0995       |\n",
      "|    entropy_loss         | -1.31        |\n",
      "|    explained_variance   | 0.932        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.0162      |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00527     |\n",
      "|    value_loss           | 0.0168       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 854          |\n",
      "|    ep_rew_mean          | 2.19         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 641          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 76           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019274362 |\n",
      "|    clip_fraction        | 0.0465       |\n",
      "|    clip_range           | 0.0995       |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.934        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.015       |\n",
      "|    n_updates            | 115          |\n",
      "|    policy_gradient_loss | -0.00307     |\n",
      "|    value_loss           | 0.0122       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=1.60 +/- 1.20\n",
      "Episode length: 742.20 +/- 180.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 742          |\n",
      "|    mean_reward          | 1.6          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029601417 |\n",
      "|    clip_fraction        | 0.0919       |\n",
      "|    clip_range           | 0.0995       |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.938        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.00368     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00445     |\n",
      "|    value_loss           | 0.0127       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 2.11     |\n",
      "| time/              |          |\n",
      "|    fps             | 631      |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 845          |\n",
      "|    ep_rew_mean          | 2.15         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 635          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 83           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019054387 |\n",
      "|    clip_fraction        | 0.104        |\n",
      "|    clip_range           | 0.0995       |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 0.959        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.00832     |\n",
      "|    n_updates            | 125          |\n",
      "|    policy_gradient_loss | -0.00368     |\n",
      "|    value_loss           | 0.00894      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 853          |\n",
      "|    ep_rew_mean          | 2.22         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 640          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 86           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022011753 |\n",
      "|    clip_fraction        | 0.0707       |\n",
      "|    clip_range           | 0.0995       |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 0.945        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.0195      |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00392     |\n",
      "|    value_loss           | 0.00958      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 818          |\n",
      "|    ep_rew_mean          | 1.95         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 643          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 89           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028262453 |\n",
      "|    clip_fraction        | 0.139        |\n",
      "|    clip_range           | 0.0994       |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 0.941        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.0162      |\n",
      "|    n_updates            | 135          |\n",
      "|    policy_gradient_loss | -0.00583     |\n",
      "|    value_loss           | 0.0122       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 819          |\n",
      "|    ep_rew_mean          | 1.94         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 647          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 91           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019469126 |\n",
      "|    clip_fraction        | 0.0983       |\n",
      "|    clip_range           | 0.0994       |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.0138      |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00568     |\n",
      "|    value_loss           | 0.0203       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=1.60 +/- 1.50\n",
      "Episode length: 769.80 +/- 245.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 770          |\n",
      "|    mean_reward          | 1.6          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022224544 |\n",
      "|    clip_fraction        | 0.0888       |\n",
      "|    clip_range           | 0.0994       |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | -0.0195      |\n",
      "|    n_updates            | 145          |\n",
      "|    policy_gradient_loss | -0.00639     |\n",
      "|    value_loss           | 0.0252       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 844      |\n",
      "|    ep_rew_mean     | 2.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 637      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 96       |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 864          |\n",
      "|    ep_rew_mean          | 2.27         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 641          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 98           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024044374 |\n",
      "|    clip_fraction        | 0.114        |\n",
      "|    clip_range           | 0.0994       |\n",
      "|    entropy_loss         | -1.31        |\n",
      "|    explained_variance   | 0.833        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | -0.021       |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00792     |\n",
      "|    value_loss           | 0.026        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 873          |\n",
      "|    ep_rew_mean          | 2.3          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 645          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 101          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017910374 |\n",
      "|    clip_fraction        | 0.159        |\n",
      "|    clip_range           | 0.0994       |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.771        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | -0.0167      |\n",
      "|    n_updates            | 155          |\n",
      "|    policy_gradient_loss | -0.00577     |\n",
      "|    value_loss           | 0.039        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 907          |\n",
      "|    ep_rew_mean          | 2.53         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 648          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 104          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017316898 |\n",
      "|    clip_fraction        | 0.0677       |\n",
      "|    clip_range           | 0.0993       |\n",
      "|    entropy_loss         | -1.31        |\n",
      "|    explained_variance   | 0.693        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | -0.00182     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0057      |\n",
      "|    value_loss           | 0.0498       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 870         |\n",
      "|    ep_rew_mean          | 2.3         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 650         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002655837 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.0993      |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.625       |\n",
      "|    learning_rate        | 0.000248    |\n",
      "|    loss                 | -0.00667    |\n",
      "|    n_updates            | 165         |\n",
      "|    policy_gradient_loss | -0.00678    |\n",
      "|    value_loss           | 0.0475      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=3.40 +/- 1.62\n",
      "Episode length: 1083.80 +/- 248.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.08e+03     |\n",
      "|    mean_reward          | 3.4          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 70000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026175135 |\n",
      "|    clip_fraction        | 0.148        |\n",
      "|    clip_range           | 0.0993       |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.693        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | -0.0191      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00689     |\n",
      "|    value_loss           | 0.0446       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 2.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 636      |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 880          |\n",
      "|    ep_rew_mean          | 2.36         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 639          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 115          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021512639 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.0993       |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.687        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | -0.0115      |\n",
      "|    n_updates            | 175          |\n",
      "|    policy_gradient_loss | -0.0077      |\n",
      "|    value_loss           | 0.0492       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 903          |\n",
      "|    ep_rew_mean          | 2.51         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 642          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 118          |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017708887 |\n",
      "|    clip_fraction        | 0.0916       |\n",
      "|    clip_range           | 0.0993       |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 0.737        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | -0.0186      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00771     |\n",
      "|    value_loss           | 0.0292       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 906         |\n",
      "|    ep_rew_mean          | 2.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 645         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 120         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002283905 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.0992      |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 0.000248    |\n",
      "|    loss                 | -0.00977    |\n",
      "|    n_updates            | 185         |\n",
      "|    policy_gradient_loss | -0.00721    |\n",
      "|    value_loss           | 0.0394      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 907          |\n",
      "|    ep_rew_mean          | 2.55         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 649          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 123          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028820783 |\n",
      "|    clip_fraction        | 0.13         |\n",
      "|    clip_range           | 0.0992       |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0.745        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | -0.0151      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00896     |\n",
      "|    value_loss           | 0.0467       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=4.20 +/- 3.12\n",
      "Episode length: 1138.80 +/- 464.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.14e+03     |\n",
      "|    mean_reward          | 4.2          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026511704 |\n",
      "|    clip_fraction        | 0.117        |\n",
      "|    clip_range           | 0.0992       |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0.781        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | -0.0173      |\n",
      "|    n_updates            | 195          |\n",
      "|    policy_gradient_loss | -0.0102      |\n",
      "|    value_loss           | 0.0323       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 921      |\n",
      "|    ep_rew_mean     | 2.6      |\n",
      "| time/              |          |\n",
      "|    fps             | 634      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 129      |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 933         |\n",
      "|    ep_rew_mean          | 2.71        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 638         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003207182 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.0992      |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.000248    |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00711    |\n",
      "|    value_loss           | 0.0501      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 925          |\n",
      "|    ep_rew_mean          | 2.7          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 641          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 134          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019283049 |\n",
      "|    clip_fraction        | 0.0899       |\n",
      "|    clip_range           | 0.0992       |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 0.687        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | -0.00282     |\n",
      "|    n_updates            | 205          |\n",
      "|    policy_gradient_loss | -0.00681     |\n",
      "|    value_loss           | 0.0462       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 926          |\n",
      "|    ep_rew_mean          | 2.77         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 644          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 136          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025701453 |\n",
      "|    clip_fraction        | 0.151        |\n",
      "|    clip_range           | 0.0991       |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0.655        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | -0.0115      |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0112      |\n",
      "|    value_loss           | 0.0533       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=4.20 +/- 2.48\n",
      "Episode length: 1247.80 +/- 458.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.25e+03     |\n",
      "|    mean_reward          | 4.2          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 90000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021488885 |\n",
      "|    clip_fraction        | 0.131        |\n",
      "|    clip_range           | 0.0991       |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 0.722        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | -0.00234     |\n",
      "|    n_updates            | 215          |\n",
      "|    policy_gradient_loss | -0.00816     |\n",
      "|    value_loss           | 0.0533       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 915      |\n",
      "|    ep_rew_mean     | 2.69     |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 142      |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 934          |\n",
      "|    ep_rew_mean          | 2.81         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 636          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 144          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031052595 |\n",
      "|    clip_fraction        | 0.117        |\n",
      "|    clip_range           | 0.0991       |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 0.706        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | -0.00745     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00863     |\n",
      "|    value_loss           | 0.0597       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 985          |\n",
      "|    ep_rew_mean          | 3.13         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 638          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 147          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036073723 |\n",
      "|    clip_fraction        | 0.17         |\n",
      "|    clip_range           | 0.0991       |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0.748        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | -0.0156      |\n",
      "|    n_updates            | 225          |\n",
      "|    policy_gradient_loss | -0.012       |\n",
      "|    value_loss           | 0.0524       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.01e+03    |\n",
      "|    ep_rew_mean          | 3.27        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 640         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 150         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003455489 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.0991      |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.757       |\n",
      "|    learning_rate        | 0.000248    |\n",
      "|    loss                 | -0.00824    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00897    |\n",
      "|    value_loss           | 0.047       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.01e+03     |\n",
      "|    ep_rew_mean          | 3.27         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 643          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 152          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033257464 |\n",
      "|    clip_fraction        | 0.175        |\n",
      "|    clip_range           | 0.099        |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0.768        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | -0.0132      |\n",
      "|    n_updates            | 235          |\n",
      "|    policy_gradient_loss | -0.0124      |\n",
      "|    value_loss           | 0.0544       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=6.20 +/- 3.49\n",
      "Episode length: 1397.20 +/- 419.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.4e+03      |\n",
      "|    mean_reward          | 6.2          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046106367 |\n",
      "|    clip_fraction        | 0.211        |\n",
      "|    clip_range           | 0.099        |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0.739        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | -0.0181      |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0113      |\n",
      "|    value_loss           | 0.0446       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.04e+03 |\n",
      "|    ep_rew_mean     | 3.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 630      |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 159      |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.06e+03     |\n",
      "|    ep_rew_mean          | 3.59         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 633          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 161          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027842894 |\n",
      "|    clip_fraction        | 0.177        |\n",
      "|    clip_range           | 0.099        |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0.787        |\n",
      "|    learning_rate        | 0.000247     |\n",
      "|    loss                 | -0.0186      |\n",
      "|    n_updates            | 245          |\n",
      "|    policy_gradient_loss | -0.0112      |\n",
      "|    value_loss           | 0.0402       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.07e+03     |\n",
      "|    ep_rew_mean          | 3.64         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 635          |\n",
      "|    iterations           | 51           |\n",
      "|    time_elapsed         | 164          |\n",
      "|    total_timesteps      | 104448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042120256 |\n",
      "|    clip_fraction        | 0.203        |\n",
      "|    clip_range           | 0.099        |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.000247     |\n",
      "|    loss                 | -0.0104      |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.0118      |\n",
      "|    value_loss           | 0.0424       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.09e+03     |\n",
      "|    ep_rew_mean          | 3.8          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 638          |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 166          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030535222 |\n",
      "|    clip_fraction        | 0.157        |\n",
      "|    clip_range           | 0.099        |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0.642        |\n",
      "|    learning_rate        | 0.000247     |\n",
      "|    loss                 | -0.0113      |\n",
      "|    n_updates            | 255          |\n",
      "|    policy_gradient_loss | -0.00926     |\n",
      "|    value_loss           | 0.0457       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.12e+03     |\n",
      "|    ep_rew_mean          | 4.11         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 641          |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 169          |\n",
      "|    total_timesteps      | 108544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034942888 |\n",
      "|    clip_fraction        | 0.164        |\n",
      "|    clip_range           | 0.0989       |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0.682        |\n",
      "|    learning_rate        | 0.000247     |\n",
      "|    loss                 | -0.0139      |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0106      |\n",
      "|    value_loss           | 0.0455       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=4.40 +/- 3.01\n",
      "Episode length: 1164.60 +/- 466.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.16e+03    |\n",
      "|    mean_reward          | 4.4         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 110000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003237625 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.0989      |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.000247    |\n",
      "|    loss                 | -0.0171     |\n",
      "|    n_updates            | 265         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    value_loss           | 0.0464      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.13e+03 |\n",
      "|    ep_rew_mean     | 4.26     |\n",
      "| time/              |          |\n",
      "|    fps             | 632      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 174      |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.12e+03     |\n",
      "|    ep_rew_mean          | 4.2          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 634          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 177          |\n",
      "|    total_timesteps      | 112640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036377863 |\n",
      "|    clip_fraction        | 0.172        |\n",
      "|    clip_range           | 0.0989       |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0.759        |\n",
      "|    learning_rate        | 0.000247     |\n",
      "|    loss                 | -0.0212      |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.012       |\n",
      "|    value_loss           | 0.0455       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.12e+03    |\n",
      "|    ep_rew_mean          | 4.14        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 637         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 180         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003844465 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.0989      |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.000247    |\n",
      "|    loss                 | -0.0158     |\n",
      "|    n_updates            | 275         |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 0.038       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.15e+03     |\n",
      "|    ep_rew_mean          | 4.43         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 638          |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 182          |\n",
      "|    total_timesteps      | 116736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035053699 |\n",
      "|    clip_fraction        | 0.153        |\n",
      "|    clip_range           | 0.0989       |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0.734        |\n",
      "|    learning_rate        | 0.000247     |\n",
      "|    loss                 | -0.0155      |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.0111      |\n",
      "|    value_loss           | 0.0393       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.15e+03     |\n",
      "|    ep_rew_mean          | 4.45         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 640          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 185          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035551381 |\n",
      "|    clip_fraction        | 0.164        |\n",
      "|    clip_range           | 0.0988       |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0.752        |\n",
      "|    learning_rate        | 0.000247     |\n",
      "|    loss                 | -0.0162      |\n",
      "|    n_updates            | 285          |\n",
      "|    policy_gradient_loss | -0.0122      |\n",
      "|    value_loss           | 0.0473       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=3.60 +/- 1.50\n",
      "Episode length: 1028.40 +/- 221.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.03e+03    |\n",
      "|    mean_reward          | 3.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004142902 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.0988      |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.000247    |\n",
      "|    loss                 | -0.0183     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 0.0505      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.15e+03 |\n",
      "|    ep_rew_mean     | 4.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 632      |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 191      |\n",
      "|    total_timesteps | 120832   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.16e+03     |\n",
      "|    ep_rew_mean          | 4.77         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 634          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 193          |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037472048 |\n",
      "|    clip_fraction        | 0.212        |\n",
      "|    clip_range           | 0.0988       |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0.743        |\n",
      "|    learning_rate        | 0.000247     |\n",
      "|    loss                 | -0.0225      |\n",
      "|    n_updates            | 295          |\n",
      "|    policy_gradient_loss | -0.0125      |\n",
      "|    value_loss           | 0.0459       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.17e+03    |\n",
      "|    ep_rew_mean          | 4.87        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 636         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 196         |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003644328 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.0988      |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.000247    |\n",
      "|    loss                 | -0.0145     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 0.0486      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.2e+03     |\n",
      "|    ep_rew_mean          | 5.11        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 638         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 198         |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003198299 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.0988      |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 0.000247    |\n",
      "|    loss                 | -0.00984    |\n",
      "|    n_updates            | 305         |\n",
      "|    policy_gradient_loss | -0.00978    |\n",
      "|    value_loss           | 0.0484      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.22e+03    |\n",
      "|    ep_rew_mean          | 5.28        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 640         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 201         |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003998985 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.0987      |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.000247    |\n",
      "|    loss                 | -0.0188     |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 0.0432      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=4.40 +/- 3.26\n",
      "Episode length: 1138.00 +/- 457.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.14e+03   |\n",
      "|    mean_reward          | 4.4        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 130000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00412588 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.0987     |\n",
      "|    entropy_loss         | -1.21      |\n",
      "|    explained_variance   | 0.81       |\n",
      "|    learning_rate        | 0.000247   |\n",
      "|    loss                 | -0.0247    |\n",
      "|    n_updates            | 315        |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    value_loss           | 0.0412     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.22e+03 |\n",
      "|    ep_rew_mean     | 5.34     |\n",
      "| time/              |          |\n",
      "|    fps             | 631      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 207      |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.23e+03     |\n",
      "|    ep_rew_mean          | 5.46         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 633          |\n",
      "|    iterations           | 65           |\n",
      "|    time_elapsed         | 209          |\n",
      "|    total_timesteps      | 133120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033716534 |\n",
      "|    clip_fraction        | 0.162        |\n",
      "|    clip_range           | 0.0987       |\n",
      "|    entropy_loss         | -1.18        |\n",
      "|    explained_variance   | 0.805        |\n",
      "|    learning_rate        | 0.000247     |\n",
      "|    loss                 | -0.0188      |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0116      |\n",
      "|    value_loss           | 0.0419       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.28e+03     |\n",
      "|    ep_rew_mean          | 5.8          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 635          |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 212          |\n",
      "|    total_timesteps      | 135168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042666243 |\n",
      "|    clip_fraction        | 0.169        |\n",
      "|    clip_range           | 0.0987       |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0.788        |\n",
      "|    learning_rate        | 0.000247     |\n",
      "|    loss                 | -0.0196      |\n",
      "|    n_updates            | 325          |\n",
      "|    policy_gradient_loss | -0.011       |\n",
      "|    value_loss           | 0.0385       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.27e+03    |\n",
      "|    ep_rew_mean          | 5.76        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 638         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 214         |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005072777 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.0986      |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.000247    |\n",
      "|    loss                 | -0.0184     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 0.0386      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.28e+03    |\n",
      "|    ep_rew_mean          | 5.79        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 639         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 217         |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004219396 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.0986      |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.000247    |\n",
      "|    loss                 | -0.027      |\n",
      "|    n_updates            | 335         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 0.0347      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=9.80 +/- 3.60\n",
      "Episode length: 1748.40 +/- 396.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.75e+03     |\n",
      "|    mean_reward          | 9.8          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 140000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034382816 |\n",
      "|    clip_fraction        | 0.173        |\n",
      "|    clip_range           | 0.0986       |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0.8          |\n",
      "|    learning_rate        | 0.000247     |\n",
      "|    loss                 | -0.0152      |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.0122      |\n",
      "|    value_loss           | 0.0377       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 5.85     |\n",
      "| time/              |          |\n",
      "|    fps             | 628      |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 224      |\n",
      "|    total_timesteps | 141312   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.32e+03     |\n",
      "|    ep_rew_mean          | 5.89         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 630          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 227          |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037134402 |\n",
      "|    clip_fraction        | 0.177        |\n",
      "|    clip_range           | 0.0986       |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0.74         |\n",
      "|    learning_rate        | 0.000246     |\n",
      "|    loss                 | -0.0149      |\n",
      "|    n_updates            | 345          |\n",
      "|    policy_gradient_loss | -0.0109      |\n",
      "|    value_loss           | 0.0471       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.37e+03     |\n",
      "|    ep_rew_mean          | 6.29         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 631          |\n",
      "|    iterations           | 71           |\n",
      "|    time_elapsed         | 230          |\n",
      "|    total_timesteps      | 145408       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030521434 |\n",
      "|    clip_fraction        | 0.147        |\n",
      "|    clip_range           | 0.0986       |\n",
      "|    entropy_loss         | -1.19        |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 0.000246     |\n",
      "|    loss                 | -0.0167      |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.0097      |\n",
      "|    value_loss           | 0.0435       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.37e+03     |\n",
      "|    ep_rew_mean          | 6.26         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 633          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 232          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035368085 |\n",
      "|    clip_fraction        | 0.197        |\n",
      "|    clip_range           | 0.0985       |\n",
      "|    entropy_loss         | -1.18        |\n",
      "|    explained_variance   | 0.732        |\n",
      "|    learning_rate        | 0.000246     |\n",
      "|    loss                 | -0.0244      |\n",
      "|    n_updates            | 355          |\n",
      "|    policy_gradient_loss | -0.0124      |\n",
      "|    value_loss           | 0.0421       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.36e+03     |\n",
      "|    ep_rew_mean          | 6.36         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 635          |\n",
      "|    iterations           | 73           |\n",
      "|    time_elapsed         | 235          |\n",
      "|    total_timesteps      | 149504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032509305 |\n",
      "|    clip_fraction        | 0.177        |\n",
      "|    clip_range           | 0.0985       |\n",
      "|    entropy_loss         | -1.17        |\n",
      "|    explained_variance   | 0.753        |\n",
      "|    learning_rate        | 0.000246     |\n",
      "|    loss                 | -0.0188      |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.0105      |\n",
      "|    value_loss           | 0.0454       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=7.60 +/- 4.80\n",
      "Episode length: 1427.00 +/- 529.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.43e+03     |\n",
      "|    mean_reward          | 7.6          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 150000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037880205 |\n",
      "|    clip_fraction        | 0.185        |\n",
      "|    clip_range           | 0.0985       |\n",
      "|    entropy_loss         | -1.19        |\n",
      "|    explained_variance   | 0.775        |\n",
      "|    learning_rate        | 0.000246     |\n",
      "|    loss                 | -0.0159      |\n",
      "|    n_updates            | 365          |\n",
      "|    policy_gradient_loss | -0.0135      |\n",
      "|    value_loss           | 0.0397       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.42e+03 |\n",
      "|    ep_rew_mean     | 6.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 627      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 241      |\n",
      "|    total_timesteps | 151552   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.41e+03     |\n",
      "|    ep_rew_mean          | 6.76         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 629          |\n",
      "|    iterations           | 75           |\n",
      "|    time_elapsed         | 244          |\n",
      "|    total_timesteps      | 153600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042697433 |\n",
      "|    clip_fraction        | 0.223        |\n",
      "|    clip_range           | 0.0985       |\n",
      "|    entropy_loss         | -1.18        |\n",
      "|    explained_variance   | 0.675        |\n",
      "|    learning_rate        | 0.000246     |\n",
      "|    loss                 | -0.0118      |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.0136      |\n",
      "|    value_loss           | 0.0511       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.42e+03    |\n",
      "|    ep_rew_mean          | 6.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 631         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 246         |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004638125 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.0985      |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.000246    |\n",
      "|    loss                 | -0.00467    |\n",
      "|    n_updates            | 375         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 0.0532      |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 1.42e+03 |\n",
      "|    ep_rew_mean          | 6.69     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 633      |\n",
      "|    iterations           | 77       |\n",
      "|    time_elapsed         | 249      |\n",
      "|    total_timesteps      | 157696   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.003556 |\n",
      "|    clip_fraction        | 0.191    |\n",
      "|    clip_range           | 0.0984   |\n",
      "|    entropy_loss         | -1.17    |\n",
      "|    explained_variance   | 0.673    |\n",
      "|    learning_rate        | 0.000246 |\n",
      "|    loss                 | -0.0225  |\n",
      "|    n_updates            | 380      |\n",
      "|    policy_gradient_loss | -0.0127  |\n",
      "|    value_loss           | 0.0541   |\n",
      "--------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.42e+03     |\n",
      "|    ep_rew_mean          | 6.77         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 634          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 251          |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042373016 |\n",
      "|    clip_fraction        | 0.202        |\n",
      "|    clip_range           | 0.0984       |\n",
      "|    entropy_loss         | -1.15        |\n",
      "|    explained_variance   | 0.722        |\n",
      "|    learning_rate        | 0.000246     |\n",
      "|    loss                 | -0.0157      |\n",
      "|    n_updates            | 385          |\n",
      "|    policy_gradient_loss | -0.0139      |\n",
      "|    value_loss           | 0.0541       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=10.40 +/- 5.31\n",
      "Episode length: 1887.80 +/- 519.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.89e+03     |\n",
      "|    mean_reward          | 10.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046666227 |\n",
      "|    clip_fraction        | 0.2          |\n",
      "|    clip_range           | 0.0984       |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0.812        |\n",
      "|    learning_rate        | 0.000246     |\n",
      "|    loss                 | -0.0151      |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.0117      |\n",
      "|    value_loss           | 0.0448       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.42e+03 |\n",
      "|    ep_rew_mean     | 6.71     |\n",
      "| time/              |          |\n",
      "|    fps             | 624      |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 259      |\n",
      "|    total_timesteps | 161792   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.43e+03    |\n",
      "|    ep_rew_mean          | 6.87        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 626         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 261         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004865307 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.0984      |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.000246    |\n",
      "|    loss                 | -0.0244     |\n",
      "|    n_updates            | 395         |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 0.0375      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.45e+03     |\n",
      "|    ep_rew_mean          | 7.03         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 627          |\n",
      "|    iterations           | 81           |\n",
      "|    time_elapsed         | 264          |\n",
      "|    total_timesteps      | 165888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042792414 |\n",
      "|    clip_fraction        | 0.194        |\n",
      "|    clip_range           | 0.0984       |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.817        |\n",
      "|    learning_rate        | 0.000246     |\n",
      "|    loss                 | -0.0234      |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.012       |\n",
      "|    value_loss           | 0.0331       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.48e+03    |\n",
      "|    ep_rew_mean          | 7.25        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 629         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 266         |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004246217 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.0983      |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.774       |\n",
      "|    learning_rate        | 0.000246    |\n",
      "|    loss                 | -0.0223     |\n",
      "|    n_updates            | 405         |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 0.0439      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.49e+03    |\n",
      "|    ep_rew_mean          | 7.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 630         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004260448 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.0983      |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.000246    |\n",
      "|    loss                 | -0.0187     |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 0.0494      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=6.60 +/- 2.42\n",
      "Episode length: 1449.80 +/- 90.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.45e+03    |\n",
      "|    mean_reward          | 6.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 170000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004491184 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.0983      |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.000246    |\n",
      "|    loss                 | -0.0257     |\n",
      "|    n_updates            | 415         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 0.049       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.48e+03 |\n",
      "|    ep_rew_mean     | 7.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 622      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 276      |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.49e+03     |\n",
      "|    ep_rew_mean          | 7.6          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 624          |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 278          |\n",
      "|    total_timesteps      | 174080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051853587 |\n",
      "|    clip_fraction        | 0.186        |\n",
      "|    clip_range           | 0.0983       |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 0.771        |\n",
      "|    learning_rate        | 0.000246     |\n",
      "|    loss                 | -0.0184      |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.012       |\n",
      "|    value_loss           | 0.0482       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.48e+03    |\n",
      "|    ep_rew_mean          | 7.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 626         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 281         |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003867371 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.0983      |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.529       |\n",
      "|    learning_rate        | 0.000246    |\n",
      "|    loss                 | -0.0164     |\n",
      "|    n_updates            | 425         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 0.0676      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | 7.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 627         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 283         |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004432192 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.0982      |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 0.000246    |\n",
      "|    loss                 | -0.0235     |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 0.0548      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=8.80 +/- 2.56\n",
      "Episode length: 1786.00 +/- 347.23\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.79e+03     |\n",
      "|    mean_reward          | 8.8          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 180000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042622844 |\n",
      "|    clip_fraction        | 0.193        |\n",
      "|    clip_range           | 0.0982       |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 0.736        |\n",
      "|    learning_rate        | 0.000246     |\n",
      "|    loss                 | -0.0201      |\n",
      "|    n_updates            | 435          |\n",
      "|    policy_gradient_loss | -0.0125      |\n",
      "|    value_loss           | 0.0494       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.51e+03 |\n",
      "|    ep_rew_mean     | 7.92     |\n",
      "| time/              |          |\n",
      "|    fps             | 618      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 291      |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.52e+03     |\n",
      "|    ep_rew_mean          | 8.01         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 619          |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 294          |\n",
      "|    total_timesteps      | 182272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063824733 |\n",
      "|    clip_fraction        | 0.241        |\n",
      "|    clip_range           | 0.0982       |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0.71         |\n",
      "|    learning_rate        | 0.000245     |\n",
      "|    loss                 | -0.0162      |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.0104      |\n",
      "|    value_loss           | 0.0497       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.53e+03     |\n",
      "|    ep_rew_mean          | 8.16         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 621          |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 296          |\n",
      "|    total_timesteps      | 184320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038994926 |\n",
      "|    clip_fraction        | 0.179        |\n",
      "|    clip_range           | 0.0982       |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 0.817        |\n",
      "|    learning_rate        | 0.000245     |\n",
      "|    loss                 | -0.0218      |\n",
      "|    n_updates            | 445          |\n",
      "|    policy_gradient_loss | -0.00917     |\n",
      "|    value_loss           | 0.0479       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.56e+03    |\n",
      "|    ep_rew_mean          | 8.29        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 622         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 299         |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005095627 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.0982      |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.000245    |\n",
      "|    loss                 | -0.0155     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 0.0389      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.59e+03     |\n",
      "|    ep_rew_mean          | 8.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 624          |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 301          |\n",
      "|    total_timesteps      | 188416       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041865166 |\n",
      "|    clip_fraction        | 0.186        |\n",
      "|    clip_range           | 0.0981       |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.797        |\n",
      "|    learning_rate        | 0.000245     |\n",
      "|    loss                 | -0.0205      |\n",
      "|    n_updates            | 455          |\n",
      "|    policy_gradient_loss | -0.0128      |\n",
      "|    value_loss           | 0.0412       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=11.20 +/- 4.53\n",
      "Episode length: 1715.60 +/- 476.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.72e+03    |\n",
      "|    mean_reward          | 11.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 190000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004424672 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.0981      |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.744       |\n",
      "|    learning_rate        | 0.000245    |\n",
      "|    loss                 | -0.0197     |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 0.0409      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.59e+03 |\n",
      "|    ep_rew_mean     | 8.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 615      |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 309      |\n",
      "|    total_timesteps | 190464   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.61e+03    |\n",
      "|    ep_rew_mean          | 8.88        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 617         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 311         |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004255594 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.0981      |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.000245    |\n",
      "|    loss                 | -0.0238     |\n",
      "|    n_updates            | 465         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 0.0441      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.62e+03     |\n",
      "|    ep_rew_mean          | 8.94         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 618          |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 314          |\n",
      "|    total_timesteps      | 194560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043929457 |\n",
      "|    clip_fraction        | 0.18         |\n",
      "|    clip_range           | 0.0981       |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 0.000245     |\n",
      "|    loss                 | -0.0193      |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.0121      |\n",
      "|    value_loss           | 0.0432       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.67e+03     |\n",
      "|    ep_rew_mean          | 9.36         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 620          |\n",
      "|    iterations           | 96           |\n",
      "|    time_elapsed         | 316          |\n",
      "|    total_timesteps      | 196608       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044363905 |\n",
      "|    clip_fraction        | 0.171        |\n",
      "|    clip_range           | 0.0981       |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.781        |\n",
      "|    learning_rate        | 0.000245     |\n",
      "|    loss                 | -0.0218      |\n",
      "|    n_updates            | 475          |\n",
      "|    policy_gradient_loss | -0.0116      |\n",
      "|    value_loss           | 0.0498       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.7e+03     |\n",
      "|    ep_rew_mean          | 9.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 621         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 319         |\n",
      "|    total_timesteps      | 198656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003908509 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.098       |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 0.000245    |\n",
      "|    loss                 | -0.0191     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 0.0481      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=5.00 +/- 2.10\n",
      "Episode length: 1382.60 +/- 379.65\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.38e+03     |\n",
      "|    mean_reward          | 5            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 200000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043549556 |\n",
      "|    clip_fraction        | 0.186        |\n",
      "|    clip_range           | 0.098        |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.715        |\n",
      "|    learning_rate        | 0.000245     |\n",
      "|    loss                 | -0.0345      |\n",
      "|    n_updates            | 485          |\n",
      "|    policy_gradient_loss | -0.0135      |\n",
      "|    value_loss           | 0.0461       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.74e+03 |\n",
      "|    ep_rew_mean     | 9.95     |\n",
      "| time/              |          |\n",
      "|    fps             | 615      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 326      |\n",
      "|    total_timesteps | 200704   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.76e+03     |\n",
      "|    ep_rew_mean          | 10.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 616          |\n",
      "|    iterations           | 99           |\n",
      "|    time_elapsed         | 328          |\n",
      "|    total_timesteps      | 202752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055904007 |\n",
      "|    clip_fraction        | 0.234        |\n",
      "|    clip_range           | 0.098        |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.811        |\n",
      "|    learning_rate        | 0.000245     |\n",
      "|    loss                 | -0.0229      |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.0142      |\n",
      "|    value_loss           | 0.0439       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.75e+03    |\n",
      "|    ep_rew_mean          | 10.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 617         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 331         |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005049347 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.098       |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 0.000245    |\n",
      "|    loss                 | -0.0156     |\n",
      "|    n_updates            | 495         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    value_loss           | 0.062       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.76e+03     |\n",
      "|    ep_rew_mean          | 10.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 619          |\n",
      "|    iterations           | 101          |\n",
      "|    time_elapsed         | 333          |\n",
      "|    total_timesteps      | 206848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049270475 |\n",
      "|    clip_fraction        | 0.176        |\n",
      "|    clip_range           | 0.098        |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.741        |\n",
      "|    learning_rate        | 0.000245     |\n",
      "|    loss                 | -0.0178      |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0139      |\n",
      "|    value_loss           | 0.0569       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.76e+03     |\n",
      "|    ep_rew_mean          | 10.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 620          |\n",
      "|    iterations           | 102          |\n",
      "|    time_elapsed         | 336          |\n",
      "|    total_timesteps      | 208896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048520286 |\n",
      "|    clip_fraction        | 0.183        |\n",
      "|    clip_range           | 0.0979       |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.719        |\n",
      "|    learning_rate        | 0.000245     |\n",
      "|    loss                 | -0.0187      |\n",
      "|    n_updates            | 505          |\n",
      "|    policy_gradient_loss | -0.0118      |\n",
      "|    value_loss           | 0.0534       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=13.40 +/- 5.24\n",
      "Episode length: 1869.60 +/- 500.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.87e+03    |\n",
      "|    mean_reward          | 13.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 210000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004197442 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.0979      |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.000245    |\n",
      "|    loss                 | -0.0257     |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 0.0477      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.77e+03 |\n",
      "|    ep_rew_mean     | 10.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 613      |\n",
      "|    iterations      | 103      |\n",
      "|    time_elapsed    | 344      |\n",
      "|    total_timesteps | 210944   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.77e+03     |\n",
      "|    ep_rew_mean          | 10.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 614          |\n",
      "|    iterations           | 104          |\n",
      "|    time_elapsed         | 346          |\n",
      "|    total_timesteps      | 212992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041989526 |\n",
      "|    clip_fraction        | 0.196        |\n",
      "|    clip_range           | 0.0979       |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.778        |\n",
      "|    learning_rate        | 0.000245     |\n",
      "|    loss                 | -0.0257      |\n",
      "|    n_updates            | 515          |\n",
      "|    policy_gradient_loss | -0.0148      |\n",
      "|    value_loss           | 0.0448       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.77e+03    |\n",
      "|    ep_rew_mean          | 10.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 616         |\n",
      "|    iterations           | 105         |\n",
      "|    time_elapsed         | 349         |\n",
      "|    total_timesteps      | 215040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003482664 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.0979      |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.744       |\n",
      "|    learning_rate        | 0.000245    |\n",
      "|    loss                 | -0.0177     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 0.0524      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.77e+03    |\n",
      "|    ep_rew_mean          | 10.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 617         |\n",
      "|    iterations           | 106         |\n",
      "|    time_elapsed         | 351         |\n",
      "|    total_timesteps      | 217088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005191702 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.0978      |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.000245    |\n",
      "|    loss                 | -0.0177     |\n",
      "|    n_updates            | 525         |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 0.0456      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.76e+03     |\n",
      "|    ep_rew_mean          | 10.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 618          |\n",
      "|    iterations           | 107          |\n",
      "|    time_elapsed         | 354          |\n",
      "|    total_timesteps      | 219136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049432293 |\n",
      "|    clip_fraction        | 0.236        |\n",
      "|    clip_range           | 0.0978       |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.748        |\n",
      "|    learning_rate        | 0.000245     |\n",
      "|    loss                 | -0.0165      |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.0113      |\n",
      "|    value_loss           | 0.0474       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=10.60 +/- 3.07\n",
      "Episode length: 1795.00 +/- 383.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.8e+03      |\n",
      "|    mean_reward          | 10.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 220000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056635365 |\n",
      "|    clip_fraction        | 0.222        |\n",
      "|    clip_range           | 0.0978       |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 0.000245     |\n",
      "|    loss                 | -0.0277      |\n",
      "|    n_updates            | 535          |\n",
      "|    policy_gradient_loss | -0.0143      |\n",
      "|    value_loss           | 0.0372       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.77e+03 |\n",
      "|    ep_rew_mean     | 10.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 611      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 361      |\n",
      "|    total_timesteps | 221184   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.76e+03     |\n",
      "|    ep_rew_mean          | 10.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 612          |\n",
      "|    iterations           | 109          |\n",
      "|    time_elapsed         | 364          |\n",
      "|    total_timesteps      | 223232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035917922 |\n",
      "|    clip_fraction        | 0.178        |\n",
      "|    clip_range           | 0.0978       |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.748        |\n",
      "|    learning_rate        | 0.000244     |\n",
      "|    loss                 | -0.0178      |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0128      |\n",
      "|    value_loss           | 0.0449       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.8e+03      |\n",
      "|    ep_rew_mean          | 10.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 613          |\n",
      "|    iterations           | 110          |\n",
      "|    time_elapsed         | 366          |\n",
      "|    total_timesteps      | 225280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049366294 |\n",
      "|    clip_fraction        | 0.208        |\n",
      "|    clip_range           | 0.0978       |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.818        |\n",
      "|    learning_rate        | 0.000244     |\n",
      "|    loss                 | -0.0242      |\n",
      "|    n_updates            | 545          |\n",
      "|    policy_gradient_loss | -0.0146      |\n",
      "|    value_loss           | 0.0413       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.81e+03     |\n",
      "|    ep_rew_mean          | 10.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 615          |\n",
      "|    iterations           | 111          |\n",
      "|    time_elapsed         | 369          |\n",
      "|    total_timesteps      | 227328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041223373 |\n",
      "|    clip_fraction        | 0.192        |\n",
      "|    clip_range           | 0.0977       |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.000244     |\n",
      "|    loss                 | -0.0184      |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.0125      |\n",
      "|    value_loss           | 0.0336       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.81e+03     |\n",
      "|    ep_rew_mean          | 10.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 616          |\n",
      "|    iterations           | 112          |\n",
      "|    time_elapsed         | 372          |\n",
      "|    total_timesteps      | 229376       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049077375 |\n",
      "|    clip_fraction        | 0.237        |\n",
      "|    clip_range           | 0.0977       |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 0.000244     |\n",
      "|    loss                 | -0.0223      |\n",
      "|    n_updates            | 555          |\n",
      "|    policy_gradient_loss | -0.015       |\n",
      "|    value_loss           | 0.0389       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=16.40 +/- 6.83\n",
      "Episode length: 2378.20 +/- 723.89\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.38e+03     |\n",
      "|    mean_reward          | 16.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 230000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042818147 |\n",
      "|    clip_fraction        | 0.173        |\n",
      "|    clip_range           | 0.0977       |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.781        |\n",
      "|    learning_rate        | 0.000244     |\n",
      "|    loss                 | -0.0171      |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0117      |\n",
      "|    value_loss           | 0.0446       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.82e+03 |\n",
      "|    ep_rew_mean     | 10.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 607      |\n",
      "|    iterations      | 113      |\n",
      "|    time_elapsed    | 380      |\n",
      "|    total_timesteps | 231424   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.82e+03    |\n",
      "|    ep_rew_mean          | 10.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 608         |\n",
      "|    iterations           | 114         |\n",
      "|    time_elapsed         | 383         |\n",
      "|    total_timesteps      | 233472      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005006182 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.0977      |\n",
      "|    entropy_loss         | -0.985      |\n",
      "|    explained_variance   | 0.718       |\n",
      "|    learning_rate        | 0.000244    |\n",
      "|    loss                 | -0.0191     |\n",
      "|    n_updates            | 565         |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 0.0496      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.83e+03    |\n",
      "|    ep_rew_mean          | 10.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 609         |\n",
      "|    iterations           | 115         |\n",
      "|    time_elapsed         | 386         |\n",
      "|    total_timesteps      | 235520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004270516 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.0977      |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.000244    |\n",
      "|    loss                 | -0.0277     |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 0.0442      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.81e+03    |\n",
      "|    ep_rew_mean          | 10.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 611         |\n",
      "|    iterations           | 116         |\n",
      "|    time_elapsed         | 388         |\n",
      "|    total_timesteps      | 237568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005598872 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.0976      |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.000244    |\n",
      "|    loss                 | -0.0103     |\n",
      "|    n_updates            | 575         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 0.0481      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.79e+03     |\n",
      "|    ep_rew_mean          | 10.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 612          |\n",
      "|    iterations           | 117          |\n",
      "|    time_elapsed         | 391          |\n",
      "|    total_timesteps      | 239616       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061582094 |\n",
      "|    clip_fraction        | 0.223        |\n",
      "|    clip_range           | 0.0976       |\n",
      "|    entropy_loss         | -0.999       |\n",
      "|    explained_variance   | 0.735        |\n",
      "|    learning_rate        | 0.000244     |\n",
      "|    loss                 | -0.0126      |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0111      |\n",
      "|    value_loss           | 0.0521       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=11.20 +/- 2.79\n",
      "Episode length: 2048.00 +/- 171.72\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.05e+03     |\n",
      "|    mean_reward          | 11.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 240000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042298604 |\n",
      "|    clip_fraction        | 0.197        |\n",
      "|    clip_range           | 0.0976       |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.771        |\n",
      "|    learning_rate        | 0.000244     |\n",
      "|    loss                 | -0.0252      |\n",
      "|    n_updates            | 585          |\n",
      "|    policy_gradient_loss | -0.0147      |\n",
      "|    value_loss           | 0.0484       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.75e+03 |\n",
      "|    ep_rew_mean     | 9.98     |\n",
      "| time/              |          |\n",
      "|    fps             | 606      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 398      |\n",
      "|    total_timesteps | 241664   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.74e+03     |\n",
      "|    ep_rew_mean          | 9.86         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 607          |\n",
      "|    iterations           | 119          |\n",
      "|    time_elapsed         | 401          |\n",
      "|    total_timesteps      | 243712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050377054 |\n",
      "|    clip_fraction        | 0.197        |\n",
      "|    clip_range           | 0.0976       |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.794        |\n",
      "|    learning_rate        | 0.000244     |\n",
      "|    loss                 | -0.0276      |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.0127      |\n",
      "|    value_loss           | 0.0425       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.74e+03     |\n",
      "|    ep_rew_mean          | 9.98         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 608          |\n",
      "|    iterations           | 120          |\n",
      "|    time_elapsed         | 403          |\n",
      "|    total_timesteps      | 245760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044826805 |\n",
      "|    clip_fraction        | 0.19         |\n",
      "|    clip_range           | 0.0976       |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.749        |\n",
      "|    learning_rate        | 0.000244     |\n",
      "|    loss                 | -0.0188      |\n",
      "|    n_updates            | 595          |\n",
      "|    policy_gradient_loss | -0.0128      |\n",
      "|    value_loss           | 0.0516       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.75e+03     |\n",
      "|    ep_rew_mean          | 10           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 609          |\n",
      "|    iterations           | 121          |\n",
      "|    time_elapsed         | 406          |\n",
      "|    total_timesteps      | 247808       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055845166 |\n",
      "|    clip_fraction        | 0.222        |\n",
      "|    clip_range           | 0.0975       |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.711        |\n",
      "|    learning_rate        | 0.000244     |\n",
      "|    loss                 | -0.0215      |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.0152      |\n",
      "|    value_loss           | 0.0462       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.75e+03    |\n",
      "|    ep_rew_mean          | 10.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 611         |\n",
      "|    iterations           | 122         |\n",
      "|    time_elapsed         | 408         |\n",
      "|    total_timesteps      | 249856      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003975602 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.0975      |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.752       |\n",
      "|    learning_rate        | 0.000244    |\n",
      "|    loss                 | -0.0185     |\n",
      "|    n_updates            | 605         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 0.0469      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=10.60 +/- 3.72\n",
      "Episode length: 1918.60 +/- 398.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.92e+03    |\n",
      "|    mean_reward          | 10.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 250000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004598646 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.0975      |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.000244    |\n",
      "|    loss                 | -0.0202     |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 0.0414      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.75e+03 |\n",
      "|    ep_rew_mean     | 10.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 123      |\n",
      "|    time_elapsed    | 416      |\n",
      "|    total_timesteps | 251904   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.73e+03    |\n",
      "|    ep_rew_mean          | 10.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 606         |\n",
      "|    iterations           | 124         |\n",
      "|    time_elapsed         | 418         |\n",
      "|    total_timesteps      | 253952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006559439 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.0975      |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.000244    |\n",
      "|    loss                 | -0.022      |\n",
      "|    n_updates            | 615         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 0.0397      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.74e+03    |\n",
      "|    ep_rew_mean          | 10.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 608         |\n",
      "|    iterations           | 125         |\n",
      "|    time_elapsed         | 420         |\n",
      "|    total_timesteps      | 256000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005342685 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.0975      |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.000244    |\n",
      "|    loss                 | -0.0222     |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 0.0421      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.75e+03    |\n",
      "|    ep_rew_mean          | 10.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 609         |\n",
      "|    iterations           | 126         |\n",
      "|    time_elapsed         | 423         |\n",
      "|    total_timesteps      | 258048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003672111 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.0974      |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 0.000244    |\n",
      "|    loss                 | -0.0273     |\n",
      "|    n_updates            | 625         |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 0.0388      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=11.00 +/- 4.00\n",
      "Episode length: 1999.00 +/- 409.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2e+03       |\n",
      "|    mean_reward          | 11          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 260000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004915217 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.0974      |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 0.000244    |\n",
      "|    loss                 | -0.019      |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 0.0371      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.77e+03 |\n",
      "|    ep_rew_mean     | 10.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 127      |\n",
      "|    time_elapsed    | 430      |\n",
      "|    total_timesteps | 260096   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.79e+03     |\n",
      "|    ep_rew_mean          | 10.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 605          |\n",
      "|    iterations           | 128          |\n",
      "|    time_elapsed         | 432          |\n",
      "|    total_timesteps      | 262144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054903063 |\n",
      "|    clip_fraction        | 0.232        |\n",
      "|    clip_range           | 0.0974       |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.77         |\n",
      "|    learning_rate        | 0.000243     |\n",
      "|    loss                 | -0.021       |\n",
      "|    n_updates            | 635          |\n",
      "|    policy_gradient_loss | -0.0149      |\n",
      "|    value_loss           | 0.0358       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.8e+03     |\n",
      "|    ep_rew_mean          | 10.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 606         |\n",
      "|    iterations           | 129         |\n",
      "|    time_elapsed         | 435         |\n",
      "|    total_timesteps      | 264192      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005151596 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.0974      |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.000243    |\n",
      "|    loss                 | -0.0326     |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 0.0379      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.8e+03     |\n",
      "|    ep_rew_mean          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 608         |\n",
      "|    iterations           | 130         |\n",
      "|    time_elapsed         | 437         |\n",
      "|    total_timesteps      | 266240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005104771 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.0974      |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.768       |\n",
      "|    learning_rate        | 0.000243    |\n",
      "|    loss                 | -0.0194     |\n",
      "|    n_updates            | 645         |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 0.0382      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.79e+03    |\n",
      "|    ep_rew_mean          | 10.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 609         |\n",
      "|    iterations           | 131         |\n",
      "|    time_elapsed         | 439         |\n",
      "|    total_timesteps      | 268288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005883712 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.0973      |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.642       |\n",
      "|    learning_rate        | 0.000243    |\n",
      "|    loss                 | -0.0182     |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    value_loss           | 0.0576      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=11.20 +/- 3.06\n",
      "Episode length: 1963.80 +/- 372.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.96e+03    |\n",
      "|    mean_reward          | 11.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 270000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004962171 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.0973      |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.000243    |\n",
      "|    loss                 | -0.0257     |\n",
      "|    n_updates            | 655         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    value_loss           | 0.042       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.78e+03 |\n",
      "|    ep_rew_mean     | 10.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 447      |\n",
      "|    total_timesteps | 270336   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.78e+03     |\n",
      "|    ep_rew_mean          | 10.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 605          |\n",
      "|    iterations           | 133          |\n",
      "|    time_elapsed         | 449          |\n",
      "|    total_timesteps      | 272384       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055298745 |\n",
      "|    clip_fraction        | 0.225        |\n",
      "|    clip_range           | 0.0973       |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.781        |\n",
      "|    learning_rate        | 0.000243     |\n",
      "|    loss                 | -0.0174      |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.0118      |\n",
      "|    value_loss           | 0.0432       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.79e+03     |\n",
      "|    ep_rew_mean          | 10.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 607          |\n",
      "|    iterations           | 134          |\n",
      "|    time_elapsed         | 452          |\n",
      "|    total_timesteps      | 274432       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046843532 |\n",
      "|    clip_fraction        | 0.205        |\n",
      "|    clip_range           | 0.0973       |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.78         |\n",
      "|    learning_rate        | 0.000243     |\n",
      "|    loss                 | -0.0247      |\n",
      "|    n_updates            | 665          |\n",
      "|    policy_gradient_loss | -0.0153      |\n",
      "|    value_loss           | 0.0405       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.77e+03    |\n",
      "|    ep_rew_mean          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 608         |\n",
      "|    iterations           | 135         |\n",
      "|    time_elapsed         | 454         |\n",
      "|    total_timesteps      | 276480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005549079 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.0973      |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.000243    |\n",
      "|    loss                 | -0.0239     |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 0.0447      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.78e+03     |\n",
      "|    ep_rew_mean          | 10.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 609          |\n",
      "|    iterations           | 136          |\n",
      "|    time_elapsed         | 456          |\n",
      "|    total_timesteps      | 278528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064087585 |\n",
      "|    clip_fraction        | 0.225        |\n",
      "|    clip_range           | 0.0972       |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.798        |\n",
      "|    learning_rate        | 0.000243     |\n",
      "|    loss                 | -0.0263      |\n",
      "|    n_updates            | 675          |\n",
      "|    policy_gradient_loss | -0.0136      |\n",
      "|    value_loss           | 0.0444       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=9.80 +/- 4.45\n",
      "Episode length: 1687.20 +/- 369.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.69e+03    |\n",
      "|    mean_reward          | 9.8         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 280000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005150488 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.0972      |\n",
      "|    entropy_loss         | -0.971      |\n",
      "|    explained_variance   | 0.731       |\n",
      "|    learning_rate        | 0.000243    |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 0.0584      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.78e+03 |\n",
      "|    ep_rew_mean     | 10.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 137      |\n",
      "|    time_elapsed    | 463      |\n",
      "|    total_timesteps | 280576   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.8e+03      |\n",
      "|    ep_rew_mean          | 10.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 606          |\n",
      "|    iterations           | 138          |\n",
      "|    time_elapsed         | 465          |\n",
      "|    total_timesteps      | 282624       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055975798 |\n",
      "|    clip_fraction        | 0.246        |\n",
      "|    clip_range           | 0.0972       |\n",
      "|    entropy_loss         | -0.978       |\n",
      "|    explained_variance   | 0.8          |\n",
      "|    learning_rate        | 0.000243     |\n",
      "|    loss                 | -0.0259      |\n",
      "|    n_updates            | 685          |\n",
      "|    policy_gradient_loss | -0.0128      |\n",
      "|    value_loss           | 0.0397       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.84e+03    |\n",
      "|    ep_rew_mean          | 11.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 608         |\n",
      "|    iterations           | 139         |\n",
      "|    time_elapsed         | 468         |\n",
      "|    total_timesteps      | 284672      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005620213 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.0972      |\n",
      "|    entropy_loss         | -0.968      |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.000243    |\n",
      "|    loss                 | -0.0207     |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 0.0394      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.86e+03     |\n",
      "|    ep_rew_mean          | 11.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 609          |\n",
      "|    iterations           | 140          |\n",
      "|    time_elapsed         | 470          |\n",
      "|    total_timesteps      | 286720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076634856 |\n",
      "|    clip_fraction        | 0.256        |\n",
      "|    clip_range           | 0.0972       |\n",
      "|    entropy_loss         | -0.983       |\n",
      "|    explained_variance   | 0.8          |\n",
      "|    learning_rate        | 0.000243     |\n",
      "|    loss                 | -0.0228      |\n",
      "|    n_updates            | 695          |\n",
      "|    policy_gradient_loss | -0.0149      |\n",
      "|    value_loss           | 0.0401       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.88e+03    |\n",
      "|    ep_rew_mean          | 11.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 610         |\n",
      "|    iterations           | 141         |\n",
      "|    time_elapsed         | 472         |\n",
      "|    total_timesteps      | 288768      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007025892 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.0971      |\n",
      "|    entropy_loss         | -0.974      |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.000243    |\n",
      "|    loss                 | -0.0144     |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    value_loss           | 0.0473      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=13.40 +/- 5.54\n",
      "Episode length: 1821.80 +/- 406.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.82e+03    |\n",
      "|    mean_reward          | 13.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 290000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004068506 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.0971      |\n",
      "|    entropy_loss         | -0.989      |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 0.000243    |\n",
      "|    loss                 | -0.023      |\n",
      "|    n_updates            | 705         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 0.0432      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.88e+03 |\n",
      "|    ep_rew_mean     | 11.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 606      |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 479      |\n",
      "|    total_timesteps | 290816   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.9e+03     |\n",
      "|    ep_rew_mean          | 11.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 607         |\n",
      "|    iterations           | 143         |\n",
      "|    time_elapsed         | 481         |\n",
      "|    total_timesteps      | 292864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005361587 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.0971      |\n",
      "|    entropy_loss         | -0.991      |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.000243    |\n",
      "|    loss                 | -0.0242     |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 0.046       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.91e+03    |\n",
      "|    ep_rew_mean          | 11.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 608         |\n",
      "|    iterations           | 144         |\n",
      "|    time_elapsed         | 484         |\n",
      "|    total_timesteps      | 294912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006406782 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.0971      |\n",
      "|    entropy_loss         | -0.989      |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 0.000243    |\n",
      "|    loss                 | -0.0238     |\n",
      "|    n_updates            | 715         |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 0.032       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.91e+03    |\n",
      "|    ep_rew_mean          | 11.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 610         |\n",
      "|    iterations           | 145         |\n",
      "|    time_elapsed         | 486         |\n",
      "|    total_timesteps      | 296960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005280778 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.0971      |\n",
      "|    entropy_loss         | -0.944      |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.000243    |\n",
      "|    loss                 | -0.0262     |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 0.0356      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.91e+03    |\n",
      "|    ep_rew_mean          | 11.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 611         |\n",
      "|    iterations           | 146         |\n",
      "|    time_elapsed         | 488         |\n",
      "|    total_timesteps      | 299008      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005188235 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.097       |\n",
      "|    entropy_loss         | -0.963      |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.000243    |\n",
      "|    loss                 | -0.0216     |\n",
      "|    n_updates            | 725         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 0.0425      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=11.60 +/- 2.06\n",
      "Episode length: 1890.00 +/- 211.95\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.89e+03     |\n",
      "|    mean_reward          | 11.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 300000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045290585 |\n",
      "|    clip_fraction        | 0.173        |\n",
      "|    clip_range           | 0.097        |\n",
      "|    entropy_loss         | -0.979       |\n",
      "|    explained_variance   | 0.79         |\n",
      "|    learning_rate        | 0.000243     |\n",
      "|    loss                 | -0.0233      |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.0131      |\n",
      "|    value_loss           | 0.0388       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.92e+03 |\n",
      "|    ep_rew_mean     | 11.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 607      |\n",
      "|    iterations      | 147      |\n",
      "|    time_elapsed    | 495      |\n",
      "|    total_timesteps | 301056   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.91e+03     |\n",
      "|    ep_rew_mean          | 11.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 608          |\n",
      "|    iterations           | 148          |\n",
      "|    time_elapsed         | 498          |\n",
      "|    total_timesteps      | 303104       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058179563 |\n",
      "|    clip_fraction        | 0.187        |\n",
      "|    clip_range           | 0.097        |\n",
      "|    entropy_loss         | -0.941       |\n",
      "|    explained_variance   | 0.704        |\n",
      "|    learning_rate        | 0.000242     |\n",
      "|    loss                 | -0.0223      |\n",
      "|    n_updates            | 735          |\n",
      "|    policy_gradient_loss | -0.0135      |\n",
      "|    value_loss           | 0.0446       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 11.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 609         |\n",
      "|    iterations           | 149         |\n",
      "|    time_elapsed         | 500         |\n",
      "|    total_timesteps      | 305152      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004330827 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.097       |\n",
      "|    entropy_loss         | -0.964      |\n",
      "|    explained_variance   | 0.714       |\n",
      "|    learning_rate        | 0.000242    |\n",
      "|    loss                 | -0.0181     |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 0.0456      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.91e+03   |\n",
      "|    ep_rew_mean          | 11.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 610        |\n",
      "|    iterations           | 150        |\n",
      "|    time_elapsed         | 503        |\n",
      "|    total_timesteps      | 307200     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00578069 |\n",
      "|    clip_fraction        | 0.208      |\n",
      "|    clip_range           | 0.0969     |\n",
      "|    entropy_loss         | -0.984     |\n",
      "|    explained_variance   | 0.747      |\n",
      "|    learning_rate        | 0.000242   |\n",
      "|    loss                 | -0.0191    |\n",
      "|    n_updates            | 745        |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    value_loss           | 0.0513     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.91e+03     |\n",
      "|    ep_rew_mean          | 11.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 611          |\n",
      "|    iterations           | 151          |\n",
      "|    time_elapsed         | 505          |\n",
      "|    total_timesteps      | 309248       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070646773 |\n",
      "|    clip_fraction        | 0.217        |\n",
      "|    clip_range           | 0.0969       |\n",
      "|    entropy_loss         | -0.953       |\n",
      "|    explained_variance   | 0.729        |\n",
      "|    learning_rate        | 0.000242     |\n",
      "|    loss                 | -0.0186      |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.0125      |\n",
      "|    value_loss           | 0.0526       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=13.60 +/- 3.07\n",
      "Episode length: 2164.80 +/- 399.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.16e+03    |\n",
      "|    mean_reward          | 13.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 310000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005401986 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.0969      |\n",
      "|    entropy_loss         | -0.99       |\n",
      "|    explained_variance   | 0.757       |\n",
      "|    learning_rate        | 0.000242    |\n",
      "|    loss                 | -0.0247     |\n",
      "|    n_updates            | 755         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    value_loss           | 0.0438      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.91e+03 |\n",
      "|    ep_rew_mean     | 11.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 606      |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 512      |\n",
      "|    total_timesteps | 311296   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.91e+03    |\n",
      "|    ep_rew_mean          | 11.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 608         |\n",
      "|    iterations           | 153         |\n",
      "|    time_elapsed         | 515         |\n",
      "|    total_timesteps      | 313344      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005778251 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.0969      |\n",
      "|    entropy_loss         | -0.96       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.000242    |\n",
      "|    loss                 | -0.0163     |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 0.0437      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.94e+03   |\n",
      "|    ep_rew_mean          | 11.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 609        |\n",
      "|    iterations           | 154        |\n",
      "|    time_elapsed         | 517        |\n",
      "|    total_timesteps      | 315392     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00461697 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.0969     |\n",
      "|    entropy_loss         | -0.931     |\n",
      "|    explained_variance   | 0.795      |\n",
      "|    learning_rate        | 0.000242   |\n",
      "|    loss                 | -0.0204    |\n",
      "|    n_updates            | 765        |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    value_loss           | 0.0361     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.94e+03     |\n",
      "|    ep_rew_mean          | 11.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 610          |\n",
      "|    iterations           | 155          |\n",
      "|    time_elapsed         | 520          |\n",
      "|    total_timesteps      | 317440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051471516 |\n",
      "|    clip_fraction        | 0.221        |\n",
      "|    clip_range           | 0.0968       |\n",
      "|    entropy_loss         | -0.95        |\n",
      "|    explained_variance   | 0.808        |\n",
      "|    learning_rate        | 0.000242     |\n",
      "|    loss                 | -0.0227      |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.0152      |\n",
      "|    value_loss           | 0.0412       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.94e+03     |\n",
      "|    ep_rew_mean          | 11.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 611          |\n",
      "|    iterations           | 156          |\n",
      "|    time_elapsed         | 522          |\n",
      "|    total_timesteps      | 319488       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061181225 |\n",
      "|    clip_fraction        | 0.223        |\n",
      "|    clip_range           | 0.0968       |\n",
      "|    entropy_loss         | -0.984       |\n",
      "|    explained_variance   | 0.788        |\n",
      "|    learning_rate        | 0.000242     |\n",
      "|    loss                 | -0.02        |\n",
      "|    n_updates            | 775          |\n",
      "|    policy_gradient_loss | -0.0101      |\n",
      "|    value_loss           | 0.0415       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=13.00 +/- 5.87\n",
      "Episode length: 1906.60 +/- 513.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.91e+03     |\n",
      "|    mean_reward          | 13           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 320000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052834544 |\n",
      "|    clip_fraction        | 0.207        |\n",
      "|    clip_range           | 0.0968       |\n",
      "|    entropy_loss         | -0.989       |\n",
      "|    explained_variance   | 0.773        |\n",
      "|    learning_rate        | 0.000242     |\n",
      "|    loss                 | -0.0289      |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.015       |\n",
      "|    value_loss           | 0.0373       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.96e+03 |\n",
      "|    ep_rew_mean     | 11.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 607      |\n",
      "|    iterations      | 157      |\n",
      "|    time_elapsed    | 529      |\n",
      "|    total_timesteps | 321536   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.97e+03     |\n",
      "|    ep_rew_mean          | 11.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 608          |\n",
      "|    iterations           | 158          |\n",
      "|    time_elapsed         | 531          |\n",
      "|    total_timesteps      | 323584       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050016684 |\n",
      "|    clip_fraction        | 0.184        |\n",
      "|    clip_range           | 0.0968       |\n",
      "|    entropy_loss         | -0.936       |\n",
      "|    explained_variance   | 0.792        |\n",
      "|    learning_rate        | 0.000242     |\n",
      "|    loss                 | -0.0204      |\n",
      "|    n_updates            | 785          |\n",
      "|    policy_gradient_loss | -0.013       |\n",
      "|    value_loss           | 0.0349       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.96e+03    |\n",
      "|    ep_rew_mean          | 11.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 609         |\n",
      "|    iterations           | 159         |\n",
      "|    time_elapsed         | 534         |\n",
      "|    total_timesteps      | 325632      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004653718 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.0968      |\n",
      "|    entropy_loss         | -0.901      |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.000242    |\n",
      "|    loss                 | -0.0231     |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 0.0298      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.98e+03    |\n",
      "|    ep_rew_mean          | 11.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 610         |\n",
      "|    iterations           | 160         |\n",
      "|    time_elapsed         | 536         |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006259634 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.0967      |\n",
      "|    entropy_loss         | -0.93       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.000242    |\n",
      "|    loss                 | -0.0247     |\n",
      "|    n_updates            | 795         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 0.0357      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2e+03       |\n",
      "|    ep_rew_mean          | 11.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 611         |\n",
      "|    iterations           | 161         |\n",
      "|    time_elapsed         | 538         |\n",
      "|    total_timesteps      | 329728      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005151053 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.0967      |\n",
      "|    entropy_loss         | -0.951      |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.000242    |\n",
      "|    loss                 | -0.0251     |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 0.0295      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=10.80 +/- 2.40\n",
      "Episode length: 2030.40 +/- 332.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.03e+03    |\n",
      "|    mean_reward          | 10.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 330000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005055814 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.0967      |\n",
      "|    entropy_loss         | -0.995      |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.000242    |\n",
      "|    loss                 | -0.0227     |\n",
      "|    n_updates            | 805         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 0.0355      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 11.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 607      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 545      |\n",
      "|    total_timesteps | 331776   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.99e+03    |\n",
      "|    ep_rew_mean          | 11.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 608         |\n",
      "|    iterations           | 163         |\n",
      "|    time_elapsed         | 548         |\n",
      "|    total_timesteps      | 333824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005232186 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.0967      |\n",
      "|    entropy_loss         | -0.929      |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.000242    |\n",
      "|    loss                 | -0.0121     |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 0.0526      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 11.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 609          |\n",
      "|    iterations           | 164          |\n",
      "|    time_elapsed         | 550          |\n",
      "|    total_timesteps      | 335872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074704513 |\n",
      "|    clip_fraction        | 0.23         |\n",
      "|    clip_range           | 0.0967       |\n",
      "|    entropy_loss         | -0.959       |\n",
      "|    explained_variance   | 0.789        |\n",
      "|    learning_rate        | 0.000242     |\n",
      "|    loss                 | -0.0191      |\n",
      "|    n_updates            | 815          |\n",
      "|    policy_gradient_loss | -0.0143      |\n",
      "|    value_loss           | 0.0398       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.02e+03     |\n",
      "|    ep_rew_mean          | 12.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 610          |\n",
      "|    iterations           | 165          |\n",
      "|    time_elapsed         | 553          |\n",
      "|    total_timesteps      | 337920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048996885 |\n",
      "|    clip_fraction        | 0.216        |\n",
      "|    clip_range           | 0.0966       |\n",
      "|    entropy_loss         | -0.93        |\n",
      "|    explained_variance   | 0.785        |\n",
      "|    learning_rate        | 0.000242     |\n",
      "|    loss                 | -0.023       |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0133      |\n",
      "|    value_loss           | 0.047        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.04e+03    |\n",
      "|    ep_rew_mean          | 12.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 611         |\n",
      "|    iterations           | 166         |\n",
      "|    time_elapsed         | 555         |\n",
      "|    total_timesteps      | 339968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006890882 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.0966      |\n",
      "|    entropy_loss         | -0.938      |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.000242    |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 825         |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 0.0393      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=13.40 +/- 3.93\n",
      "Episode length: 2352.60 +/- 340.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.35e+03     |\n",
      "|    mean_reward          | 13.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 340000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053925663 |\n",
      "|    clip_fraction        | 0.231        |\n",
      "|    clip_range           | 0.0966       |\n",
      "|    entropy_loss         | -0.95        |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 0.000242     |\n",
      "|    loss                 | -0.0243      |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.0162      |\n",
      "|    value_loss           | 0.035        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.04e+03 |\n",
      "|    ep_rew_mean     | 12.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 606      |\n",
      "|    iterations      | 167      |\n",
      "|    time_elapsed    | 563      |\n",
      "|    total_timesteps | 342016   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.03e+03    |\n",
      "|    ep_rew_mean          | 12.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 607         |\n",
      "|    iterations           | 168         |\n",
      "|    time_elapsed         | 565         |\n",
      "|    total_timesteps      | 344064      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005315412 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.0966      |\n",
      "|    entropy_loss         | -0.914      |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.000241    |\n",
      "|    loss                 | -0.0248     |\n",
      "|    n_updates            | 835         |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    value_loss           | 0.0364      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.04e+03    |\n",
      "|    ep_rew_mean          | 12.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 608         |\n",
      "|    iterations           | 169         |\n",
      "|    time_elapsed         | 568         |\n",
      "|    total_timesteps      | 346112      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005711789 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.0966      |\n",
      "|    entropy_loss         | -0.945      |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.000241    |\n",
      "|    loss                 | -0.0295     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 0.0485      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.05e+03     |\n",
      "|    ep_rew_mean          | 12.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 609          |\n",
      "|    iterations           | 170          |\n",
      "|    time_elapsed         | 570          |\n",
      "|    total_timesteps      | 348160       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067791026 |\n",
      "|    clip_fraction        | 0.216        |\n",
      "|    clip_range           | 0.0965       |\n",
      "|    entropy_loss         | -0.987       |\n",
      "|    explained_variance   | 0.753        |\n",
      "|    learning_rate        | 0.000241     |\n",
      "|    loss                 | -0.0198      |\n",
      "|    n_updates            | 845          |\n",
      "|    policy_gradient_loss | -0.0146      |\n",
      "|    value_loss           | 0.0505       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=11.80 +/- 2.14\n",
      "Episode length: 2072.20 +/- 210.34\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.07e+03     |\n",
      "|    mean_reward          | 11.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 350000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056404565 |\n",
      "|    clip_fraction        | 0.226        |\n",
      "|    clip_range           | 0.0965       |\n",
      "|    entropy_loss         | -0.983       |\n",
      "|    explained_variance   | 0.765        |\n",
      "|    learning_rate        | 0.000241     |\n",
      "|    loss                 | -0.0282      |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.0147      |\n",
      "|    value_loss           | 0.0404       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.07e+03 |\n",
      "|    ep_rew_mean     | 12.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 171      |\n",
      "|    time_elapsed    | 577      |\n",
      "|    total_timesteps | 350208   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.06e+03    |\n",
      "|    ep_rew_mean          | 12.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 606         |\n",
      "|    iterations           | 172         |\n",
      "|    time_elapsed         | 580         |\n",
      "|    total_timesteps      | 352256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005720784 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.0965      |\n",
      "|    entropy_loss         | -0.957      |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.000241    |\n",
      "|    loss                 | -0.0202     |\n",
      "|    n_updates            | 855         |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 0.0444      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.05e+03     |\n",
      "|    ep_rew_mean          | 12.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 607          |\n",
      "|    iterations           | 173          |\n",
      "|    time_elapsed         | 582          |\n",
      "|    total_timesteps      | 354304       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053385715 |\n",
      "|    clip_fraction        | 0.19         |\n",
      "|    clip_range           | 0.0965       |\n",
      "|    entropy_loss         | -0.95        |\n",
      "|    explained_variance   | 0.77         |\n",
      "|    learning_rate        | 0.000241     |\n",
      "|    loss                 | -0.0178      |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0141      |\n",
      "|    value_loss           | 0.0532       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.07e+03     |\n",
      "|    ep_rew_mean          | 12.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 608          |\n",
      "|    iterations           | 174          |\n",
      "|    time_elapsed         | 585          |\n",
      "|    total_timesteps      | 356352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070329527 |\n",
      "|    clip_fraction        | 0.239        |\n",
      "|    clip_range           | 0.0965       |\n",
      "|    entropy_loss         | -0.949       |\n",
      "|    explained_variance   | 0.829        |\n",
      "|    learning_rate        | 0.000241     |\n",
      "|    loss                 | -0.0285      |\n",
      "|    n_updates            | 865          |\n",
      "|    policy_gradient_loss | -0.0154      |\n",
      "|    value_loss           | 0.036        |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.08e+03   |\n",
      "|    ep_rew_mean          | 12.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 609        |\n",
      "|    iterations           | 175        |\n",
      "|    time_elapsed         | 587        |\n",
      "|    total_timesteps      | 358400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00542243 |\n",
      "|    clip_fraction        | 0.19       |\n",
      "|    clip_range           | 0.0964     |\n",
      "|    entropy_loss         | -0.937     |\n",
      "|    explained_variance   | 0.797      |\n",
      "|    learning_rate        | 0.000241   |\n",
      "|    loss                 | -0.0223    |\n",
      "|    n_updates            | 870        |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    value_loss           | 0.0365     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=13.80 +/- 1.83\n",
      "Episode length: 1736.60 +/- 279.84\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.74e+03     |\n",
      "|    mean_reward          | 13.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 360000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045196675 |\n",
      "|    clip_fraction        | 0.192        |\n",
      "|    clip_range           | 0.0964       |\n",
      "|    entropy_loss         | -0.954       |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 0.000241     |\n",
      "|    loss                 | -0.0116      |\n",
      "|    n_updates            | 875          |\n",
      "|    policy_gradient_loss | -0.0132      |\n",
      "|    value_loss           | 0.038        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.08e+03 |\n",
      "|    ep_rew_mean     | 12.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 606      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 594      |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.08e+03     |\n",
      "|    ep_rew_mean          | 12.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 607          |\n",
      "|    iterations           | 177          |\n",
      "|    time_elapsed         | 596          |\n",
      "|    total_timesteps      | 362496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061240345 |\n",
      "|    clip_fraction        | 0.199        |\n",
      "|    clip_range           | 0.0964       |\n",
      "|    entropy_loss         | -0.95        |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 0.000241     |\n",
      "|    loss                 | -0.0262      |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.014       |\n",
      "|    value_loss           | 0.0351       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.08e+03     |\n",
      "|    ep_rew_mean          | 12.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 608          |\n",
      "|    iterations           | 178          |\n",
      "|    time_elapsed         | 599          |\n",
      "|    total_timesteps      | 364544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056687817 |\n",
      "|    clip_fraction        | 0.191        |\n",
      "|    clip_range           | 0.0964       |\n",
      "|    entropy_loss         | -0.946       |\n",
      "|    explained_variance   | 0.766        |\n",
      "|    learning_rate        | 0.000241     |\n",
      "|    loss                 | -0.0081      |\n",
      "|    n_updates            | 885          |\n",
      "|    policy_gradient_loss | -0.0132      |\n",
      "|    value_loss           | 0.0486       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.08e+03    |\n",
      "|    ep_rew_mean          | 12.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 609         |\n",
      "|    iterations           | 179         |\n",
      "|    time_elapsed         | 601         |\n",
      "|    total_timesteps      | 366592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006930207 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.0964      |\n",
      "|    entropy_loss         | -0.935      |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.000241    |\n",
      "|    loss                 | -0.0228     |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 0.0359      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.1e+03      |\n",
      "|    ep_rew_mean          | 13.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 610          |\n",
      "|    iterations           | 180          |\n",
      "|    time_elapsed         | 604          |\n",
      "|    total_timesteps      | 368640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058015483 |\n",
      "|    clip_fraction        | 0.193        |\n",
      "|    clip_range           | 0.0963       |\n",
      "|    entropy_loss         | -0.912       |\n",
      "|    explained_variance   | 0.741        |\n",
      "|    learning_rate        | 0.000241     |\n",
      "|    loss                 | -0.0236      |\n",
      "|    n_updates            | 895          |\n",
      "|    policy_gradient_loss | -0.014       |\n",
      "|    value_loss           | 0.039        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=17.00 +/- 4.60\n",
      "Episode length: 2465.20 +/- 392.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.47e+03    |\n",
      "|    mean_reward          | 17          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 370000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005591044 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.0963      |\n",
      "|    entropy_loss         | -0.923      |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 0.000241    |\n",
      "|    loss                 | -0.0197     |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 0.0337      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.11e+03 |\n",
      "|    ep_rew_mean     | 13.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 181      |\n",
      "|    time_elapsed    | 612      |\n",
      "|    total_timesteps | 370688   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.12e+03     |\n",
      "|    ep_rew_mean          | 13.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 606          |\n",
      "|    iterations           | 182          |\n",
      "|    time_elapsed         | 614          |\n",
      "|    total_timesteps      | 372736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052452516 |\n",
      "|    clip_fraction        | 0.239        |\n",
      "|    clip_range           | 0.0963       |\n",
      "|    entropy_loss         | -0.921       |\n",
      "|    explained_variance   | 0.751        |\n",
      "|    learning_rate        | 0.000241     |\n",
      "|    loss                 | -0.0255      |\n",
      "|    n_updates            | 905          |\n",
      "|    policy_gradient_loss | -0.0135      |\n",
      "|    value_loss           | 0.0386       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.13e+03     |\n",
      "|    ep_rew_mean          | 13.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 607          |\n",
      "|    iterations           | 183          |\n",
      "|    time_elapsed         | 617          |\n",
      "|    total_timesteps      | 374784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059128986 |\n",
      "|    clip_fraction        | 0.2          |\n",
      "|    clip_range           | 0.0963       |\n",
      "|    entropy_loss         | -0.923       |\n",
      "|    explained_variance   | 0.741        |\n",
      "|    learning_rate        | 0.000241     |\n",
      "|    loss                 | -0.027       |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.0141      |\n",
      "|    value_loss           | 0.0487       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.13e+03     |\n",
      "|    ep_rew_mean          | 13.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 608          |\n",
      "|    iterations           | 184          |\n",
      "|    time_elapsed         | 619          |\n",
      "|    total_timesteps      | 376832       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061335387 |\n",
      "|    clip_fraction        | 0.232        |\n",
      "|    clip_range           | 0.0963       |\n",
      "|    entropy_loss         | -0.921       |\n",
      "|    explained_variance   | 0.737        |\n",
      "|    learning_rate        | 0.000241     |\n",
      "|    loss                 | -0.0252      |\n",
      "|    n_updates            | 915          |\n",
      "|    policy_gradient_loss | -0.0133      |\n",
      "|    value_loss           | 0.0408       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.13e+03     |\n",
      "|    ep_rew_mean          | 13.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 609          |\n",
      "|    iterations           | 185          |\n",
      "|    time_elapsed         | 621          |\n",
      "|    total_timesteps      | 378880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060592415 |\n",
      "|    clip_fraction        | 0.215        |\n",
      "|    clip_range           | 0.0962       |\n",
      "|    entropy_loss         | -0.909       |\n",
      "|    explained_variance   | 0.774        |\n",
      "|    learning_rate        | 0.000241     |\n",
      "|    loss                 | -0.0249      |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.0132      |\n",
      "|    value_loss           | 0.0404       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=12.60 +/- 2.42\n",
      "Episode length: 2188.80 +/- 108.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.19e+03    |\n",
      "|    mean_reward          | 12.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 380000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006043438 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.0962      |\n",
      "|    entropy_loss         | -0.926      |\n",
      "|    explained_variance   | 0.756       |\n",
      "|    learning_rate        | 0.000241    |\n",
      "|    loss                 | -0.013      |\n",
      "|    n_updates            | 925         |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 0.0546      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.12e+03 |\n",
      "|    ep_rew_mean     | 13.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 629      |\n",
      "|    total_timesteps | 380928   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.1e+03     |\n",
      "|    ep_rew_mean          | 12.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 606         |\n",
      "|    iterations           | 187         |\n",
      "|    time_elapsed         | 631         |\n",
      "|    total_timesteps      | 382976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006091428 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.0962      |\n",
      "|    entropy_loss         | -0.936      |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 0.00024     |\n",
      "|    loss                 | -0.0215     |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 0.042       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.12e+03    |\n",
      "|    ep_rew_mean          | 13.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 607         |\n",
      "|    iterations           | 188         |\n",
      "|    time_elapsed         | 634         |\n",
      "|    total_timesteps      | 385024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006143107 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.0962      |\n",
      "|    entropy_loss         | -0.929      |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.00024     |\n",
      "|    loss                 | -0.0247     |\n",
      "|    n_updates            | 935         |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 0.0416      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.12e+03    |\n",
      "|    ep_rew_mean          | 13.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 608         |\n",
      "|    iterations           | 189         |\n",
      "|    time_elapsed         | 636         |\n",
      "|    total_timesteps      | 387072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005893117 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.0961      |\n",
      "|    entropy_loss         | -0.969      |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.00024     |\n",
      "|    loss                 | -0.0262     |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 0.0382      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.11e+03     |\n",
      "|    ep_rew_mean          | 13           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 608          |\n",
      "|    iterations           | 190          |\n",
      "|    time_elapsed         | 638          |\n",
      "|    total_timesteps      | 389120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068252757 |\n",
      "|    clip_fraction        | 0.227        |\n",
      "|    clip_range           | 0.0961       |\n",
      "|    entropy_loss         | -0.949       |\n",
      "|    explained_variance   | 0.747        |\n",
      "|    learning_rate        | 0.00024      |\n",
      "|    loss                 | -0.027       |\n",
      "|    n_updates            | 945          |\n",
      "|    policy_gradient_loss | -0.016       |\n",
      "|    value_loss           | 0.0421       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=14.00 +/- 5.62\n",
      "Episode length: 2121.80 +/- 436.88\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.12e+03   |\n",
      "|    mean_reward          | 14         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 390000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00652578 |\n",
      "|    clip_fraction        | 0.234      |\n",
      "|    clip_range           | 0.0961     |\n",
      "|    entropy_loss         | -0.921     |\n",
      "|    explained_variance   | 0.729      |\n",
      "|    learning_rate        | 0.00024    |\n",
      "|    loss                 | -0.0197    |\n",
      "|    n_updates            | 950        |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    value_loss           | 0.0475     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.12e+03 |\n",
      "|    ep_rew_mean     | 13.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 191      |\n",
      "|    time_elapsed    | 646      |\n",
      "|    total_timesteps | 391168   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.13e+03     |\n",
      "|    ep_rew_mean          | 12.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 605          |\n",
      "|    iterations           | 192          |\n",
      "|    time_elapsed         | 649          |\n",
      "|    total_timesteps      | 393216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048955516 |\n",
      "|    clip_fraction        | 0.214        |\n",
      "|    clip_range           | 0.0961       |\n",
      "|    entropy_loss         | -0.924       |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 0.00024      |\n",
      "|    loss                 | -0.0258      |\n",
      "|    n_updates            | 955          |\n",
      "|    policy_gradient_loss | -0.0135      |\n",
      "|    value_loss           | 0.0394       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.15e+03    |\n",
      "|    ep_rew_mean          | 13.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 606         |\n",
      "|    iterations           | 193         |\n",
      "|    time_elapsed         | 651         |\n",
      "|    total_timesteps      | 395264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006117633 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.0961      |\n",
      "|    entropy_loss         | -0.938      |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.00024     |\n",
      "|    loss                 | -0.0217     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 0.0427      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.15e+03     |\n",
      "|    ep_rew_mean          | 13.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 607          |\n",
      "|    iterations           | 194          |\n",
      "|    time_elapsed         | 654          |\n",
      "|    total_timesteps      | 397312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066861995 |\n",
      "|    clip_fraction        | 0.197        |\n",
      "|    clip_range           | 0.096        |\n",
      "|    entropy_loss         | -0.906       |\n",
      "|    explained_variance   | 0.79         |\n",
      "|    learning_rate        | 0.00024      |\n",
      "|    loss                 | -0.026       |\n",
      "|    n_updates            | 965          |\n",
      "|    policy_gradient_loss | -0.0147      |\n",
      "|    value_loss           | 0.0387       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.15e+03    |\n",
      "|    ep_rew_mean          | 13.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 608         |\n",
      "|    iterations           | 195         |\n",
      "|    time_elapsed         | 656         |\n",
      "|    total_timesteps      | 399360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005374509 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.096       |\n",
      "|    entropy_loss         | -0.944      |\n",
      "|    explained_variance   | 0.744       |\n",
      "|    learning_rate        | 0.00024     |\n",
      "|    loss                 | -0.0287     |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 0.0448      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=16.40 +/- 2.50\n",
      "Episode length: 2220.60 +/- 416.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.22e+03    |\n",
      "|    mean_reward          | 16.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005452254 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.096       |\n",
      "|    entropy_loss         | -0.941      |\n",
      "|    explained_variance   | 0.768       |\n",
      "|    learning_rate        | 0.00024     |\n",
      "|    loss                 | -0.0225     |\n",
      "|    n_updates            | 975         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    value_loss           | 0.0408      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.14e+03 |\n",
      "|    ep_rew_mean     | 13.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 603      |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 664      |\n",
      "|    total_timesteps | 401408   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.15e+03     |\n",
      "|    ep_rew_mean          | 13.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 604          |\n",
      "|    iterations           | 197          |\n",
      "|    time_elapsed         | 667          |\n",
      "|    total_timesteps      | 403456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068933377 |\n",
      "|    clip_fraction        | 0.197        |\n",
      "|    clip_range           | 0.096        |\n",
      "|    entropy_loss         | -0.927       |\n",
      "|    explained_variance   | 0.809        |\n",
      "|    learning_rate        | 0.00024      |\n",
      "|    loss                 | -0.0242      |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.0138      |\n",
      "|    value_loss           | 0.0322       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.19e+03    |\n",
      "|    ep_rew_mean          | 13.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 605         |\n",
      "|    iterations           | 198         |\n",
      "|    time_elapsed         | 669         |\n",
      "|    total_timesteps      | 405504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005064355 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.096       |\n",
      "|    entropy_loss         | -0.962      |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.00024     |\n",
      "|    loss                 | -0.0328     |\n",
      "|    n_updates            | 985         |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 0.0256      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.19e+03   |\n",
      "|    ep_rew_mean          | 13.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 606        |\n",
      "|    iterations           | 199        |\n",
      "|    time_elapsed         | 671        |\n",
      "|    total_timesteps      | 407552     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00643926 |\n",
      "|    clip_fraction        | 0.217      |\n",
      "|    clip_range           | 0.0959     |\n",
      "|    entropy_loss         | -0.959     |\n",
      "|    explained_variance   | 0.804      |\n",
      "|    learning_rate        | 0.00024    |\n",
      "|    loss                 | -0.0251    |\n",
      "|    n_updates            | 990        |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    value_loss           | 0.0391     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.18e+03     |\n",
      "|    ep_rew_mean          | 13.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 607          |\n",
      "|    iterations           | 200          |\n",
      "|    time_elapsed         | 674          |\n",
      "|    total_timesteps      | 409600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061579878 |\n",
      "|    clip_fraction        | 0.241        |\n",
      "|    clip_range           | 0.0959       |\n",
      "|    entropy_loss         | -0.947       |\n",
      "|    explained_variance   | 0.809        |\n",
      "|    learning_rate        | 0.00024      |\n",
      "|    loss                 | -0.0288      |\n",
      "|    n_updates            | 995          |\n",
      "|    policy_gradient_loss | -0.0144      |\n",
      "|    value_loss           | 0.0381       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=14.60 +/- 1.02\n",
      "Episode length: 1982.00 +/- 369.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.98e+03     |\n",
      "|    mean_reward          | 14.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 410000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059720473 |\n",
      "|    clip_fraction        | 0.233        |\n",
      "|    clip_range           | 0.0959       |\n",
      "|    entropy_loss         | -0.938       |\n",
      "|    explained_variance   | 0.656        |\n",
      "|    learning_rate        | 0.00024      |\n",
      "|    loss                 | -0.0217      |\n",
      "|    n_updates            | 1000         |\n",
      "|    policy_gradient_loss | -0.0137      |\n",
      "|    value_loss           | 0.0525       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.18e+03 |\n",
      "|    ep_rew_mean     | 13.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 201      |\n",
      "|    time_elapsed    | 681      |\n",
      "|    total_timesteps | 411648   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.22e+03     |\n",
      "|    ep_rew_mean          | 13.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 604          |\n",
      "|    iterations           | 202          |\n",
      "|    time_elapsed         | 683          |\n",
      "|    total_timesteps      | 413696       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065287957 |\n",
      "|    clip_fraction        | 0.238        |\n",
      "|    clip_range           | 0.0959       |\n",
      "|    entropy_loss         | -0.939       |\n",
      "|    explained_variance   | 0.82         |\n",
      "|    learning_rate        | 0.00024      |\n",
      "|    loss                 | -0.0164      |\n",
      "|    n_updates            | 1005         |\n",
      "|    policy_gradient_loss | -0.015       |\n",
      "|    value_loss           | 0.0369       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.22e+03     |\n",
      "|    ep_rew_mean          | 13.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 605          |\n",
      "|    iterations           | 203          |\n",
      "|    time_elapsed         | 686          |\n",
      "|    total_timesteps      | 415744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072295126 |\n",
      "|    clip_fraction        | 0.235        |\n",
      "|    clip_range           | 0.0959       |\n",
      "|    entropy_loss         | -0.89        |\n",
      "|    explained_variance   | 0.769        |\n",
      "|    learning_rate        | 0.00024      |\n",
      "|    loss                 | -0.023       |\n",
      "|    n_updates            | 1010         |\n",
      "|    policy_gradient_loss | -0.0172      |\n",
      "|    value_loss           | 0.0421       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.22e+03     |\n",
      "|    ep_rew_mean          | 14           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 606          |\n",
      "|    iterations           | 204          |\n",
      "|    time_elapsed         | 688          |\n",
      "|    total_timesteps      | 417792       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067680236 |\n",
      "|    clip_fraction        | 0.212        |\n",
      "|    clip_range           | 0.0958       |\n",
      "|    entropy_loss         | -0.919       |\n",
      "|    explained_variance   | 0.802        |\n",
      "|    learning_rate        | 0.00024      |\n",
      "|    loss                 | -0.0224      |\n",
      "|    n_updates            | 1015         |\n",
      "|    policy_gradient_loss | -0.0153      |\n",
      "|    value_loss           | 0.0459       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.24e+03    |\n",
      "|    ep_rew_mean          | 14.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 607         |\n",
      "|    iterations           | 205         |\n",
      "|    time_elapsed         | 691         |\n",
      "|    total_timesteps      | 419840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006719347 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.0958      |\n",
      "|    entropy_loss         | -0.919      |\n",
      "|    explained_variance   | 0.703       |\n",
      "|    learning_rate        | 0.00024     |\n",
      "|    loss                 | -0.0126     |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    value_loss           | 0.0573      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=14.40 +/- 6.09\n",
      "Episode length: 2188.00 +/- 471.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.19e+03    |\n",
      "|    mean_reward          | 14.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 420000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007834002 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.0958      |\n",
      "|    entropy_loss         | -0.912      |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.00024     |\n",
      "|    loss                 | -0.0293     |\n",
      "|    n_updates            | 1025        |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 0.0351      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.22e+03 |\n",
      "|    ep_rew_mean     | 13.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 602      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 699      |\n",
      "|    total_timesteps | 421888   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.21e+03    |\n",
      "|    ep_rew_mean          | 13.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 603         |\n",
      "|    iterations           | 207         |\n",
      "|    time_elapsed         | 702         |\n",
      "|    total_timesteps      | 423936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007321351 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.0958      |\n",
      "|    entropy_loss         | -0.882      |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.000239    |\n",
      "|    loss                 | -0.0103     |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 0.0633      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.21e+03     |\n",
      "|    ep_rew_mean          | 13.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 604          |\n",
      "|    iterations           | 208          |\n",
      "|    time_elapsed         | 704          |\n",
      "|    total_timesteps      | 425984       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066600787 |\n",
      "|    clip_fraction        | 0.204        |\n",
      "|    clip_range           | 0.0958       |\n",
      "|    entropy_loss         | -0.871       |\n",
      "|    explained_variance   | 0.825        |\n",
      "|    learning_rate        | 0.000239     |\n",
      "|    loss                 | -0.0215      |\n",
      "|    n_updates            | 1035         |\n",
      "|    policy_gradient_loss | -0.0152      |\n",
      "|    value_loss           | 0.041        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.18e+03     |\n",
      "|    ep_rew_mean          | 13.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 605          |\n",
      "|    iterations           | 209          |\n",
      "|    time_elapsed         | 707          |\n",
      "|    total_timesteps      | 428032       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068590473 |\n",
      "|    clip_fraction        | 0.22         |\n",
      "|    clip_range           | 0.0957       |\n",
      "|    entropy_loss         | -0.822       |\n",
      "|    explained_variance   | 0.727        |\n",
      "|    learning_rate        | 0.000239     |\n",
      "|    loss                 | -0.0144      |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0127      |\n",
      "|    value_loss           | 0.0535       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=14.60 +/- 2.73\n",
      "Episode length: 2292.40 +/- 399.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.29e+03     |\n",
      "|    mean_reward          | 14.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 430000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078692995 |\n",
      "|    clip_fraction        | 0.226        |\n",
      "|    clip_range           | 0.0957       |\n",
      "|    entropy_loss         | -0.829       |\n",
      "|    explained_variance   | 0.742        |\n",
      "|    learning_rate        | 0.000239     |\n",
      "|    loss                 | -0.0243      |\n",
      "|    n_updates            | 1045         |\n",
      "|    policy_gradient_loss | -0.0117      |\n",
      "|    value_loss           | 0.0434       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.18e+03 |\n",
      "|    ep_rew_mean     | 13.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 600      |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 715      |\n",
      "|    total_timesteps | 430080   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.19e+03     |\n",
      "|    ep_rew_mean          | 13.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 601          |\n",
      "|    iterations           | 211          |\n",
      "|    time_elapsed         | 718          |\n",
      "|    total_timesteps      | 432128       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066237887 |\n",
      "|    clip_fraction        | 0.193        |\n",
      "|    clip_range           | 0.0957       |\n",
      "|    entropy_loss         | -0.878       |\n",
      "|    explained_variance   | 0.726        |\n",
      "|    learning_rate        | 0.000239     |\n",
      "|    loss                 | -0.0194      |\n",
      "|    n_updates            | 1050         |\n",
      "|    policy_gradient_loss | -0.0138      |\n",
      "|    value_loss           | 0.0414       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.19e+03    |\n",
      "|    ep_rew_mean          | 13.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 602         |\n",
      "|    iterations           | 212         |\n",
      "|    time_elapsed         | 720         |\n",
      "|    total_timesteps      | 434176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007403881 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.0957      |\n",
      "|    entropy_loss         | -0.852      |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.000239    |\n",
      "|    loss                 | -0.0237     |\n",
      "|    n_updates            | 1055        |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 0.04        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.22e+03     |\n",
      "|    ep_rew_mean          | 14.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 603          |\n",
      "|    iterations           | 213          |\n",
      "|    time_elapsed         | 723          |\n",
      "|    total_timesteps      | 436224       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066665886 |\n",
      "|    clip_fraction        | 0.231        |\n",
      "|    clip_range           | 0.0957       |\n",
      "|    entropy_loss         | -0.864       |\n",
      "|    explained_variance   | 0.701        |\n",
      "|    learning_rate        | 0.000239     |\n",
      "|    loss                 | -0.0203      |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0122      |\n",
      "|    value_loss           | 0.0431       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.21e+03    |\n",
      "|    ep_rew_mean          | 14.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 604         |\n",
      "|    iterations           | 214         |\n",
      "|    time_elapsed         | 725         |\n",
      "|    total_timesteps      | 438272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005603213 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.0956      |\n",
      "|    entropy_loss         | -0.842      |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.000239    |\n",
      "|    loss                 | -0.0266     |\n",
      "|    n_updates            | 1065        |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 0.038       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=14.40 +/- 5.28\n",
      "Episode length: 2311.20 +/- 320.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.31e+03     |\n",
      "|    mean_reward          | 14.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 440000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065682153 |\n",
      "|    clip_fraction        | 0.215        |\n",
      "|    clip_range           | 0.0956       |\n",
      "|    entropy_loss         | -0.91        |\n",
      "|    explained_variance   | 0.731        |\n",
      "|    learning_rate        | 0.000239     |\n",
      "|    loss                 | -0.0162      |\n",
      "|    n_updates            | 1070         |\n",
      "|    policy_gradient_loss | -0.0142      |\n",
      "|    value_loss           | 0.0572       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.21e+03 |\n",
      "|    ep_rew_mean     | 14.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 599      |\n",
      "|    iterations      | 215      |\n",
      "|    time_elapsed    | 734      |\n",
      "|    total_timesteps | 440320   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.2e+03    |\n",
      "|    ep_rew_mean          | 14.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 600        |\n",
      "|    iterations           | 216        |\n",
      "|    time_elapsed         | 737        |\n",
      "|    total_timesteps      | 442368     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00638584 |\n",
      "|    clip_fraction        | 0.264      |\n",
      "|    clip_range           | 0.0956     |\n",
      "|    entropy_loss         | -0.918     |\n",
      "|    explained_variance   | 0.744      |\n",
      "|    learning_rate        | 0.000239   |\n",
      "|    loss                 | -0.0206    |\n",
      "|    n_updates            | 1075       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    value_loss           | 0.0487     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.2e+03     |\n",
      "|    ep_rew_mean          | 14.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 601         |\n",
      "|    iterations           | 217         |\n",
      "|    time_elapsed         | 739         |\n",
      "|    total_timesteps      | 444416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005952926 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.0956      |\n",
      "|    entropy_loss         | -0.904      |\n",
      "|    explained_variance   | 0.758       |\n",
      "|    learning_rate        | 0.000239    |\n",
      "|    loss                 | -0.0257     |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 0.0561      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.19e+03    |\n",
      "|    ep_rew_mean          | 14.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 601         |\n",
      "|    iterations           | 218         |\n",
      "|    time_elapsed         | 741         |\n",
      "|    total_timesteps      | 446464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007011443 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.0956      |\n",
      "|    entropy_loss         | -0.831      |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 0.000239    |\n",
      "|    loss                 | -0.032      |\n",
      "|    n_updates            | 1085        |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 0.044       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.21e+03     |\n",
      "|    ep_rew_mean          | 14.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 602          |\n",
      "|    iterations           | 219          |\n",
      "|    time_elapsed         | 744          |\n",
      "|    total_timesteps      | 448512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068605524 |\n",
      "|    clip_fraction        | 0.251        |\n",
      "|    clip_range           | 0.0955       |\n",
      "|    entropy_loss         | -0.842       |\n",
      "|    explained_variance   | 0.774        |\n",
      "|    learning_rate        | 0.000239     |\n",
      "|    loss                 | -0.0161      |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | -0.0133      |\n",
      "|    value_loss           | 0.0452       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=16.80 +/- 4.71\n",
      "Episode length: 2417.80 +/- 620.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.42e+03    |\n",
      "|    mean_reward          | 16.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 450000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005580056 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.0955      |\n",
      "|    entropy_loss         | -0.875      |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 0.000239    |\n",
      "|    loss                 | -0.0267     |\n",
      "|    n_updates            | 1095        |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 0.0425      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.22e+03 |\n",
      "|    ep_rew_mean     | 14.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 597      |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 753      |\n",
      "|    total_timesteps | 450560   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.23e+03     |\n",
      "|    ep_rew_mean          | 14.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 598          |\n",
      "|    iterations           | 221          |\n",
      "|    time_elapsed         | 756          |\n",
      "|    total_timesteps      | 452608       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052031283 |\n",
      "|    clip_fraction        | 0.209        |\n",
      "|    clip_range           | 0.0955       |\n",
      "|    entropy_loss         | -0.954       |\n",
      "|    explained_variance   | 0.734        |\n",
      "|    learning_rate        | 0.000239     |\n",
      "|    loss                 | -0.0147      |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0148      |\n",
      "|    value_loss           | 0.0781       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.23e+03     |\n",
      "|    ep_rew_mean          | 14.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 599          |\n",
      "|    iterations           | 222          |\n",
      "|    time_elapsed         | 758          |\n",
      "|    total_timesteps      | 454656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057393396 |\n",
      "|    clip_fraction        | 0.228        |\n",
      "|    clip_range           | 0.0955       |\n",
      "|    entropy_loss         | -0.928       |\n",
      "|    explained_variance   | 0.688        |\n",
      "|    learning_rate        | 0.000239     |\n",
      "|    loss                 | -0.0238      |\n",
      "|    n_updates            | 1105         |\n",
      "|    policy_gradient_loss | -0.0173      |\n",
      "|    value_loss           | 0.0522       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.23e+03     |\n",
      "|    ep_rew_mean          | 14.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 599          |\n",
      "|    iterations           | 223          |\n",
      "|    time_elapsed         | 761          |\n",
      "|    total_timesteps      | 456704       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066785733 |\n",
      "|    clip_fraction        | 0.246        |\n",
      "|    clip_range           | 0.0955       |\n",
      "|    entropy_loss         | -0.881       |\n",
      "|    explained_variance   | 0.742        |\n",
      "|    learning_rate        | 0.000239     |\n",
      "|    loss                 | -0.02        |\n",
      "|    n_updates            | 1110         |\n",
      "|    policy_gradient_loss | -0.0172      |\n",
      "|    value_loss           | 0.044        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.24e+03     |\n",
      "|    ep_rew_mean          | 14.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 600          |\n",
      "|    iterations           | 224          |\n",
      "|    time_elapsed         | 763          |\n",
      "|    total_timesteps      | 458752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073474017 |\n",
      "|    clip_fraction        | 0.258        |\n",
      "|    clip_range           | 0.0954       |\n",
      "|    entropy_loss         | -0.903       |\n",
      "|    explained_variance   | 0.812        |\n",
      "|    learning_rate        | 0.000239     |\n",
      "|    loss                 | -0.0154      |\n",
      "|    n_updates            | 1115         |\n",
      "|    policy_gradient_loss | -0.0126      |\n",
      "|    value_loss           | 0.0411       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=16.60 +/- 3.67\n",
      "Episode length: 2026.00 +/- 209.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.03e+03    |\n",
      "|    mean_reward          | 16.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 460000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007339797 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.0954      |\n",
      "|    entropy_loss         | -0.895      |\n",
      "|    explained_variance   | 0.737       |\n",
      "|    learning_rate        | 0.000239    |\n",
      "|    loss                 | -0.0195     |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 0.0566      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.23e+03 |\n",
      "|    ep_rew_mean     | 14.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 596      |\n",
      "|    iterations      | 225      |\n",
      "|    time_elapsed    | 771      |\n",
      "|    total_timesteps | 460800   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.23e+03     |\n",
      "|    ep_rew_mean          | 14.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 597          |\n",
      "|    iterations           | 226          |\n",
      "|    time_elapsed         | 774          |\n",
      "|    total_timesteps      | 462848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060639326 |\n",
      "|    clip_fraction        | 0.216        |\n",
      "|    clip_range           | 0.0954       |\n",
      "|    entropy_loss         | -0.888       |\n",
      "|    explained_variance   | 0.797        |\n",
      "|    learning_rate        | 0.000238     |\n",
      "|    loss                 | -0.0223      |\n",
      "|    n_updates            | 1125         |\n",
      "|    policy_gradient_loss | -0.0169      |\n",
      "|    value_loss           | 0.0418       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.24e+03    |\n",
      "|    ep_rew_mean          | 14.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 598         |\n",
      "|    iterations           | 227         |\n",
      "|    time_elapsed         | 776         |\n",
      "|    total_timesteps      | 464896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007906062 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.0954      |\n",
      "|    entropy_loss         | -0.86       |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.000238    |\n",
      "|    loss                 | -0.0258     |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 0.0378      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.23e+03   |\n",
      "|    ep_rew_mean          | 14.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 599        |\n",
      "|    iterations           | 228        |\n",
      "|    time_elapsed         | 779        |\n",
      "|    total_timesteps      | 466944     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00779788 |\n",
      "|    clip_fraction        | 0.23       |\n",
      "|    clip_range           | 0.0954     |\n",
      "|    entropy_loss         | -0.824     |\n",
      "|    explained_variance   | 0.732      |\n",
      "|    learning_rate        | 0.000238   |\n",
      "|    loss                 | -0.024     |\n",
      "|    n_updates            | 1135       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    value_loss           | 0.0476     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.23e+03     |\n",
      "|    ep_rew_mean          | 14.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 599          |\n",
      "|    iterations           | 229          |\n",
      "|    time_elapsed         | 781          |\n",
      "|    total_timesteps      | 468992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071279914 |\n",
      "|    clip_fraction        | 0.238        |\n",
      "|    clip_range           | 0.0953       |\n",
      "|    entropy_loss         | -0.88        |\n",
      "|    explained_variance   | 0.749        |\n",
      "|    learning_rate        | 0.000238     |\n",
      "|    loss                 | -0.0296      |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.0152      |\n",
      "|    value_loss           | 0.0432       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=17.20 +/- 5.84\n",
      "Episode length: 2671.00 +/- 549.64\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.67e+03     |\n",
      "|    mean_reward          | 17.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 470000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061536385 |\n",
      "|    clip_fraction        | 0.209        |\n",
      "|    clip_range           | 0.0953       |\n",
      "|    entropy_loss         | -0.918       |\n",
      "|    explained_variance   | 0.829        |\n",
      "|    learning_rate        | 0.000238     |\n",
      "|    loss                 | -0.02        |\n",
      "|    n_updates            | 1145         |\n",
      "|    policy_gradient_loss | -0.0136      |\n",
      "|    value_loss           | 0.0388       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.23e+03 |\n",
      "|    ep_rew_mean     | 14.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 594      |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 791      |\n",
      "|    total_timesteps | 471040   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.21e+03    |\n",
      "|    ep_rew_mean          | 14.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 595         |\n",
      "|    iterations           | 231         |\n",
      "|    time_elapsed         | 794         |\n",
      "|    total_timesteps      | 473088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006597572 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.0953      |\n",
      "|    entropy_loss         | -0.903      |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.000238    |\n",
      "|    loss                 | -0.0195     |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 0.0475      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.22e+03     |\n",
      "|    ep_rew_mean          | 14.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 596          |\n",
      "|    iterations           | 232          |\n",
      "|    time_elapsed         | 796          |\n",
      "|    total_timesteps      | 475136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0112361405 |\n",
      "|    clip_fraction        | 0.251        |\n",
      "|    clip_range           | 0.0953       |\n",
      "|    entropy_loss         | -0.868       |\n",
      "|    explained_variance   | 0.669        |\n",
      "|    learning_rate        | 0.000238     |\n",
      "|    loss                 | -0.0147      |\n",
      "|    n_updates            | 1155         |\n",
      "|    policy_gradient_loss | -0.0111      |\n",
      "|    value_loss           | 0.0715       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.22e+03    |\n",
      "|    ep_rew_mean          | 14.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 596         |\n",
      "|    iterations           | 233         |\n",
      "|    time_elapsed         | 799         |\n",
      "|    total_timesteps      | 477184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007322196 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.0952      |\n",
      "|    entropy_loss         | -0.883      |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.000238    |\n",
      "|    loss                 | -0.021      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 0.0465      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.24e+03    |\n",
      "|    ep_rew_mean          | 14.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 597         |\n",
      "|    iterations           | 234         |\n",
      "|    time_elapsed         | 801         |\n",
      "|    total_timesteps      | 479232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005748422 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.0952      |\n",
      "|    entropy_loss         | -0.89       |\n",
      "|    explained_variance   | 0.748       |\n",
      "|    learning_rate        | 0.000238    |\n",
      "|    loss                 | -0.0293     |\n",
      "|    n_updates            | 1165        |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 0.0441      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=21.60 +/- 6.34\n",
      "Episode length: 2622.40 +/- 549.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.62e+03    |\n",
      "|    mean_reward          | 21.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 480000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009521311 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.0952      |\n",
      "|    entropy_loss         | -0.835      |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.000238    |\n",
      "|    loss                 | -0.0281     |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 0.0299      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.24e+03 |\n",
      "|    ep_rew_mean     | 14.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 592      |\n",
      "|    iterations      | 235      |\n",
      "|    time_elapsed    | 811      |\n",
      "|    total_timesteps | 481280   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.25e+03     |\n",
      "|    ep_rew_mean          | 14.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 593          |\n",
      "|    iterations           | 236          |\n",
      "|    time_elapsed         | 814          |\n",
      "|    total_timesteps      | 483328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072618295 |\n",
      "|    clip_fraction        | 0.251        |\n",
      "|    clip_range           | 0.0952       |\n",
      "|    entropy_loss         | -0.877       |\n",
      "|    explained_variance   | 0.762        |\n",
      "|    learning_rate        | 0.000238     |\n",
      "|    loss                 | -0.0221      |\n",
      "|    n_updates            | 1175         |\n",
      "|    policy_gradient_loss | -0.0134      |\n",
      "|    value_loss           | 0.0361       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.25e+03    |\n",
      "|    ep_rew_mean          | 14.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 594         |\n",
      "|    iterations           | 237         |\n",
      "|    time_elapsed         | 816         |\n",
      "|    total_timesteps      | 485376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005977638 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.0952      |\n",
      "|    entropy_loss         | -0.886      |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.000238    |\n",
      "|    loss                 | -0.0247     |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 0.0349      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.23e+03     |\n",
      "|    ep_rew_mean          | 14.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 595          |\n",
      "|    iterations           | 238          |\n",
      "|    time_elapsed         | 818          |\n",
      "|    total_timesteps      | 487424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069506736 |\n",
      "|    clip_fraction        | 0.236        |\n",
      "|    clip_range           | 0.0951       |\n",
      "|    entropy_loss         | -0.859       |\n",
      "|    explained_variance   | 0.816        |\n",
      "|    learning_rate        | 0.000238     |\n",
      "|    loss                 | -0.0166      |\n",
      "|    n_updates            | 1185         |\n",
      "|    policy_gradient_loss | -0.0159      |\n",
      "|    value_loss           | 0.0468       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.24e+03    |\n",
      "|    ep_rew_mean          | 14.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 595         |\n",
      "|    iterations           | 239         |\n",
      "|    time_elapsed         | 821         |\n",
      "|    total_timesteps      | 489472      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004878791 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.0951      |\n",
      "|    entropy_loss         | -0.83       |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.000238    |\n",
      "|    loss                 | -0.0214     |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 0.0475      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=17.20 +/- 5.88\n",
      "Episode length: 2660.60 +/- 339.65\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.66e+03   |\n",
      "|    mean_reward          | 17.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 490000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00696545 |\n",
      "|    clip_fraction        | 0.221      |\n",
      "|    clip_range           | 0.0951     |\n",
      "|    entropy_loss         | -0.872     |\n",
      "|    explained_variance   | 0.849      |\n",
      "|    learning_rate        | 0.000238   |\n",
      "|    loss                 | -0.0284    |\n",
      "|    n_updates            | 1195       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    value_loss           | 0.0302     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.24e+03 |\n",
      "|    ep_rew_mean     | 14.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 591      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 831      |\n",
      "|    total_timesteps | 491520   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.24e+03    |\n",
      "|    ep_rew_mean          | 14.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 591         |\n",
      "|    iterations           | 241         |\n",
      "|    time_elapsed         | 833         |\n",
      "|    total_timesteps      | 493568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007144724 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.0951      |\n",
      "|    entropy_loss         | -0.901      |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 0.000238    |\n",
      "|    loss                 | -0.0202     |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 0.0539      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.24e+03    |\n",
      "|    ep_rew_mean          | 14.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 592         |\n",
      "|    iterations           | 242         |\n",
      "|    time_elapsed         | 836         |\n",
      "|    total_timesteps      | 495616      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008217421 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.0951      |\n",
      "|    entropy_loss         | -0.867      |\n",
      "|    explained_variance   | 0.725       |\n",
      "|    learning_rate        | 0.000238    |\n",
      "|    loss                 | -0.0263     |\n",
      "|    n_updates            | 1205        |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 0.0479      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.27e+03     |\n",
      "|    ep_rew_mean          | 14.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 593          |\n",
      "|    iterations           | 243          |\n",
      "|    time_elapsed         | 838          |\n",
      "|    total_timesteps      | 497664       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072275405 |\n",
      "|    clip_fraction        | 0.216        |\n",
      "|    clip_range           | 0.095        |\n",
      "|    entropy_loss         | -0.892       |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 0.000238     |\n",
      "|    loss                 | -0.0239      |\n",
      "|    n_updates            | 1210         |\n",
      "|    policy_gradient_loss | -0.015       |\n",
      "|    value_loss           | 0.0369       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.27e+03     |\n",
      "|    ep_rew_mean          | 14.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 594          |\n",
      "|    iterations           | 244          |\n",
      "|    time_elapsed         | 841          |\n",
      "|    total_timesteps      | 499712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051174387 |\n",
      "|    clip_fraction        | 0.196        |\n",
      "|    clip_range           | 0.095        |\n",
      "|    entropy_loss         | -0.9         |\n",
      "|    explained_variance   | 0.815        |\n",
      "|    learning_rate        | 0.000238     |\n",
      "|    loss                 | -0.0294      |\n",
      "|    n_updates            | 1215         |\n",
      "|    policy_gradient_loss | -0.0136      |\n",
      "|    value_loss           | 0.0404       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=18.20 +/- 4.87\n",
      "Episode length: 2312.60 +/- 309.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.31e+03     |\n",
      "|    mean_reward          | 18.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 500000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060313744 |\n",
      "|    clip_fraction        | 0.197        |\n",
      "|    clip_range           | 0.095        |\n",
      "|    entropy_loss         | -0.879       |\n",
      "|    explained_variance   | 0.771        |\n",
      "|    learning_rate        | 0.000238     |\n",
      "|    loss                 | -0.0142      |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0144      |\n",
      "|    value_loss           | 0.0433       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.28e+03 |\n",
      "|    ep_rew_mean     | 14.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 590      |\n",
      "|    iterations      | 245      |\n",
      "|    time_elapsed    | 849      |\n",
      "|    total_timesteps | 501760   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.26e+03    |\n",
      "|    ep_rew_mean          | 15.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 591         |\n",
      "|    iterations           | 246         |\n",
      "|    time_elapsed         | 852         |\n",
      "|    total_timesteps      | 503808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008143399 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.095       |\n",
      "|    entropy_loss         | -0.908      |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.000237    |\n",
      "|    loss                 | -0.0198     |\n",
      "|    n_updates            | 1225        |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 0.0442      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.27e+03    |\n",
      "|    ep_rew_mean          | 15.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 591         |\n",
      "|    iterations           | 247         |\n",
      "|    time_elapsed         | 854         |\n",
      "|    total_timesteps      | 505856      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007232744 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.095       |\n",
      "|    entropy_loss         | -0.892      |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.000237    |\n",
      "|    loss                 | -0.028      |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 0.039       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.3e+03     |\n",
      "|    ep_rew_mean          | 15.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 592         |\n",
      "|    iterations           | 248         |\n",
      "|    time_elapsed         | 857         |\n",
      "|    total_timesteps      | 507904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007008378 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.0949      |\n",
      "|    entropy_loss         | -0.88       |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.000237    |\n",
      "|    loss                 | -0.0271     |\n",
      "|    n_updates            | 1235        |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    value_loss           | 0.0377      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | 15.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 593         |\n",
      "|    iterations           | 249         |\n",
      "|    time_elapsed         | 859         |\n",
      "|    total_timesteps      | 509952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007992484 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.0949      |\n",
      "|    entropy_loss         | -0.837      |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.000237    |\n",
      "|    loss                 | -0.0314     |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 0.0448      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=18.60 +/- 7.39\n",
      "Episode length: 2620.20 +/- 463.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.62e+03     |\n",
      "|    mean_reward          | 18.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 510000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066858297 |\n",
      "|    clip_fraction        | 0.233        |\n",
      "|    clip_range           | 0.0949       |\n",
      "|    entropy_loss         | -0.899       |\n",
      "|    explained_variance   | 0.717        |\n",
      "|    learning_rate        | 0.000237     |\n",
      "|    loss                 | -0.0327      |\n",
      "|    n_updates            | 1245         |\n",
      "|    policy_gradient_loss | -0.0137      |\n",
      "|    value_loss           | 0.043        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.28e+03 |\n",
      "|    ep_rew_mean     | 15.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 588      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 869      |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.27e+03    |\n",
      "|    ep_rew_mean          | 15.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 589         |\n",
      "|    iterations           | 251         |\n",
      "|    time_elapsed         | 871         |\n",
      "|    total_timesteps      | 514048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006421866 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.0949      |\n",
      "|    entropy_loss         | -0.861      |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.000237    |\n",
      "|    loss                 | -0.0185     |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 0.0517      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.27e+03    |\n",
      "|    ep_rew_mean          | 15.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 590         |\n",
      "|    iterations           | 252         |\n",
      "|    time_elapsed         | 874         |\n",
      "|    total_timesteps      | 516096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006321978 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.0949      |\n",
      "|    entropy_loss         | -0.871      |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 0.000237    |\n",
      "|    loss                 | -0.0205     |\n",
      "|    n_updates            | 1255        |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 0.0401      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.29e+03     |\n",
      "|    ep_rew_mean          | 15.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 590          |\n",
      "|    iterations           | 253          |\n",
      "|    time_elapsed         | 876          |\n",
      "|    total_timesteps      | 518144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066927345 |\n",
      "|    clip_fraction        | 0.235        |\n",
      "|    clip_range           | 0.0948       |\n",
      "|    entropy_loss         | -0.855       |\n",
      "|    explained_variance   | 0.688        |\n",
      "|    learning_rate        | 0.000237     |\n",
      "|    loss                 | -0.0135      |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | -0.0131      |\n",
      "|    value_loss           | 0.0683       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=15.00 +/- 5.51\n",
      "Episode length: 2177.40 +/- 428.48\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.18e+03     |\n",
      "|    mean_reward          | 15           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 520000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074346815 |\n",
      "|    clip_fraction        | 0.222        |\n",
      "|    clip_range           | 0.0948       |\n",
      "|    entropy_loss         | -0.813       |\n",
      "|    explained_variance   | 0.791        |\n",
      "|    learning_rate        | 0.000237     |\n",
      "|    loss                 | -0.0209      |\n",
      "|    n_updates            | 1265         |\n",
      "|    policy_gradient_loss | -0.0143      |\n",
      "|    value_loss           | 0.0531       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.32e+03 |\n",
      "|    ep_rew_mean     | 16.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 587      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 885      |\n",
      "|    total_timesteps | 520192   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.33e+03    |\n",
      "|    ep_rew_mean          | 16.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 588         |\n",
      "|    iterations           | 255         |\n",
      "|    time_elapsed         | 887         |\n",
      "|    total_timesteps      | 522240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006749205 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.0948      |\n",
      "|    entropy_loss         | -0.812      |\n",
      "|    explained_variance   | 0.68        |\n",
      "|    learning_rate        | 0.000237    |\n",
      "|    loss                 | -0.0142     |\n",
      "|    n_updates            | 1270        |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 0.0611      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.34e+03     |\n",
      "|    ep_rew_mean          | 16.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 589          |\n",
      "|    iterations           | 256          |\n",
      "|    time_elapsed         | 890          |\n",
      "|    total_timesteps      | 524288       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068455627 |\n",
      "|    clip_fraction        | 0.208        |\n",
      "|    clip_range           | 0.0948       |\n",
      "|    entropy_loss         | -0.863       |\n",
      "|    explained_variance   | 0.777        |\n",
      "|    learning_rate        | 0.000237     |\n",
      "|    loss                 | -0.0229      |\n",
      "|    n_updates            | 1275         |\n",
      "|    policy_gradient_loss | -0.015       |\n",
      "|    value_loss           | 0.06         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.33e+03     |\n",
      "|    ep_rew_mean          | 16.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 589          |\n",
      "|    iterations           | 257          |\n",
      "|    time_elapsed         | 892          |\n",
      "|    total_timesteps      | 526336       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072573125 |\n",
      "|    clip_fraction        | 0.228        |\n",
      "|    clip_range           | 0.0948       |\n",
      "|    entropy_loss         | -0.864       |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 0.000237     |\n",
      "|    loss                 | -0.025       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0139      |\n",
      "|    value_loss           | 0.0453       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.31e+03    |\n",
      "|    ep_rew_mean          | 16.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 590         |\n",
      "|    iterations           | 258         |\n",
      "|    time_elapsed         | 894         |\n",
      "|    total_timesteps      | 528384      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006629437 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.0947      |\n",
      "|    entropy_loss         | -0.845      |\n",
      "|    explained_variance   | 0.639       |\n",
      "|    learning_rate        | 0.000237    |\n",
      "|    loss                 | -0.0138     |\n",
      "|    n_updates            | 1285        |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 0.0703      |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ALGO = \"ppo\"\n",
    "TAGS = f\"{ROM} {ALGO.upper()}\"\n",
    "CONFIG_PATH = f\"{CONFIG_DIR}/{ALGO}.yml\"\n",
    "CMD = f\"{CMD} --algo {ALGO} --conf {CONFIG_PATH} -tags {TAGS}\"\n",
    "\n",
    "!{CMD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGO = \"dqn\"\n",
    "TAGS = f\"{ROM} {ALGO.upper()}\"\n",
    "CONFIG_PATH = f\"{CONFIG_DIR}/{ALGO}.yml\"\n",
    "CMD = f\"{CMD} --algo {ALGO} --conf {CONFIG_PATH} -tags {TAGS} --save-replay-buffer\"\n",
    "\n",
    "!{CMD}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pong\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROM = \"Pong\"\n",
    "\n",
    "# ENV_ID = f\"{ROM}NoFrameskip-v4\"\n",
    "\n",
    "# CMD = f\"cd {RUN_DIR} && python {ZOO_DIR}/train.py\"\n",
    "# CMD += f\" --env {ENV_ID}\"\n",
    "# CMD += f\" --log-folder {LOG_DIR}\"\n",
    "# CMD += f\" --tensorboard-log {TENSORBOARD_DIR}\"\n",
    "# CMD += f\" --wandb-project-name {PROJECT_NAME}\"\n",
    "# CMD += f\" --seed {SEED}\"\n",
    "# CMD += f\" --save-freq {SAVE_FREQ}\"\n",
    "# CMD += f\" --eval-freq {EVAL_FREQ}\"\n",
    "# CMD += f\" --eval-episodes {EVAL_EPISODES}\"\n",
    "# CMD += f\" --verbose {VERBOSE}\"\n",
    "# CMD += f\" --device cuda\"\n",
    "# CMD += f\" --track\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGO = \"a2c\"\n",
    "# TAGS = f\"{ROM} {ALGO.upper()}\"\n",
    "# CONFIG_PATH = f\"{CONFIG_DIR}/{ALGO}.yml\"\n",
    "# CMD = f\"{CMD} --algo {ALGO} --conf {CONFIG_PATH} -tags {TAGS}\"\n",
    "\n",
    "# !{CMD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGO = \"ppo\"\n",
    "# TAGS = f\"{ROM} {ALGO.upper()}\"\n",
    "# CONFIG_PATH = f\"{CONFIG_DIR}/{ALGO}.yml\"\n",
    "# CMD = f\"{CMD} --algo {ALGO} --conf {CONFIG_PATH} -tags {TAGS}\"\n",
    "\n",
    "# !{CMD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGO = \"dqn\"\n",
    "# TAGS = f\"{ROM} {ALGO.upper()}\"\n",
    "# CONFIG_PATH = f\"{CONFIG_DIR}/{ALGO}.yml\"\n",
    "# CMD = f\"{CMD} --algo {ALGO} --conf {CONFIG_PATH} -tags {TAGS} --save-replay-buffer\"\n",
    "\n",
    "# !{CMD}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaceInvaders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROM = \"SpaceInvaders\"\n",
    "\n",
    "# ENV_ID = f\"{ROM}NoFrameskip-v4\"\n",
    "\n",
    "# CMD = f\"cd {RUN_DIR} && python {ZOO_DIR}/train.py\"\n",
    "# CMD += f\" --env {ENV_ID}\"\n",
    "# CMD += f\" --log-folder {LOG_DIR}\"\n",
    "# CMD += f\" --tensorboard-log {TENSORBOARD_DIR}\"\n",
    "# CMD += f\" --wandb-project-name {PROJECT_NAME}\"\n",
    "# CMD += f\" --seed {SEED}\"\n",
    "# CMD += f\" --save-freq {SAVE_FREQ}\"\n",
    "# CMD += f\" --eval-freq {EVAL_FREQ}\"\n",
    "# CMD += f\" --eval-episodes {EVAL_EPISODES}\"\n",
    "# CMD += f\" --verbose {VERBOSE}\"\n",
    "# CMD += f\" --device cuda\"\n",
    "# CMD += f\" --track\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGO = \"a2c\"\n",
    "# TAGS = f\"{ROM} {ALGO.upper()}\"\n",
    "# CONFIG_PATH = f\"{CONFIG_DIR}/{ALGO}.yml\"\n",
    "# CMD = f\"{CMD} --algo {ALGO} --conf {CONFIG_PATH} -tags {TAGS}\"\n",
    "\n",
    "# !{CMD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGO = \"ppo\"\n",
    "# TAGS = f\"{ROM} {ALGO.upper()}\"\n",
    "# CONFIG_PATH = f\"{CONFIG_DIR}/{ALGO}.yml\"\n",
    "# CMD = f\"{CMD} --algo {ALGO} --conf {CONFIG_PATH} -tags {TAGS}\"\n",
    "\n",
    "# !{CMD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGO = \"dqn\"\n",
    "# TAGS = f\"{ROM} {ALGO.upper()}\"\n",
    "# CONFIG_PATH = f\"{CONFIG_DIR}/{ALGO}.yml\"\n",
    "# CMD = f\"{CMD} --algo {ALGO} --conf {CONFIG_PATH} -tags {TAGS} --save-replay-buffer\"\n",
    "\n",
    "# !{CMD}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MsPacman\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROM = \"MsPacman\"\n",
    "\n",
    "# ENV_ID = f\"{ROM}NoFrameskip-v4\"\n",
    "\n",
    "# CMD = f\"cd {RUN_DIR} && python {ZOO_DIR}/train.py\"\n",
    "# CMD += f\" --env {ENV_ID}\"\n",
    "# CMD += f\" --log-folder {LOG_DIR}\"\n",
    "# CMD += f\" --tensorboard-log {TENSORBOARD_DIR}\"\n",
    "# CMD += f\" --wandb-project-name {PROJECT_NAME}\"\n",
    "# CMD += f\" --seed {SEED}\"\n",
    "# CMD += f\" --save-freq {SAVE_FREQ}\"\n",
    "# CMD += f\" --eval-freq {EVAL_FREQ}\"\n",
    "# CMD += f\" --eval-episodes {EVAL_EPISODES}\"\n",
    "# CMD += f\" --verbose {VERBOSE}\"\n",
    "# CMD += f\" --device cuda\"\n",
    "# CMD += f\" --track\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGO = \"a2c\"\n",
    "# TAGS = f\"{ROM} {ALGO.upper()}\"\n",
    "# CONFIG_PATH = f\"{CONFIG_DIR}/{ALGO}.yml\"\n",
    "# CMD = f\"{CMD} --algo {ALGO} --conf {CONFIG_PATH} -tags {TAGS}\"\n",
    "\n",
    "# !{CMD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGO = \"ppo\"\n",
    "# TAGS = f\"{ROM} {ALGO.upper()}\"\n",
    "# CONFIG_PATH = f\"{CONFIG_DIR}/{ALGO}.yml\"\n",
    "# CMD = f\"{CMD} --algo {ALGO} --conf {CONFIG_PATH} -tags {TAGS}\"\n",
    "\n",
    "# !{CMD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGO = \"dqn\"\n",
    "# TAGS = f\"{ROM} {ALGO.upper()}\"\n",
    "# CONFIG_PATH = f\"{CONFIG_DIR}/{ALGO}.yml\"\n",
    "# CMD = f\"{CMD} --algo {ALGO} --conf {CONFIG_PATH} -tags {TAGS} --save-replay-buffer\"\n",
    "\n",
    "# !{CMD}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
