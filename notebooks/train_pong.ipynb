{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "RUN_DIR = \"/home/sequenzia/dev/rl-project\"\n",
    "ZOO_DIR = \"/home/sequenzia/dev/repos/rl-baselines3-zoo/rl_zoo3\"\n",
    "LOG_DIR = \"/home/sequenzia/dev/rl-project/trained-agents\"\n",
    "CONFIG_DIR = \"/home/sequenzia/dev/rl-project/configs\"\n",
    "TENSORBOARD_DIR = \"/home/sequenzia/dev/rl-project/logs/tensorboard\"\n",
    "\n",
    "ENV_ID = \"PongNoFrameskip-v4\"\n",
    "ROM = \"pong\"\n",
    "\n",
    "PROJECT_NAME = \"Solen-RL-Project\"\n",
    "\n",
    "SEED = 43\n",
    "\n",
    "SAVE_FREQ = 10000\n",
    "EVAL_FREQ = 10000\n",
    "EVAL_EPISODES = 5\n",
    "VERBOSE = 1\n",
    "\n",
    "CMD = f\"cd {RUN_DIR} && python {ZOO_DIR}/train.py\"\n",
    "CMD += f\" --env {ENV_ID}\"\n",
    "CMD += f\" --log-folder {LOG_DIR}\"\n",
    "CMD += f\" --tensorboard-log {TENSORBOARD_DIR}\"\n",
    "CMD += f\" --wandb-project-name {PROJECT_NAME}\"\n",
    "CMD += f\" --seed {SEED}\"\n",
    "CMD += f\" --save-freq {SAVE_FREQ}\"\n",
    "CMD += f\" --eval-freq {EVAL_FREQ}\"\n",
    "CMD += f\" --eval-episodes {EVAL_EPISODES}\"\n",
    "CMD += f\" --verbose {VERBOSE}\"\n",
    "CMD += f\" --device cuda\"\n",
    "CMD += f\" --track\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== PongNoFrameskip-v4 ==========\n",
      "Seed: 43\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mappliedtheta\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sequenzia/dev/rl-project/logs/wandb/run-20231218_073435-4r30pbj8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mPongNoFrameskip-v4__dqn__1702902874\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/appliedtheta/Solen-RL-Project\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/appliedtheta/Solen-RL-Project/runs/4r30pbj8\u001b[0m\n",
      "Loading hyperparameters from: /home/sequenzia/dev/rl-project/configs/dqn.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 32),\n",
      "             ('buffer_size', 100000),\n",
      "             ('env_wrapper',\n",
      "              ['stable_baselines3.common.atari_wrappers.AtariWrapper']),\n",
      "             ('exploration_final_eps', 0.01),\n",
      "             ('exploration_fraction', 0.1),\n",
      "             ('frame_stack', 4),\n",
      "             ('gradient_steps', 1),\n",
      "             ('learning_rate', 0.0001),\n",
      "             ('learning_starts', 100000),\n",
      "             ('n_timesteps', 20000000.0),\n",
      "             ('optimize_memory_usage', False),\n",
      "             ('policy', 'CnnPolicy'),\n",
      "             ('target_update_interval', 1000),\n",
      "             ('train_freq', 1)])\n",
      "Using 1 environments\n",
      "Creating test environment\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "Stacking 4 frames\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Stacking 4 frames\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Using cuda device\n",
      "Log path: /home/sequenzia/dev/rl-project/trained-agents/dqn/PongNoFrameskip-v4_1\n",
      "Logging to runs/PongNoFrameskip-v4__dqn__1702902874/PongNoFrameskip-v4/DQN_1\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.53e+03 |\n",
      "|    ep_rew_mean      | -20.5    |\n",
      "|    exploration_rate | 0.998    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 1125     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 3508     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.6e+03  |\n",
      "|    ep_rew_mean      | -20.4    |\n",
      "|    exploration_rate | 0.996    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 1171     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 7152     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-20.20 +/- 1.17\n",
      "Episode length: 4009.20 +/- 346.41\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.01e+03 |\n",
      "|    mean_reward      | -20.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.995    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.64e+03 |\n",
      "|    ep_rew_mean      | -20.5    |\n",
      "|    exploration_rate | 0.995    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 816      |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 10875    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.73e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.993    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 893      |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 14821    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.991    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 942      |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 18610    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-20.20 +/- 1.17\n",
      "Episode length: 3851.60 +/- 270.05\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.85e+03 |\n",
      "|    mean_reward      | -20.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.99     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.989    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 840      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 22304    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.71e+03 |\n",
      "|    ep_rew_mean      | -20.4    |\n",
      "|    exploration_rate | 0.987    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 875      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 25852    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.7e+03  |\n",
      "|    ep_rew_mean      | -20.4    |\n",
      "|    exploration_rate | 0.985    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 898      |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 29417    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 3430.00 +/- 155.49\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.43e+03 |\n",
      "|    mean_reward      | -20.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.985    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.72e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.984    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 846      |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 33251    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.68e+03 |\n",
      "|    ep_rew_mean      | -20.4    |\n",
      "|    exploration_rate | 0.982    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 868      |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 36586    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-20.00 +/- 0.89\n",
      "Episode length: 3721.60 +/- 279.50\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.72e+03 |\n",
      "|    mean_reward      | -20      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.98     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40000    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.71e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.98     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 825      |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 40600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.72e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.978    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 848      |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 44377    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.72e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.976    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 867      |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 48060    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-20.20 +/- 0.75\n",
      "Episode length: 3793.60 +/- 427.46\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.79e+03 |\n",
      "|    mean_reward      | -20.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.975    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 50000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.71e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.974    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 830      |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 51639    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.7e+03  |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.973    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 844      |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 55130    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.71e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.971    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 861      |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 58971    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-19.80 +/- 0.98\n",
      "Episode length: 3763.40 +/- 291.85\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.76e+03 |\n",
      "|    mean_reward      | -19.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.97     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60000    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.72e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.969    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 831      |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 62838    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.72e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.967    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 843      |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 66489    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-20.40 +/- 0.49\n",
      "Episode length: 3672.80 +/- 369.49\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.67e+03 |\n",
      "|    mean_reward      | -20.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.965    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 70000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.71e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.965    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 815      |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 70143    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.73e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.963    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 830      |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 74132    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.72e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.962    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 841      |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 77667    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 3643.40 +/- 416.82\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.64e+03 |\n",
      "|    mean_reward      | -20.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.96     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 80000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.71e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.96     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 818      |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 81141    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.72e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.958    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 828      |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 85109    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.72e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.956    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 837      |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total_timesteps  | 88859    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-20.20 +/- 0.75\n",
      "Episode length: 3791.40 +/- 466.49\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.79e+03 |\n",
      "|    mean_reward      | -20.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.955    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.73e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.954    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 819      |\n",
      "|    time_elapsed     | 113      |\n",
      "|    total_timesteps  | 92695    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.952    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 829      |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 96534    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-20.20 +/- 0.40\n",
      "Episode length: 4073.40 +/- 290.46\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.07e+03 |\n",
      "|    mean_reward      | -20.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.951    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.95     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 804      |\n",
      "|    time_elapsed     | 124      |\n",
      "|    total_timesteps  | 100180   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000304 |\n",
      "|    n_updates        | 179      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.949    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 731      |\n",
      "|    time_elapsed     | 141      |\n",
      "|    total_timesteps  | 103775   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0149   |\n",
      "|    n_updates        | 3774     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.947    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 674      |\n",
      "|    time_elapsed     | 159      |\n",
      "|    total_timesteps  | 107665   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00197  |\n",
      "|    n_updates        | 7664     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-20.00 +/- 0.63\n",
      "Episode length: 3652.80 +/- 414.83\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.65e+03 |\n",
      "|    mean_reward      | -20      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.946    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 110000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00508  |\n",
      "|    n_updates        | 9999     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.945    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 613      |\n",
      "|    time_elapsed     | 181      |\n",
      "|    total_timesteps  | 111582   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00852  |\n",
      "|    n_updates        | 11581    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.76e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.943    |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 573      |\n",
      "|    time_elapsed     | 201      |\n",
      "|    total_timesteps  | 115719   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000951 |\n",
      "|    n_updates        | 15718    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.75e+03 |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.941    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 547      |\n",
      "|    time_elapsed     | 217      |\n",
      "|    total_timesteps  | 119078   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0168   |\n",
      "|    n_updates        | 19077    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-20.00 +/- 1.26\n",
      "Episode length: 3812.00 +/- 460.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.81e+03 |\n",
      "|    mean_reward      | -20      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.941    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 120000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.027    |\n",
      "|    n_updates        | 19999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.77e+03 |\n",
      "|    ep_rew_mean      | -20.2    |\n",
      "|    exploration_rate | 0.939    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 510      |\n",
      "|    time_elapsed     | 241      |\n",
      "|    total_timesteps  | 123153   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0022   |\n",
      "|    n_updates        | 23152    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.77e+03 |\n",
      "|    ep_rew_mean      | -20.2    |\n",
      "|    exploration_rate | 0.937    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 490      |\n",
      "|    time_elapsed     | 258      |\n",
      "|    total_timesteps  | 126862   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00208  |\n",
      "|    n_updates        | 26861    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-20.80 +/- 0.40\n",
      "Episode length: 3337.60 +/- 223.62\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.34e+03 |\n",
      "|    mean_reward      | -20.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.936    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 130000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00221  |\n",
      "|    n_updates        | 29999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.79e+03 |\n",
      "|    ep_rew_mean      | -20.2    |\n",
      "|    exploration_rate | 0.935    |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 466      |\n",
      "|    time_elapsed     | 280      |\n",
      "|    total_timesteps  | 130844   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00144  |\n",
      "|    n_updates        | 30843    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.79e+03 |\n",
      "|    ep_rew_mean      | -20.2    |\n",
      "|    exploration_rate | 0.933    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 451      |\n",
      "|    time_elapsed     | 298      |\n",
      "|    total_timesteps  | 134689   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00731  |\n",
      "|    n_updates        | 34688    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.78e+03 |\n",
      "|    ep_rew_mean      | -20.2    |\n",
      "|    exploration_rate | 0.932    |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 439      |\n",
      "|    time_elapsed     | 314      |\n",
      "|    total_timesteps  | 138210   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00425  |\n",
      "|    n_updates        | 38209    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-20.20 +/- 1.60\n",
      "Episode length: 3782.00 +/- 1014.25\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.78e+03 |\n",
      "|    mean_reward      | -20.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.931    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 140000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0176   |\n",
      "|    n_updates        | 39999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.78e+03 |\n",
      "|    ep_rew_mean      | -20.2    |\n",
      "|    exploration_rate | 0.93     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 421      |\n",
      "|    time_elapsed     | 336      |\n",
      "|    total_timesteps  | 142106   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00187  |\n",
      "|    n_updates        | 42105    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.8e+03  |\n",
      "|    ep_rew_mean      | -20.2    |\n",
      "|    exploration_rate | 0.928    |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 411      |\n",
      "|    time_elapsed     | 355      |\n",
      "|    total_timesteps  | 146206   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00188  |\n",
      "|    n_updates        | 46205    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 3635.00 +/- 279.80\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.64e+03 |\n",
      "|    mean_reward      | -20.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.926    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 150000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00233  |\n",
      "|    n_updates        | 49999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.82e+03 |\n",
      "|    ep_rew_mean      | -20.2    |\n",
      "|    exploration_rate | 0.926    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 397      |\n",
      "|    time_elapsed     | 377      |\n",
      "|    total_timesteps  | 150173   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00277  |\n",
      "|    n_updates        | 50172    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.81e+03 |\n",
      "|    ep_rew_mean      | -20.2    |\n",
      "|    exploration_rate | 0.924    |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 390      |\n",
      "|    time_elapsed     | 393      |\n",
      "|    total_timesteps  | 153664   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0037   |\n",
      "|    n_updates        | 53663    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.79e+03 |\n",
      "|    ep_rew_mean      | -20.2    |\n",
      "|    exploration_rate | 0.922    |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 383      |\n",
      "|    time_elapsed     | 409      |\n",
      "|    total_timesteps  | 157118   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00347  |\n",
      "|    n_updates        | 57117    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 3796.00 +/- 369.87\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.8e+03  |\n",
      "|    mean_reward      | -20.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.921    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 160000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0047   |\n",
      "|    n_updates        | 59999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.8e+03  |\n",
      "|    ep_rew_mean      | -20.2    |\n",
      "|    exploration_rate | 0.92     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 432      |\n",
      "|    total_timesteps  | 160974   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00241  |\n",
      "|    n_updates        | 60973    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.81e+03 |\n",
      "|    ep_rew_mean      | -20.2    |\n",
      "|    exploration_rate | 0.918    |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 365      |\n",
      "|    time_elapsed     | 450      |\n",
      "|    total_timesteps  | 164799   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00611  |\n",
      "|    n_updates        | 64798    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.81e+03 |\n",
      "|    ep_rew_mean      | -20.2    |\n",
      "|    exploration_rate | 0.916    |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 359      |\n",
      "|    time_elapsed     | 469      |\n",
      "|    total_timesteps  | 168773   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0022   |\n",
      "|    n_updates        | 68772    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-20.60 +/- 0.80\n",
      "Episode length: 3489.00 +/- 215.63\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.49e+03 |\n",
      "|    mean_reward      | -20.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.916    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 170000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00121  |\n",
      "|    n_updates        | 69999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.81e+03 |\n",
      "|    ep_rew_mean      | -20.2    |\n",
      "|    exploration_rate | 0.915    |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 351      |\n",
      "|    time_elapsed     | 490      |\n",
      "|    total_timesteps  | 172363   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00307  |\n",
      "|    n_updates        | 72362    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.85e+03 |\n",
      "|    ep_rew_mean      | -20.2    |\n",
      "|    exploration_rate | 0.913    |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 345      |\n",
      "|    time_elapsed     | 512      |\n",
      "|    total_timesteps  | 176767   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00403  |\n",
      "|    n_updates        | 76766    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-19.80 +/- 0.75\n",
      "Episode length: 3984.40 +/- 324.61\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.98e+03 |\n",
      "|    mean_reward      | -19.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.911    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 180000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00486  |\n",
      "|    n_updates        | 79999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.86e+03 |\n",
      "|    ep_rew_mean      | -20.2    |\n",
      "|    exploration_rate | 0.91     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 336      |\n",
      "|    time_elapsed     | 537      |\n",
      "|    total_timesteps  | 181116   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00129  |\n",
      "|    n_updates        | 81115    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.87e+03 |\n",
      "|    ep_rew_mean      | -20.1    |\n",
      "|    exploration_rate | 0.908    |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 556      |\n",
      "|    total_timesteps  | 185174   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00281  |\n",
      "|    n_updates        | 85173    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.87e+03 |\n",
      "|    ep_rew_mean      | -20.1    |\n",
      "|    exploration_rate | 0.906    |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 328      |\n",
      "|    time_elapsed     | 575      |\n",
      "|    total_timesteps  | 188995   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00178  |\n",
      "|    n_updates        | 88994    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 3913.20 +/- 570.27\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.91e+03 |\n",
      "|    mean_reward      | -20.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.906    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 190000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00108  |\n",
      "|    n_updates        | 89999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.87e+03 |\n",
      "|    ep_rew_mean      | -20.2    |\n",
      "|    exploration_rate | 0.905    |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 322      |\n",
      "|    time_elapsed     | 597      |\n",
      "|    total_timesteps  | 192798   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00427  |\n",
      "|    n_updates        | 92797    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.89e+03 |\n",
      "|    ep_rew_mean      | -20.1    |\n",
      "|    exploration_rate | 0.903    |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 318      |\n",
      "|    time_elapsed     | 617      |\n",
      "|    total_timesteps  | 196816   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00124  |\n",
      "|    n_updates        | 96815    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-20.40 +/- 0.80\n",
      "Episode length: 3892.20 +/- 336.17\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.89e+03 |\n",
      "|    mean_reward      | -20.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.901    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 200000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00437  |\n",
      "|    n_updates        | 99999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.91e+03 |\n",
      "|    ep_rew_mean      | -20.1    |\n",
      "|    exploration_rate | 0.9      |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 312      |\n",
      "|    time_elapsed     | 642      |\n",
      "|    total_timesteps  | 201036   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00216  |\n",
      "|    n_updates        | 101035   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.93e+03 |\n",
      "|    ep_rew_mean      | -20.1    |\n",
      "|    exploration_rate | 0.898    |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 663      |\n",
      "|    total_timesteps  | 205346   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0033   |\n",
      "|    n_updates        | 105345   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.94e+03 |\n",
      "|    ep_rew_mean      | -20      |\n",
      "|    exploration_rate | 0.896    |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 306      |\n",
      "|    time_elapsed     | 683      |\n",
      "|    total_timesteps  | 209659   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00118  |\n",
      "|    n_updates        | 109658   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-20.40 +/- 1.20\n",
      "Episode length: 3900.20 +/- 418.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.9e+03  |\n",
      "|    mean_reward      | -20.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.896    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 210000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00119  |\n",
      "|    n_updates        | 109999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.94e+03 |\n",
      "|    ep_rew_mean      | -20.1    |\n",
      "|    exploration_rate | 0.894    |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 301      |\n",
      "|    time_elapsed     | 708      |\n",
      "|    total_timesteps  | 213781   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00101  |\n",
      "|    n_updates        | 113780   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.98e+03 |\n",
      "|    ep_rew_mean      | -20      |\n",
      "|    exploration_rate | 0.892    |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 299      |\n",
      "|    time_elapsed     | 727      |\n",
      "|    total_timesteps  | 217947   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00175  |\n",
      "|    n_updates        | 117946   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-19.80 +/- 1.47\n",
      "Episode length: 4009.20 +/- 822.78\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.01e+03 |\n",
      "|    mean_reward      | -19.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.891    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 220000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00889  |\n",
      "|    n_updates        | 119999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.98e+03 |\n",
      "|    ep_rew_mean      | -20      |\n",
      "|    exploration_rate | 0.89     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 295      |\n",
      "|    time_elapsed     | 750      |\n",
      "|    total_timesteps  | 222132   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00284  |\n",
      "|    n_updates        | 122131   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.01e+03 |\n",
      "|    ep_rew_mean      | -20      |\n",
      "|    exploration_rate | 0.888    |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 294      |\n",
      "|    time_elapsed     | 770      |\n",
      "|    total_timesteps  | 226472   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00324  |\n",
      "|    n_updates        | 126471   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=-19.80 +/- 1.60\n",
      "Episode length: 4549.60 +/- 606.35\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.55e+03 |\n",
      "|    mean_reward      | -19.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.886    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 230000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00258  |\n",
      "|    n_updates        | 129999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.01e+03 |\n",
      "|    ep_rew_mean      | -20      |\n",
      "|    exploration_rate | 0.886    |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 290      |\n",
      "|    time_elapsed     | 794      |\n",
      "|    total_timesteps  | 230499   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00335  |\n",
      "|    n_updates        | 130498   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.01e+03 |\n",
      "|    ep_rew_mean      | -20      |\n",
      "|    exploration_rate | 0.884    |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 288      |\n",
      "|    time_elapsed     | 812      |\n",
      "|    total_timesteps  | 234534   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00239  |\n",
      "|    n_updates        | 134533   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.04e+03 |\n",
      "|    ep_rew_mean      | -20      |\n",
      "|    exploration_rate | 0.882    |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 287      |\n",
      "|    time_elapsed     | 831      |\n",
      "|    total_timesteps  | 238737   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000785 |\n",
      "|    n_updates        | 138736   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-20.00 +/- 0.63\n",
      "Episode length: 4001.20 +/- 533.95\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4e+03    |\n",
      "|    mean_reward      | -20      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.881    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 240000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00547  |\n",
      "|    n_updates        | 139999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.07e+03 |\n",
      "|    ep_rew_mean      | -20      |\n",
      "|    exploration_rate | 0.88     |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 283      |\n",
      "|    time_elapsed     | 857      |\n",
      "|    total_timesteps  | 243316   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00218  |\n",
      "|    n_updates        | 143315   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.07e+03 |\n",
      "|    ep_rew_mean      | -19.9    |\n",
      "|    exploration_rate | 0.877    |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 282      |\n",
      "|    time_elapsed     | 876      |\n",
      "|    total_timesteps  | 247484   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00647  |\n",
      "|    n_updates        | 147483   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=-19.40 +/- 1.02\n",
      "Episode length: 4332.40 +/- 395.73\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.33e+03 |\n",
      "|    mean_reward      | -19.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.876    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 250000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00818  |\n",
      "|    n_updates        | 149999   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.06e+03 |\n",
      "|    ep_rew_mean      | -19.9    |\n",
      "|    exploration_rate | 0.876    |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 279      |\n",
      "|    time_elapsed     | 899      |\n",
      "|    total_timesteps  | 251211   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0023   |\n",
      "|    n_updates        | 151210   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.08e+03 |\n",
      "|    ep_rew_mean      | -19.9    |\n",
      "|    exploration_rate | 0.874    |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 278      |\n",
      "|    time_elapsed     | 916      |\n",
      "|    total_timesteps  | 255129   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00115  |\n",
      "|    n_updates        | 155128   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.13e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.871    |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 276      |\n",
      "|    time_elapsed     | 938      |\n",
      "|    total_timesteps  | 259879   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00324  |\n",
      "|    n_updates        | 159878   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-19.60 +/- 0.49\n",
      "Episode length: 4282.60 +/- 276.98\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.28e+03 |\n",
      "|    mean_reward      | -19.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.871    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 260000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00513  |\n",
      "|    n_updates        | 159999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.17e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.869    |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 273      |\n",
      "|    time_elapsed     | 965      |\n",
      "|    total_timesteps  | 264572   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00349  |\n",
      "|    n_updates        | 164571   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.17e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.867    |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 272      |\n",
      "|    time_elapsed     | 983      |\n",
      "|    total_timesteps  | 268538   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00595  |\n",
      "|    n_updates        | 168537   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=-20.00 +/- 1.10\n",
      "Episode length: 3963.00 +/- 306.67\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 3.96e+03 |\n",
      "|    mean_reward      | -20      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.866    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 270000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0013   |\n",
      "|    n_updates        | 169999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.17e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.865    |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 270      |\n",
      "|    time_elapsed     | 1006     |\n",
      "|    total_timesteps  | 272389   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00633  |\n",
      "|    n_updates        | 172388   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.2e+03  |\n",
      "|    ep_rew_mean      | -19.7    |\n",
      "|    exploration_rate | 0.863    |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 269      |\n",
      "|    time_elapsed     | 1026     |\n",
      "|    total_timesteps  | 276725   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00943  |\n",
      "|    n_updates        | 176724   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-19.60 +/- 1.20\n",
      "Episode length: 4219.20 +/- 276.58\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.22e+03 |\n",
      "|    mean_reward      | -19.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.861    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 280000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00101  |\n",
      "|    n_updates        | 179999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.18e+03 |\n",
      "|    ep_rew_mean      | -19.7    |\n",
      "|    exploration_rate | 0.861    |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 267      |\n",
      "|    time_elapsed     | 1049     |\n",
      "|    total_timesteps  | 280735   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00211  |\n",
      "|    n_updates        | 180734   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.16e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.859    |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 266      |\n",
      "|    time_elapsed     | 1068     |\n",
      "|    total_timesteps  | 284634   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00186  |\n",
      "|    n_updates        | 184633   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.17e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.857    |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 265      |\n",
      "|    time_elapsed     | 1087     |\n",
      "|    total_timesteps  | 288886   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00212  |\n",
      "|    n_updates        | 188885   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=-19.60 +/- 0.80\n",
      "Episode length: 4044.00 +/- 503.18\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.04e+03 |\n",
      "|    mean_reward      | -19.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.856    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 290000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00222  |\n",
      "|    n_updates        | 189999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.18e+03 |\n",
      "|    ep_rew_mean      | -19.7    |\n",
      "|    exploration_rate | 0.855    |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 263      |\n",
      "|    time_elapsed     | 1110     |\n",
      "|    total_timesteps  | 292855   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00214  |\n",
      "|    n_updates        | 192854   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.18e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.853    |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 263      |\n",
      "|    time_elapsed     | 1127     |\n",
      "|    total_timesteps  | 296621   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00551  |\n",
      "|    n_updates        | 196620   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-18.80 +/- 1.17\n",
      "Episode length: 4557.60 +/- 335.46\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.56e+03 |\n",
      "|    mean_reward      | -18.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.852    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 300000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00548  |\n",
      "|    n_updates        | 199999   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.16e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.851    |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 261      |\n",
      "|    time_elapsed     | 1149     |\n",
      "|    total_timesteps  | 300197   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00134  |\n",
      "|    n_updates        | 200196   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.16e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.849    |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 260      |\n",
      "|    time_elapsed     | 1168     |\n",
      "|    total_timesteps  | 304345   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00714  |\n",
      "|    n_updates        | 204344   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.13e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.847    |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 259      |\n",
      "|    time_elapsed     | 1185     |\n",
      "|    total_timesteps  | 308130   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00258  |\n",
      "|    n_updates        | 208129   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=-19.60 +/- 1.02\n",
      "Episode length: 4193.40 +/- 384.95\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.19e+03 |\n",
      "|    mean_reward      | -19.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.847    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 310000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00122  |\n",
      "|    n_updates        | 209999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.11e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.846    |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 258      |\n",
      "|    time_elapsed     | 1207     |\n",
      "|    total_timesteps  | 311872   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00248  |\n",
      "|    n_updates        | 211871   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.11e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.844    |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 257      |\n",
      "|    time_elapsed     | 1225     |\n",
      "|    total_timesteps  | 315912   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00131  |\n",
      "|    n_updates        | 215911   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-19.60 +/- 0.80\n",
      "Episode length: 4337.00 +/- 386.41\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.34e+03 |\n",
      "|    mean_reward      | -19.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.842    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 320000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00133  |\n",
      "|    n_updates        | 219999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.11e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.842    |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 256      |\n",
      "|    time_elapsed     | 1250     |\n",
      "|    total_timesteps  | 320201   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00169  |\n",
      "|    n_updates        | 220200   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.11e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.839    |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 1270     |\n",
      "|    total_timesteps  | 324423   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00879  |\n",
      "|    n_updates        | 224422   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.11e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.837    |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 1290     |\n",
      "|    total_timesteps  | 328780   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0034   |\n",
      "|    n_updates        | 228779   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=-19.40 +/- 0.80\n",
      "Episode length: 4399.20 +/- 247.02\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.4e+03  |\n",
      "|    mean_reward      | -19.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.837    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 330000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00532  |\n",
      "|    n_updates        | 229999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.16e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.835    |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1319     |\n",
      "|    total_timesteps  | 333873   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00632  |\n",
      "|    n_updates        | 233872   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.16e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.833    |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1338     |\n",
      "|    total_timesteps  | 338065   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00419  |\n",
      "|    n_updates        | 238064   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-19.00 +/- 0.89\n",
      "Episode length: 4534.80 +/- 325.85\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.53e+03 |\n",
      "|    mean_reward      | -19      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.832    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 340000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00433  |\n",
      "|    n_updates        | 239999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.16e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.831    |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 1362     |\n",
      "|    total_timesteps  | 342184   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00324  |\n",
      "|    n_updates        | 242183   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.17e+03 |\n",
      "|    ep_rew_mean      | -19.7    |\n",
      "|    exploration_rate | 0.828    |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 1384     |\n",
      "|    total_timesteps  | 346955   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.018    |\n",
      "|    n_updates        | 246954   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=-20.00 +/- 0.63\n",
      "Episode length: 4187.20 +/- 369.38\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.19e+03 |\n",
      "|    mean_reward      | -20      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.827    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 350000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0116   |\n",
      "|    n_updates        | 249999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.18e+03 |\n",
      "|    ep_rew_mean      | -19.7    |\n",
      "|    exploration_rate | 0.826    |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 1410     |\n",
      "|    total_timesteps  | 351347   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00479  |\n",
      "|    n_updates        | 251346   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.19e+03 |\n",
      "|    ep_rew_mean      | -19.7    |\n",
      "|    exploration_rate | 0.824    |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 248      |\n",
      "|    time_elapsed     | 1429     |\n",
      "|    total_timesteps  | 355347   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00518  |\n",
      "|    n_updates        | 255346   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-18.00 +/- 1.41\n",
      "Episode length: 4749.20 +/- 427.99\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.75e+03 |\n",
      "|    mean_reward      | -18      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.822    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 360000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00466  |\n",
      "|    n_updates        | 259999   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.22e+03 |\n",
      "|    ep_rew_mean      | -19.6    |\n",
      "|    exploration_rate | 0.822    |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 1456     |\n",
      "|    total_timesteps  | 360016   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00464  |\n",
      "|    n_updates        | 260015   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.19e+03 |\n",
      "|    ep_rew_mean      | -19.7    |\n",
      "|    exploration_rate | 0.82     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 1475     |\n",
      "|    total_timesteps  | 364039   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00228  |\n",
      "|    n_updates        | 264038   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.15e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.818    |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 1493     |\n",
      "|    total_timesteps  | 367907   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00604  |\n",
      "|    n_updates        | 267906   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=-19.60 +/- 1.36\n",
      "Episode length: 4532.40 +/- 507.02\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.53e+03 |\n",
      "|    mean_reward      | -19.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.817    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 370000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00377  |\n",
      "|    n_updates        | 269999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.16e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.816    |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 245      |\n",
      "|    time_elapsed     | 1518     |\n",
      "|    total_timesteps  | 372104   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00464  |\n",
      "|    n_updates        | 272103   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.18e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.814    |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 244      |\n",
      "|    time_elapsed     | 1538     |\n",
      "|    total_timesteps  | 376383   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00284  |\n",
      "|    n_updates        | 276382   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-19.60 +/- 1.02\n",
      "Episode length: 4157.20 +/- 470.89\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.16e+03 |\n",
      "|    mean_reward      | -19.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.812    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 380000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00738  |\n",
      "|    n_updates        | 279999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.16e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.812    |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 243      |\n",
      "|    time_elapsed     | 1561     |\n",
      "|    total_timesteps  | 380318   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00701  |\n",
      "|    n_updates        | 280317   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.18e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 243      |\n",
      "|    time_elapsed     | 1581     |\n",
      "|    total_timesteps  | 384588   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.011    |\n",
      "|    n_updates        | 284587   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.21e+03 |\n",
      "|    ep_rew_mean      | -19.7    |\n",
      "|    exploration_rate | 0.807    |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 242      |\n",
      "|    time_elapsed     | 1604     |\n",
      "|    total_timesteps  | 389470   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00401  |\n",
      "|    n_updates        | 289469   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=-20.40 +/- 0.80\n",
      "Episode length: 4519.60 +/- 742.92\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.52e+03 |\n",
      "|    mean_reward      | -20.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.807    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 390000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00392  |\n",
      "|    n_updates        | 289999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.23e+03 |\n",
      "|    ep_rew_mean      | -19.7    |\n",
      "|    exploration_rate | 0.805    |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 241      |\n",
      "|    time_elapsed     | 1629     |\n",
      "|    total_timesteps  | 394116   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00186  |\n",
      "|    n_updates        | 294115   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.23e+03 |\n",
      "|    ep_rew_mean      | -19.7    |\n",
      "|    exploration_rate | 0.803    |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 241      |\n",
      "|    time_elapsed     | 1646     |\n",
      "|    total_timesteps  | 398163   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000968 |\n",
      "|    n_updates        | 298162   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-18.60 +/- 1.62\n",
      "Episode length: 4552.00 +/- 745.72\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.55e+03 |\n",
      "|    mean_reward      | -18.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.802    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 400000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00319  |\n",
      "|    n_updates        | 299999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.25e+03 |\n",
      "|    ep_rew_mean      | -19.7    |\n",
      "|    exploration_rate | 0.801    |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 240      |\n",
      "|    time_elapsed     | 1669     |\n",
      "|    total_timesteps  | 402355   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00496  |\n",
      "|    n_updates        | 302354   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.27e+03 |\n",
      "|    ep_rew_mean      | -19.7    |\n",
      "|    exploration_rate | 0.799    |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 240      |\n",
      "|    time_elapsed     | 1687     |\n",
      "|    total_timesteps  | 406459   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00416  |\n",
      "|    n_updates        | 306458   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=-19.40 +/- 1.02\n",
      "Episode length: 4615.80 +/- 343.96\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.62e+03 |\n",
      "|    mean_reward      | -19.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.797    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 410000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00119  |\n",
      "|    n_updates        | 309999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.3e+03  |\n",
      "|    ep_rew_mean      | -19.6    |\n",
      "|    exploration_rate | 0.796    |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 240      |\n",
      "|    time_elapsed     | 1713     |\n",
      "|    total_timesteps  | 411400   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00632  |\n",
      "|    n_updates        | 311399   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.34e+03 |\n",
      "|    ep_rew_mean      | -19.6    |\n",
      "|    exploration_rate | 0.794    |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 240      |\n",
      "|    time_elapsed     | 1733     |\n",
      "|    total_timesteps  | 416058   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00245  |\n",
      "|    n_updates        | 316057   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-19.00 +/- 0.89\n",
      "Episode length: 4590.60 +/- 248.56\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.59e+03 |\n",
      "|    mean_reward      | -19      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.792    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 420000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00184  |\n",
      "|    n_updates        | 319999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.36e+03 |\n",
      "|    ep_rew_mean      | -19.6    |\n",
      "|    exploration_rate | 0.792    |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 239      |\n",
      "|    time_elapsed     | 1756     |\n",
      "|    total_timesteps  | 420254   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00265  |\n",
      "|    n_updates        | 320253   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.36e+03 |\n",
      "|    ep_rew_mean      | -19.6    |\n",
      "|    exploration_rate | 0.79     |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 239      |\n",
      "|    time_elapsed     | 1775     |\n",
      "|    total_timesteps  | 424473   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00643  |\n",
      "|    n_updates        | 324472   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.37e+03 |\n",
      "|    ep_rew_mean      | -19.6    |\n",
      "|    exploration_rate | 0.788    |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 239      |\n",
      "|    time_elapsed     | 1794     |\n",
      "|    total_timesteps  | 428876   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00395  |\n",
      "|    n_updates        | 328875   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=-19.20 +/- 0.75\n",
      "Episode length: 4731.80 +/- 489.89\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.73e+03 |\n",
      "|    mean_reward      | -19.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.787    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 430000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00521  |\n",
      "|    n_updates        | 329999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.39e+03 |\n",
      "|    ep_rew_mean      | -19.5    |\n",
      "|    exploration_rate | 0.785    |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 238      |\n",
      "|    time_elapsed     | 1820     |\n",
      "|    total_timesteps  | 433691   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00228  |\n",
      "|    n_updates        | 333690   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.39e+03 |\n",
      "|    ep_rew_mean      | -19.5    |\n",
      "|    exploration_rate | 0.783    |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 238      |\n",
      "|    time_elapsed     | 1838     |\n",
      "|    total_timesteps  | 438010   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00483  |\n",
      "|    n_updates        | 338009   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-19.60 +/- 1.02\n",
      "Episode length: 4384.20 +/- 431.45\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.38e+03 |\n",
      "|    mean_reward      | -19.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.782    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 440000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00181  |\n",
      "|    n_updates        | 339999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.35e+03 |\n",
      "|    ep_rew_mean      | -19.6    |\n",
      "|    exploration_rate | 0.781    |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 237      |\n",
      "|    time_elapsed     | 1861     |\n",
      "|    total_timesteps  | 442150   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00157  |\n",
      "|    n_updates        | 342149   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.36e+03 |\n",
      "|    ep_rew_mean      | -19.6    |\n",
      "|    exploration_rate | 0.779    |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 237      |\n",
      "|    time_elapsed     | 1880     |\n",
      "|    total_timesteps  | 446521   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00349  |\n",
      "|    n_updates        | 346520   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=-19.00 +/- 1.26\n",
      "Episode length: 4744.40 +/- 505.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.74e+03 |\n",
      "|    mean_reward      | -19      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.777    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 450000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0121   |\n",
      "|    n_updates        | 349999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.39e+03 |\n",
      "|    ep_rew_mean      | -19.6    |\n",
      "|    exploration_rate | 0.777    |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 236      |\n",
      "|    time_elapsed     | 1906     |\n",
      "|    total_timesteps  | 451502   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00639  |\n",
      "|    n_updates        | 351501   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.38e+03 |\n",
      "|    ep_rew_mean      | -19.6    |\n",
      "|    exploration_rate | 0.774    |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 236      |\n",
      "|    time_elapsed     | 1926     |\n",
      "|    total_timesteps  | 456020   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.004    |\n",
      "|    n_updates        | 356019   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=-19.60 +/- 1.02\n",
      "Episode length: 4120.80 +/- 461.62\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.12e+03 |\n",
      "|    mean_reward      | -19.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.772    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 460000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00225  |\n",
      "|    n_updates        | 359999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.39e+03 |\n",
      "|    ep_rew_mean      | -19.6    |\n",
      "|    exploration_rate | 0.772    |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 236      |\n",
      "|    time_elapsed     | 1950     |\n",
      "|    total_timesteps  | 460477   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00621  |\n",
      "|    n_updates        | 360476   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.4e+03  |\n",
      "|    ep_rew_mean      | -19.5    |\n",
      "|    exploration_rate | 0.77     |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 236      |\n",
      "|    time_elapsed     | 1968     |\n",
      "|    total_timesteps  | 464789   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00245  |\n",
      "|    n_updates        | 364788   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.39e+03 |\n",
      "|    ep_rew_mean      | -19.6    |\n",
      "|    exploration_rate | 0.768    |\n",
      "| time/               |          |\n",
      "|    episodes         | 464      |\n",
      "|    fps              | 236      |\n",
      "|    time_elapsed     | 1987     |\n",
      "|    total_timesteps  | 469131   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00231  |\n",
      "|    n_updates        | 369130   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=-18.60 +/- 2.50\n",
      "Episode length: 4612.80 +/- 953.30\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.61e+03 |\n",
      "|    mean_reward      | -18.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.767    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 470000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00279  |\n",
      "|    n_updates        | 369999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.4e+03  |\n",
      "|    ep_rew_mean      | -19.6    |\n",
      "|    exploration_rate | 0.766    |\n",
      "| time/               |          |\n",
      "|    episodes         | 468      |\n",
      "|    fps              | 235      |\n",
      "|    time_elapsed     | 2011     |\n",
      "|    total_timesteps  | 473553   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0042   |\n",
      "|    n_updates        | 373552   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.43e+03 |\n",
      "|    ep_rew_mean      | -19.5    |\n",
      "|    exploration_rate | 0.763    |\n",
      "| time/               |          |\n",
      "|    episodes         | 472      |\n",
      "|    fps              | 235      |\n",
      "|    time_elapsed     | 2031     |\n",
      "|    total_timesteps  | 478022   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00214  |\n",
      "|    n_updates        | 378021   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-18.60 +/- 2.24\n",
      "Episode length: 5068.60 +/- 1160.80\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.07e+03 |\n",
      "|    mean_reward      | -18.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.762    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 480000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00288  |\n",
      "|    n_updates        | 379999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.44e+03 |\n",
      "|    ep_rew_mean      | -19.4    |\n",
      "|    exploration_rate | 0.761    |\n",
      "| time/               |          |\n",
      "|    episodes         | 476      |\n",
      "|    fps              | 234      |\n",
      "|    time_elapsed     | 2056     |\n",
      "|    total_timesteps  | 482646   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00285  |\n",
      "|    n_updates        | 382645   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.44e+03 |\n",
      "|    ep_rew_mean      | -19.4    |\n",
      "|    exploration_rate | 0.759    |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 234      |\n",
      "|    time_elapsed     | 2075     |\n",
      "|    total_timesteps  | 486892   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0122   |\n",
      "|    n_updates        | 386891   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=-19.20 +/- 1.60\n",
      "Episode length: 4618.40 +/- 799.16\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.62e+03 |\n",
      "|    mean_reward      | -19.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.757    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 490000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00317  |\n",
      "|    n_updates        | 389999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.48e+03 |\n",
      "|    ep_rew_mean      | -19.4    |\n",
      "|    exploration_rate | 0.757    |\n",
      "| time/               |          |\n",
      "|    episodes         | 484      |\n",
      "|    fps              | 233      |\n",
      "|    time_elapsed     | 2101     |\n",
      "|    total_timesteps  | 491695   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00146  |\n",
      "|    n_updates        | 391694   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.47e+03 |\n",
      "|    ep_rew_mean      | -19.4    |\n",
      "|    exploration_rate | 0.755    |\n",
      "| time/               |          |\n",
      "|    episodes         | 488      |\n",
      "|    fps              | 233      |\n",
      "|    time_elapsed     | 2119     |\n",
      "|    total_timesteps  | 495872   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00614  |\n",
      "|    n_updates        | 395871   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-19.60 +/- 1.02\n",
      "Episode length: 4569.00 +/- 552.48\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.57e+03 |\n",
      "|    mean_reward      | -19.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.753    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 500000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00396  |\n",
      "|    n_updates        | 399999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.52e+03 |\n",
      "|    ep_rew_mean      | -19.3    |\n",
      "|    exploration_rate | 0.752    |\n",
      "| time/               |          |\n",
      "|    episodes         | 492      |\n",
      "|    fps              | 233      |\n",
      "|    time_elapsed     | 2150     |\n",
      "|    total_timesteps  | 501870   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00313  |\n",
      "|    n_updates        | 401869   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.53e+03 |\n",
      "|    ep_rew_mean      | -19.2    |\n",
      "|    exploration_rate | 0.749    |\n",
      "| time/               |          |\n",
      "|    episodes         | 496      |\n",
      "|    fps              | 233      |\n",
      "|    time_elapsed     | 2171     |\n",
      "|    total_timesteps  | 506831   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00222  |\n",
      "|    n_updates        | 406830   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=-18.40 +/- 1.36\n",
      "Episode length: 5198.20 +/- 767.29\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.2e+03  |\n",
      "|    mean_reward      | -18.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.748    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 510000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00222  |\n",
      "|    n_updates        | 409999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.57e+03 |\n",
      "|    ep_rew_mean      | -19.2    |\n",
      "|    exploration_rate | 0.747    |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 232      |\n",
      "|    time_elapsed     | 2198     |\n",
      "|    total_timesteps  | 511788   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00356  |\n",
      "|    n_updates        | 411787   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.58e+03 |\n",
      "|    ep_rew_mean      | -19.2    |\n",
      "|    exploration_rate | 0.744    |\n",
      "| time/               |          |\n",
      "|    episodes         | 504      |\n",
      "|    fps              | 232      |\n",
      "|    time_elapsed     | 2219     |\n",
      "|    total_timesteps  | 516428   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00265  |\n",
      "|    n_updates        | 416427   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-19.00 +/- 1.41\n",
      "Episode length: 4764.00 +/- 634.81\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.76e+03 |\n",
      "|    mean_reward      | -19      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.743    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 520000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00313  |\n",
      "|    n_updates        | 419999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.58e+03 |\n",
      "|    ep_rew_mean      | -19.1    |\n",
      "|    exploration_rate | 0.742    |\n",
      "| time/               |          |\n",
      "|    episodes         | 508      |\n",
      "|    fps              | 232      |\n",
      "|    time_elapsed     | 2241     |\n",
      "|    total_timesteps  | 520451   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00178  |\n",
      "|    n_updates        | 420450   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.59e+03 |\n",
      "|    ep_rew_mean      | -19.1    |\n",
      "|    exploration_rate | 0.74     |\n",
      "| time/               |          |\n",
      "|    episodes         | 512      |\n",
      "|    fps              | 232      |\n",
      "|    time_elapsed     | 2263     |\n",
      "|    total_timesteps  | 525528   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00177  |\n",
      "|    n_updates        | 425527   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=-17.80 +/- 0.98\n",
      "Episode length: 5209.40 +/- 501.02\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.21e+03 |\n",
      "|    mean_reward      | -17.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.738    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 530000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00238  |\n",
      "|    n_updates        | 429999   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.59e+03 |\n",
      "|    ep_rew_mean      | -19.1    |\n",
      "|    exploration_rate | 0.738    |\n",
      "| time/               |          |\n",
      "|    episodes         | 516      |\n",
      "|    fps              | 231      |\n",
      "|    time_elapsed     | 2290     |\n",
      "|    total_timesteps  | 530261   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00165  |\n",
      "|    n_updates        | 430260   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.61e+03 |\n",
      "|    ep_rew_mean      | -19.1    |\n",
      "|    exploration_rate | 0.735    |\n",
      "| time/               |          |\n",
      "|    episodes         | 520      |\n",
      "|    fps              | 231      |\n",
      "|    time_elapsed     | 2310     |\n",
      "|    total_timesteps  | 534941   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00381  |\n",
      "|    n_updates        | 434940   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.62e+03 |\n",
      "|    ep_rew_mean      | -19.1    |\n",
      "|    exploration_rate | 0.733    |\n",
      "| time/               |          |\n",
      "|    episodes         | 524      |\n",
      "|    fps              | 231      |\n",
      "|    time_elapsed     | 2330     |\n",
      "|    total_timesteps  | 539472   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00302  |\n",
      "|    n_updates        | 439471   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-19.40 +/- 1.62\n",
      "Episode length: 4320.40 +/- 792.78\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.32e+03 |\n",
      "|    mean_reward      | -19.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.733    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 540000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00311  |\n",
      "|    n_updates        | 439999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.65e+03 |\n",
      "|    ep_rew_mean      | -19.1    |\n",
      "|    exploration_rate | 0.73     |\n",
      "| time/               |          |\n",
      "|    episodes         | 528      |\n",
      "|    fps              | 231      |\n",
      "|    time_elapsed     | 2357     |\n",
      "|    total_timesteps  | 544615   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00543  |\n",
      "|    n_updates        | 444614   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.64e+03 |\n",
      "|    ep_rew_mean      | -19.1    |\n",
      "|    exploration_rate | 0.728    |\n",
      "| time/               |          |\n",
      "|    episodes         | 532      |\n",
      "|    fps              | 231      |\n",
      "|    time_elapsed     | 2377     |\n",
      "|    total_timesteps  | 549146   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00296  |\n",
      "|    n_updates        | 449145   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=-19.20 +/- 1.72\n",
      "Episode length: 4828.60 +/- 902.56\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.83e+03 |\n",
      "|    mean_reward      | -19.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.728    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 550000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0052   |\n",
      "|    n_updates        | 449999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.66e+03 |\n",
      "|    ep_rew_mean      | -19.1    |\n",
      "|    exploration_rate | 0.726    |\n",
      "| time/               |          |\n",
      "|    episodes         | 536      |\n",
      "|    fps              | 230      |\n",
      "|    time_elapsed     | 2406     |\n",
      "|    total_timesteps  | 553850   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00341  |\n",
      "|    n_updates        | 453849   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.68e+03 |\n",
      "|    ep_rew_mean      | -19.1    |\n",
      "|    exploration_rate | 0.724    |\n",
      "| time/               |          |\n",
      "|    episodes         | 540      |\n",
      "|    fps              | 229      |\n",
      "|    time_elapsed     | 2429     |\n",
      "|    total_timesteps  | 558575   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00208  |\n",
      "|    n_updates        | 458574   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=-19.20 +/- 1.83\n",
      "Episode length: 4879.20 +/- 575.44\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.88e+03 |\n",
      "|    mean_reward      | -19.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.723    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 560000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00321  |\n",
      "|    n_updates        | 459999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.67e+03 |\n",
      "|    ep_rew_mean      | -19.1    |\n",
      "|    exploration_rate | 0.721    |\n",
      "| time/               |          |\n",
      "|    episodes         | 544      |\n",
      "|    fps              | 229      |\n",
      "|    time_elapsed     | 2457     |\n",
      "|    total_timesteps  | 562846   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00313  |\n",
      "|    n_updates        | 462845   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.67e+03 |\n",
      "|    ep_rew_mean      | -19.1    |\n",
      "|    exploration_rate | 0.719    |\n",
      "| time/               |          |\n",
      "|    episodes         | 548      |\n",
      "|    fps              | 228      |\n",
      "|    time_elapsed     | 2479     |\n",
      "|    total_timesteps  | 567590   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00695  |\n",
      "|    n_updates        | 467589   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=-19.00 +/- 1.67\n",
      "Episode length: 4561.20 +/- 1068.65\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.56e+03 |\n",
      "|    mean_reward      | -19      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.718    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 570000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00773  |\n",
      "|    n_updates        | 469999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.67e+03 |\n",
      "|    ep_rew_mean      | -19.1    |\n",
      "|    exploration_rate | 0.717    |\n",
      "| time/               |          |\n",
      "|    episodes         | 552      |\n",
      "|    fps              | 228      |\n",
      "|    time_elapsed     | 2507     |\n",
      "|    total_timesteps  | 572117   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0123   |\n",
      "|    n_updates        | 472116   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.68e+03 |\n",
      "|    ep_rew_mean      | -19      |\n",
      "|    exploration_rate | 0.714    |\n",
      "| time/               |          |\n",
      "|    episodes         | 556      |\n",
      "|    fps              | 227      |\n",
      "|    time_elapsed     | 2531     |\n",
      "|    total_timesteps  | 577009   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00406  |\n",
      "|    n_updates        | 477008   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-19.40 +/- 0.49\n",
      "Episode length: 4854.20 +/- 441.43\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.85e+03 |\n",
      "|    mean_reward      | -19.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.713    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 580000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0023   |\n",
      "|    n_updates        | 479999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.71e+03 |\n",
      "|    ep_rew_mean      | -19      |\n",
      "|    exploration_rate | 0.712    |\n",
      "| time/               |          |\n",
      "|    episodes         | 560      |\n",
      "|    fps              | 227      |\n",
      "|    time_elapsed     | 2561     |\n",
      "|    total_timesteps  | 581891   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00573  |\n",
      "|    n_updates        | 481890   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.73e+03 |\n",
      "|    ep_rew_mean      | -18.9    |\n",
      "|    exploration_rate | 0.709    |\n",
      "| time/               |          |\n",
      "|    episodes         | 564      |\n",
      "|    fps              | 227      |\n",
      "|    time_elapsed     | 2584     |\n",
      "|    total_timesteps  | 586886   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00212  |\n",
      "|    n_updates        | 486885   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=-18.40 +/- 2.42\n",
      "Episode length: 4954.60 +/- 862.28\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.95e+03 |\n",
      "|    mean_reward      | -18.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.708    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 590000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00298  |\n",
      "|    n_updates        | 489999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.79e+03 |\n",
      "|    ep_rew_mean      | -18.8    |\n",
      "|    exploration_rate | 0.707    |\n",
      "| time/               |          |\n",
      "|    episodes         | 568      |\n",
      "|    fps              | 226      |\n",
      "|    time_elapsed     | 2618     |\n",
      "|    total_timesteps  | 592724   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00413  |\n",
      "|    n_updates        | 492723   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.82e+03 |\n",
      "|    ep_rew_mean      | -18.8    |\n",
      "|    exploration_rate | 0.704    |\n",
      "| time/               |          |\n",
      "|    episodes         | 572      |\n",
      "|    fps              | 226      |\n",
      "|    time_elapsed     | 2644     |\n",
      "|    total_timesteps  | 598098   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00399  |\n",
      "|    n_updates        | 498097   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-18.00 +/- 0.63\n",
      "Episode length: 5106.20 +/- 678.22\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.11e+03 |\n",
      "|    mean_reward      | -18      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.703    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 600000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00262  |\n",
      "|    n_updates        | 499999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.84e+03 |\n",
      "|    ep_rew_mean      | -18.8    |\n",
      "|    exploration_rate | 0.701    |\n",
      "| time/               |          |\n",
      "|    episodes         | 576      |\n",
      "|    fps              | 225      |\n",
      "|    time_elapsed     | 2674     |\n",
      "|    total_timesteps  | 603064   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00485  |\n",
      "|    n_updates        | 503063   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.88e+03 |\n",
      "|    ep_rew_mean      | -18.8    |\n",
      "|    exploration_rate | 0.699    |\n",
      "| time/               |          |\n",
      "|    episodes         | 580      |\n",
      "|    fps              | 225      |\n",
      "|    time_elapsed     | 2698     |\n",
      "|    total_timesteps  | 608231   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00176  |\n",
      "|    n_updates        | 508230   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=-19.80 +/- 0.98\n",
      "Episode length: 4476.00 +/- 456.99\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.48e+03 |\n",
      "|    mean_reward      | -19.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.698    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 610000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00263  |\n",
      "|    n_updates        | 509999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.92e+03 |\n",
      "|    ep_rew_mean      | -18.7    |\n",
      "|    exploration_rate | 0.696    |\n",
      "| time/               |          |\n",
      "|    episodes         | 584      |\n",
      "|    fps              | 224      |\n",
      "|    time_elapsed     | 2732     |\n",
      "|    total_timesteps  | 614133   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00341  |\n",
      "|    n_updates        | 514132   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.94e+03 |\n",
      "|    ep_rew_mean      | -18.7    |\n",
      "|    exploration_rate | 0.694    |\n",
      "| time/               |          |\n",
      "|    episodes         | 588      |\n",
      "|    fps              | 224      |\n",
      "|    time_elapsed     | 2755     |\n",
      "|    total_timesteps  | 618760   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00298  |\n",
      "|    n_updates        | 518759   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=-19.80 +/- 0.75\n",
      "Episode length: 4131.20 +/- 441.77\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.13e+03 |\n",
      "|    mean_reward      | -19.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.693    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 620000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00358  |\n",
      "|    n_updates        | 519999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.9e+03  |\n",
      "|    ep_rew_mean      | -18.8    |\n",
      "|    exploration_rate | 0.691    |\n",
      "| time/               |          |\n",
      "|    episodes         | 592      |\n",
      "|    fps              | 223      |\n",
      "|    time_elapsed     | 2785     |\n",
      "|    total_timesteps  | 623951   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0124   |\n",
      "|    n_updates        | 523950   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.91e+03 |\n",
      "|    ep_rew_mean      | -18.7    |\n",
      "|    exploration_rate | 0.689    |\n",
      "| time/               |          |\n",
      "|    episodes         | 596      |\n",
      "|    fps              | 223      |\n",
      "|    time_elapsed     | 2809     |\n",
      "|    total_timesteps  | 628965   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00228  |\n",
      "|    n_updates        | 528964   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=-18.60 +/- 1.85\n",
      "Episode length: 5478.40 +/- 992.47\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.48e+03 |\n",
      "|    mean_reward      | -18.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.688    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 630000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00376  |\n",
      "|    n_updates        | 529999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.89e+03 |\n",
      "|    ep_rew_mean      | -18.8    |\n",
      "|    exploration_rate | 0.686    |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 223      |\n",
      "|    time_elapsed     | 2838     |\n",
      "|    total_timesteps  | 633510   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00173  |\n",
      "|    n_updates        | 533509   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.89e+03 |\n",
      "|    ep_rew_mean      | -18.8    |\n",
      "|    exploration_rate | 0.684    |\n",
      "| time/               |          |\n",
      "|    episodes         | 604      |\n",
      "|    fps              | 223      |\n",
      "|    time_elapsed     | 2860     |\n",
      "|    total_timesteps  | 638018   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00778  |\n",
      "|    n_updates        | 538017   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=-19.80 +/- 0.75\n",
      "Episode length: 4261.20 +/- 368.69\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.26e+03 |\n",
      "|    mean_reward      | -19.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.683    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 640000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00807  |\n",
      "|    n_updates        | 539999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.92e+03 |\n",
      "|    ep_rew_mean      | -18.8    |\n",
      "|    exploration_rate | 0.682    |\n",
      "| time/               |          |\n",
      "|    episodes         | 608      |\n",
      "|    fps              | 222      |\n",
      "|    time_elapsed     | 2890     |\n",
      "|    total_timesteps  | 643033   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00589  |\n",
      "|    n_updates        | 543032   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.93e+03 |\n",
      "|    ep_rew_mean      | -18.8    |\n",
      "|    exploration_rate | 0.679    |\n",
      "| time/               |          |\n",
      "|    episodes         | 612      |\n",
      "|    fps              | 222      |\n",
      "|    time_elapsed     | 2915     |\n",
      "|    total_timesteps  | 648184   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0172   |\n",
      "|    n_updates        | 548183   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=-18.60 +/- 0.80\n",
      "Episode length: 5255.80 +/- 809.56\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.26e+03 |\n",
      "|    mean_reward      | -18.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.678    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 650000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00773  |\n",
      "|    n_updates        | 549999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.96e+03 |\n",
      "|    ep_rew_mean      | -18.7    |\n",
      "|    exploration_rate | 0.676    |\n",
      "| time/               |          |\n",
      "|    episodes         | 616      |\n",
      "|    fps              | 221      |\n",
      "|    time_elapsed     | 2948     |\n",
      "|    total_timesteps  | 653733   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00419  |\n",
      "|    n_updates        | 553732   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.99e+03 |\n",
      "|    ep_rew_mean      | -18.7    |\n",
      "|    exploration_rate | 0.674    |\n",
      "| time/               |          |\n",
      "|    episodes         | 620      |\n",
      "|    fps              | 221      |\n",
      "|    time_elapsed     | 2975     |\n",
      "|    total_timesteps  | 659255   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00861  |\n",
      "|    n_updates        | 559254   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-17.40 +/- 1.02\n",
      "Episode length: 5529.40 +/- 583.47\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.53e+03 |\n",
      "|    mean_reward      | -17.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.673    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 660000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00816  |\n",
      "|    n_updates        | 559999   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.02e+03 |\n",
      "|    ep_rew_mean      | -18.6    |\n",
      "|    exploration_rate | 0.671    |\n",
      "| time/               |          |\n",
      "|    episodes         | 624      |\n",
      "|    fps              | 220      |\n",
      "|    time_elapsed     | 3006     |\n",
      "|    total_timesteps  | 664304   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00644  |\n",
      "|    n_updates        | 564303   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.97e+03 |\n",
      "|    ep_rew_mean      | -18.7    |\n",
      "|    exploration_rate | 0.669    |\n",
      "| time/               |          |\n",
      "|    episodes         | 628      |\n",
      "|    fps              | 220      |\n",
      "|    time_elapsed     | 3026     |\n",
      "|    total_timesteps  | 668445   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00578  |\n",
      "|    n_updates        | 568444   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=-19.20 +/- 1.17\n",
      "Episode length: 4894.40 +/- 1003.78\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.89e+03 |\n",
      "|    mean_reward      | -19.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.668    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 670000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0077   |\n",
      "|    n_updates        | 569999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.97e+03 |\n",
      "|    ep_rew_mean      | -18.7    |\n",
      "|    exploration_rate | 0.667    |\n",
      "| time/               |          |\n",
      "|    episodes         | 632      |\n",
      "|    fps              | 220      |\n",
      "|    time_elapsed     | 3054     |\n",
      "|    total_timesteps  | 672780   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00555  |\n",
      "|    n_updates        | 572779   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.99e+03 |\n",
      "|    ep_rew_mean      | -18.7    |\n",
      "|    exploration_rate | 0.664    |\n",
      "| time/               |          |\n",
      "|    episodes         | 636      |\n",
      "|    fps              | 220      |\n",
      "|    time_elapsed     | 3079     |\n",
      "|    total_timesteps  | 677941   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00173  |\n",
      "|    n_updates        | 577940   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=-19.00 +/- 1.67\n",
      "Episode length: 4705.60 +/- 973.92\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.71e+03 |\n",
      "|    mean_reward      | -19      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.663    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 680000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00568  |\n",
      "|    n_updates        | 579999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5e+03    |\n",
      "|    ep_rew_mean      | -18.6    |\n",
      "|    exploration_rate | 0.662    |\n",
      "| time/               |          |\n",
      "|    episodes         | 640      |\n",
      "|    fps              | 219      |\n",
      "|    time_elapsed     | 3110     |\n",
      "|    total_timesteps  | 683153   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.004    |\n",
      "|    n_updates        | 583152   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.06e+03 |\n",
      "|    ep_rew_mean      | -18.5    |\n",
      "|    exploration_rate | 0.659    |\n",
      "| time/               |          |\n",
      "|    episodes         | 644      |\n",
      "|    fps              | 219      |\n",
      "|    time_elapsed     | 3137     |\n",
      "|    total_timesteps  | 688717   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00212  |\n",
      "|    n_updates        | 588716   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=-17.80 +/- 1.94\n",
      "Episode length: 5335.60 +/- 713.64\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.34e+03 |\n",
      "|    mean_reward      | -17.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.658    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 690000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00447  |\n",
      "|    n_updates        | 589999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.05e+03 |\n",
      "|    ep_rew_mean      | -18.5    |\n",
      "|    exploration_rate | 0.657    |\n",
      "| time/               |          |\n",
      "|    episodes         | 648      |\n",
      "|    fps              | 218      |\n",
      "|    time_elapsed     | 3167     |\n",
      "|    total_timesteps  | 693432   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00451  |\n",
      "|    n_updates        | 593431   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.11e+03 |\n",
      "|    ep_rew_mean      | -18.5    |\n",
      "|    exploration_rate | 0.654    |\n",
      "| time/               |          |\n",
      "|    episodes         | 652      |\n",
      "|    fps              | 218      |\n",
      "|    time_elapsed     | 3196     |\n",
      "|    total_timesteps  | 699361   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0308   |\n",
      "|    n_updates        | 599360   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=-18.00 +/- 2.45\n",
      "Episode length: 5413.80 +/- 1103.78\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.41e+03 |\n",
      "|    mean_reward      | -18      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.654    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 700000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00555  |\n",
      "|    n_updates        | 599999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.15e+03 |\n",
      "|    ep_rew_mean      | -18.4    |\n",
      "|    exploration_rate | 0.651    |\n",
      "| time/               |          |\n",
      "|    episodes         | 656      |\n",
      "|    fps              | 218      |\n",
      "|    time_elapsed     | 3232     |\n",
      "|    total_timesteps  | 705201   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00628  |\n",
      "|    n_updates        | 605200   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=-19.80 +/- 0.75\n",
      "Episode length: 4979.40 +/- 344.86\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.98e+03 |\n",
      "|    mean_reward      | -19.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.649    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 710000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00337  |\n",
      "|    n_updates        | 609999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.18e+03 |\n",
      "|    ep_rew_mean      | -18.4    |\n",
      "|    exploration_rate | 0.648    |\n",
      "| time/               |          |\n",
      "|    episodes         | 660      |\n",
      "|    fps              | 217      |\n",
      "|    time_elapsed     | 3265     |\n",
      "|    total_timesteps  | 710782   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00163  |\n",
      "|    n_updates        | 610781   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.2e+03  |\n",
      "|    ep_rew_mean      | -18.4    |\n",
      "|    exploration_rate | 0.645    |\n",
      "| time/               |          |\n",
      "|    episodes         | 664      |\n",
      "|    fps              | 217      |\n",
      "|    time_elapsed     | 3293     |\n",
      "|    total_timesteps  | 716333   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00546  |\n",
      "|    n_updates        | 616332   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=-18.00 +/- 2.10\n",
      "Episode length: 5765.20 +/- 675.40\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.77e+03 |\n",
      "|    mean_reward      | -18      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.644    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 720000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00597  |\n",
      "|    n_updates        | 619999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.2e+03  |\n",
      "|    ep_rew_mean      | -18.4    |\n",
      "|    exploration_rate | 0.643    |\n",
      "| time/               |          |\n",
      "|    episodes         | 668      |\n",
      "|    fps              | 216      |\n",
      "|    time_elapsed     | 3328     |\n",
      "|    total_timesteps  | 722108   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0101   |\n",
      "|    n_updates        | 622107   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.23e+03 |\n",
      "|    ep_rew_mean      | -18.3    |\n",
      "|    exploration_rate | 0.64     |\n",
      "| time/               |          |\n",
      "|    episodes         | 672      |\n",
      "|    fps              | 216      |\n",
      "|    time_elapsed     | 3358     |\n",
      "|    total_timesteps  | 728234   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0203   |\n",
      "|    n_updates        | 628233   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=-19.00 +/- 1.26\n",
      "Episode length: 4885.20 +/- 691.13\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.89e+03 |\n",
      "|    mean_reward      | -19      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.639    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 730000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00385  |\n",
      "|    n_updates        | 629999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.26e+03 |\n",
      "|    ep_rew_mean      | -18.2    |\n",
      "|    exploration_rate | 0.637    |\n",
      "| time/               |          |\n",
      "|    episodes         | 676      |\n",
      "|    fps              | 216      |\n",
      "|    time_elapsed     | 3394     |\n",
      "|    total_timesteps  | 734019   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0114   |\n",
      "|    n_updates        | 634018   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.27e+03 |\n",
      "|    ep_rew_mean      | -18.2    |\n",
      "|    exploration_rate | 0.634    |\n",
      "| time/               |          |\n",
      "|    episodes         | 680      |\n",
      "|    fps              | 216      |\n",
      "|    time_elapsed     | 3421     |\n",
      "|    total_timesteps  | 739349   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00427  |\n",
      "|    n_updates        | 639348   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=-19.20 +/- 1.33\n",
      "Episode length: 5221.20 +/- 681.70\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.22e+03 |\n",
      "|    mean_reward      | -19.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.634    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 740000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00673  |\n",
      "|    n_updates        | 639999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.27e+03 |\n",
      "|    ep_rew_mean      | -18.2    |\n",
      "|    exploration_rate | 0.631    |\n",
      "| time/               |          |\n",
      "|    episodes         | 684      |\n",
      "|    fps              | 215      |\n",
      "|    time_elapsed     | 3457     |\n",
      "|    total_timesteps  | 745367   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00409  |\n",
      "|    n_updates        | 645366   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=-17.80 +/- 1.47\n",
      "Episode length: 5764.20 +/- 778.34\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.76e+03 |\n",
      "|    mean_reward      | -17.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.629    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 750000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00365  |\n",
      "|    n_updates        | 649999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.35e+03 |\n",
      "|    ep_rew_mean      | -18.1    |\n",
      "|    exploration_rate | 0.628    |\n",
      "| time/               |          |\n",
      "|    episodes         | 688      |\n",
      "|    fps              | 215      |\n",
      "|    time_elapsed     | 3495     |\n",
      "|    total_timesteps  | 751950   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00529  |\n",
      "|    n_updates        | 651949   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.34e+03 |\n",
      "|    ep_rew_mean      | -18.1    |\n",
      "|    exploration_rate | 0.625    |\n",
      "| time/               |          |\n",
      "|    episodes         | 692      |\n",
      "|    fps              | 215      |\n",
      "|    time_elapsed     | 3519     |\n",
      "|    total_timesteps  | 756945   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00528  |\n",
      "|    n_updates        | 656944   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=-17.00 +/- 1.10\n",
      "Episode length: 6502.60 +/- 1148.73\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.5e+03  |\n",
      "|    mean_reward      | -17      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.624    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 760000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0114   |\n",
      "|    n_updates        | 659999   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.4e+03  |\n",
      "|    ep_rew_mean      | -18.1    |\n",
      "|    exploration_rate | 0.622    |\n",
      "| time/               |          |\n",
      "|    episodes         | 696      |\n",
      "|    fps              | 214      |\n",
      "|    time_elapsed     | 3559     |\n",
      "|    total_timesteps  | 763458   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00313  |\n",
      "|    n_updates        | 663457   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.46e+03 |\n",
      "|    ep_rew_mean      | -17.9    |\n",
      "|    exploration_rate | 0.619    |\n",
      "| time/               |          |\n",
      "|    episodes         | 700      |\n",
      "|    fps              | 214      |\n",
      "|    time_elapsed     | 3587     |\n",
      "|    total_timesteps  | 769424   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00269  |\n",
      "|    n_updates        | 669423   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=-18.40 +/- 2.06\n",
      "Episode length: 5144.80 +/- 675.30\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.14e+03 |\n",
      "|    mean_reward      | -18.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.619    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 770000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0141   |\n",
      "|    n_updates        | 669999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.51e+03 |\n",
      "|    ep_rew_mean      | -17.9    |\n",
      "|    exploration_rate | 0.616    |\n",
      "| time/               |          |\n",
      "|    episodes         | 704      |\n",
      "|    fps              | 214      |\n",
      "|    time_elapsed     | 3622     |\n",
      "|    total_timesteps  | 775230   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00269  |\n",
      "|    n_updates        | 675229   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=-17.20 +/- 2.04\n",
      "Episode length: 6257.00 +/- 272.04\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.26e+03 |\n",
      "|    mean_reward      | -17.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.614    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 780000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0021   |\n",
      "|    n_updates        | 679999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.55e+03 |\n",
      "|    ep_rew_mean      | -17.8    |\n",
      "|    exploration_rate | 0.613    |\n",
      "| time/               |          |\n",
      "|    episodes         | 708      |\n",
      "|    fps              | 213      |\n",
      "|    time_elapsed     | 3659     |\n",
      "|    total_timesteps  | 781160   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00508  |\n",
      "|    n_updates        | 681159   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.54e+03 |\n",
      "|    ep_rew_mean      | -17.8    |\n",
      "|    exploration_rate | 0.611    |\n",
      "| time/               |          |\n",
      "|    episodes         | 712      |\n",
      "|    fps              | 213      |\n",
      "|    time_elapsed     | 3683     |\n",
      "|    total_timesteps  | 786230   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00455  |\n",
      "|    n_updates        | 686229   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=-17.20 +/- 2.04\n",
      "Episode length: 6063.80 +/- 1203.46\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.06e+03 |\n",
      "|    mean_reward      | -17.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.609    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 790000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00755  |\n",
      "|    n_updates        | 689999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.57e+03 |\n",
      "|    ep_rew_mean      | -17.8    |\n",
      "|    exploration_rate | 0.608    |\n",
      "| time/               |          |\n",
      "|    episodes         | 716      |\n",
      "|    fps              | 212      |\n",
      "|    time_elapsed     | 3721     |\n",
      "|    total_timesteps  | 792406   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00835  |\n",
      "|    n_updates        | 692405   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.59e+03 |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.605    |\n",
      "| time/               |          |\n",
      "|    episodes         | 720      |\n",
      "|    fps              | 212      |\n",
      "|    time_elapsed     | 3751     |\n",
      "|    total_timesteps  | 798467   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00576  |\n",
      "|    n_updates        | 698466   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=-16.80 +/- 1.94\n",
      "Episode length: 6230.80 +/- 1446.15\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.23e+03 |\n",
      "|    mean_reward      | -16.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.604    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 800000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00368  |\n",
      "|    n_updates        | 699999   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.6e+03  |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.602    |\n",
      "| time/               |          |\n",
      "|    episodes         | 724      |\n",
      "|    fps              | 212      |\n",
      "|    time_elapsed     | 3785     |\n",
      "|    total_timesteps  | 803681   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00458  |\n",
      "|    n_updates        | 703680   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.66e+03 |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.599    |\n",
      "| time/               |          |\n",
      "|    episodes         | 728      |\n",
      "|    fps              | 212      |\n",
      "|    time_elapsed     | 3813     |\n",
      "|    total_timesteps  | 809438   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00585  |\n",
      "|    n_updates        | 709437   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=-18.40 +/- 2.50\n",
      "Episode length: 5837.40 +/- 783.92\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.84e+03 |\n",
      "|    mean_reward      | -18.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.599    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 810000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00434  |\n",
      "|    n_updates        | 709999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.71e+03 |\n",
      "|    ep_rew_mean      | -17.5    |\n",
      "|    exploration_rate | 0.597    |\n",
      "| time/               |          |\n",
      "|    episodes         | 732      |\n",
      "|    fps              | 211      |\n",
      "|    time_elapsed     | 3849     |\n",
      "|    total_timesteps  | 814899   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00764  |\n",
      "|    n_updates        | 714898   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=-17.60 +/- 0.80\n",
      "Episode length: 5859.00 +/- 597.14\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.86e+03 |\n",
      "|    mean_reward      | -17.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.594    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 820000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00447  |\n",
      "|    n_updates        | 719999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.74e+03 |\n",
      "|    ep_rew_mean      | -17.5    |\n",
      "|    exploration_rate | 0.594    |\n",
      "| time/               |          |\n",
      "|    episodes         | 736      |\n",
      "|    fps              | 211      |\n",
      "|    time_elapsed     | 3887     |\n",
      "|    total_timesteps  | 820812   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00289  |\n",
      "|    n_updates        | 720811   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.76e+03 |\n",
      "|    ep_rew_mean      | -17.5    |\n",
      "|    exploration_rate | 0.591    |\n",
      "| time/               |          |\n",
      "|    episodes         | 740      |\n",
      "|    fps              | 211      |\n",
      "|    time_elapsed     | 3917     |\n",
      "|    total_timesteps  | 826515   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00619  |\n",
      "|    n_updates        | 726514   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=-17.20 +/- 1.72\n",
      "Episode length: 6168.20 +/- 854.83\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.17e+03 |\n",
      "|    mean_reward      | -17.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.589    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 830000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00597  |\n",
      "|    n_updates        | 729999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.73e+03 |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.588    |\n",
      "| time/               |          |\n",
      "|    episodes         | 744      |\n",
      "|    fps              | 210      |\n",
      "|    time_elapsed     | 3950     |\n",
      "|    total_timesteps  | 831395   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00675  |\n",
      "|    n_updates        | 731394   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.77e+03 |\n",
      "|    ep_rew_mean      | -17.5    |\n",
      "|    exploration_rate | 0.586    |\n",
      "| time/               |          |\n",
      "|    episodes         | 748      |\n",
      "|    fps              | 210      |\n",
      "|    time_elapsed     | 3979     |\n",
      "|    total_timesteps  | 837162   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00695  |\n",
      "|    n_updates        | 737161   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=-17.20 +/- 1.72\n",
      "Episode length: 5963.40 +/- 356.86\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.96e+03 |\n",
      "|    mean_reward      | -17.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.584    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 840000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00404  |\n",
      "|    n_updates        | 739999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.77e+03 |\n",
      "|    ep_rew_mean      | -17.5    |\n",
      "|    exploration_rate | 0.583    |\n",
      "| time/               |          |\n",
      "|    episodes         | 752      |\n",
      "|    fps              | 209      |\n",
      "|    time_elapsed     | 4017     |\n",
      "|    total_timesteps  | 843094   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0158   |\n",
      "|    n_updates        | 743093   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.76e+03 |\n",
      "|    ep_rew_mean      | -17.5    |\n",
      "|    exploration_rate | 0.58     |\n",
      "| time/               |          |\n",
      "|    episodes         | 756      |\n",
      "|    fps              | 209      |\n",
      "|    time_elapsed     | 4044     |\n",
      "|    total_timesteps  | 848678   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00562  |\n",
      "|    n_updates        | 748677   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=-18.20 +/- 1.17\n",
      "Episode length: 4932.60 +/- 661.46\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.93e+03 |\n",
      "|    mean_reward      | -18.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.579    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 850000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00352  |\n",
      "|    n_updates        | 749999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.75e+03 |\n",
      "|    ep_rew_mean      | -17.7    |\n",
      "|    exploration_rate | 0.577    |\n",
      "| time/               |          |\n",
      "|    episodes         | 760      |\n",
      "|    fps              | 209      |\n",
      "|    time_elapsed     | 4075     |\n",
      "|    total_timesteps  | 853850   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00687  |\n",
      "|    n_updates        | 753849   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=-17.60 +/- 1.02\n",
      "Episode length: 6029.60 +/- 263.39\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.03e+03 |\n",
      "|    mean_reward      | -17.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.574    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 860000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00233  |\n",
      "|    n_updates        | 759999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.8e+03  |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.574    |\n",
      "| time/               |          |\n",
      "|    episodes         | 764      |\n",
      "|    fps              | 209      |\n",
      "|    time_elapsed     | 4115     |\n",
      "|    total_timesteps  | 860647   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00911  |\n",
      "|    n_updates        | 760646   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.8e+03  |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.571    |\n",
      "| time/               |          |\n",
      "|    episodes         | 768      |\n",
      "|    fps              | 209      |\n",
      "|    time_elapsed     | 4144     |\n",
      "|    total_timesteps  | 866432   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00354  |\n",
      "|    n_updates        | 766431   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=-16.80 +/- 2.56\n",
      "Episode length: 6204.60 +/- 797.17\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.2e+03  |\n",
      "|    mean_reward      | -16.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.569    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 870000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0385   |\n",
      "|    n_updates        | 769999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.8e+03  |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.568    |\n",
      "| time/               |          |\n",
      "|    episodes         | 772      |\n",
      "|    fps              | 208      |\n",
      "|    time_elapsed     | 4184     |\n",
      "|    total_timesteps  | 872639   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00676  |\n",
      "|    n_updates        | 772638   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.76e+03 |\n",
      "|    ep_rew_mean      | -17.7    |\n",
      "|    exploration_rate | 0.566    |\n",
      "| time/               |          |\n",
      "|    episodes         | 776      |\n",
      "|    fps              | 208      |\n",
      "|    time_elapsed     | 4208     |\n",
      "|    total_timesteps  | 877441   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00311  |\n",
      "|    n_updates        | 777440   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=-16.20 +/- 2.48\n",
      "Episode length: 5980.60 +/- 1119.92\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.98e+03 |\n",
      "|    mean_reward      | -16.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.564    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 880000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00258  |\n",
      "|    n_updates        | 779999   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.8e+03  |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.563    |\n",
      "| time/               |          |\n",
      "|    episodes         | 780      |\n",
      "|    fps              | 207      |\n",
      "|    time_elapsed     | 4248     |\n",
      "|    total_timesteps  | 883646   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00684  |\n",
      "|    n_updates        | 783645   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.79e+03 |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.56     |\n",
      "| time/               |          |\n",
      "|    episodes         | 784      |\n",
      "|    fps              | 207      |\n",
      "|    time_elapsed     | 4277     |\n",
      "|    total_timesteps  | 889511   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0264   |\n",
      "|    n_updates        | 789510   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=-16.00 +/- 2.45\n",
      "Episode length: 7171.40 +/- 1008.44\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.17e+03 |\n",
      "|    mean_reward      | -16      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.559    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 890000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00515  |\n",
      "|    n_updates        | 789999   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.78e+03 |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.557    |\n",
      "| time/               |          |\n",
      "|    episodes         | 788      |\n",
      "|    fps              | 207      |\n",
      "|    time_elapsed     | 4321     |\n",
      "|    total_timesteps  | 895819   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.009    |\n",
      "|    n_updates        | 795818   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=-18.00 +/- 0.63\n",
      "Episode length: 5750.80 +/- 686.99\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.75e+03 |\n",
      "|    mean_reward      | -18      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.555    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 900000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0073   |\n",
      "|    n_updates        | 799999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.78e+03 |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.554    |\n",
      "| time/               |          |\n",
      "|    episodes         | 792      |\n",
      "|    fps              | 206      |\n",
      "|    time_elapsed     | 4352     |\n",
      "|    total_timesteps  | 900754   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00672  |\n",
      "|    n_updates        | 800753   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.75e+03 |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.551    |\n",
      "| time/               |          |\n",
      "|    episodes         | 796      |\n",
      "|    fps              | 206      |\n",
      "|    time_elapsed     | 4380     |\n",
      "|    total_timesteps  | 906577   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00995  |\n",
      "|    n_updates        | 806576   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=-16.80 +/- 1.17\n",
      "Episode length: 6175.40 +/- 700.97\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.18e+03 |\n",
      "|    mean_reward      | -16.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.55     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 910000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00919  |\n",
      "|    n_updates        | 809999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.77e+03 |\n",
      "|    ep_rew_mean      | -17.7    |\n",
      "|    exploration_rate | 0.548    |\n",
      "| time/               |          |\n",
      "|    episodes         | 800      |\n",
      "|    fps              | 206      |\n",
      "|    time_elapsed     | 4420     |\n",
      "|    total_timesteps  | 913058   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0128   |\n",
      "|    n_updates        | 813057   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.74e+03 |\n",
      "|    ep_rew_mean      | -17.7    |\n",
      "|    exploration_rate | 0.546    |\n",
      "| time/               |          |\n",
      "|    episodes         | 804      |\n",
      "|    fps              | 206      |\n",
      "|    time_elapsed     | 4444     |\n",
      "|    total_timesteps  | 918143   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00601  |\n",
      "|    n_updates        | 818142   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=-16.00 +/- 2.19\n",
      "Episode length: 6154.20 +/- 960.69\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.15e+03 |\n",
      "|    mean_reward      | -16      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.545    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 920000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00474  |\n",
      "|    n_updates        | 819999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.75e+03 |\n",
      "|    ep_rew_mean      | -17.7    |\n",
      "|    exploration_rate | 0.542    |\n",
      "| time/               |          |\n",
      "|    episodes         | 808      |\n",
      "|    fps              | 206      |\n",
      "|    time_elapsed     | 4483     |\n",
      "|    total_timesteps  | 924350   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0179   |\n",
      "|    n_updates        | 824349   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.76e+03 |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.54     |\n",
      "| time/               |          |\n",
      "|    episodes         | 812      |\n",
      "|    fps              | 206      |\n",
      "|    time_elapsed     | 4509     |\n",
      "|    total_timesteps  | 929745   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00413  |\n",
      "|    n_updates        | 829744   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=-16.00 +/- 2.28\n",
      "Episode length: 6180.20 +/- 826.99\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.18e+03 |\n",
      "|    mean_reward      | -16      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.54     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 930000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00429  |\n",
      "|    n_updates        | 829999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.75e+03 |\n",
      "|    ep_rew_mean      | -17.7    |\n",
      "|    exploration_rate | 0.537    |\n",
      "| time/               |          |\n",
      "|    episodes         | 816      |\n",
      "|    fps              | 205      |\n",
      "|    time_elapsed     | 4548     |\n",
      "|    total_timesteps  | 935580   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0026   |\n",
      "|    n_updates        | 835579   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=-18.40 +/- 1.50\n",
      "Episode length: 5169.00 +/- 853.07\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.17e+03 |\n",
      "|    mean_reward      | -18.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.535    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 940000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00641  |\n",
      "|    n_updates        | 839999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.72e+03 |\n",
      "|    ep_rew_mean      | -17.8    |\n",
      "|    exploration_rate | 0.534    |\n",
      "| time/               |          |\n",
      "|    episodes         | 820      |\n",
      "|    fps              | 205      |\n",
      "|    time_elapsed     | 4583     |\n",
      "|    total_timesteps  | 940807   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0065   |\n",
      "|    n_updates        | 840806   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.72e+03 |\n",
      "|    ep_rew_mean      | -17.8    |\n",
      "|    exploration_rate | 0.532    |\n",
      "| time/               |          |\n",
      "|    episodes         | 824      |\n",
      "|    fps              | 205      |\n",
      "|    time_elapsed     | 4611     |\n",
      "|    total_timesteps  | 946210   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00496  |\n",
      "|    n_updates        | 846209   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=-17.00 +/- 1.79\n",
      "Episode length: 6015.80 +/- 589.77\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.02e+03 |\n",
      "|    mean_reward      | -17      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.53     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 950000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00646  |\n",
      "|    n_updates        | 849999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.73e+03 |\n",
      "|    ep_rew_mean      | -17.8    |\n",
      "|    exploration_rate | 0.529    |\n",
      "| time/               |          |\n",
      "|    episodes         | 828      |\n",
      "|    fps              | 204      |\n",
      "|    time_elapsed     | 4649     |\n",
      "|    total_timesteps  | 952045   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0178   |\n",
      "|    n_updates        | 852044   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.7e+03  |\n",
      "|    ep_rew_mean      | -17.8    |\n",
      "|    exploration_rate | 0.526    |\n",
      "| time/               |          |\n",
      "|    episodes         | 832      |\n",
      "|    fps              | 204      |\n",
      "|    time_elapsed     | 4673     |\n",
      "|    total_timesteps  | 956902   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00591  |\n",
      "|    n_updates        | 856901   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=-18.80 +/- 1.33\n",
      "Episode length: 4841.80 +/- 521.59\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.84e+03 |\n",
      "|    mean_reward      | -18.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.525    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 960000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00994  |\n",
      "|    n_updates        | 859999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.74e+03 |\n",
      "|    ep_rew_mean      | -17.8    |\n",
      "|    exploration_rate | 0.523    |\n",
      "| time/               |          |\n",
      "|    episodes         | 836      |\n",
      "|    fps              | 204      |\n",
      "|    time_elapsed     | 4713     |\n",
      "|    total_timesteps  | 963810   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00371  |\n",
      "|    n_updates        | 863809   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.76e+03 |\n",
      "|    ep_rew_mean      | -17.8    |\n",
      "|    exploration_rate | 0.52     |\n",
      "| time/               |          |\n",
      "|    episodes         | 840      |\n",
      "|    fps              | 204      |\n",
      "|    time_elapsed     | 4743     |\n",
      "|    total_timesteps  | 969851   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0029   |\n",
      "|    n_updates        | 869850   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=-17.80 +/- 2.04\n",
      "Episode length: 5414.40 +/- 741.42\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.41e+03 |\n",
      "|    mean_reward      | -17.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.52     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 970000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00354  |\n",
      "|    n_updates        | 869999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.77e+03 |\n",
      "|    ep_rew_mean      | -17.8    |\n",
      "|    exploration_rate | 0.517    |\n",
      "| time/               |          |\n",
      "|    episodes         | 844      |\n",
      "|    fps              | 204      |\n",
      "|    time_elapsed     | 4778     |\n",
      "|    total_timesteps  | 975006   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0203   |\n",
      "|    n_updates        | 875005   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=-19.00 +/- 0.89\n",
      "Episode length: 5141.20 +/- 323.41\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.14e+03 |\n",
      "|    mean_reward      | -19      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.515    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 980000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00627  |\n",
      "|    n_updates        | 879999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.74e+03 |\n",
      "|    ep_rew_mean      | -17.9    |\n",
      "|    exploration_rate | 0.515    |\n",
      "| time/               |          |\n",
      "|    episodes         | 848      |\n",
      "|    fps              | 203      |\n",
      "|    time_elapsed     | 4810     |\n",
      "|    total_timesteps  | 980205   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00567  |\n",
      "|    n_updates        | 880204   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.74e+03 |\n",
      "|    ep_rew_mean      | -17.8    |\n",
      "|    exploration_rate | 0.512    |\n",
      "| time/               |          |\n",
      "|    episodes         | 852      |\n",
      "|    fps              | 203      |\n",
      "|    time_elapsed     | 4839     |\n",
      "|    total_timesteps  | 986145   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00631  |\n",
      "|    n_updates        | 886144   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=-17.80 +/- 1.72\n",
      "Episode length: 5021.00 +/- 674.62\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.02e+03 |\n",
      "|    mean_reward      | -17.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.51     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 990000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00671  |\n",
      "|    n_updates        | 889999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.72e+03 |\n",
      "|    ep_rew_mean      | -17.9    |\n",
      "|    exploration_rate | 0.509    |\n",
      "| time/               |          |\n",
      "|    episodes         | 856      |\n",
      "|    fps              | 203      |\n",
      "|    time_elapsed     | 4870     |\n",
      "|    total_timesteps  | 991068   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0132   |\n",
      "|    n_updates        | 891067   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.8e+03  |\n",
      "|    ep_rew_mean      | -17.7    |\n",
      "|    exploration_rate | 0.506    |\n",
      "| time/               |          |\n",
      "|    episodes         | 860      |\n",
      "|    fps              | 203      |\n",
      "|    time_elapsed     | 4905     |\n",
      "|    total_timesteps  | 998287   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00291  |\n",
      "|    n_updates        | 898286   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=-17.00 +/- 1.67\n",
      "Episode length: 6503.60 +/- 538.51\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.5e+03  |\n",
      "|    mean_reward      | -17      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.505    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1000000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00843  |\n",
      "|    n_updates        | 899999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.77e+03 |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.503    |\n",
      "| time/               |          |\n",
      "|    episodes         | 864      |\n",
      "|    fps              | 203      |\n",
      "|    time_elapsed     | 4944     |\n",
      "|    total_timesteps  | 1004483  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0113   |\n",
      "|    n_updates        | 904482   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1010000, episode_reward=-16.40 +/- 2.24\n",
      "Episode length: 6584.00 +/- 1377.84\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.58e+03 |\n",
      "|    mean_reward      | -16.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.5      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1010000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00593  |\n",
      "|    n_updates        | 909999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.78e+03 |\n",
      "|    ep_rew_mean      | -17.7    |\n",
      "|    exploration_rate | 0.5      |\n",
      "| time/               |          |\n",
      "|    episodes         | 868      |\n",
      "|    fps              | 202      |\n",
      "|    time_elapsed     | 4983     |\n",
      "|    total_timesteps  | 1010419  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00719  |\n",
      "|    n_updates        | 910418   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.76e+03 |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.497    |\n",
      "| time/               |          |\n",
      "|    episodes         | 872      |\n",
      "|    fps              | 202      |\n",
      "|    time_elapsed     | 5011     |\n",
      "|    total_timesteps  | 1016207  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0139   |\n",
      "|    n_updates        | 916206   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=-17.80 +/- 1.72\n",
      "Episode length: 5462.40 +/- 471.68\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.46e+03 |\n",
      "|    mean_reward      | -17.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.495    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1020000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0109   |\n",
      "|    n_updates        | 919999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.8e+03  |\n",
      "|    ep_rew_mean      | -17.5    |\n",
      "|    exploration_rate | 0.494    |\n",
      "| time/               |          |\n",
      "|    episodes         | 876      |\n",
      "|    fps              | 202      |\n",
      "|    time_elapsed     | 5046     |\n",
      "|    total_timesteps  | 1021878  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00499  |\n",
      "|    n_updates        | 921877   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.77e+03 |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.491    |\n",
      "| time/               |          |\n",
      "|    episodes         | 880      |\n",
      "|    fps              | 202      |\n",
      "|    time_elapsed     | 5073     |\n",
      "|    total_timesteps  | 1027375  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00405  |\n",
      "|    n_updates        | 927374   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1030000, episode_reward=-18.00 +/- 2.10\n",
      "Episode length: 5520.20 +/- 1106.34\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.52e+03 |\n",
      "|    mean_reward      | -18      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.49     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1030000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0864   |\n",
      "|    n_updates        | 929999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.75e+03 |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.489    |\n",
      "| time/               |          |\n",
      "|    episodes         | 884      |\n",
      "|    fps              | 202      |\n",
      "|    time_elapsed     | 5107     |\n",
      "|    total_timesteps  | 1032776  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00388  |\n",
      "|    n_updates        | 932775   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.7e+03  |\n",
      "|    ep_rew_mean      | -17.7    |\n",
      "|    exploration_rate | 0.486    |\n",
      "| time/               |          |\n",
      "|    episodes         | 888      |\n",
      "|    fps              | 202      |\n",
      "|    time_elapsed     | 5131     |\n",
      "|    total_timesteps  | 1037773  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00149  |\n",
      "|    n_updates        | 937772   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=-16.80 +/- 1.60\n",
      "Episode length: 6397.60 +/- 1112.29\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.4e+03  |\n",
      "|    mean_reward      | -16.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.485    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1040000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00568  |\n",
      "|    n_updates        | 939999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.74e+03 |\n",
      "|    ep_rew_mean      | -17.7    |\n",
      "|    exploration_rate | 0.483    |\n",
      "| time/               |          |\n",
      "|    episodes         | 892      |\n",
      "|    fps              | 201      |\n",
      "|    time_elapsed     | 5169     |\n",
      "|    total_timesteps  | 1043618  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0393   |\n",
      "|    n_updates        | 943617   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.74e+03 |\n",
      "|    ep_rew_mean      | -17.7    |\n",
      "|    exploration_rate | 0.48     |\n",
      "| time/               |          |\n",
      "|    episodes         | 896      |\n",
      "|    fps              | 201      |\n",
      "|    time_elapsed     | 5198     |\n",
      "|    total_timesteps  | 1049593  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0316   |\n",
      "|    n_updates        | 949592   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1050000, episode_reward=-17.40 +/- 0.80\n",
      "Episode length: 5791.20 +/- 546.41\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.79e+03 |\n",
      "|    mean_reward      | -17.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.48     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1050000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00941  |\n",
      "|    n_updates        | 949999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.72e+03 |\n",
      "|    ep_rew_mean      | -17.7    |\n",
      "|    exploration_rate | 0.478    |\n",
      "| time/               |          |\n",
      "|    episodes         | 900      |\n",
      "|    fps              | 201      |\n",
      "|    time_elapsed     | 5235     |\n",
      "|    total_timesteps  | 1055418  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0357   |\n",
      "|    n_updates        | 955417   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=-17.20 +/- 1.72\n",
      "Episode length: 5973.20 +/- 884.04\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.97e+03 |\n",
      "|    mean_reward      | -17.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.475    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1060000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0137   |\n",
      "|    n_updates        | 959999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.75e+03 |\n",
      "|    ep_rew_mean      | -17.7    |\n",
      "|    exploration_rate | 0.475    |\n",
      "| time/               |          |\n",
      "|    episodes         | 904      |\n",
      "|    fps              | 201      |\n",
      "|    time_elapsed     | 5274     |\n",
      "|    total_timesteps  | 1061488  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0173   |\n",
      "|    n_updates        | 961487   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.77e+03 |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.471    |\n",
      "| time/               |          |\n",
      "|    episodes         | 908      |\n",
      "|    fps              | 201      |\n",
      "|    time_elapsed     | 5306     |\n",
      "|    total_timesteps  | 1068149  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00509  |\n",
      "|    n_updates        | 968148   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1070000, episode_reward=-16.60 +/- 1.20\n",
      "Episode length: 6169.80 +/- 619.87\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.17e+03 |\n",
      "|    mean_reward      | -16.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.47     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1070000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0076   |\n",
      "|    n_updates        | 969999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.77e+03 |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.469    |\n",
      "| time/               |          |\n",
      "|    episodes         | 912      |\n",
      "|    fps              | 200      |\n",
      "|    time_elapsed     | 5341     |\n",
      "|    total_timesteps  | 1073572  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0122   |\n",
      "|    n_updates        | 973571   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.79e+03 |\n",
      "|    ep_rew_mean      | -17.5    |\n",
      "|    exploration_rate | 0.466    |\n",
      "| time/               |          |\n",
      "|    episodes         | 916      |\n",
      "|    fps              | 201      |\n",
      "|    time_elapsed     | 5371     |\n",
      "|    total_timesteps  | 1079770  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00896  |\n",
      "|    n_updates        | 979769   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=-16.80 +/- 1.72\n",
      "Episode length: 5829.20 +/- 524.27\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.83e+03 |\n",
      "|    mean_reward      | -16.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.465    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1080000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0108   |\n",
      "|    n_updates        | 979999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.82e+03 |\n",
      "|    ep_rew_mean      | -17.5    |\n",
      "|    exploration_rate | 0.463    |\n",
      "| time/               |          |\n",
      "|    episodes         | 920      |\n",
      "|    fps              | 200      |\n",
      "|    time_elapsed     | 5408     |\n",
      "|    total_timesteps  | 1085693  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00916  |\n",
      "|    n_updates        | 985692   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1090000, episode_reward=-15.80 +/- 2.14\n",
      "Episode length: 6571.60 +/- 596.31\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.57e+03 |\n",
      "|    mean_reward      | -15.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.46     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1090000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00525  |\n",
      "|    n_updates        | 989999   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.87e+03 |\n",
      "|    ep_rew_mean      | -17.4    |\n",
      "|    exploration_rate | 0.459    |\n",
      "| time/               |          |\n",
      "|    episodes         | 924      |\n",
      "|    fps              | 200      |\n",
      "|    time_elapsed     | 5450     |\n",
      "|    total_timesteps  | 1092307  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0163   |\n",
      "|    n_updates        | 992306   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.88e+03 |\n",
      "|    ep_rew_mean      | -17.3    |\n",
      "|    exploration_rate | 0.456    |\n",
      "| time/               |          |\n",
      "|    episodes         | 928      |\n",
      "|    fps              | 200      |\n",
      "|    time_elapsed     | 5481     |\n",
      "|    total_timesteps  | 1098545  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00976  |\n",
      "|    n_updates        | 998544   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=-16.20 +/- 2.32\n",
      "Episode length: 6778.20 +/- 1402.78\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.78e+03 |\n",
      "|    mean_reward      | -16.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.456    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1100000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00424  |\n",
      "|    n_updates        | 999999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.95e+03 |\n",
      "|    ep_rew_mean      | -17.2    |\n",
      "|    exploration_rate | 0.453    |\n",
      "| time/               |          |\n",
      "|    episodes         | 932      |\n",
      "|    fps              | 200      |\n",
      "|    time_elapsed     | 5523     |\n",
      "|    total_timesteps  | 1105033  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0103   |\n",
      "|    n_updates        | 1005032  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1110000, episode_reward=-16.20 +/- 3.43\n",
      "Episode length: 6070.00 +/- 1145.64\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.07e+03 |\n",
      "|    mean_reward      | -16.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.451    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1110000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00174  |\n",
      "|    n_updates        | 1009999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.89e+03 |\n",
      "|    ep_rew_mean      | -17.2    |\n",
      "|    exploration_rate | 0.45     |\n",
      "| time/               |          |\n",
      "|    episodes         | 936      |\n",
      "|    fps              | 199      |\n",
      "|    time_elapsed     | 5558     |\n",
      "|    total_timesteps  | 1110410  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0155   |\n",
      "|    n_updates        | 1010409  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.89e+03 |\n",
      "|    ep_rew_mean      | -17.1    |\n",
      "|    exploration_rate | 0.447    |\n",
      "| time/               |          |\n",
      "|    episodes         | 940      |\n",
      "|    fps              | 199      |\n",
      "|    time_elapsed     | 5588     |\n",
      "|    total_timesteps  | 1116473  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00285  |\n",
      "|    n_updates        | 1016472  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=-16.20 +/- 2.48\n",
      "Episode length: 6992.60 +/- 1617.05\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.99e+03 |\n",
      "|    mean_reward      | -16.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.446    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1120000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00936  |\n",
      "|    n_updates        | 1019999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.96e+03 |\n",
      "|    ep_rew_mean      | -17      |\n",
      "|    exploration_rate | 0.444    |\n",
      "| time/               |          |\n",
      "|    episodes         | 944      |\n",
      "|    fps              | 199      |\n",
      "|    time_elapsed     | 5632     |\n",
      "|    total_timesteps  | 1123352  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0329   |\n",
      "|    n_updates        | 1023351  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.99e+03 |\n",
      "|    ep_rew_mean      | -16.9    |\n",
      "|    exploration_rate | 0.441    |\n",
      "| time/               |          |\n",
      "|    episodes         | 948      |\n",
      "|    fps              | 199      |\n",
      "|    time_elapsed     | 5662     |\n",
      "|    total_timesteps  | 1129454  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00626  |\n",
      "|    n_updates        | 1029453  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1130000, episode_reward=-15.20 +/- 1.72\n",
      "Episode length: 6553.60 +/- 388.14\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.55e+03 |\n",
      "|    mean_reward      | -15.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.441    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1130000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00103  |\n",
      "|    n_updates        | 1029999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.02e+03 |\n",
      "|    ep_rew_mean      | -16.9    |\n",
      "|    exploration_rate | 0.438    |\n",
      "| time/               |          |\n",
      "|    episodes         | 952      |\n",
      "|    fps              | 199      |\n",
      "|    time_elapsed     | 5703     |\n",
      "|    total_timesteps  | 1135981  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0067   |\n",
      "|    n_updates        | 1035980  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=-16.40 +/- 0.80\n",
      "Episode length: 5946.00 +/- 395.97\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.95e+03 |\n",
      "|    mean_reward      | -16.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.436    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1140000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00663  |\n",
      "|    n_updates        | 1039999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.08e+03 |\n",
      "|    ep_rew_mean      | -16.8    |\n",
      "|    exploration_rate | 0.435    |\n",
      "| time/               |          |\n",
      "|    episodes         | 956      |\n",
      "|    fps              | 198      |\n",
      "|    time_elapsed     | 5742     |\n",
      "|    total_timesteps  | 1142419  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00974  |\n",
      "|    n_updates        | 1042418  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.06e+03 |\n",
      "|    ep_rew_mean      | -16.8    |\n",
      "|    exploration_rate | 0.431    |\n",
      "| time/               |          |\n",
      "|    episodes         | 960      |\n",
      "|    fps              | 199      |\n",
      "|    time_elapsed     | 5772     |\n",
      "|    total_timesteps  | 1149215  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00342  |\n",
      "|    n_updates        | 1049214  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1150000, episode_reward=-15.80 +/- 2.93\n",
      "Episode length: 6430.40 +/- 1034.82\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.43e+03 |\n",
      "|    mean_reward      | -15.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.431    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1150000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0101   |\n",
      "|    n_updates        | 1049999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.06e+03 |\n",
      "|    ep_rew_mean      | -16.8    |\n",
      "|    exploration_rate | 0.428    |\n",
      "| time/               |          |\n",
      "|    episodes         | 964      |\n",
      "|    fps              | 198      |\n",
      "|    time_elapsed     | 5811     |\n",
      "|    total_timesteps  | 1155548  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0078   |\n",
      "|    n_updates        | 1055547  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=-17.20 +/- 2.14\n",
      "Episode length: 5935.40 +/- 903.49\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 5.94e+03 |\n",
      "|    mean_reward      | -17.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.426    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1160000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00615  |\n",
      "|    n_updates        | 1059999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.1e+03  |\n",
      "|    ep_rew_mean      | -16.8    |\n",
      "|    exploration_rate | 0.425    |\n",
      "| time/               |          |\n",
      "|    episodes         | 968      |\n",
      "|    fps              | 198      |\n",
      "|    time_elapsed     | 5851     |\n",
      "|    total_timesteps  | 1162432  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00424  |\n",
      "|    n_updates        | 1062431  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.15e+03 |\n",
      "|    ep_rew_mean      | -16.7    |\n",
      "|    exploration_rate | 0.421    |\n",
      "| time/               |          |\n",
      "|    episodes         | 972      |\n",
      "|    fps              | 198      |\n",
      "|    time_elapsed     | 5883     |\n",
      "|    total_timesteps  | 1169469  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0485   |\n",
      "|    n_updates        | 1069468  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1170000, episode_reward=-13.80 +/- 2.79\n",
      "Episode length: 7181.20 +/- 865.30\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.18e+03 |\n",
      "|    mean_reward      | -13.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.421    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1170000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00687  |\n",
      "|    n_updates        | 1069999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.23e+03 |\n",
      "|    ep_rew_mean      | -16.6    |\n",
      "|    exploration_rate | 0.417    |\n",
      "| time/               |          |\n",
      "|    episodes         | 976      |\n",
      "|    fps              | 198      |\n",
      "|    time_elapsed     | 5928     |\n",
      "|    total_timesteps  | 1177176  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00291  |\n",
      "|    n_updates        | 1077175  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1180000, episode_reward=-15.00 +/- 1.10\n",
      "Episode length: 7217.80 +/- 673.43\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.22e+03 |\n",
      "|    mean_reward      | -15      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.416    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1180000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00413  |\n",
      "|    n_updates        | 1079999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.3e+03  |\n",
      "|    ep_rew_mean      | -16.5    |\n",
      "|    exploration_rate | 0.414    |\n",
      "| time/               |          |\n",
      "|    episodes         | 980      |\n",
      "|    fps              | 198      |\n",
      "|    time_elapsed     | 5972     |\n",
      "|    total_timesteps  | 1184433  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00579  |\n",
      "|    n_updates        | 1084432  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1190000, episode_reward=-14.40 +/- 2.24\n",
      "Episode length: 6842.20 +/- 1157.46\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.84e+03 |\n",
      "|    mean_reward      | -14.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.411    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1190000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00868  |\n",
      "|    n_updates        | 1089999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.37e+03 |\n",
      "|    ep_rew_mean      | -16.4    |\n",
      "|    exploration_rate | 0.41     |\n",
      "| time/               |          |\n",
      "|    episodes         | 984      |\n",
      "|    fps              | 198      |\n",
      "|    time_elapsed     | 6014     |\n",
      "|    total_timesteps  | 1191417  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00268  |\n",
      "|    n_updates        | 1091416  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.48e+03 |\n",
      "|    ep_rew_mean      | -16.3    |\n",
      "|    exploration_rate | 0.406    |\n",
      "| time/               |          |\n",
      "|    episodes         | 988      |\n",
      "|    fps              | 198      |\n",
      "|    time_elapsed     | 6049     |\n",
      "|    total_timesteps  | 1199141  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00389  |\n",
      "|    n_updates        | 1099140  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=-15.20 +/- 4.35\n",
      "Episode length: 6976.40 +/- 1847.06\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.98e+03 |\n",
      "|    mean_reward      | -15.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.406    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1200000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00423  |\n",
      "|    n_updates        | 1099999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.52e+03 |\n",
      "|    ep_rew_mean      | -16.1    |\n",
      "|    exploration_rate | 0.403    |\n",
      "| time/               |          |\n",
      "|    episodes         | 992      |\n",
      "|    fps              | 197      |\n",
      "|    time_elapsed     | 6090     |\n",
      "|    total_timesteps  | 1205997  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0259   |\n",
      "|    n_updates        | 1105996  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1210000, episode_reward=-15.20 +/- 1.47\n",
      "Episode length: 7121.80 +/- 787.41\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.12e+03 |\n",
      "|    mean_reward      | -15.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.401    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1210000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00357  |\n",
      "|    n_updates        | 1109999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.57e+03 |\n",
      "|    ep_rew_mean      | -16.1    |\n",
      "|    exploration_rate | 0.399    |\n",
      "| time/               |          |\n",
      "|    episodes         | 996      |\n",
      "|    fps              | 197      |\n",
      "|    time_elapsed     | 6134     |\n",
      "|    total_timesteps  | 1213282  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0342   |\n",
      "|    n_updates        | 1113281  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=-15.00 +/- 2.61\n",
      "Episode length: 7711.40 +/- 1164.25\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.71e+03 |\n",
      "|    mean_reward      | -15      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.396    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1220000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0109   |\n",
      "|    n_updates        | 1119999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.67e+03 |\n",
      "|    ep_rew_mean      | -15.8    |\n",
      "|    exploration_rate | 0.395    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1000     |\n",
      "|    fps              | 197      |\n",
      "|    time_elapsed     | 6184     |\n",
      "|    total_timesteps  | 1221685  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0149   |\n",
      "|    n_updates        | 1121684  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.72e+03 |\n",
      "|    ep_rew_mean      | -15.7    |\n",
      "|    exploration_rate | 0.392    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1004     |\n",
      "|    fps              | 197      |\n",
      "|    time_elapsed     | 6218     |\n",
      "|    total_timesteps  | 1228829  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0129   |\n",
      "|    n_updates        | 1128828  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1230000, episode_reward=-15.20 +/- 2.93\n",
      "Episode length: 7321.60 +/- 1439.27\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.32e+03 |\n",
      "|    mean_reward      | -15.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.391    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1230000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00286  |\n",
      "|    n_updates        | 1129999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.75e+03 |\n",
      "|    ep_rew_mean      | -15.7    |\n",
      "|    exploration_rate | 0.388    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1008     |\n",
      "|    fps              | 197      |\n",
      "|    time_elapsed     | 6267     |\n",
      "|    total_timesteps  | 1236324  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00361  |\n",
      "|    n_updates        | 1136323  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=-13.80 +/- 3.54\n",
      "Episode length: 7848.20 +/- 1279.99\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.85e+03 |\n",
      "|    mean_reward      | -13.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.386    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1240000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00454  |\n",
      "|    n_updates        | 1139999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.9e+03  |\n",
      "|    ep_rew_mean      | -15.4    |\n",
      "|    exploration_rate | 0.383    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1012     |\n",
      "|    fps              | 196      |\n",
      "|    time_elapsed     | 6327     |\n",
      "|    total_timesteps  | 1245526  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00451  |\n",
      "|    n_updates        | 1145525  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1250000, episode_reward=-12.00 +/- 4.69\n",
      "Episode length: 8659.40 +/- 1950.16\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 8.66e+03 |\n",
      "|    mean_reward      | -12      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.381    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1250000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0058   |\n",
      "|    n_updates        | 1149999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.89e+03 |\n",
      "|    ep_rew_mean      | -15.4    |\n",
      "|    exploration_rate | 0.381    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1016     |\n",
      "|    fps              | 196      |\n",
      "|    time_elapsed     | 6373     |\n",
      "|    total_timesteps  | 1251501  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00235  |\n",
      "|    n_updates        | 1151500  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.91e+03 |\n",
      "|    ep_rew_mean      | -15.4    |\n",
      "|    exploration_rate | 0.377    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1020     |\n",
      "|    fps              | 196      |\n",
      "|    time_elapsed     | 6407     |\n",
      "|    total_timesteps  | 1257897  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00793  |\n",
      "|    n_updates        | 1157896  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=-16.00 +/- 1.26\n",
      "Episode length: 7568.60 +/- 1096.72\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.57e+03 |\n",
      "|    mean_reward      | -16      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.376    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1260000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00262  |\n",
      "|    n_updates        | 1159999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.94e+03 |\n",
      "|    ep_rew_mean      | -15.3    |\n",
      "|    exploration_rate | 0.374    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1024     |\n",
      "|    fps              | 195      |\n",
      "|    time_elapsed     | 6457     |\n",
      "|    total_timesteps  | 1265289  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00402  |\n",
      "|    n_updates        | 1165288  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1270000, episode_reward=-15.00 +/- 2.83\n",
      "Episode length: 6603.00 +/- 1704.31\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.6e+03  |\n",
      "|    mean_reward      | -15      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.371    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1270000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00468  |\n",
      "|    n_updates        | 1169999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.01e+03 |\n",
      "|    ep_rew_mean      | -15.2    |\n",
      "|    exploration_rate | 0.37     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1028     |\n",
      "|    fps              | 195      |\n",
      "|    time_elapsed     | 6504     |\n",
      "|    total_timesteps  | 1273165  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00591  |\n",
      "|    n_updates        | 1173164  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=-15.80 +/- 1.94\n",
      "Episode length: 7646.80 +/- 1229.54\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.65e+03 |\n",
      "|    mean_reward      | -15.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.366    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1280000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00386  |\n",
      "|    n_updates        | 1179999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.08e+03 |\n",
      "|    ep_rew_mean      | -15      |\n",
      "|    exploration_rate | 0.366    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1032     |\n",
      "|    fps              | 195      |\n",
      "|    time_elapsed     | 6552     |\n",
      "|    total_timesteps  | 1281444  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0024   |\n",
      "|    n_updates        | 1181443  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1290000, episode_reward=-15.20 +/- 2.14\n",
      "Episode length: 7147.40 +/- 1003.39\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.15e+03 |\n",
      "|    mean_reward      | -15.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.361    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1290000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00179  |\n",
      "|    n_updates        | 1189999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.21e+03 |\n",
      "|    ep_rew_mean      | -14.7    |\n",
      "|    exploration_rate | 0.361    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1036     |\n",
      "|    fps              | 195      |\n",
      "|    time_elapsed     | 6602     |\n",
      "|    total_timesteps  | 1290147  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00791  |\n",
      "|    n_updates        | 1190146  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.27e+03 |\n",
      "|    ep_rew_mean      | -14.6    |\n",
      "|    exploration_rate | 0.358    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1040     |\n",
      "|    fps              | 195      |\n",
      "|    time_elapsed     | 6635     |\n",
      "|    total_timesteps  | 1297591  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0414   |\n",
      "|    n_updates        | 1197590  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=-10.20 +/- 4.71\n",
      "Episode length: 9122.60 +/- 2475.17\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.12e+03 |\n",
      "|    mean_reward      | -10.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.357    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1300000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00279  |\n",
      "|    n_updates        | 1199999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.35e+03 |\n",
      "|    ep_rew_mean      | -14.4    |\n",
      "|    exploration_rate | 0.353    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1044     |\n",
      "|    fps              | 195      |\n",
      "|    time_elapsed     | 6689     |\n",
      "|    total_timesteps  | 1306585  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00475  |\n",
      "|    n_updates        | 1206584  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1310000, episode_reward=-9.80 +/- 4.26\n",
      "Episode length: 8867.40 +/- 1981.22\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 8.87e+03 |\n",
      "|    mean_reward      | -9.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.352    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1310000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00455  |\n",
      "|    n_updates        | 1209999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.46e+03 |\n",
      "|    ep_rew_mean      | -14.2    |\n",
      "|    exploration_rate | 0.349    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1048     |\n",
      "|    fps              | 195      |\n",
      "|    time_elapsed     | 6741     |\n",
      "|    total_timesteps  | 1315293  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00311  |\n",
      "|    n_updates        | 1215292  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=-12.20 +/- 0.75\n",
      "Episode length: 8923.00 +/- 617.91\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 8.92e+03 |\n",
      "|    mean_reward      | -12.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.347    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1320000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00603  |\n",
      "|    n_updates        | 1219999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.52e+03 |\n",
      "|    ep_rew_mean      | -14.1    |\n",
      "|    exploration_rate | 0.345    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1052     |\n",
      "|    fps              | 194      |\n",
      "|    time_elapsed     | 6791     |\n",
      "|    total_timesteps  | 1323527  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0127   |\n",
      "|    n_updates        | 1223526  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1330000, episode_reward=-12.20 +/- 2.23\n",
      "Episode length: 8272.40 +/- 901.43\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 8.27e+03 |\n",
      "|    mean_reward      | -12.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.342    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1330000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00177  |\n",
      "|    n_updates        | 1229999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.58e+03 |\n",
      "|    ep_rew_mean      | -13.9    |\n",
      "|    exploration_rate | 0.341    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1056     |\n",
      "|    fps              | 194      |\n",
      "|    time_elapsed     | 6838     |\n",
      "|    total_timesteps  | 1331286  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00843  |\n",
      "|    n_updates        | 1231285  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=-13.00 +/- 3.16\n",
      "Episode length: 8037.00 +/- 910.13\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 8.04e+03 |\n",
      "|    mean_reward      | -13      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.337    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1340000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00321  |\n",
      "|    n_updates        | 1239999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.69e+03 |\n",
      "|    ep_rew_mean      | -13.6    |\n",
      "|    exploration_rate | 0.336    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1060     |\n",
      "|    fps              | 194      |\n",
      "|    time_elapsed     | 6893     |\n",
      "|    total_timesteps  | 1340807  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0113   |\n",
      "|    n_updates        | 1240806  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1350000, episode_reward=-9.60 +/- 4.59\n",
      "Episode length: 9344.20 +/- 1931.68\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.34e+03 |\n",
      "|    mean_reward      | -9.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.332    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1350000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00244  |\n",
      "|    n_updates        | 1249999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.86e+03 |\n",
      "|    ep_rew_mean      | -13.3    |\n",
      "|    exploration_rate | 0.331    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1064     |\n",
      "|    fps              | 194      |\n",
      "|    time_elapsed     | 6955     |\n",
      "|    total_timesteps  | 1351393  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00379  |\n",
      "|    n_updates        | 1251392  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.91e+03 |\n",
      "|    ep_rew_mean      | -13.1    |\n",
      "|    exploration_rate | 0.327    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1068     |\n",
      "|    fps              | 194      |\n",
      "|    time_elapsed     | 6993     |\n",
      "|    total_timesteps  | 1359658  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00866  |\n",
      "|    n_updates        | 1259657  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=-11.20 +/- 5.46\n",
      "Episode length: 8292.20 +/- 2469.63\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 8.29e+03 |\n",
      "|    mean_reward      | -11.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.327    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1360000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00176  |\n",
      "|    n_updates        | 1259999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.03e+03 |\n",
      "|    ep_rew_mean      | -12.8    |\n",
      "|    exploration_rate | 0.322    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1072     |\n",
      "|    fps              | 194      |\n",
      "|    time_elapsed     | 7050     |\n",
      "|    total_timesteps  | 1369614  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00245  |\n",
      "|    n_updates        | 1269613  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1370000, episode_reward=-9.40 +/- 4.45\n",
      "Episode length: 10720.20 +/- 2911.74\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.07e+04 |\n",
      "|    mean_reward      | -9.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.322    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1370000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00543  |\n",
      "|    n_updates        | 1269999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.11e+03 |\n",
      "|    ep_rew_mean      | -12.3    |\n",
      "|    exploration_rate | 0.317    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1076     |\n",
      "|    fps              | 193      |\n",
      "|    time_elapsed     | 7111     |\n",
      "|    total_timesteps  | 1379393  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00826  |\n",
      "|    n_updates        | 1279392  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=-11.80 +/- 3.19\n",
      "Episode length: 8728.00 +/- 1568.56\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 8.73e+03 |\n",
      "|    mean_reward      | -11.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.317    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1380000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0216   |\n",
      "|    n_updates        | 1279999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.22e+03 |\n",
      "|    ep_rew_mean      | -12      |\n",
      "|    exploration_rate | 0.312    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1080     |\n",
      "|    fps              | 193      |\n",
      "|    time_elapsed     | 7169     |\n",
      "|    total_timesteps  | 1389430  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00935  |\n",
      "|    n_updates        | 1289429  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1390000, episode_reward=-5.80 +/- 6.01\n",
      "Episode length: 10377.20 +/- 1757.38\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.04e+04 |\n",
      "|    mean_reward      | -5.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.312    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1390000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00923  |\n",
      "|    n_updates        | 1289999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.29e+03 |\n",
      "|    ep_rew_mean      | -11.9    |\n",
      "|    exploration_rate | 0.308    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1084     |\n",
      "|    fps              | 193      |\n",
      "|    time_elapsed     | 7225     |\n",
      "|    total_timesteps  | 1398180  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0521   |\n",
      "|    n_updates        | 1298179  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1400000, episode_reward=-8.80 +/- 1.94\n",
      "Episode length: 10425.20 +/- 855.55\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.04e+04 |\n",
      "|    mean_reward      | -8.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.307    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1400000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.004    |\n",
      "|    n_updates        | 1299999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.33e+03 |\n",
      "|    ep_rew_mean      | -11.7    |\n",
      "|    exploration_rate | 0.304    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1088     |\n",
      "|    fps              | 193      |\n",
      "|    time_elapsed     | 7279     |\n",
      "|    total_timesteps  | 1406728  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00205  |\n",
      "|    n_updates        | 1306727  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1410000, episode_reward=-3.60 +/- 3.61\n",
      "Episode length: 11562.40 +/- 1744.68\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.16e+04 |\n",
      "|    mean_reward      | -3.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.302    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1410000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00394  |\n",
      "|    n_updates        | 1309999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.47e+03 |\n",
      "|    ep_rew_mean      | -11.3    |\n",
      "|    exploration_rate | 0.298    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1092     |\n",
      "|    fps              | 192      |\n",
      "|    time_elapsed     | 7344     |\n",
      "|    total_timesteps  | 1417173  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00547  |\n",
      "|    n_updates        | 1317172  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1420000, episode_reward=-7.00 +/- 4.43\n",
      "Episode length: 10903.00 +/- 1874.09\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.09e+04 |\n",
      "|    mean_reward      | -7       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.297    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1420000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0019   |\n",
      "|    n_updates        | 1319999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.6e+03  |\n",
      "|    ep_rew_mean      | -11.1    |\n",
      "|    exploration_rate | 0.293    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1096     |\n",
      "|    fps              | 192      |\n",
      "|    time_elapsed     | 7408     |\n",
      "|    total_timesteps  | 1427609  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0078   |\n",
      "|    n_updates        | 1327608  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1430000, episode_reward=-10.00 +/- 1.90\n",
      "Episode length: 8900.80 +/- 530.13\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 8.9e+03  |\n",
      "|    mean_reward      | -10      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.292    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1430000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00313  |\n",
      "|    n_updates        | 1329999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.74e+03 |\n",
      "|    ep_rew_mean      | -10.7    |\n",
      "|    exploration_rate | 0.287    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1100     |\n",
      "|    fps              | 192      |\n",
      "|    time_elapsed     | 7475     |\n",
      "|    total_timesteps  | 1439602  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00767  |\n",
      "|    n_updates        | 1339601  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=-2.80 +/- 7.33\n",
      "Episode length: 10915.20 +/- 720.55\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.09e+04 |\n",
      "|    mean_reward      | -2.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.287    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1440000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00364  |\n",
      "|    n_updates        | 1339999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1450000, episode_reward=-6.80 +/- 6.65\n",
      "Episode length: 9649.40 +/- 1590.68\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.65e+03 |\n",
      "|    mean_reward      | -6.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.282    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1450000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00378  |\n",
      "|    n_updates        | 1349999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.87e+03 |\n",
      "|    ep_rew_mean      | -10.3    |\n",
      "|    exploration_rate | 0.282    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1104     |\n",
      "|    fps              | 191      |\n",
      "|    time_elapsed     | 7554     |\n",
      "|    total_timesteps  | 1450059  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00315  |\n",
      "|    n_updates        | 1350058  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.95e+03 |\n",
      "|    ep_rew_mean      | -10.2    |\n",
      "|    exploration_rate | 0.278    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1108     |\n",
      "|    fps              | 192      |\n",
      "|    time_elapsed     | 7595     |\n",
      "|    total_timesteps  | 1459392  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00379  |\n",
      "|    n_updates        | 1359391  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1460000, episode_reward=-0.80 +/- 6.18\n",
      "Episode length: 11094.80 +/- 703.03\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.11e+04 |\n",
      "|    mean_reward      | -0.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.277    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1460000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0012   |\n",
      "|    n_updates        | 1359999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9e+03    |\n",
      "|    ep_rew_mean      | -10.1    |\n",
      "|    exploration_rate | 0.272    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1112     |\n",
      "|    fps              | 191      |\n",
      "|    time_elapsed     | 7659     |\n",
      "|    total_timesteps  | 1469905  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00363  |\n",
      "|    n_updates        | 1369904  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1470000, episode_reward=-5.60 +/- 7.06\n",
      "Episode length: 10901.00 +/- 1302.60\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.09e+04 |\n",
      "|    mean_reward      | -5.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.272    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1470000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00521  |\n",
      "|    n_updates        | 1369999  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1480000, episode_reward=3.60 +/- 7.20\n",
      "Episode length: 10624.00 +/- 952.87\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.06e+04 |\n",
      "|    mean_reward      | 3.6      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.267    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1480000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00196  |\n",
      "|    n_updates        | 1379999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.2e+03  |\n",
      "|    ep_rew_mean      | -9.4     |\n",
      "|    exploration_rate | 0.267    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1116     |\n",
      "|    fps              | 191      |\n",
      "|    time_elapsed     | 7744     |\n",
      "|    total_timesteps  | 1481043  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0031   |\n",
      "|    n_updates        | 1381042  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1490000, episode_reward=5.80 +/- 5.11\n",
      "Episode length: 11552.60 +/- 1577.68\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.16e+04 |\n",
      "|    mean_reward      | 5.8      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.262    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1490000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00233  |\n",
      "|    n_updates        | 1389999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.37e+03 |\n",
      "|    ep_rew_mean      | -8.79    |\n",
      "|    exploration_rate | 0.262    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1120     |\n",
      "|    fps              | 190      |\n",
      "|    time_elapsed     | 7809     |\n",
      "|    total_timesteps  | 1491488  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00336  |\n",
      "|    n_updates        | 1391487  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=-9.20 +/- 4.35\n",
      "Episode length: 9044.60 +/- 1996.47\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.04e+03 |\n",
      "|    mean_reward      | -9.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.258    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1500000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00243  |\n",
      "|    n_updates        | 1399999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.48e+03 |\n",
      "|    ep_rew_mean      | -8.52    |\n",
      "|    exploration_rate | 0.257    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1124     |\n",
      "|    fps              | 190      |\n",
      "|    time_elapsed     | 7868     |\n",
      "|    total_timesteps  | 1501672  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00157  |\n",
      "|    n_updates        | 1401671  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1510000, episode_reward=-3.00 +/- 7.46\n",
      "Episode length: 10916.60 +/- 1543.78\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.09e+04 |\n",
      "|    mean_reward      | -3       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.253    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1510000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00201  |\n",
      "|    n_updates        | 1409999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.58e+03 |\n",
      "|    ep_rew_mean      | -8.26    |\n",
      "|    exploration_rate | 0.252    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1128     |\n",
      "|    fps              | 190      |\n",
      "|    time_elapsed     | 7932     |\n",
      "|    total_timesteps  | 1512108  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00836  |\n",
      "|    n_updates        | 1412107  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1520000, episode_reward=-4.60 +/- 4.32\n",
      "Episode length: 12378.60 +/- 1794.35\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.24e+04 |\n",
      "|    mean_reward      | -4.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.248    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1520000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0021   |\n",
      "|    n_updates        | 1419999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.68e+03 |\n",
      "|    ep_rew_mean      | -7.88    |\n",
      "|    exploration_rate | 0.246    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1132     |\n",
      "|    fps              | 190      |\n",
      "|    time_elapsed     | 8000     |\n",
      "|    total_timesteps  | 1522870  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000993 |\n",
      "|    n_updates        | 1422869  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1530000, episode_reward=-0.20 +/- 5.31\n",
      "Episode length: 11711.40 +/- 896.72\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.17e+04 |\n",
      "|    mean_reward      | -0.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.243    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1530000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00315  |\n",
      "|    n_updates        | 1429999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.76e+03 |\n",
      "|    ep_rew_mean      | -7.66    |\n",
      "|    exploration_rate | 0.241    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1136     |\n",
      "|    fps              | 190      |\n",
      "|    time_elapsed     | 8067     |\n",
      "|    total_timesteps  | 1533480  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00193  |\n",
      "|    n_updates        | 1433479  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1540000, episode_reward=3.00 +/- 11.47\n",
      "Episode length: 9740.40 +/- 1467.92\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.74e+03 |\n",
      "|    mean_reward      | 3        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.238    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1540000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00113  |\n",
      "|    n_updates        | 1439999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.9e+03  |\n",
      "|    ep_rew_mean      | -6.86    |\n",
      "|    exploration_rate | 0.236    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1140     |\n",
      "|    fps              | 189      |\n",
      "|    time_elapsed     | 8131     |\n",
      "|    total_timesteps  | 1544418  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000951 |\n",
      "|    n_updates        | 1444417  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1550000, episode_reward=2.60 +/- 5.95\n",
      "Episode length: 11849.60 +/- 1764.71\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.18e+04 |\n",
      "|    mean_reward      | 2.6      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.233    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1550000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00168  |\n",
      "|    n_updates        | 1449999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+04    |\n",
      "|    ep_rew_mean      | -6.32    |\n",
      "|    exploration_rate | 0.23     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1144     |\n",
      "|    fps              | 189      |\n",
      "|    time_elapsed     | 8202     |\n",
      "|    total_timesteps  | 1556114  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00157  |\n",
      "|    n_updates        | 1456113  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1560000, episode_reward=2.40 +/- 4.36\n",
      "Episode length: 11122.80 +/- 704.31\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.11e+04 |\n",
      "|    mean_reward      | 2.4      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.228    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1560000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00607  |\n",
      "|    n_updates        | 1459999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.01e+04 |\n",
      "|    ep_rew_mean      | -5.76    |\n",
      "|    exploration_rate | 0.224    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1148     |\n",
      "|    fps              | 189      |\n",
      "|    time_elapsed     | 8273     |\n",
      "|    total_timesteps  | 1567900  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00172  |\n",
      "|    n_updates        | 1467899  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1570000, episode_reward=0.00 +/- 5.69\n",
      "Episode length: 11301.40 +/- 909.39\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.13e+04 |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.223    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1570000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00338  |\n",
      "|    n_updates        | 1469999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.03e+04 |\n",
      "|    ep_rew_mean      | -5.38    |\n",
      "|    exploration_rate | 0.218    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1152     |\n",
      "|    fps              | 189      |\n",
      "|    time_elapsed     | 8342     |\n",
      "|    total_timesteps  | 1579300  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00146  |\n",
      "|    n_updates        | 1479299  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1580000, episode_reward=4.60 +/- 4.13\n",
      "Episode length: 11386.60 +/- 1306.50\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.14e+04 |\n",
      "|    mean_reward      | 4.6      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.218    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1580000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00121  |\n",
      "|    n_updates        | 1479999  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1590000, episode_reward=3.60 +/- 9.97\n",
      "Episode length: 10411.20 +/- 1507.99\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.04e+04 |\n",
      "|    mean_reward      | 3.6      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.213    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1590000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00408  |\n",
      "|    n_updates        | 1489999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.04e+04 |\n",
      "|    ep_rew_mean      | -5.07    |\n",
      "|    exploration_rate | 0.212    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1156     |\n",
      "|    fps              | 188      |\n",
      "|    time_elapsed     | 8429     |\n",
      "|    total_timesteps  | 1590954  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00409  |\n",
      "|    n_updates        | 1490953  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=3.80 +/- 5.34\n",
      "Episode length: 11339.40 +/- 1149.39\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.13e+04 |\n",
      "|    mean_reward      | 3.8      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.208    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1600000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00338  |\n",
      "|    n_updates        | 1499999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.05e+04 |\n",
      "|    ep_rew_mean      | -4.51    |\n",
      "|    exploration_rate | 0.207    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1160     |\n",
      "|    fps              | 188      |\n",
      "|    time_elapsed     | 8499     |\n",
      "|    total_timesteps  | 1602588  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00143  |\n",
      "|    n_updates        | 1502587  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1610000, episode_reward=-2.00 +/- 5.76\n",
      "Episode length: 11105.60 +/- 941.70\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.11e+04 |\n",
      "|    mean_reward      | -2       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.203    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1610000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00118  |\n",
      "|    n_updates        | 1509999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.05e+04 |\n",
      "|    ep_rew_mean      | -4.01    |\n",
      "|    exploration_rate | 0.202    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1164     |\n",
      "|    fps              | 188      |\n",
      "|    time_elapsed     | 8562     |\n",
      "|    total_timesteps  | 1612546  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000879 |\n",
      "|    n_updates        | 1512545  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1620000, episode_reward=12.00 +/- 4.05\n",
      "Episode length: 9718.00 +/- 1233.51\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.72e+03 |\n",
      "|    mean_reward      | 12       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.198    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1620000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0108   |\n",
      "|    n_updates        | 1519999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.06e+04 |\n",
      "|    ep_rew_mean      | -3.68    |\n",
      "|    exploration_rate | 0.196    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1168     |\n",
      "|    fps              | 188      |\n",
      "|    time_elapsed     | 8627     |\n",
      "|    total_timesteps  | 1623732  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00159  |\n",
      "|    n_updates        | 1523731  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1630000, episode_reward=7.60 +/- 4.50\n",
      "Episode length: 10708.80 +/- 1281.58\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.07e+04 |\n",
      "|    mean_reward      | 7.6      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.193    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1630000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00379  |\n",
      "|    n_updates        | 1529999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.06e+04 |\n",
      "|    ep_rew_mean      | -3.15    |\n",
      "|    exploration_rate | 0.191    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1172     |\n",
      "|    fps              | 188      |\n",
      "|    time_elapsed     | 8692     |\n",
      "|    total_timesteps  | 1634295  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00208  |\n",
      "|    n_updates        | 1534294  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1640000, episode_reward=9.40 +/- 3.38\n",
      "Episode length: 10476.20 +/- 1394.15\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.05e+04 |\n",
      "|    mean_reward      | 9.4      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.188    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1640000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00105  |\n",
      "|    n_updates        | 1539999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.06e+04 |\n",
      "|    ep_rew_mean      | -2.59    |\n",
      "|    exploration_rate | 0.186    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1176     |\n",
      "|    fps              | 187      |\n",
      "|    time_elapsed     | 8754     |\n",
      "|    total_timesteps  | 1644494  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00378  |\n",
      "|    n_updates        | 1544493  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1650000, episode_reward=9.20 +/- 7.03\n",
      "Episode length: 9580.00 +/- 851.47\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.58e+03 |\n",
      "|    mean_reward      | 9.2      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.183    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1650000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00115  |\n",
      "|    n_updates        | 1549999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.06e+04 |\n",
      "|    ep_rew_mean      | -1.85    |\n",
      "|    exploration_rate | 0.181    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1180     |\n",
      "|    fps              | 187      |\n",
      "|    time_elapsed     | 8814     |\n",
      "|    total_timesteps  | 1654357  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00108  |\n",
      "|    n_updates        | 1554356  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1660000, episode_reward=6.60 +/- 5.78\n",
      "Episode length: 10685.20 +/- 527.72\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.07e+04 |\n",
      "|    mean_reward      | 6.6      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.178    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1660000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00194  |\n",
      "|    n_updates        | 1559999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.07e+04 |\n",
      "|    ep_rew_mean      | -0.88    |\n",
      "|    exploration_rate | 0.176    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1184     |\n",
      "|    fps              | 187      |\n",
      "|    time_elapsed     | 8877     |\n",
      "|    total_timesteps  | 1664572  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00142  |\n",
      "|    n_updates        | 1564571  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1670000, episode_reward=5.60 +/- 3.72\n",
      "Episode length: 11661.80 +/- 790.66\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.17e+04 |\n",
      "|    mean_reward      | 5.6      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.173    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1670000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00188  |\n",
      "|    n_updates        | 1569999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.08e+04 |\n",
      "|    ep_rew_mean      | -0.16    |\n",
      "|    exploration_rate | 0.171    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1188     |\n",
      "|    fps              | 187      |\n",
      "|    time_elapsed     | 8944     |\n",
      "|    total_timesteps  | 1675268  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00129  |\n",
      "|    n_updates        | 1575267  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1680000, episode_reward=9.00 +/- 2.83\n",
      "Episode length: 10109.60 +/- 900.12\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.01e+04 |\n",
      "|    mean_reward      | 9        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.168    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1680000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00178  |\n",
      "|    n_updates        | 1579999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.08e+04 |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.165    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1192     |\n",
      "|    fps              | 187      |\n",
      "|    time_elapsed     | 9015     |\n",
      "|    total_timesteps  | 1687460  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00167  |\n",
      "|    n_updates        | 1587459  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1690000, episode_reward=0.00 +/- 4.82\n",
      "Episode length: 12087.40 +/- 1416.29\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.21e+04 |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.163    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1690000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00255  |\n",
      "|    n_updates        | 1589999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.08e+04 |\n",
      "|    ep_rew_mean      | 0.27     |\n",
      "|    exploration_rate | 0.16     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1196     |\n",
      "|    fps              | 186      |\n",
      "|    time_elapsed     | 9082     |\n",
      "|    total_timesteps  | 1697947  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00168  |\n",
      "|    n_updates        | 1597946  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1700000, episode_reward=8.80 +/- 2.99\n",
      "Episode length: 10524.40 +/- 611.13\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.05e+04 |\n",
      "|    mean_reward      | 8.8      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.159    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1700000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00213  |\n",
      "|    n_updates        | 1599999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.08e+04 |\n",
      "|    ep_rew_mean      | 0.67     |\n",
      "|    exploration_rate | 0.154    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1200     |\n",
      "|    fps              | 186      |\n",
      "|    time_elapsed     | 9149     |\n",
      "|    total_timesteps  | 1709235  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00315  |\n",
      "|    n_updates        | 1609234  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1710000, episode_reward=11.40 +/- 3.44\n",
      "Episode length: 9698.40 +/- 1343.55\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.7e+03  |\n",
      "|    mean_reward      | 11.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.154    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1710000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00458  |\n",
      "|    n_updates        | 1609999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.08e+04 |\n",
      "|    ep_rew_mean      | 1.35     |\n",
      "|    exploration_rate | 0.149    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1204     |\n",
      "|    fps              | 186      |\n",
      "|    time_elapsed     | 9209     |\n",
      "|    total_timesteps  | 1718962  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00213  |\n",
      "|    n_updates        | 1618961  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1720000, episode_reward=11.20 +/- 2.93\n",
      "Episode length: 10113.40 +/- 1156.08\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.01e+04 |\n",
      "|    mean_reward      | 11.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.149    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1720000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00125  |\n",
      "|    n_updates        | 1619999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.08e+04 |\n",
      "|    ep_rew_mean      | 2.14     |\n",
      "|    exploration_rate | 0.144    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1208     |\n",
      "|    fps              | 186      |\n",
      "|    time_elapsed     | 9273     |\n",
      "|    total_timesteps  | 1729318  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00108  |\n",
      "|    n_updates        | 1629317  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1730000, episode_reward=13.40 +/- 4.76\n",
      "Episode length: 9424.60 +/- 1205.15\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.42e+03 |\n",
      "|    mean_reward      | 13.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.144    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1730000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00168  |\n",
      "|    n_updates        | 1629999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.08e+04 |\n",
      "|    ep_rew_mean      | 2.76     |\n",
      "|    exploration_rate | 0.139    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1212     |\n",
      "|    fps              | 186      |\n",
      "|    time_elapsed     | 9336     |\n",
      "|    total_timesteps  | 1739754  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00103  |\n",
      "|    n_updates        | 1639753  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1740000, episode_reward=8.20 +/- 5.27\n",
      "Episode length: 10747.40 +/- 1648.54\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.07e+04 |\n",
      "|    mean_reward      | 8.2      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.139    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1740000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0021   |\n",
      "|    n_updates        | 1639999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.08e+04 |\n",
      "|    ep_rew_mean      | 3.23     |\n",
      "|    exploration_rate | 0.134    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1216     |\n",
      "|    fps              | 186      |\n",
      "|    time_elapsed     | 9399     |\n",
      "|    total_timesteps  | 1749726  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000949 |\n",
      "|    n_updates        | 1649725  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1750000, episode_reward=11.00 +/- 3.41\n",
      "Episode length: 9950.80 +/- 1465.70\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.95e+03 |\n",
      "|    mean_reward      | 11       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.134    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1750000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00179  |\n",
      "|    n_updates        | 1649999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.07e+04 |\n",
      "|    ep_rew_mean      | 3.85     |\n",
      "|    exploration_rate | 0.129    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1220     |\n",
      "|    fps              | 185      |\n",
      "|    time_elapsed     | 9456     |\n",
      "|    total_timesteps  | 1758662  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00133  |\n",
      "|    n_updates        | 1658661  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1760000, episode_reward=-6.00 +/- 1.26\n",
      "Episode length: 11871.60 +/- 800.16\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.19e+04 |\n",
      "|    mean_reward      | -6       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.129    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1760000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00321  |\n",
      "|    n_updates        | 1659999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.07e+04 |\n",
      "|    ep_rew_mean      | 4.6      |\n",
      "|    exploration_rate | 0.124    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1224     |\n",
      "|    fps              | 185      |\n",
      "|    time_elapsed     | 9522     |\n",
      "|    total_timesteps  | 1768818  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000893 |\n",
      "|    n_updates        | 1668817  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1770000, episode_reward=6.80 +/- 4.45\n",
      "Episode length: 11331.60 +/- 1374.21\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.13e+04 |\n",
      "|    mean_reward      | 6.8      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.124    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1770000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0014   |\n",
      "|    n_updates        | 1669999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.07e+04 |\n",
      "|    ep_rew_mean      | 5.33     |\n",
      "|    exploration_rate | 0.12     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1228     |\n",
      "|    fps              | 185      |\n",
      "|    time_elapsed     | 9583     |\n",
      "|    total_timesteps  | 1777967  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00161  |\n",
      "|    n_updates        | 1677966  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1780000, episode_reward=7.60 +/- 2.33\n",
      "Episode length: 11897.20 +/- 900.43\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.19e+04 |\n",
      "|    mean_reward      | 7.6      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.119    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1780000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0012   |\n",
      "|    n_updates        | 1679999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.07e+04 |\n",
      "|    ep_rew_mean      | 5.79     |\n",
      "|    exploration_rate | 0.115    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1232     |\n",
      "|    fps              | 185      |\n",
      "|    time_elapsed     | 9650     |\n",
      "|    total_timesteps  | 1788549  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00336  |\n",
      "|    n_updates        | 1688548  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1790000, episode_reward=13.80 +/- 2.04\n",
      "Episode length: 9377.60 +/- 881.38\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.38e+03 |\n",
      "|    mean_reward      | 13.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.114    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1790000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00154  |\n",
      "|    n_updates        | 1689999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.07e+04 |\n",
      "|    ep_rew_mean      | 6.25     |\n",
      "|    exploration_rate | 0.109    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1236     |\n",
      "|    fps              | 185      |\n",
      "|    time_elapsed     | 9716     |\n",
      "|    total_timesteps  | 1799715  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00124  |\n",
      "|    n_updates        | 1699714  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1800000, episode_reward=11.40 +/- 3.44\n",
      "Episode length: 9877.40 +/- 1364.43\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.88e+03 |\n",
      "|    mean_reward      | 11.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.109    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1800000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00074  |\n",
      "|    n_updates        | 1699999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.06e+04 |\n",
      "|    ep_rew_mean      | 6.43     |\n",
      "|    exploration_rate | 0.104    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1240     |\n",
      "|    fps              | 185      |\n",
      "|    time_elapsed     | 9776     |\n",
      "|    total_timesteps  | 1809375  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000614 |\n",
      "|    n_updates        | 1709374  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1810000, episode_reward=3.20 +/- 2.99\n",
      "Episode length: 11966.60 +/- 451.27\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.2e+04  |\n",
      "|    mean_reward      | 3.2      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.104    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1810000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000982 |\n",
      "|    n_updates        | 1709999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.05e+04 |\n",
      "|    ep_rew_mean      | 6.92     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1244     |\n",
      "|    fps              | 184      |\n",
      "|    time_elapsed     | 9835     |\n",
      "|    total_timesteps  | 1817941  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00616  |\n",
      "|    n_updates        | 1717940  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1820000, episode_reward=14.20 +/- 3.19\n",
      "Episode length: 8984.40 +/- 890.98\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 8.98e+03 |\n",
      "|    mean_reward      | 14.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0991   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1820000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000586 |\n",
      "|    n_updates        | 1719999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.04e+04 |\n",
      "|    ep_rew_mean      | 7.38     |\n",
      "|    exploration_rate | 0.096    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1248     |\n",
      "|    fps              | 184      |\n",
      "|    time_elapsed     | 9888     |\n",
      "|    total_timesteps  | 1826190  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00131  |\n",
      "|    n_updates        | 1726189  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1830000, episode_reward=12.00 +/- 2.37\n",
      "Episode length: 9970.20 +/- 1062.45\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.97e+03 |\n",
      "|    mean_reward      | 12       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0942   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1830000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00131  |\n",
      "|    n_updates        | 1729999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.03e+04 |\n",
      "|    ep_rew_mean      | 8.03     |\n",
      "|    exploration_rate | 0.0914   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1252     |\n",
      "|    fps              | 184      |\n",
      "|    time_elapsed     | 9947     |\n",
      "|    total_timesteps  | 1835542  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000781 |\n",
      "|    n_updates        | 1735541  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1840000, episode_reward=12.00 +/- 2.61\n",
      "Episode length: 9769.00 +/- 1295.03\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.77e+03 |\n",
      "|    mean_reward      | 12       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0892   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1840000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000892 |\n",
      "|    n_updates        | 1739999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.02e+04 |\n",
      "|    ep_rew_mean      | 8.8      |\n",
      "|    exploration_rate | 0.0867   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1256     |\n",
      "|    fps              | 184      |\n",
      "|    time_elapsed     | 10007    |\n",
      "|    total_timesteps  | 1845028  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00266  |\n",
      "|    n_updates        | 1745027  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1850000, episode_reward=14.00 +/- 2.28\n",
      "Episode length: 9128.60 +/- 677.75\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.13e+03 |\n",
      "|    mean_reward      | 14       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0843   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1850000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000618 |\n",
      "|    n_updates        | 1749999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.01e+04 |\n",
      "|    ep_rew_mean      | 9.08     |\n",
      "|    exploration_rate | 0.0818   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1260     |\n",
      "|    fps              | 184      |\n",
      "|    time_elapsed     | 10067    |\n",
      "|    total_timesteps  | 1854872  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000557 |\n",
      "|    n_updates        | 1754871  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1860000, episode_reward=14.60 +/- 2.06\n",
      "Episode length: 9410.40 +/- 774.75\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.41e+03 |\n",
      "|    mean_reward      | 14.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0793   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1860000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00164  |\n",
      "|    n_updates        | 1759999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.01e+04 |\n",
      "|    ep_rew_mean      | 9.38     |\n",
      "|    exploration_rate | 0.0772   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1264     |\n",
      "|    fps              | 184      |\n",
      "|    time_elapsed     | 10125    |\n",
      "|    total_timesteps  | 1864157  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00127  |\n",
      "|    n_updates        | 1764156  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1870000, episode_reward=18.00 +/- 1.26\n",
      "Episode length: 7862.60 +/- 359.93\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.86e+03 |\n",
      "|    mean_reward      | 18       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0744   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1870000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00107  |\n",
      "|    n_updates        | 1769999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+04    |\n",
      "|    ep_rew_mean      | 10.1     |\n",
      "|    exploration_rate | 0.0728   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1268     |\n",
      "|    fps              | 184      |\n",
      "|    time_elapsed     | 10180    |\n",
      "|    total_timesteps  | 1873199  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00378  |\n",
      "|    n_updates        | 1773198  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1880000, episode_reward=17.00 +/- 2.10\n",
      "Episode length: 8002.20 +/- 783.31\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 8e+03    |\n",
      "|    mean_reward      | 17       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0694   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1880000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000775 |\n",
      "|    n_updates        | 1779999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.97e+03 |\n",
      "|    ep_rew_mean      | 10.3     |\n",
      "|    exploration_rate | 0.0679   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1272     |\n",
      "|    fps              | 183      |\n",
      "|    time_elapsed     | 10239    |\n",
      "|    total_timesteps  | 1883071  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0016   |\n",
      "|    n_updates        | 1783070  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1890000, episode_reward=15.60 +/- 2.80\n",
      "Episode length: 8469.80 +/- 1245.19\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 8.47e+03 |\n",
      "|    mean_reward      | 15.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0645   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1890000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000438 |\n",
      "|    n_updates        | 1789999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.91e+03 |\n",
      "|    ep_rew_mean      | 10.4     |\n",
      "|    exploration_rate | 0.0636   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1276     |\n",
      "|    fps              | 183      |\n",
      "|    time_elapsed     | 10292    |\n",
      "|    total_timesteps  | 1891732  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00105  |\n",
      "|    n_updates        | 1791731  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1900000, episode_reward=13.40 +/- 6.41\n",
      "Episode length: 9561.80 +/- 2043.66\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.56e+03 |\n",
      "|    mean_reward      | 13.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0595   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1900000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000585 |\n",
      "|    n_updates        | 1799999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.87e+03 |\n",
      "|    ep_rew_mean      | 10.6     |\n",
      "|    exploration_rate | 0.0592   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1280     |\n",
      "|    fps              | 183      |\n",
      "|    time_elapsed     | 10349    |\n",
      "|    total_timesteps  | 1900594  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00115  |\n",
      "|    n_updates        | 1800593  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.79e+03 |\n",
      "|    ep_rew_mean      | 10.9     |\n",
      "|    exploration_rate | 0.0551   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1284     |\n",
      "|    fps              | 183      |\n",
      "|    time_elapsed     | 10386    |\n",
      "|    total_timesteps  | 1908801  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000882 |\n",
      "|    n_updates        | 1808800  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1910000, episode_reward=15.60 +/- 4.41\n",
      "Episode length: 9120.60 +/- 1670.19\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.12e+03 |\n",
      "|    mean_reward      | 15.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0546   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1910000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000563 |\n",
      "|    n_updates        | 1809999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.71e+03 |\n",
      "|    ep_rew_mean      | 11.2     |\n",
      "|    exploration_rate | 0.0508   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1288     |\n",
      "|    fps              | 183      |\n",
      "|    time_elapsed     | 10442    |\n",
      "|    total_timesteps  | 1917544  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00368  |\n",
      "|    n_updates        | 1817543  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1920000, episode_reward=18.20 +/- 0.75\n",
      "Episode length: 7871.80 +/- 462.47\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.87e+03 |\n",
      "|    mean_reward      | 18.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0496   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1920000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000511 |\n",
      "|    n_updates        | 1819999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.54e+03 |\n",
      "|    ep_rew_mean      | 12       |\n",
      "|    exploration_rate | 0.047    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1292     |\n",
      "|    fps              | 183      |\n",
      "|    time_elapsed     | 10490    |\n",
      "|    total_timesteps  | 1925311  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000792 |\n",
      "|    n_updates        | 1825310  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1930000, episode_reward=17.40 +/- 1.74\n",
      "Episode length: 7870.40 +/- 451.11\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.87e+03 |\n",
      "|    mean_reward      | 17.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0447   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1930000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000986 |\n",
      "|    n_updates        | 1829999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.41e+03 |\n",
      "|    ep_rew_mean      | 12.8     |\n",
      "|    exploration_rate | 0.0434   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1296     |\n",
      "|    fps              | 183      |\n",
      "|    time_elapsed     | 10538    |\n",
      "|    total_timesteps  | 1932623  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00103  |\n",
      "|    n_updates        | 1832622  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1940000, episode_reward=18.20 +/- 1.47\n",
      "Episode length: 7524.20 +/- 624.14\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.52e+03 |\n",
      "|    mean_reward      | 18.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0397   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1940000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00175  |\n",
      "|    n_updates        | 1839999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.27e+03 |\n",
      "|    ep_rew_mean      | 13.2     |\n",
      "|    exploration_rate | 0.0395   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1300     |\n",
      "|    fps              | 183      |\n",
      "|    time_elapsed     | 10586    |\n",
      "|    total_timesteps  | 1940374  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000703 |\n",
      "|    n_updates        | 1840373  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.22e+03 |\n",
      "|    ep_rew_mean      | 13.4     |\n",
      "|    exploration_rate | 0.0353   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1304     |\n",
      "|    fps              | 183      |\n",
      "|    time_elapsed     | 10625    |\n",
      "|    total_timesteps  | 1948869  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000587 |\n",
      "|    n_updates        | 1848868  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1950000, episode_reward=4.40 +/- 2.06\n",
      "Episode length: 12507.80 +/- 550.11\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.25e+04 |\n",
      "|    mean_reward      | 4.4      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0348   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1950000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00158  |\n",
      "|    n_updates        | 1849999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.12e+03 |\n",
      "|    ep_rew_mean      | 13.8     |\n",
      "|    exploration_rate | 0.0315   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1308     |\n",
      "|    fps              | 183      |\n",
      "|    time_elapsed     | 10681    |\n",
      "|    total_timesteps  | 1956655  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00123  |\n",
      "|    n_updates        | 1856654  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1960000, episode_reward=20.00 +/- 0.89\n",
      "Episode length: 6960.00 +/- 324.69\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.96e+03 |\n",
      "|    mean_reward      | 20       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0298   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1960000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000811 |\n",
      "|    n_updates        | 1859999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.99e+03 |\n",
      "|    ep_rew_mean      | 14.2     |\n",
      "|    exploration_rate | 0.0278   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1312     |\n",
      "|    fps              | 183      |\n",
      "|    time_elapsed     | 10727    |\n",
      "|    total_timesteps  | 1964058  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000654 |\n",
      "|    n_updates        | 1864057  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1970000, episode_reward=9.80 +/- 5.19\n",
      "Episode length: 10577.80 +/- 1413.41\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.06e+04 |\n",
      "|    mean_reward      | 9.8      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0249   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1970000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00107  |\n",
      "|    n_updates        | 1869999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.92e+03 |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.0237   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1316     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 10783    |\n",
      "|    total_timesteps  | 1972261  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00125  |\n",
      "|    n_updates        | 1872260  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1980000, episode_reward=16.60 +/- 2.65\n",
      "Episode length: 8105.40 +/- 1026.49\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 8.11e+03 |\n",
      "|    mean_reward      | 16.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0199   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1980000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00132  |\n",
      "|    n_updates        | 1879999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.89e+03 |\n",
      "|    ep_rew_mean      | 14.4     |\n",
      "|    exploration_rate | 0.0197   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1320     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 10834    |\n",
      "|    total_timesteps  | 1980451  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000418 |\n",
      "|    n_updates        | 1880450  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.81e+03 |\n",
      "|    ep_rew_mean      | 14.6     |\n",
      "|    exploration_rate | 0.0157   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1324     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 10871    |\n",
      "|    total_timesteps  | 1988410  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000617 |\n",
      "|    n_updates        | 1888409  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1990000, episode_reward=19.80 +/- 0.98\n",
      "Episode length: 6979.60 +/- 316.86\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.98e+03 |\n",
      "|    mean_reward      | 19.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.015    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1990000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000285 |\n",
      "|    n_updates        | 1889999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.75e+03 |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.0119   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1328     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 10918    |\n",
      "|    total_timesteps  | 1996170  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000519 |\n",
      "|    n_updates        | 1896169  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2000000, episode_reward=19.60 +/- 1.36\n",
      "Episode length: 7007.40 +/- 507.98\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.01e+03 |\n",
      "|    mean_reward      | 19.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2000000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00126  |\n",
      "|    n_updates        | 1899999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.62e+03 |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1332     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 10964    |\n",
      "|    total_timesteps  | 2003560  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000416 |\n",
      "|    n_updates        | 1903559  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2010000, episode_reward=19.40 +/- 0.49\n",
      "Episode length: 7134.00 +/- 169.37\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.13e+03 |\n",
      "|    mean_reward      | 19.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2010000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000414 |\n",
      "|    n_updates        | 1909999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.46e+03 |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1336     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11009    |\n",
      "|    total_timesteps  | 2010554  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000202 |\n",
      "|    n_updates        | 1910553  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.35e+03 |\n",
      "|    ep_rew_mean      | 16.1     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1340     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11042    |\n",
      "|    total_timesteps  | 2017586  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000347 |\n",
      "|    n_updates        | 1917585  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2020000, episode_reward=19.80 +/- 0.75\n",
      "Episode length: 6910.80 +/- 349.67\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.91e+03 |\n",
      "|    mean_reward      | 19.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2020000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000224 |\n",
      "|    n_updates        | 1919999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.29e+03 |\n",
      "|    ep_rew_mean      | 16.3     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1344     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11086    |\n",
      "|    total_timesteps  | 2024526  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000736 |\n",
      "|    n_updates        | 1924525  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2030000, episode_reward=20.40 +/- 0.49\n",
      "Episode length: 6880.00 +/- 279.06\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.88e+03 |\n",
      "|    mean_reward      | 20.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2030000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000216 |\n",
      "|    n_updates        | 1929999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.25e+03 |\n",
      "|    ep_rew_mean      | 16.4     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1348     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11132    |\n",
      "|    total_timesteps  | 2032004  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00148  |\n",
      "|    n_updates        | 1932003  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.16e+03 |\n",
      "|    ep_rew_mean      | 16.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1352     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11165    |\n",
      "|    total_timesteps  | 2039038  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000613 |\n",
      "|    n_updates        | 1939037  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2040000, episode_reward=20.40 +/- 0.49\n",
      "Episode length: 6686.00 +/- 146.18\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.69e+03 |\n",
      "|    mean_reward      | 20.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2040000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000527 |\n",
      "|    n_updates        | 1939999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.07e+03 |\n",
      "|    ep_rew_mean      | 17       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1356     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11209    |\n",
      "|    total_timesteps  | 2046300  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000384 |\n",
      "|    n_updates        | 1946299  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2050000, episode_reward=20.60 +/- 0.49\n",
      "Episode length: 6756.00 +/- 230.86\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.76e+03 |\n",
      "|    mean_reward      | 20.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2050000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000423 |\n",
      "|    n_updates        | 1949999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.97e+03 |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1360     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11255    |\n",
      "|    total_timesteps  | 2053640  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000354 |\n",
      "|    n_updates        | 1953639  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2060000, episode_reward=20.60 +/- 0.49\n",
      "Episode length: 6602.80 +/- 109.78\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.6e+03  |\n",
      "|    mean_reward      | 20.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2060000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000982 |\n",
      "|    n_updates        | 1959999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.88e+03 |\n",
      "|    ep_rew_mean      | 17.5     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1364     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11299    |\n",
      "|    total_timesteps  | 2060650  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000211 |\n",
      "|    n_updates        | 1960649  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.8e+03  |\n",
      "|    ep_rew_mean      | 17.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1368     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11332    |\n",
      "|    total_timesteps  | 2067748  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000437 |\n",
      "|    n_updates        | 1967747  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2070000, episode_reward=18.40 +/- 1.36\n",
      "Episode length: 7556.20 +/- 582.10\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.56e+03 |\n",
      "|    mean_reward      | 18.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2070000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00019  |\n",
      "|    n_updates        | 1969999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.7e+03  |\n",
      "|    ep_rew_mean      | 18       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1372     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11378    |\n",
      "|    total_timesteps  | 2074960  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00138  |\n",
      "|    n_updates        | 1974959  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2080000, episode_reward=18.40 +/- 2.87\n",
      "Episode length: 7471.80 +/- 1034.75\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.47e+03 |\n",
      "|    mean_reward      | 18.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2080000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000171 |\n",
      "|    n_updates        | 1979999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.65e+03 |\n",
      "|    ep_rew_mean      | 18.2     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1376     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11425    |\n",
      "|    total_timesteps  | 2082427  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000234 |\n",
      "|    n_updates        | 1982426  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.57e+03 |\n",
      "|    ep_rew_mean      | 18.3     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1380     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11458    |\n",
      "|    total_timesteps  | 2089435  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000363 |\n",
      "|    n_updates        | 1989434  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2090000, episode_reward=19.80 +/- 0.98\n",
      "Episode length: 7209.00 +/- 584.19\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.21e+03 |\n",
      "|    mean_reward      | 19.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2090000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000228 |\n",
      "|    n_updates        | 1989999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.53e+03 |\n",
      "|    ep_rew_mean      | 18.4     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1384     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11503    |\n",
      "|    total_timesteps  | 2096555  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000248 |\n",
      "|    n_updates        | 1996554  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2100000, episode_reward=19.60 +/- 0.80\n",
      "Episode length: 7063.80 +/- 191.44\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.06e+03 |\n",
      "|    mean_reward      | 19.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2100000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00053  |\n",
      "|    n_updates        | 1999999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.47e+03 |\n",
      "|    ep_rew_mean      | 18.5     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1388     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11548    |\n",
      "|    total_timesteps  | 2103807  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000555 |\n",
      "|    n_updates        | 2003806  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2110000, episode_reward=18.40 +/- 2.65\n",
      "Episode length: 7278.40 +/- 807.27\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.28e+03 |\n",
      "|    mean_reward      | 18.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2110000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00184  |\n",
      "|    n_updates        | 2009999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.44e+03 |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1392     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11593    |\n",
      "|    total_timesteps  | 2110829  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000154 |\n",
      "|    n_updates        | 2010828  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.42e+03 |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1396     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11625    |\n",
      "|    total_timesteps  | 2117591  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000499 |\n",
      "|    n_updates        | 2017590  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2120000, episode_reward=20.20 +/- 1.17\n",
      "Episode length: 6766.40 +/- 510.15\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.77e+03 |\n",
      "|    mean_reward      | 20.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2120000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00034  |\n",
      "|    n_updates        | 2019999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.37e+03 |\n",
      "|    ep_rew_mean      | 18.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1400     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11666    |\n",
      "|    total_timesteps  | 2124192  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000945 |\n",
      "|    n_updates        | 2024191  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2130000, episode_reward=20.60 +/- 0.80\n",
      "Episode length: 6645.20 +/- 206.30\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.65e+03 |\n",
      "|    mean_reward      | 20.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2130000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000119 |\n",
      "|    n_updates        | 2029999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.3e+03  |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1404     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 11709    |\n",
      "|    total_timesteps  | 2130861  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000134 |\n",
      "|    n_updates        | 2030860  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.26e+03 |\n",
      "|    ep_rew_mean      | 19       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1408     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11740    |\n",
      "|    total_timesteps  | 2137575  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000136 |\n",
      "|    n_updates        | 2037574  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2140000, episode_reward=20.40 +/- 1.20\n",
      "Episode length: 6792.20 +/- 603.30\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.79e+03 |\n",
      "|    mean_reward      | 20.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2140000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000276 |\n",
      "|    n_updates        | 2039999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.25e+03 |\n",
      "|    ep_rew_mean      | 19       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1412     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 11784    |\n",
      "|    total_timesteps  | 2144692  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000161 |\n",
      "|    n_updates        | 2044691  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2150000, episode_reward=20.60 +/- 0.49\n",
      "Episode length: 6713.20 +/- 180.43\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.71e+03 |\n",
      "|    mean_reward      | 20.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2150000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000173 |\n",
      "|    n_updates        | 2049999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.19e+03 |\n",
      "|    ep_rew_mean      | 19.2     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1416     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 11827    |\n",
      "|    total_timesteps  | 2151450  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000241 |\n",
      "|    n_updates        | 2051449  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.18e+03 |\n",
      "|    ep_rew_mean      | 19.2     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1420     |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 11864    |\n",
      "|    total_timesteps  | 2159410  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000156 |\n",
      "|    n_updates        | 2059409  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2160000, episode_reward=20.60 +/- 0.49\n",
      "Episode length: 6599.00 +/- 185.10\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.6e+03  |\n",
      "|    mean_reward      | 20.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2160000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00078  |\n",
      "|    n_updates        | 2059999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.12e+03 |\n",
      "|    ep_rew_mean      | 19.4     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1424     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 11906    |\n",
      "|    total_timesteps  | 2165998  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0002   |\n",
      "|    n_updates        | 2065997  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2170000, episode_reward=20.40 +/- 0.49\n",
      "Episode length: 6726.00 +/- 129.69\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.73e+03 |\n",
      "|    mean_reward      | 20.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2170000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000176 |\n",
      "|    n_updates        | 2069999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.08e+03 |\n",
      "|    ep_rew_mean      | 19.5     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1428     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 11948    |\n",
      "|    total_timesteps  | 2172735  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00019  |\n",
      "|    n_updates        | 2072734  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.06e+03 |\n",
      "|    ep_rew_mean      | 19.5     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1432     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 11979    |\n",
      "|    total_timesteps  | 2179508  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00016  |\n",
      "|    n_updates        | 2079507  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2180000, episode_reward=19.20 +/- 1.33\n",
      "Episode length: 7046.60 +/- 396.04\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.05e+03 |\n",
      "|    mean_reward      | 19.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2180000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000135 |\n",
      "|    n_updates        | 2079999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.08e+03 |\n",
      "|    ep_rew_mean      | 19.4     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1436     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12026    |\n",
      "|    total_timesteps  | 2186921  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000435 |\n",
      "|    n_updates        | 2086920  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2190000, episode_reward=20.60 +/- 0.49\n",
      "Episode length: 6742.40 +/- 272.43\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.74e+03 |\n",
      "|    mean_reward      | 20.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2190000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.28e-05 |\n",
      "|    n_updates        | 2089999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.07e+03 |\n",
      "|    ep_rew_mean      | 19.5     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1440     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12069    |\n",
      "|    total_timesteps  | 2193746  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.19e-05 |\n",
      "|    n_updates        | 2093745  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2200000, episode_reward=18.60 +/- 2.73\n",
      "Episode length: 7197.60 +/- 762.69\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.2e+03  |\n",
      "|    mean_reward      | 18.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2200000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000239 |\n",
      "|    n_updates        | 2099999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.05e+03 |\n",
      "|    ep_rew_mean      | 19.5     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1444     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12110    |\n",
      "|    total_timesteps  | 2200156  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000563 |\n",
      "|    n_updates        | 2100155  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.02e+03 |\n",
      "|    ep_rew_mean      | 19.6     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1448     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12143    |\n",
      "|    total_timesteps  | 2207026  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00052  |\n",
      "|    n_updates        | 2107025  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2210000, episode_reward=20.40 +/- 0.49\n",
      "Episode length: 6612.20 +/- 68.50\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.61e+03 |\n",
      "|    mean_reward      | 20.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2210000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.6e-05  |\n",
      "|    n_updates        | 2109999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.02e+03 |\n",
      "|    ep_rew_mean      | 19.6     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1452     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12186    |\n",
      "|    total_timesteps  | 2213905  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000248 |\n",
      "|    n_updates        | 2113904  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2220000, episode_reward=18.80 +/- 1.17\n",
      "Episode length: 7397.80 +/- 575.95\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.4e+03  |\n",
      "|    mean_reward      | 18.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2220000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0001   |\n",
      "|    n_updates        | 2119999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.99e+03 |\n",
      "|    ep_rew_mean      | 19.6     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1456     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12229    |\n",
      "|    total_timesteps  | 2220540  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.99e-05 |\n",
      "|    n_updates        | 2120539  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.98e+03 |\n",
      "|    ep_rew_mean      | 19.6     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1460     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12262    |\n",
      "|    total_timesteps  | 2227662  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.9e-05  |\n",
      "|    n_updates        | 2127661  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2230000, episode_reward=20.60 +/- 0.49\n",
      "Episode length: 6802.40 +/- 298.82\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.8e+03  |\n",
      "|    mean_reward      | 20.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2230000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000124 |\n",
      "|    n_updates        | 2129999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.97e+03 |\n",
      "|    ep_rew_mean      | 19.6     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1464     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12305    |\n",
      "|    total_timesteps  | 2234435  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000352 |\n",
      "|    n_updates        | 2134434  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2240000, episode_reward=21.00 +/- 0.00\n",
      "Episode length: 6573.40 +/- 116.42\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.57e+03 |\n",
      "|    mean_reward      | 21       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2240000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000141 |\n",
      "|    n_updates        | 2139999  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.95e+03 |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1468     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12346    |\n",
      "|    total_timesteps  | 2240934  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000136 |\n",
      "|    n_updates        | 2140933  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.94e+03 |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1472     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12378    |\n",
      "|    total_timesteps  | 2247815  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000726 |\n",
      "|    n_updates        | 2147814  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2250000, episode_reward=20.80 +/- 0.40\n",
      "Episode length: 6511.20 +/- 115.35\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.51e+03 |\n",
      "|    mean_reward      | 20.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2250000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000108 |\n",
      "|    n_updates        | 2149999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.92e+03 |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1476     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12422    |\n",
      "|    total_timesteps  | 2254935  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000156 |\n",
      "|    n_updates        | 2154934  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2260000, episode_reward=20.40 +/- 0.80\n",
      "Episode length: 6765.00 +/- 414.28\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.76e+03 |\n",
      "|    mean_reward      | 20.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2260000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000306 |\n",
      "|    n_updates        | 2159999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.92e+03 |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1480     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12465    |\n",
      "|    total_timesteps  | 2261907  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000701 |\n",
      "|    n_updates        | 2161906  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.9e+03  |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1484     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12496    |\n",
      "|    total_timesteps  | 2268620  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000181 |\n",
      "|    n_updates        | 2168619  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2270000, episode_reward=19.40 +/- 1.36\n",
      "Episode length: 7104.20 +/- 479.88\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.1e+03  |\n",
      "|    mean_reward      | 19.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2270000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000455 |\n",
      "|    n_updates        | 2169999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.88e+03 |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1488     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12539    |\n",
      "|    total_timesteps  | 2275283  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000165 |\n",
      "|    n_updates        | 2175282  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2280000, episode_reward=20.20 +/- 0.75\n",
      "Episode length: 6815.00 +/- 242.63\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.82e+03 |\n",
      "|    mean_reward      | 20.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2280000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000106 |\n",
      "|    n_updates        | 2179999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.87e+03 |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1492     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12582    |\n",
      "|    total_timesteps  | 2282107  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000272 |\n",
      "|    n_updates        | 2182106  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.88e+03 |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1496     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12615    |\n",
      "|    total_timesteps  | 2289187  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000805 |\n",
      "|    n_updates        | 2189186  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2290000, episode_reward=16.80 +/- 3.12\n",
      "Episode length: 7816.60 +/- 1037.46\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.82e+03 |\n",
      "|    mean_reward      | 16.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2290000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000226 |\n",
      "|    n_updates        | 2189999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.91e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1500     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12661    |\n",
      "|    total_timesteps  | 2296366  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000223 |\n",
      "|    n_updates        | 2196365  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2300000, episode_reward=18.80 +/- 1.83\n",
      "Episode length: 7256.00 +/- 659.49\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.26e+03 |\n",
      "|    mean_reward      | 18.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2300000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.45e-05 |\n",
      "|    n_updates        | 2199999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.92e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1504     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12705    |\n",
      "|    total_timesteps  | 2303221  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000125 |\n",
      "|    n_updates        | 2203220  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2310000, episode_reward=20.00 +/- 1.10\n",
      "Episode length: 6922.60 +/- 524.85\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.92e+03 |\n",
      "|    mean_reward      | 20       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2310000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.57e-05 |\n",
      "|    n_updates        | 2209999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.94e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1508     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12751    |\n",
      "|    total_timesteps  | 2310519  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.22e-05 |\n",
      "|    n_updates        | 2210518  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.94e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1512     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12784    |\n",
      "|    total_timesteps  | 2317647  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000256 |\n",
      "|    n_updates        | 2217646  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2320000, episode_reward=19.40 +/- 1.85\n",
      "Episode length: 7212.20 +/- 699.58\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.21e+03 |\n",
      "|    mean_reward      | 19.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2320000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00015  |\n",
      "|    n_updates        | 2219999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.96e+03 |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1516     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12830    |\n",
      "|    total_timesteps  | 2324950  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000125 |\n",
      "|    n_updates        | 2224949  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2330000, episode_reward=20.40 +/- 0.80\n",
      "Episode length: 6670.00 +/- 185.82\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.67e+03 |\n",
      "|    mean_reward      | 20.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2330000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00015  |\n",
      "|    n_updates        | 2229999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.91e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1520     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12872    |\n",
      "|    total_timesteps  | 2331610  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000149 |\n",
      "|    n_updates        | 2231609  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.93e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1524     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12905    |\n",
      "|    total_timesteps  | 2338716  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000351 |\n",
      "|    n_updates        | 2238715  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2340000, episode_reward=20.00 +/- 0.89\n",
      "Episode length: 6844.60 +/- 210.38\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.84e+03 |\n",
      "|    mean_reward      | 20       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2340000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000148 |\n",
      "|    n_updates        | 2239999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.94e+03 |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1528     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12948    |\n",
      "|    total_timesteps  | 2345623  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000494 |\n",
      "|    n_updates        | 2245622  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2350000, episode_reward=20.40 +/- 0.80\n",
      "Episode length: 6599.60 +/- 148.40\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.6e+03  |\n",
      "|    mean_reward      | 20.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2350000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000221 |\n",
      "|    n_updates        | 2249999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.94e+03 |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1532     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 12992    |\n",
      "|    total_timesteps  | 2352488  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000205 |\n",
      "|    n_updates        | 2252487  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.93e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1536     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 13024    |\n",
      "|    total_timesteps  | 2359553  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000541 |\n",
      "|    n_updates        | 2259552  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2360000, episode_reward=19.80 +/- 1.47\n",
      "Episode length: 6781.60 +/- 378.27\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.78e+03 |\n",
      "|    mean_reward      | 19.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2360000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00014  |\n",
      "|    n_updates        | 2259999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.94e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1540     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 13069    |\n",
      "|    total_timesteps  | 2366632  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000272 |\n",
      "|    n_updates        | 2266631  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2370000, episode_reward=20.00 +/- 1.55\n",
      "Episode length: 6777.60 +/- 620.89\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.78e+03 |\n",
      "|    mean_reward      | 20       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2370000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000153 |\n",
      "|    n_updates        | 2269999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.95e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1544     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 13111    |\n",
      "|    total_timesteps  | 2373378  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000598 |\n",
      "|    n_updates        | 2273377  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2380000, episode_reward=19.20 +/- 1.33\n",
      "Episode length: 6943.20 +/- 282.82\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.94e+03 |\n",
      "|    mean_reward      | 19.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2380000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00016  |\n",
      "|    n_updates        | 2279999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.95e+03 |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1548     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13155    |\n",
      "|    total_timesteps  | 2380250  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000478 |\n",
      "|    n_updates        | 2280249  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.94e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1552     |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 13186    |\n",
      "|    total_timesteps  | 2386915  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000536 |\n",
      "|    n_updates        | 2286914  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2390000, episode_reward=19.00 +/- 2.19\n",
      "Episode length: 7366.60 +/- 879.15\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.37e+03 |\n",
      "|    mean_reward      | 19       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2390000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.57e-05 |\n",
      "|    n_updates        | 2289999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.96e+03 |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1556     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13231    |\n",
      "|    total_timesteps  | 2394087  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000384 |\n",
      "|    n_updates        | 2294086  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2400000, episode_reward=20.80 +/- 0.40\n",
      "Episode length: 6641.80 +/- 143.70\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.64e+03 |\n",
      "|    mean_reward      | 20.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2400000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000127 |\n",
      "|    n_updates        | 2299999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.95e+03 |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1560     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13274    |\n",
      "|    total_timesteps  | 2400960  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000206 |\n",
      "|    n_updates        | 2300959  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.94e+03 |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1564     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13304    |\n",
      "|    total_timesteps  | 2407496  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000109 |\n",
      "|    n_updates        | 2307495  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2410000, episode_reward=19.40 +/- 1.36\n",
      "Episode length: 6899.40 +/- 387.26\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.9e+03  |\n",
      "|    mean_reward      | 19.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2410000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0001   |\n",
      "|    n_updates        | 2309999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.95e+03 |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1568     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13347    |\n",
      "|    total_timesteps  | 2414127  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000564 |\n",
      "|    n_updates        | 2314126  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2420000, episode_reward=20.60 +/- 0.49\n",
      "Episode length: 6647.00 +/- 219.58\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.65e+03 |\n",
      "|    mean_reward      | 20.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2420000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000406 |\n",
      "|    n_updates        | 2319999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.94e+03 |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1572     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13390    |\n",
      "|    total_timesteps  | 2420847  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00372  |\n",
      "|    n_updates        | 2320846  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.92e+03 |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1576     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13421    |\n",
      "|    total_timesteps  | 2427491  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000394 |\n",
      "|    n_updates        | 2327490  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2430000, episode_reward=20.60 +/- 0.49\n",
      "Episode length: 6690.80 +/- 236.33\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.69e+03 |\n",
      "|    mean_reward      | 20.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2430000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000309 |\n",
      "|    n_updates        | 2329999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.92e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1580     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13463    |\n",
      "|    total_timesteps  | 2434252  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000282 |\n",
      "|    n_updates        | 2334251  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2440000, episode_reward=21.00 +/- 0.00\n",
      "Episode length: 6557.80 +/- 78.43\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.56e+03 |\n",
      "|    mean_reward      | 21       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2440000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.93e-05 |\n",
      "|    n_updates        | 2339999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.92e+03 |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1584     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13506    |\n",
      "|    total_timesteps  | 2440980  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.82e-05 |\n",
      "|    n_updates        | 2340979  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.91e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1588     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13535    |\n",
      "|    total_timesteps  | 2447406  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000211 |\n",
      "|    n_updates        | 2347405  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2450000, episode_reward=20.20 +/- 0.98\n",
      "Episode length: 6747.20 +/- 359.18\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.75e+03 |\n",
      "|    mean_reward      | 20.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2450000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000574 |\n",
      "|    n_updates        | 2349999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.92e+03 |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1592     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13579    |\n",
      "|    total_timesteps  | 2454489  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000161 |\n",
      "|    n_updates        | 2354488  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2460000, episode_reward=18.80 +/- 2.99\n",
      "Episode length: 7148.40 +/- 811.26\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.15e+03 |\n",
      "|    mean_reward      | 18.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2460000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000128 |\n",
      "|    n_updates        | 2359999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.91e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1596     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13624    |\n",
      "|    total_timesteps  | 2461409  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.02e-05 |\n",
      "|    n_updates        | 2361408  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.89e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1600     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13655    |\n",
      "|    total_timesteps  | 2468176  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000357 |\n",
      "|    n_updates        | 2368175  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2470000, episode_reward=21.00 +/- 0.00\n",
      "Episode length: 6448.80 +/- 82.17\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.45e+03 |\n",
      "|    mean_reward      | 21       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2470000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.26e-05 |\n",
      "|    n_updates        | 2369999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.89e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1604     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13697    |\n",
      "|    total_timesteps  | 2474862  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000153 |\n",
      "|    n_updates        | 2374861  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2480000, episode_reward=20.80 +/- 0.40\n",
      "Episode length: 6558.20 +/- 143.23\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.56e+03 |\n",
      "|    mean_reward      | 20.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2480000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000316 |\n",
      "|    n_updates        | 2379999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.87e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1608     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13739    |\n",
      "|    total_timesteps  | 2481643  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000213 |\n",
      "|    n_updates        | 2381642  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.84e+03 |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1612     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13770    |\n",
      "|    total_timesteps  | 2488193  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000184 |\n",
      "|    n_updates        | 2388192  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2490000, episode_reward=20.00 +/- 1.10\n",
      "Episode length: 7179.20 +/- 450.37\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.18e+03 |\n",
      "|    mean_reward      | 20       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2490000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000135 |\n",
      "|    n_updates        | 2389999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.82e+03 |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1616     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13813    |\n",
      "|    total_timesteps  | 2494919  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000453 |\n",
      "|    n_updates        | 2394918  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500000, episode_reward=19.60 +/- 1.50\n",
      "Episode length: 6832.60 +/- 389.40\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.83e+03 |\n",
      "|    mean_reward      | 19.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2500000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000421 |\n",
      "|    n_updates        | 2399999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.82e+03 |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1620     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13855    |\n",
      "|    total_timesteps  | 2501573  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000329 |\n",
      "|    n_updates        | 2401572  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.81e+03 |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1624     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13888    |\n",
      "|    total_timesteps  | 2508528  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.37e-05 |\n",
      "|    n_updates        | 2408527  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2510000, episode_reward=21.00 +/- 0.00\n",
      "Episode length: 6488.40 +/- 71.80\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.49e+03 |\n",
      "|    mean_reward      | 21       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2510000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00026  |\n",
      "|    n_updates        | 2409999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.81e+03 |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1628     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13930    |\n",
      "|    total_timesteps  | 2515315  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000603 |\n",
      "|    n_updates        | 2415314  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=19.80 +/- 0.40\n",
      "Episode length: 6836.20 +/- 144.45\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.84e+03 |\n",
      "|    mean_reward      | 19.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2520000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.69e-05 |\n",
      "|    n_updates        | 2419999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.82e+03 |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1632     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 13974    |\n",
      "|    total_timesteps  | 2522407  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000434 |\n",
      "|    n_updates        | 2422406  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.81e+03 |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1636     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14006    |\n",
      "|    total_timesteps  | 2529285  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000223 |\n",
      "|    n_updates        | 2429284  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2530000, episode_reward=20.00 +/- 1.10\n",
      "Episode length: 6754.00 +/- 335.49\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.75e+03 |\n",
      "|    mean_reward      | 20       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2530000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.66e-05 |\n",
      "|    n_updates        | 2429999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.79e+03 |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1640     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14049    |\n",
      "|    total_timesteps  | 2535964  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000101 |\n",
      "|    n_updates        | 2435963  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2540000, episode_reward=20.00 +/- 0.63\n",
      "Episode length: 6876.80 +/- 178.60\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.88e+03 |\n",
      "|    mean_reward      | 20       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2540000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000213 |\n",
      "|    n_updates        | 2439999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.8e+03  |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1644     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14092    |\n",
      "|    total_timesteps  | 2542928  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000152 |\n",
      "|    n_updates        | 2442927  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.79e+03 |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1648     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14123    |\n",
      "|    total_timesteps  | 2549512  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000291 |\n",
      "|    n_updates        | 2449511  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2550000, episode_reward=20.00 +/- 1.26\n",
      "Episode length: 6692.60 +/- 354.13\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.69e+03 |\n",
      "|    mean_reward      | 20       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2550000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000277 |\n",
      "|    n_updates        | 2449999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.8e+03  |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1652     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14166    |\n",
      "|    total_timesteps  | 2556278  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000185 |\n",
      "|    n_updates        | 2456277  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2560000, episode_reward=19.80 +/- 1.94\n",
      "Episode length: 6949.00 +/- 785.32\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.95e+03 |\n",
      "|    mean_reward      | 19.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2560000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000113 |\n",
      "|    n_updates        | 2459999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.78e+03 |\n",
      "|    ep_rew_mean      | 20.1     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1656     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14209    |\n",
      "|    total_timesteps  | 2563103  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000128 |\n",
      "|    n_updates        | 2463102  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2570000, episode_reward=20.60 +/- 0.49\n",
      "Episode length: 6585.00 +/- 62.34\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.58e+03 |\n",
      "|    mean_reward      | 20.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2570000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000248 |\n",
      "|    n_updates        | 2469999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.79e+03 |\n",
      "|    ep_rew_mean      | 20.1     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1660     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14253    |\n",
      "|    total_timesteps  | 2570265  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000138 |\n",
      "|    n_updates        | 2470264  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.8e+03  |\n",
      "|    ep_rew_mean      | 20.1     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1664     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14284    |\n",
      "|    total_timesteps  | 2576884  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00037  |\n",
      "|    n_updates        | 2476883  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2580000, episode_reward=20.80 +/- 0.40\n",
      "Episode length: 6571.00 +/- 130.16\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.57e+03 |\n",
      "|    mean_reward      | 20.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2580000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000101 |\n",
      "|    n_updates        | 2479999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.8e+03  |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1668     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14327    |\n",
      "|    total_timesteps  | 2583733  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000189 |\n",
      "|    n_updates        | 2483732  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2590000, episode_reward=18.00 +/- 2.10\n",
      "Episode length: 7751.00 +/- 760.85\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.75e+03 |\n",
      "|    mean_reward      | 18       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2590000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00165  |\n",
      "|    n_updates        | 2489999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.8e+03  |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1672     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14371    |\n",
      "|    total_timesteps  | 2590442  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00038  |\n",
      "|    n_updates        | 2490441  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.81e+03 |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1676     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14403    |\n",
      "|    total_timesteps  | 2597294  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.59e-05 |\n",
      "|    n_updates        | 2497293  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2600000, episode_reward=19.80 +/- 0.75\n",
      "Episode length: 6959.00 +/- 329.29\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.96e+03 |\n",
      "|    mean_reward      | 19.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2600000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00811  |\n",
      "|    n_updates        | 2499999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.82e+03 |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1680     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14447    |\n",
      "|    total_timesteps  | 2604319  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000213 |\n",
      "|    n_updates        | 2504318  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2610000, episode_reward=19.60 +/- 1.02\n",
      "Episode length: 6837.40 +/- 210.59\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.84e+03 |\n",
      "|    mean_reward      | 19.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2610000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.93e-05 |\n",
      "|    n_updates        | 2509999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.82e+03 |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1684     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14491    |\n",
      "|    total_timesteps  | 2611090  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00029  |\n",
      "|    n_updates        | 2511089  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.83e+03 |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1688     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14521    |\n",
      "|    total_timesteps  | 2617619  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00132  |\n",
      "|    n_updates        | 2517618  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2620000, episode_reward=20.60 +/- 0.80\n",
      "Episode length: 6833.60 +/- 396.89\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.83e+03 |\n",
      "|    mean_reward      | 20.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2620000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000133 |\n",
      "|    n_updates        | 2519999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.81e+03 |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1692     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14563    |\n",
      "|    total_timesteps  | 2624207  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000425 |\n",
      "|    n_updates        | 2524206  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2630000, episode_reward=20.80 +/- 0.40\n",
      "Episode length: 6507.80 +/- 142.69\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.51e+03 |\n",
      "|    mean_reward      | 20.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2630000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.52e-05 |\n",
      "|    n_updates        | 2529999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.8e+03  |\n",
      "|    ep_rew_mean      | 20.1     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1696     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14605    |\n",
      "|    total_timesteps  | 2630997  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.22e-05 |\n",
      "|    n_updates        | 2530996  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.8e+03  |\n",
      "|    ep_rew_mean      | 20.1     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1700     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14637    |\n",
      "|    total_timesteps  | 2637777  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000398 |\n",
      "|    n_updates        | 2537776  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2640000, episode_reward=19.80 +/- 1.17\n",
      "Episode length: 6815.40 +/- 461.64\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.82e+03 |\n",
      "|    mean_reward      | 19.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2640000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000111 |\n",
      "|    n_updates        | 2539999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.82e+03 |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1704     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14681    |\n",
      "|    total_timesteps  | 2644776  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.93e-05 |\n",
      "|    n_updates        | 2544775  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2650000, episode_reward=20.20 +/- 0.40\n",
      "Episode length: 6653.40 +/- 133.02\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.65e+03 |\n",
      "|    mean_reward      | 20.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2650000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000158 |\n",
      "|    n_updates        | 2549999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.84e+03 |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1708     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14730    |\n",
      "|    total_timesteps  | 2652051  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000144 |\n",
      "|    n_updates        | 2552050  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.85e+03 |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1712     |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 14770    |\n",
      "|    total_timesteps  | 2658942  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.79e-05 |\n",
      "|    n_updates        | 2558941  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2660000, episode_reward=20.40 +/- 0.49\n",
      "Episode length: 6685.40 +/- 107.27\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.69e+03 |\n",
      "|    mean_reward      | 20.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2660000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.36e-05 |\n",
      "|    n_updates        | 2559999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.87e+03 |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1716     |\n",
      "|    fps              | 179      |\n",
      "|    time_elapsed     | 14822    |\n",
      "|    total_timesteps  | 2666075  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000391 |\n",
      "|    n_updates        | 2566074  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2670000, episode_reward=20.60 +/- 0.49\n",
      "Episode length: 6608.60 +/- 140.63\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.61e+03 |\n",
      "|    mean_reward      | 20.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2670000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.3e-05  |\n",
      "|    n_updates        | 2569999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.87e+03 |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1720     |\n",
      "|    fps              | 179      |\n",
      "|    time_elapsed     | 14872    |\n",
      "|    total_timesteps  | 2672872  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000455 |\n",
      "|    n_updates        | 2572871  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2680000, episode_reward=20.00 +/- 0.89\n",
      "Episode length: 6899.00 +/- 143.10\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.9e+03  |\n",
      "|    mean_reward      | 20       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2680000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000119 |\n",
      "|    n_updates        | 2579999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.89e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1724     |\n",
      "|    fps              | 179      |\n",
      "|    time_elapsed     | 14923    |\n",
      "|    total_timesteps  | 2680338  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00029  |\n",
      "|    n_updates        | 2580337  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.88e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1728     |\n",
      "|    fps              | 179      |\n",
      "|    time_elapsed     | 14957    |\n",
      "|    total_timesteps  | 2686831  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000227 |\n",
      "|    n_updates        | 2586830  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2690000, episode_reward=20.80 +/- 0.40\n",
      "Episode length: 6579.80 +/- 194.29\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.58e+03 |\n",
      "|    mean_reward      | 20.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2690000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000133 |\n",
      "|    n_updates        | 2589999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.89e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1732     |\n",
      "|    fps              | 179      |\n",
      "|    time_elapsed     | 15007    |\n",
      "|    total_timesteps  | 2694042  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000487 |\n",
      "|    n_updates        | 2594041  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2700000, episode_reward=20.20 +/- 0.75\n",
      "Episode length: 6788.20 +/- 203.48\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.79e+03 |\n",
      "|    mean_reward      | 20.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2700000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.65e-05 |\n",
      "|    n_updates        | 2599999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.91e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1736     |\n",
      "|    fps              | 179      |\n",
      "|    time_elapsed     | 15060    |\n",
      "|    total_timesteps  | 2701401  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000937 |\n",
      "|    n_updates        | 2601400  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.92e+03 |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1740     |\n",
      "|    fps              | 179      |\n",
      "|    time_elapsed     | 15098    |\n",
      "|    total_timesteps  | 2708354  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.77e-05 |\n",
      "|    n_updates        | 2608353  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2710000, episode_reward=20.40 +/- 0.80\n",
      "Episode length: 6605.00 +/- 195.07\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6.6e+03  |\n",
      "|    mean_reward      | 20.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2710000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000157 |\n",
      "|    n_updates        | 2609999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.9e+03  |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1744     |\n",
      "|    fps              | 179      |\n",
      "|    time_elapsed     | 15148    |\n",
      "|    total_timesteps  | 2714933  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.67e-05 |\n",
      "|    n_updates        | 2614932  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2720000, episode_reward=19.00 +/- 0.63\n",
      "Episode length: 7276.40 +/- 210.89\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.28e+03 |\n",
      "|    mean_reward      | 19       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2720000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000116 |\n",
      "|    n_updates        | 2619999  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.9e+03  |\n",
      "|    ep_rew_mean      | 19.5     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1748     |\n",
      "|    fps              | 179      |\n",
      "|    time_elapsed     | 15198    |\n",
      "|    total_timesteps  | 2721527  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.88e-05 |\n",
      "|    n_updates        | 2621526  |\n",
      "----------------------------------\n",
      "^C\n",
      "Saving to /home/sequenzia/dev/rl-project/trained-agents/dqn/PongNoFrameskip-v4_1\n",
      "Saving replay buffer\n"
     ]
    }
   ],
   "source": [
    "ALGO = \"dqn\"\n",
    "TAGS = f\"{ROM} {ALGO.upper()}\"\n",
    "CONFIG_PATH = f\"{CONFIG_DIR}/{ALGO}.yml\"\n",
    "CMD = f\"{CMD} --algo {ALGO} --conf {CONFIG_PATH} -tags {TAGS} --save-replay-buffer\"\n",
    "\n",
    "!{CMD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== PongNoFrameskip-v4 ==========\n",
      "Seed: 43\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mappliedtheta\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sequenzia/dev/rl-project/logs/wandb/run-20231218_114830-4w705ksr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mPongNoFrameskip-v4__a2c__1702918109\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/appliedtheta/Solen-RL-Project\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/appliedtheta/Solen-RL-Project/runs/4w705ksr\u001b[0m\n",
      "Loading hyperparameters from: /home/sequenzia/dev/rl-project/configs/a2c.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('ent_coef', 0.01),\n",
      "             ('env_wrapper',\n",
      "              ['stable_baselines3.common.atari_wrappers.AtariWrapper']),\n",
      "             ('frame_stack', 4),\n",
      "             ('n_envs', 16),\n",
      "             ('n_timesteps', 10000000.0),\n",
      "             ('policy', 'CnnPolicy'),\n",
      "             ('policy_kwargs',\n",
      "              'dict(optimizer_class=RMSpropTFLike, '\n",
      "              'optimizer_kwargs=dict(eps=1e-5))'),\n",
      "             ('vf_coef', 0.25)])\n",
      "Using 16 environments\n",
      "Creating test environment\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "Stacking 4 frames\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Stacking 4 frames\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Using cuda device\n",
      "Log path: /home/sequenzia/dev/rl-project/trained-agents/a2c/PongNoFrameskip-v4_1\n",
      "Logging to runs/PongNoFrameskip-v4__a2c__1702918109/PongNoFrameskip-v4/A2C_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 754      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.0294   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.208   |\n",
      "|    value_loss         | 0.126    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-20.40 +/- 0.80\n",
      "Episode length: 3704.20 +/- 196.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.7e+03  |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.025    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124      |\n",
      "|    policy_loss        | -0.0274  |\n",
      "|    value_loss         | 0.0353   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.55e+03 |\n",
      "|    ep_rew_mean        | -20.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 556      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.0143   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.0445  |\n",
      "|    value_loss         | 0.0563   |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ALGO = \"a2c\"\n",
    "TAGS = f\"{ROM} {ALGO.upper()}\"\n",
    "CONFIG_PATH = f\"{CONFIG_DIR}/{ALGO}.yml\"\n",
    "CMD = f\"{CMD} --algo {ALGO} --conf {CONFIG_PATH} -tags {TAGS}\"\n",
    "\n",
    "!{CMD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGO = \"ppo\"\n",
    "TAGS = f\"{ROM} {ALGO.upper()}\"\n",
    "CONFIG_PATH = f\"{CONFIG_DIR}/{ALGO}.yml\"\n",
    "CMD = f\"{CMD} --algo {ALGO} --conf {CONFIG_PATH} -tags {TAGS}\"\n",
    "\n",
    "!{CMD}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
